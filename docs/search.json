[
  {
    "objectID": "posts/EX/PRO/2024-01-22-01. 텍스트 분류.html",
    "href": "posts/EX/PRO/2024-01-22-01. 텍스트 분류.html",
    "title": "00. 텍스트 분류",
    "section": "",
    "text": "- tensorflow 튜토리얼 : 영화 리뷰를 사용한 텍스트 분류\n- environment : [colab]"
  },
  {
    "objectID": "posts/EX/PRO/2024-01-22-01. 텍스트 분류.html#리뷰-길이-분포-확인",
    "href": "posts/EX/PRO/2024-01-22-01. 텍스트 분류.html#리뷰-길이-분포-확인",
    "title": "00. 텍스트 분류",
    "section": "리뷰 길이 분포 확인",
    "text": "리뷰 길이 분포 확인\n- train 리뷰 길이 분포 시각화\n\nimport numpy as np\n\n\nreview_length = [len(i) for i in x_train]\n\nfig,axes = plt.subplots(1,2, figsize = (12,4))\n\nax1, ax2 = axes\n\nax1.hist(review_length)\n#ax1.set_title(\"리뷰길이 hist\")\nax2.boxplot(review_length)\n#ax2.set_title(\"리뷰길이 boxplot\")\nfig.tight_layout()\nplt.show()\n\nprint(f\"리뷰의 최대 길이  : {max(review_length)}\")\nprint(f\"리뷰의 평균 길이  : {np.mean(review_length)}\")\n\n\n\n\n리뷰의 최대 길이  : 2494\n리뷰의 평균 길이  : 238.71364"
  },
  {
    "objectID": "posts/EX/PRO/2024-01-22-01. 텍스트 분류.html#라벨-분포-확인",
    "href": "posts/EX/PRO/2024-01-22-01. 텍스트 분류.html#라벨-분포-확인",
    "title": "00. 텍스트 분류",
    "section": "라벨 분포 확인",
    "text": "라벨 분포 확인\n\nindex = np.unique(y_train, return_counts=True)[0]\ncounts = np.unique(y_train, return_counts=True)[1]\n\nnp.asarray((index, counts))\n\narray([[    0,     1],\n       [12500, 12500]])"
  },
  {
    "objectID": "posts/EX/PRO/2024-01-22-01. 텍스트 분류.html#정수-맵핑-단어-확인",
    "href": "posts/EX/PRO/2024-01-22-01. 텍스트 분류.html#정수-맵핑-단어-확인",
    "title": "00. 텍스트 분류",
    "section": "정수 맵핑 단어 확인",
    "text": "정수 맵핑 단어 확인\n- imdb.get_word_index()에 각 단어와 맵핑되는 정수가 저장되어 있음\n\n주의 : 저장된 값에 +3을 해야 실제 맵핑되는 정수임 (이것은 IMDB 리뷰 데이터 셋에서 정한 규칙이다.)\n\n\nword_to_index = imdb.get_word_index()\n#word_to_index\n\n\nindex_to_word = {}\n\nfor key, value in word_to_index.items() :\n    index_to_word[value+3] = key\n\n\nindex_to_word에 인덱스를 집어넣으면 전처리 전에 어떤 단어였는지 확인할 수 있음.\n\n0,1,2,3은 특별토큰, 정수 4부터가 실제 IMDB 리뷰 데이터셋에서 내림차순으로 빈도수가 영단어임\n\n\n\nprint(f\"빈도수 상위 1등 단어 : {index_to_word[4]}\" )\n\n빈도수 상위 1등 단어 : the\n\n\n\nprint(f\"빈도수 상위 3938등 단어 : {index_to_word[3941]}\")\n\n빈도수 상위 3938등 단어 : suited\n\n\n- 첫 번째 훈련용 리뷰의 각 단어가 정수로 바뀌기 전에 어떤 단어들이 었는지 확인\n\n아래작업은 0,1,2,3 특별토큰의 값을 집어넣는 과정\n\n\nfor index, token in enumerate((\"&lt;pad&gt;\", \"&lt;sos&gt;\", \"&lt;unk&gt;\")):\n  index_to_word[index] = token\n\n\nindex_to_word[0], index_to_word[1], index_to_word[2]\n\n('&lt;pad&gt;', '&lt;sos&gt;', '&lt;unk&gt;')\n\n\n\nprint(\" \".join([index_to_word[i] for i in x_train[0]]))\n\n&lt;sos&gt; this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all"
  },
  {
    "objectID": "posts/EX/PRO/2024-01-22-01. 텍스트 분류.html#loss-시각화",
    "href": "posts/EX/PRO/2024-01-22-01. 텍스트 분류.html#loss-시각화",
    "title": "00. 텍스트 분류",
    "section": "loss 시각화",
    "text": "loss 시각화\n\nplt.figure(figsize = (8,4))\nplt.plot(history1[\"loss\"], label = \"train_loss\")\nplt.plot(history1[\"val_loss\"], label = \"val_loss\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/EX/PRO/2024-01-22-01. 텍스트 분류.html#predict",
    "href": "posts/EX/PRO/2024-01-22-01. 텍스트 분류.html#predict",
    "title": "00. 텍스트 분류",
    "section": "predict",
    "text": "predict\n\nrnn_pred1 =  model.predict(x_test)\n\n782/782 [==============================] - 3s 3ms/step\n\n\n\nrnn_pred1 = np.where(rnn_pred1&gt;0.5,1,0)\n\n\nfrom sklearn.metrics import *\n\n\nprint(confusion_matrix(y_test, rnn_pred1))\nprint(classification_report(y_test, rnn_pred1))\n\n[[8756 3744]\n [3500 9000]]\n              precision    recall  f1-score   support\n\n           0       0.71      0.70      0.71     12500\n           1       0.71      0.72      0.71     12500\n\n    accuracy                           0.71     25000\n   macro avg       0.71      0.71      0.71     25000\nweighted avg       0.71      0.71      0.71     25000"
  },
  {
    "objectID": "posts/EX/PRO/2024-01-08-04. MNIST.html",
    "href": "posts/EX/PRO/2024-01-08-04. MNIST.html",
    "title": "04. MNIST",
    "section": "",
    "text": "import tensorflow as tf\nfrom tensorflow import keras"
  },
  {
    "objectID": "posts/EX/PRO/2024-01-08-04. MNIST.html#one-hot-encoding",
    "href": "posts/EX/PRO/2024-01-08-04. MNIST.html#one-hot-encoding",
    "title": "04. MNIST",
    "section": "one-hot encoding",
    "text": "one-hot encoding\n\nfrom keras.utils import to_categorical\n\n\ntrain_y_c = to_categorical(y_train, 10)\ntest_y_c = to_categorical(y_test, 10)"
  },
  {
    "objectID": "posts/EX/PRO/2024-01-08-04. MNIST.html#import-1",
    "href": "posts/EX/PRO/2024-01-08-04. MNIST.html#import-1",
    "title": "04. MNIST",
    "section": "import",
    "text": "import\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom tensorflow.keras.backend import clear_session\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense, Flatten, BatchNormalization, Dropout"
  },
  {
    "objectID": "posts/EX/PRO/2024-01-08-04. MNIST.html#모델-설계",
    "href": "posts/EX/PRO/2024-01-08-04. MNIST.html#모델-설계",
    "title": "04. MNIST",
    "section": "모델 설계",
    "text": "모델 설계\n\n# 1. 세션 클리어\nclear_session()\n\n# 2. 모델 설계\n\nmodel1 = Sequential()\n\nmodel1.add( Input(shape = (28,28,1)))\nmodel1.add( Flatten() )\nmodel1.add( Dense(1024, activation = \"relu\"))\nmodel1.add( Dense(1024, activation = \"relu\"))\nmodel1.add( BatchNormalization())\nmodel1.add( Dropout(0.25) )\n\nmodel1.add( Dense(512, activation = \"relu\"))\nmodel1.add( Dense(512, activation = \"relu\"))\nmodel1.add( BatchNormalization())\nmodel1.add( Dropout(0.25) )\n\nmodel1.add( Dense (10, activation = \"softmax\"))\n\n# 3. 모델 컴파일\nmodel1.compile(optimizer = \"adam\", loss = tf.keras.losses.categorical_crossentropy,\n               metrics = [\"accuracy\"])\n\n\nmodel1.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten (Flatten)           (None, 784)               0         \n                                                                 \n dense (Dense)               (None, 1024)              803840    \n                                                                 \n dense_1 (Dense)             (None, 1024)              1049600   \n                                                                 \n batch_normalization (Batch  (None, 1024)              4096      \n Normalization)                                                  \n                                                                 \n dropout (Dropout)           (None, 1024)              0         \n                                                                 \n dense_2 (Dense)             (None, 512)               524800    \n                                                                 \n dense_3 (Dense)             (None, 512)               262656    \n                                                                 \n batch_normalization_1 (Bat  (None, 512)               2048      \n chNormalization)                                                \n                                                                 \n dropout_1 (Dropout)         (None, 512)               0         \n                                                                 \n dense_4 (Dense)             (None, 10)                5130      \n                                                                 \n=================================================================\nTotal params: 2652170 (10.12 MB)\nTrainable params: 2649098 (10.11 MB)\nNon-trainable params: 3072 (12.00 KB)\n_________________________________________________________________"
  },
  {
    "objectID": "posts/EX/PRO/2024-01-08-04. MNIST.html#학습",
    "href": "posts/EX/PRO/2024-01-08-04. MNIST.html#학습",
    "title": "04. MNIST",
    "section": "학습",
    "text": "학습\n\nfrom keras.callbacks import EarlyStopping\n\n\nes = EarlyStopping(monitor = \"val_loss\",\n                  min_delta = 0,\n                  patience = 3,\n                   verbose = 1,\n                   restore_best_weights = True)\n\n\nhistory = model1.fit(train_x_rel, train_y_c,\n                     epochs = 10000, verbose = 1,\n                     validation_split = 0.2,\n                     callbacks = [es]).history\n\nEpoch 1/10000\n1500/1500 [==============================] - 10s 7ms/step - loss: 0.5921 - accuracy: 0.7939 - val_loss: 0.5014 - val_accuracy: 0.8234\nEpoch 2/10000\n1500/1500 [==============================] - 12s 8ms/step - loss: 0.4375 - accuracy: 0.8434 - val_loss: 0.5076 - val_accuracy: 0.8056\nEpoch 3/10000\n1500/1500 [==============================] - 14s 9ms/step - loss: 0.3908 - accuracy: 0.8593 - val_loss: 0.4336 - val_accuracy: 0.8573\nEpoch 4/10000\n1500/1500 [==============================] - 12s 8ms/step - loss: 0.3534 - accuracy: 0.8709 - val_loss: 0.3595 - val_accuracy: 0.8655\nEpoch 5/10000\n1500/1500 [==============================] - 14s 10ms/step - loss: 0.3349 - accuracy: 0.8790 - val_loss: 0.3693 - val_accuracy: 0.8729\nEpoch 6/10000\n1500/1500 [==============================] - 13s 9ms/step - loss: 0.3183 - accuracy: 0.8831 - val_loss: 0.3335 - val_accuracy: 0.8810\nEpoch 7/10000\n1500/1500 [==============================] - 11s 7ms/step - loss: 0.3068 - accuracy: 0.8870 - val_loss: 0.3397 - val_accuracy: 0.8715\nEpoch 8/10000\n1500/1500 [==============================] - 11s 7ms/step - loss: 0.2890 - accuracy: 0.8942 - val_loss: 0.3358 - val_accuracy: 0.8773\nEpoch 9/10000\n1500/1500 [==============================] - 12s 8ms/step - loss: 0.2774 - accuracy: 0.8978 - val_loss: 0.3327 - val_accuracy: 0.8827\nEpoch 10/10000\n1500/1500 [==============================] - 10s 7ms/step - loss: 0.2664 - accuracy: 0.9011 - val_loss: 0.3199 - val_accuracy: 0.8850\nEpoch 11/10000\n1500/1500 [==============================] - 11s 7ms/step - loss: 0.2569 - accuracy: 0.9037 - val_loss: 0.3585 - val_accuracy: 0.8722\nEpoch 12/10000\n1500/1500 [==============================] - 15s 10ms/step - loss: 0.2463 - accuracy: 0.9079 - val_loss: 0.3201 - val_accuracy: 0.8879\nEpoch 13/10000\n1498/1500 [============================&gt;.] - ETA: 0s - loss: 0.2410 - accuracy: 0.9102Restoring model weights from the end of the best epoch: 10.\n1500/1500 [==============================] - 10s 6ms/step - loss: 0.2413 - accuracy: 0.9100 - val_loss: 0.3282 - val_accuracy: 0.8843\nEpoch 13: early stopping"
  },
  {
    "objectID": "posts/EX/PRO/2024-01-08-04. MNIST.html#결과-시각화",
    "href": "posts/EX/PRO/2024-01-08-04. MNIST.html#결과-시각화",
    "title": "04. MNIST",
    "section": "결과 시각화",
    "text": "결과 시각화\n\nplt.figure(figsize = (12,4))\nplt.plot(history[\"loss\"],\"--.\",label = \"train_loss\")\nplt.plot(history[\"val_loss\"], \"--.\",label = \"val_loss\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/EX/PRO/2024-01-08-04. MNIST.html#예측",
    "href": "posts/EX/PRO/2024-01-08-04. MNIST.html#예측",
    "title": "04. MNIST",
    "section": "예측",
    "text": "예측\n\ny_pred =  model1.predict(test_x_rel).argmax(axis = 1)\n\n313/313 [==============================] - 1s 2ms/step\n\n\n\nfrom sklearn.metrics import *\n\n\nconfusion_matrix(y_test, y_pred)\n\narray([[875,   0,  18,  27,   1,   0,  70,   0,   9,   0],\n       [  5, 958,   3,  30,   1,   0,   2,   0,   1,   0],\n       [ 19,   0, 842,  15,  65,   0,  55,   0,   4,   0],\n       [ 26,   2,   9, 920,  17,   0,  22,   0,   4,   0],\n       [  1,   0, 146,  52, 721,   0,  73,   0,   7,   0],\n       [  0,   0,   0,   0,   0, 987,   0,   9,   2,   2],\n       [162,   0, 109,  31,  48,   0, 639,   0,  11,   0],\n       [  0,   0,   0,   0,   0,  52,   0, 921,   0,  27],\n       [  5,   0,  10,   2,   5,   4,   0,   4, 970,   0],\n       [  0,   0,   0,   0,   0,  17,   1,  27,   0, 955]])\n\n\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.80      0.88      0.84      1000\n           1       1.00      0.96      0.98      1000\n           2       0.74      0.84      0.79      1000\n           3       0.85      0.92      0.89      1000\n           4       0.84      0.72      0.78      1000\n           5       0.93      0.99      0.96      1000\n           6       0.74      0.64      0.69      1000\n           7       0.96      0.92      0.94      1000\n           8       0.96      0.97      0.97      1000\n           9       0.97      0.95      0.96      1000\n\n    accuracy                           0.88     10000\n   macro avg       0.88      0.88      0.88     10000\nweighted avg       0.88      0.88      0.88     10000"
  },
  {
    "objectID": "posts/EX/PRO/2024-01-08-04. MNIST.html#import-2",
    "href": "posts/EX/PRO/2024-01-08-04. MNIST.html#import-2",
    "title": "04. MNIST",
    "section": "import",
    "text": "import\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.backend import clear_session\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Input, Conv2D, MaxPool2D"
  },
  {
    "objectID": "posts/EX/PRO/2024-01-08-04. MNIST.html#모델-설계-1",
    "href": "posts/EX/PRO/2024-01-08-04. MNIST.html#모델-설계-1",
    "title": "04. MNIST",
    "section": "모델 설계",
    "text": "모델 설계\n\n\n# 1. 세션클리어\nkeras.backend.clear_session()\n\n# 2. 모델 설계\nmodel2 = Sequential()\n\nmodel2.add(Input(shape = (28, 28, 1)))\n\nmodel2.add( Conv2D(filters = 28, kernel_size = (3,3),\n            strides = (1,1), padding = \"same\",\n             activation = \"relu\"))\n\nmodel2.add( Conv2D(filters = 28, kernel_size = (3,3),\n            strides = (1,1), padding = \"same\",\n             activation = \"relu\"))\n\nmodel2.add( BatchNormalization() )\nmodel2.add(MaxPool2D(pool_size = (2,2), strides= (2,2)))\nmodel2.add ( keras.layers.Dropout(0.25) )\n\nmodel2.add( keras.layers.Flatten() )\n# Fully Connected Layer : 노드 1024개\nmodel2.add( keras.layers.Dense(1024, activation = \"relu\"))\n\n# BatchNormalization\n\nmodel2.add(keras.layers.BatchNormalization())\n\n# DropOut : 35% 비활성화\n\nmodel2.add( keras.layers.Dropout(0.35) )\n\n# 아웃풋레이어\nmodel2.add( keras.layers.Dense(10, activation = \"softmax\"))\n\nmodel2.compile(optimizer = \"adam\",\n                              loss = keras.losses.categorical_crossentropy,\n                              metrics = [\"accuracy\"])\nmodel2.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 28, 28, 28)        280       \n                                                                 \n conv2d_1 (Conv2D)           (None, 28, 28, 28)        7084      \n                                                                 \n batch_normalization (Batch  (None, 28, 28, 28)        112       \n Normalization)                                                  \n                                                                 \n max_pooling2d (MaxPooling2  (None, 14, 14, 28)        0         \n D)                                                              \n                                                                 \n dropout (Dropout)           (None, 14, 14, 28)        0         \n                                                                 \n flatten (Flatten)           (None, 5488)              0         \n                                                                 \n dense (Dense)               (None, 1024)              5620736   \n                                                                 \n batch_normalization_1 (Bat  (None, 1024)              4096      \n chNormalization)                                                \n                                                                 \n dropout_1 (Dropout)         (None, 1024)              0         \n                                                                 \n dense_1 (Dense)             (None, 10)                10250     \n                                                                 \n=================================================================\nTotal params: 5642558 (21.52 MB)\nTrainable params: 5640454 (21.52 MB)\nNon-trainable params: 2104 (8.22 KB)\n_________________________________________________________________"
  },
  {
    "objectID": "posts/EX/PRO/2024-01-08-04. MNIST.html#모델-학습",
    "href": "posts/EX/PRO/2024-01-08-04. MNIST.html#모델-학습",
    "title": "04. MNIST",
    "section": "모델 학습",
    "text": "모델 학습\n\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n\ntrain_x_rel.shape\n\n(60000, 28, 28, 1)\n\n\n\nes = EarlyStopping(\n      monitor = \"val_loss\",\n      min_delta = 0,\n      patience = 3,\n      verbose = 1,\n      restore_best_weights  = True\n)\n\n\nhistory = model2.fit(train_x_rel, train_y_c,\n                     epochs = 10000, verbose = 1,\n                     validation_split = 0.2,\n                     callbacks = [es]).history\n\nEpoch 1/10000\n1500/1500 [==============================] - 15s 7ms/step - loss: 0.4567 - accuracy: 0.8470 - val_loss: 0.2840 - val_accuracy: 0.9010\nEpoch 2/10000\n1500/1500 [==============================] - 11s 7ms/step - loss: 0.3097 - accuracy: 0.8893 - val_loss: 0.4721 - val_accuracy: 0.8445\nEpoch 3/10000\n1500/1500 [==============================] - 10s 7ms/step - loss: 0.2647 - accuracy: 0.9051 - val_loss: 0.2349 - val_accuracy: 0.9137\nEpoch 4/10000\n1500/1500 [==============================] - 10s 7ms/step - loss: 0.2322 - accuracy: 0.9162 - val_loss: 0.2548 - val_accuracy: 0.9100\nEpoch 5/10000\n1500/1500 [==============================] - 10s 6ms/step - loss: 0.2109 - accuracy: 0.9235 - val_loss: 0.2960 - val_accuracy: 0.8965\nEpoch 6/10000\n1496/1500 [============================&gt;.] - ETA: 0s - loss: 0.1827 - accuracy: 0.9339Restoring model weights from the end of the best epoch: 3.\n1500/1500 [==============================] - 10s 7ms/step - loss: 0.1826 - accuracy: 0.9339 - val_loss: 0.2662 - val_accuracy: 0.9133\nEpoch 6: early stopping"
  },
  {
    "objectID": "posts/EX/PRO/2024-01-08-04. MNIST.html#결과-시각화-1",
    "href": "posts/EX/PRO/2024-01-08-04. MNIST.html#결과-시각화-1",
    "title": "04. MNIST",
    "section": "결과 시각화",
    "text": "결과 시각화\n\nplt.figure(figsize = (12,4))\nplt.plot(history[\"loss\"],\"--.\",label = \"train_loss\")\nplt.plot(history[\"val_loss\"], \"--.\",label = \"val_loss\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/EX/PRO/2024-01-08-04. MNIST.html#예측-1",
    "href": "posts/EX/PRO/2024-01-08-04. MNIST.html#예측-1",
    "title": "04. MNIST",
    "section": "예측",
    "text": "예측\n\ny_pred =  model1.predict(test_x_rel).argmax(axis = 1)\n\n313/313 [==============================] - 1s 3ms/step\n\n\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.80      0.88      0.84      1000\n           1       1.00      0.96      0.98      1000\n           2       0.74      0.84      0.79      1000\n           3       0.85      0.92      0.89      1000\n           4       0.84      0.72      0.78      1000\n           5       0.93      0.99      0.96      1000\n           6       0.74      0.64      0.69      1000\n           7       0.96      0.92      0.94      1000\n           8       0.96      0.97      0.97      1000\n           9       0.97      0.95      0.96      1000\n\n    accuracy                           0.88     10000\n   macro avg       0.88      0.88      0.88     10000\nweighted avg       0.88      0.88      0.88     10000"
  },
  {
    "objectID": "posts/EX/2023-11-01-02. iris.html",
    "href": "posts/EX/2023-11-01-02. iris.html",
    "title": "02. iris",
    "section": "",
    "text": "import seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams['font.family'] = \"Malgun Gothic\"\nplt.rcParams['axes.unicode_minus'] = False\n\n\niris = sns.load_dataset(\"iris\")\n\n\niris.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa"
  },
  {
    "objectID": "posts/EX/2023-11-01-02. iris.html#species-분포-확인",
    "href": "posts/EX/2023-11-01-02. iris.html#species-분포-확인",
    "title": "02. iris",
    "section": "(1) species 분포 확인",
    "text": "(1) species 분포 확인\n- barplot\n\nplt.figure(figsize = (4,4))\nsns.countplot(x = \"species\", data = iris )\nplt.show()\n\n\n\n\n- scatterplot\n\nsns.scatterplot(x = \"sepal_length\", y = \"petal_length\", hue = \"species\", data = iris)\n\n&lt;Axes: xlabel='sepal_length', ylabel='petal_length'&gt;"
  },
  {
    "objectID": "posts/EX/2023-11-01-02. iris.html#species-라벨-인코딩",
    "href": "posts/EX/2023-11-01-02. iris.html#species-라벨-인코딩",
    "title": "02. iris",
    "section": "(1) species 라벨 인코딩",
    "text": "(1) species 라벨 인코딩\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\nle = LabelEncoder()\ny = le.fit_transform(y)\nprint(le.classes_)\n\n['setosa' 'versicolor' 'virginica']"
  },
  {
    "objectID": "posts/EX/2023-11-01-02. iris.html#tree",
    "href": "posts/EX/2023-11-01-02. iris.html#tree",
    "title": "02. iris",
    "section": "(1) tree",
    "text": "(1) tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(max_depth=5)\n\ntree.fit(X, y)\n\ntree_pred = tree.predict(X)\n\nfrom sklearn.metrics import *\n\naccuracy_score(y, tree_pred)\n\n1.0"
  },
  {
    "objectID": "posts/EX/2023-11-01-02. iris.html#rf",
    "href": "posts/EX/2023-11-01-02. iris.html#rf",
    "title": "02. iris",
    "section": "(2) RF",
    "text": "(2) RF\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(max_depth = 5, n_estimators = 10)\n\nrf.fit(X, y)\n\nrf_pred = rf.predict(X)\n\naccuracy_score(y, rf_pred)\n\n0.98\n\n\n\npredict 후 라벨 추출\n\nle.classes_\n\narray(['setosa', 'versicolor', 'virginica'], dtype=object)\n\n\n\nle.classes_[rf.predict([X[50]])[0]]\n\n'versicolor'"
  },
  {
    "objectID": "posts/EX/2023-10-30-00. Churn.html",
    "href": "posts/EX/2023-10-30-00. Churn.html",
    "title": "00. Churn",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nimport scipy.stats as spst\nimport seaborn as sns"
  },
  {
    "objectID": "posts/EX/2023-10-30-00. Churn.html#드라이브-마운트",
    "href": "posts/EX/2023-10-30-00. Churn.html#드라이브-마운트",
    "title": "00. Churn",
    "section": "드라이브 마운트",
    "text": "드라이브 마운트\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\ncd /content/drive/MyDrive/Colab Notebooks/DX/AICE\n\n/content/drive/MyDrive/Colab Notebooks/DX/AICE"
  },
  {
    "objectID": "posts/EX/2023-10-30-00. Churn.html#데이터-로드",
    "href": "posts/EX/2023-10-30-00. Churn.html#데이터-로드",
    "title": "00. Churn",
    "section": "데이터 로드",
    "text": "데이터 로드\n\ndf = pd.read_csv(\"data_v1.csv\")\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\nOnlineSecurity\n...\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\n\n\n\n\n0\n7590-VHVEG\nNaN\n0.0\nYes\nNo\n1\nNo\nNo phone service\nDSL\nNo\n...\nNo\nNo\nNo\nNo\nNaN\nYes\nElectronic check\n29.85\n29.85\nNo\n\n\n1\n5575-GNVDE\nMale\n0.0\nNo\nNo\n34\nYes\nNo\nDSL\nYes\n...\nYes\nNo\nNo\nNo\nOne year\nNo\nMailed check\n56.95\n1889.5\nNo\n\n\n2\n3668-QPYBK\nMale\n0.0\nNo\nNo\n2\nYes\nNo\nDSL\nYes\n...\nNaN\nNo\nNo\nNo\nMonth-to-month\nYes\nMailed check\n53.85\n108.15\nYes\n\n\n3\n7795-CFOCW\nMale\n0.0\nNo\nNo\n45\nNo\nNo phone service\nDSL\nYes\n...\nNaN\nYes\nNo\nNo\nOne year\nNo\nBank transfer (automatic)\n42.30\n1840.75\nNo\n\n\n4\n9237-HQITU\nFemale\n0.0\nNo\nNo\n2\nYes\nNo\nFiber optic\nNo\n...\nNaN\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n70.70\n151.65\nYes\n\n\n\n\n\n5 rows × 21 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7043 entries, 0 to 7042\nData columns (total 21 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   customerID        7043 non-null   object \n 1   gender            7034 non-null   object \n 2   SeniorCitizen     7042 non-null   float64\n 3   Partner           7043 non-null   object \n 4   Dependents        7041 non-null   object \n 5   tenure            7043 non-null   int64  \n 6   PhoneService      7040 non-null   object \n 7   MultipleLines     7043 non-null   object \n 8   InternetService   7043 non-null   object \n 9   OnlineSecurity    7043 non-null   object \n 10  OnlineBackup      7043 non-null   object \n 11  DeviceProtection  3580 non-null   object \n 12  TechSupport       7043 non-null   object \n 13  StreamingTV       7043 non-null   object \n 14  StreamingMovies   7043 non-null   object \n 15  Contract          7042 non-null   object \n 16  PaperlessBilling  7043 non-null   object \n 17  PaymentMethod     7042 non-null   object \n 18  MonthlyCharges    7042 non-null   float64\n 19  TotalCharges      7043 non-null   object \n 20  Churn             7043 non-null   object \ndtypes: float64(2), int64(1), object(18)\nmemory usage: 1.1+ MB"
  },
  {
    "objectID": "posts/EX/2023-10-30-00. Churn.html#eda-exploratory-data-analysis",
    "href": "posts/EX/2023-10-30-00. Churn.html#eda-exploratory-data-analysis",
    "title": "00. Churn",
    "section": "EDA (Exploratory Data Analysis)",
    "text": "EDA (Exploratory Data Analysis)\n\n결측치 확인\n\ndf.isnull().sum()\n\ncustomerID             0\ngender                 9\nSeniorCitizen          1\nPartner                0\nDependents             2\ntenure                 0\nPhoneService           3\nMultipleLines          0\nInternetService        0\nOnlineSecurity         0\nOnlineBackup           0\nDeviceProtection    3463\nTechSupport            0\nStreamingTV            0\nStreamingMovies        0\nContract               1\nPaperlessBilling       0\nPaymentMethod          1\nMonthlyCharges         1\nTotalCharges           0\nChurn                  0\ndtype: int64\n\n\n\n\n통계 정보 확인\n\ndf.describe().T\n\n\n  \n    \n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nSeniorCitizen\n7042.0\n0.162170\n0.368633\n0.00\n0.0\n0.00\n0.00\n1.00\n\n\ntenure\n7043.0\n32.371149\n24.559481\n0.00\n9.0\n29.00\n55.00\n72.00\n\n\nMonthlyCharges\n7042.0\n64.763256\n30.091898\n18.25\n35.5\n70.35\n89.85\n118.75\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\n전처리 수행\n\ncustomerID 컬럼을 삭제\n\n\ndf.drop(\"customerID\", axis = 1, inplace = True)\n\n\n확인\n\n\n\"customreID\" in df.columns\n\nFalse\n\n\n\nTotalCharges 컬럼의 타입을 object에서 float으로 변환\n\n\n[float(i)   for i in  df.TotalCharges][:5] ##\n\nValueError: ignored\n\n\n\n공백이 있어 error가 뜬다.\n\n\ndf.TotalCharges =  df.TotalCharges.replace(\" \", 0).replace(\"\",0)\n\n\ndf.TotalCharges = [float(i)   for i in  df.TotalCharges]\n\n\ndf.TotalCharges ## 실수형으로 바뀐 것을 확인\n\n0         29.85\n1       1889.50\n2        108.15\n3       1840.75\n4        151.65\n         ...   \n7038    1990.50\n7039    7362.90\n7040     346.45\n7041     306.60\n7042    6844.50\nName: TotalCharges, Length: 7043, dtype: float64\n\n\n- Churn 컬럼의 문자열 값을 숫자로 변경\n\ndf.Churn = df.Churn.replace([\"Yes\",\"No\"],[1,0])\ndf.Churn.dtypes\n\ndtype('int64')\n\n\n- 컬럼별로 null값이 얼마나 있는지 확인\n\ndf.isnull().sum()[df.isnull().sum() !=0]\n\ngender                 9\nSeniorCitizen          1\nDependents             2\nPhoneService           3\nDeviceProtection    3463\nContract               1\nPaymentMethod          1\nMonthlyCharges         1\ndtype: int64\n\n\n- 결측치가 많은 DeviceProtection 열을 제거\n\ndf.drop(\"DeviceProtection\", axis = 1, inplace = True)\ndf.isnull().sum()[df.isnull().sum() !=0]\n\ngender            9\nSeniorCitizen     1\nDependents        2\nPhoneService      3\nContract          1\nPaymentMethod     1\nMonthlyCharges    1\ndtype: int64\n\n\n\n\n시각화\n\nObject 컬럼을 하나씩 가져와서 Bar chart 그리기\n\n\no_col = df.select_dtypes(\"O\").columns.values\nlen(o_col)\n\n14\n\n\n\nfig, axes = plt.subplots(7, 2, figsize = (14, 20))\no_col = o_col.reshape(7,-1)\n\nfor i in range(7) :\n  for j in range(2) :\n      sns.countplot(x = o_col[i][j], data = df, ax = axes[i][j])\n      axes[i][j].set_title(o_col[i][j])\nfig.tight_layout()\n\n\n\n\n\n\n불균형이 심한 열(PhoneService)삭제\n\ndf.drop(\"PhoneService\", axis = 1, inplace = True)\n\"PhoneService\"  in  df.columns\n\nFalse\n\n\n\n\ntarget 변수 분포 파악\n\nsns.countplot(x = \"Churn\", data = df)\nplt.title(\"Churn\")\n\nText(0.5, 1.0, 'Churn')\n\n\n\n\n\n- target 변수의 분포 확인 결과 불균형을 보인다.\n\n\nSeniorCitizen 열 분포 확인\n\nsns.countplot(x = \"SeniorCitizen\", data = df)\nplt.title(\"SeniorCitizen\")\nplt.show()\n\n\n\n\n- 불균형이 심하므로 삭제\n\ndf.drop(\"SeniorCitizen\", axis = 1, inplace = True)\n\"SeniorCitizen\"  in  df.columns\n\nFalse\n\n\n\n\ntenure 열의 분포 확인\n\nfig, axes = plt.subplots(1, 2, figsize = (12, 4))\nax1, ax2 = axes\n\nsns.histplot(x= \"tenure\", data = df, hue = \"Churn\", ax = ax1)\nsns.kdeplot(x= \"tenure\", data = df, hue = \"Churn\", ax = ax2)\n\nfig.tight_layout()\nfig.show()\n\n\n\n\n\n\nTotalCharges 열의 분포 확인\n\nfig, axes = plt.subplots(1, 2, figsize = (12, 4))\nax1, ax2 = axes\n\nsns.histplot(x= \"TotalCharges\", data = df, hue = \"Churn\", ax = ax1)\nsns.kdeplot(x= \"TotalCharges\", data = df, hue = \"Churn\", ax = ax2)\n\nfig.tight_layout()\nfig.show()\n\n\n\n\n### MultipleLines 분포 확인\n\nsns.countplot(x = \"MultipleLines\", data = df, hue = \"Churn\")\nplt.show()\n\n\n\n\n\n\n상관관계 분석\n\ntenure, MonthlyCharges, TotalCharges 컬럼간의 상관관계를 heatmap으로 그려보시오.\n\n\nc_col = ['tenure', 'MonthlyCharges', 'TotalCharges']\n\n\nsns.heatmap(df[c_col].corr(),\n                        annot = True,\n                        fmt = \".2f\")\nfig.show()\n\n\n\n\n\ndf[c_col].corr()\n\n\n  \n    \n\n\n\n\n\n\ntenure\nMonthlyCharges\nTotalCharges\n\n\n\n\ntenure\n1.000000\n0.247871\n0.826178\n\n\nMonthlyCharges\n0.247871\n1.000000\n0.651167\n\n\nTotalCharges\n0.826178\n0.651167\n1.000000\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nTotalCharges boxplot\n\nsns.boxplot(x = \"Churn\", y= \"TotalCharges\", data = df)\nplt.show()\n\n\n\n\n\n\n결과저장\n\ndf.to_csv(\"data_v1_save.csv\", index = False)\n\n\n\n다시 불러와서 확인\n\npd.read_csv(\"data_v1_save.csv\").shape\n\n(7043, 17)"
  },
  {
    "objectID": "posts/EX/2023-10-30-00. Churn.html#더미변수-변환",
    "href": "posts/EX/2023-10-30-00. Churn.html#더미변수-변환",
    "title": "00. Churn",
    "section": "더미변수 변환",
    "text": "더미변수 변환\n\ndf = pd.read_csv(\"data_v1_save.csv\")\n\n\ndf = df.dropna()\n\n\nd_col = df.select_dtypes(\"O\").columns.values\nd_col\n\narray(['gender', 'Partner', 'Dependents', 'MultipleLines',\n       'InternetService', 'OnlineSecurity', 'OnlineBackup', 'TechSupport',\n       'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n       'PaymentMethod'], dtype=object)\n\n\n\ndf1 = pd.get_dummies(data = df, columns = d_col, drop_first = True)\ndf1.select_dtypes(\"O\").columns.values\n\narray([], dtype=object)\n\n\n\ndf1.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 7030 entries, 1 to 7041\nData columns (total 27 columns):\n #   Column                                 Non-Null Count  Dtype  \n---  ------                                 --------------  -----  \n 0   tenure                                 7030 non-null   int64  \n 1   MonthlyCharges                         7030 non-null   float64\n 2   TotalCharges                           7030 non-null   float64\n 3   Churn                                  7030 non-null   int64  \n 4   gender_Male                            7030 non-null   uint8  \n 5   Partner_Yes                            7030 non-null   uint8  \n 6   Dependents_Yes                         7030 non-null   uint8  \n 7   MultipleLines_No phone service         7030 non-null   uint8  \n 8   MultipleLines_Yes                      7030 non-null   uint8  \n 9   InternetService_Fiber optic            7030 non-null   uint8  \n 10  InternetService_No                     7030 non-null   uint8  \n 11  OnlineSecurity_No internet service     7030 non-null   uint8  \n 12  OnlineSecurity_Yes                     7030 non-null   uint8  \n 13  OnlineBackup_No internet service       7030 non-null   uint8  \n 14  OnlineBackup_Yes                       7030 non-null   uint8  \n 15  TechSupport_No internet service        7030 non-null   uint8  \n 16  TechSupport_Yes                        7030 non-null   uint8  \n 17  StreamingTV_No internet service        7030 non-null   uint8  \n 18  StreamingTV_Yes                        7030 non-null   uint8  \n 19  StreamingMovies_No internet service    7030 non-null   uint8  \n 20  StreamingMovies_Yes                    7030 non-null   uint8  \n 21  Contract_One year                      7030 non-null   uint8  \n 22  Contract_Two year                      7030 non-null   uint8  \n 23  PaperlessBilling_Yes                   7030 non-null   uint8  \n 24  PaymentMethod_Credit card (automatic)  7030 non-null   uint8  \n 25  PaymentMethod_Electronic check         7030 non-null   uint8  \n 26  PaymentMethod_Mailed check             7030 non-null   uint8  \ndtypes: float64(2), int64(2), uint8(23)\nmemory usage: 432.5 KB\n\n\n\ndf1.to_csv(\"전처리완료.csv\",index= False)"
  },
  {
    "objectID": "posts/EX/2023-10-30-00. Churn.html#훈련-평가-데이터-분리",
    "href": "posts/EX/2023-10-30-00. Churn.html#훈련-평가-데이터-분리",
    "title": "00. Churn",
    "section": "훈련, 평가 데이터 분리",
    "text": "훈련, 평가 데이터 분리\n\ntarget = \"Churn\"\n\nX = df1.drop(target, axis = 1)\ny = df1[target]\n\n\n# 입력 : X, y\n# Train : Test 비율 = 7:3\n# y Class 비율을 유지하면서 나누기 : stratify=y\n# 여러 번 수행해도 같은 결과 나오게 고정 : random_state=42\n# 결과 : X_train, X_test, y_train, y_test\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.3, stratify = y, random_state = 42 )\n\n\nX_train.shape\n\n(4921, 26)\n\n\n\ny_train.shape\n\n(4921,)"
  },
  {
    "objectID": "posts/EX/2023-10-30-00. Churn.html#정규화스케일링",
    "href": "posts/EX/2023-10-30-00. Churn.html#정규화스케일링",
    "title": "00. Churn",
    "section": "정규화/스케일링",
    "text": "정규화/스케일링\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nscaler = MinMaxScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)"
  },
  {
    "objectID": "posts/EX/2023-10-30-00. Churn.html#모델별-bar차트-그려주고-성능-시각화-시험-x",
    "href": "posts/EX/2023-10-30-00. Churn.html#모델별-bar차트-그려주고-성능-시각화-시험-x",
    "title": "00. Churn",
    "section": "모델별 bar차트 그려주고 성능 시각화 (시험 X)",
    "text": "모델별 bar차트 그려주고 성능 시각화 (시험 X)\n\n\nCode\n# 모델별로 Recall 점수 저장\n# 모델 Recall 점수 순서대로 바차트를 그려 모델별로 성능 확인 가능\n\nfrom sklearn.metrics import accuracy_score\n\nmy_predictions = {}\n\ncolors = ['r', 'c', 'm', 'y', 'k', 'khaki', 'teal', 'orchid', 'sandybrown',\n          'greenyellow', 'dodgerblue', 'deepskyblue', 'rosybrown', 'firebrick',\n          'deeppink', 'crimson', 'salmon', 'darkred', 'olivedrab', 'olive',\n          'forestgreen', 'royalblue', 'indigo', 'navy', 'mediumpurple', 'chocolate',\n          'gold', 'darkorange', 'seagreen', 'turquoise', 'steelblue', 'slategray',\n          'peru', 'midnightblue', 'slateblue', 'dimgray', 'cadetblue', 'tomato'\n         ]\n\n# 모델명, 예측값, 실제값을 주면 위의 plot_predictions 함수 호출하여 Scatter 그래프 그리며\n# 모델별 MSE값을 Bar chart로 그려줌\n\ndef recall_eval(name_, pred, actual):\n    global predictions\n    global colors\n\n    plt.figure(figsize=(12, 9))\n\n    #acc = accuracy_score(actual, pred)\n    acc = recall_score(actual, pred)\n    my_predictions[name_] = acc * 100\n\n    y_value = sorted(my_predictions.items(), key=lambda x: x[1], reverse=True)\n\n    df = pd.DataFrame(y_value, columns=['model', 'recall'])\n    print(df)\n\n    length = len(df)\n\n    plt.figure(figsize=(10, length))\n    ax = plt.subplot()\n    ax.set_yticks(np.arange(len(df)))\n    ax.set_yticklabels(df['model'], fontsize=15)\n    bars = ax.barh(np.arange(len(df)), df['recall'])\n\n    for i, v in enumerate(df['recall']):\n        idx = np.random.choice(len(colors))\n        bars[i].set_color(colors[idx])\n        ax.text(v + 2, i, str(round(v, 3)), color='k', fontsize=15, fontweight='bold')\n\n    plt.title('recall', fontsize=18)\n    plt.xlim(0, 100)\n\n    plt.show()"
  },
  {
    "objectID": "posts/EX/2023-10-30-00. Churn.html#모델-성능-평가",
    "href": "posts/EX/2023-10-30-00. Churn.html#모델-성능-평가",
    "title": "00. Churn",
    "section": "모델 성능 평가",
    "text": "모델 성능 평가\n\n1. 로지스틱\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\n\nmodel.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\nl_pred = model.predict(X_test)\n\n- 결과 확인\n\nfrom sklearn.metrics import *\n\n\nprint(confusion_matrix(y_test, l_pred))\nprint(classification_report(y_test, l_pred))\n\n[[1385  164]\n [ 246  314]]\n              precision    recall  f1-score   support\n\n           0       0.85      0.89      0.87      1549\n           1       0.66      0.56      0.61       560\n\n    accuracy                           0.81      2109\n   macro avg       0.75      0.73      0.74      2109\nweighted avg       0.80      0.81      0.80      2109\n\n\n\n\nrecall_eval(\"Logistic Regression\", l_pred, y_test)\n\n                 model     recall\n0  Logistic Regression  56.071429\n\n\n&lt;Figure size 1200x900 with 0 Axes&gt;\n\n\n\n\n\n\n\n2. KNN\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel  = KNeighborsClassifier(n_neighbors = 5)\n\nmodel.fit(X_train, y_train)\n\nk_pred = model.predict(X_test)\n\nprint(confusion_matrix(y_test, k_pred))\nprint(classification_report(y_test, k_pred))\n\n[[1310  239]\n [ 273  287]]\n              precision    recall  f1-score   support\n\n           0       0.83      0.85      0.84      1549\n           1       0.55      0.51      0.53       560\n\n    accuracy                           0.76      2109\n   macro avg       0.69      0.68      0.68      2109\nweighted avg       0.75      0.76      0.75      2109\n\n\n\n\nrecall_eval(\"KNN\", k_pred, y_test)\n\n                 model     recall\n0  Logistic Regression  56.071429\n1                  KNN  51.250000\n\n\n&lt;Figure size 1200x900 with 0 Axes&gt;\n\n\n\n\n\n\n\n3. Decision Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier()\n\nmodel.fit(X_train, y_train)\n\ntree_pred = model.predict(X_test)\n\nprint(confusion_matrix(y_test, tree_pred))\nprint(classification_report(y_test, tree_pred))\n\n[[1289  260]\n [ 265  295]]\n              precision    recall  f1-score   support\n\n           0       0.83      0.83      0.83      1549\n           1       0.53      0.53      0.53       560\n\n    accuracy                           0.75      2109\n   macro avg       0.68      0.68      0.68      2109\nweighted avg       0.75      0.75      0.75      2109\n\n\n\n\nrecall_eval(\"Decision Tree\", tree_pred, y_test)\n\n                 model     recall\n0  Logistic Regression  56.071429\n1        Decision Tree  52.678571\n2                  KNN  51.250000\n\n\n&lt;Figure size 1200x900 with 0 Axes&gt;\n\n\n\n\n\n주요 Hyperparameter ##### 파라미터들을 조절하면서 모델의 성능을 높이거나 과대적합/과소적합 문제를 해결합니다.\n\nrandom_state: 랜덤 시드 고정 값. 고정해두고 튜닝하세요!\nn_jobs: CPU 사용 갯수 (여러 코어를 사용하면 모델 학습이 빨라짐)\nmax_depth: 깊어질 수 있는 최대 깊이. 너무 깊은 트리는 과대적합 발생할 수 있음\nn_estimators: 앙상블하는 트리의 갯수\nmax_features: 최대로 사용할 feature의 갯수. 값이 작을수록 과대적합 방지함\nmin_samples_split: 트리가 분할할 때 필요한 최소 샘플 수. 이 값을 증가시키면 각 분할에 샘플이 많이 필요해서 과대적합 방지함\n\n\n\n4. RandomForest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators = 3, random_state = 42)\n\nmodel.fit(X_train, y_train)\n\nrf_pred = model.predict(X_test)\n\nprint(confusion_matrix(y_test, rf_pred))\nprint(classification_report(y_test, rf_pred))\n\n[[1332  217]\n [ 300  260]]\n              precision    recall  f1-score   support\n\n           0       0.82      0.86      0.84      1549\n           1       0.55      0.46      0.50       560\n\n    accuracy                           0.75      2109\n   macro avg       0.68      0.66      0.67      2109\nweighted avg       0.74      0.75      0.75      2109\n\n\n\n\nrecall_eval(\"Random Forest\", rf_pred, y_test)\n\n                 model     recall\n0  Logistic Regression  56.071429\n1        Decision Tree  52.678571\n2                  KNN  51.250000\n3        Random Forest  46.428571\n\n\n&lt;Figure size 1200x900 with 0 Axes&gt;\n\n\n\n\n\n주요 Hyperparameter - random_state: 랜덤 시드 고정 값. 고정해두고 튜닝하세요! - n_jobs: CPU 사용 갯수 (여러 코어를 사용하면 모델 학습이 빨라짐) - learning_rate: 학습율. 너무 큰 학습율은 성능이 떨어질 수 있고 너무 낮으면 학습이 느려져서 적절한 값을 찾아야 함, default=0.1 - n_estimators: 부스팅 스테이지 수. (랜덤포레스트 트리의 갯수 설정과 비슷한 개념). default=100 - max_depth: 트리의 깊이. 너무 높으면 과적합, 너무 낮으면 성능이 떨어짐. default=3. - subsample: 샘플 사용 비율(0~1 사이의 값), 과대적합 방지용. - max_features: 최대로 사용할 feature의 비율. 과대적합 방지용. default=1.0\n\n\n5. XGboost\n\n#!pip install xgboost\n\n\nfrom xgboost import XGBClassifier\n\nmodel = XGBClassifier()\n\n\nmodel.fit(X_train, y_train)\n\nx_pred = model.predict(X_test)\n\nprint(confusion_matrix(y_test, x_pred))\nprint(classification_report(y_test, x_pred))\n\n[[1359  190]\n [ 278  282]]\n              precision    recall  f1-score   support\n\n           0       0.83      0.88      0.85      1549\n           1       0.60      0.50      0.55       560\n\n    accuracy                           0.78      2109\n   macro avg       0.71      0.69      0.70      2109\nweighted avg       0.77      0.78      0.77      2109\n\n\n\n\nrecall_eval(\"XGboost\", x_pred, y_test)\n\n                 model     recall\n0  Logistic Regression  56.071429\n1        Decision Tree  52.678571\n2                  KNN  51.250000\n3              XGboost  50.357143\n4        Random Forest  46.428571\n\n\n&lt;Figure size 1200x900 with 0 Axes&gt;\n\n\n\n\n\n주요 Hyperparameter - random_state: 랜덤 시드 고정 값. 고정해두고 튜닝하세요! - n_jobs: CPU 사용 갯수 (여러 코어를 사용하면 모델 학습이 빨라짐) - learning_rate: 학습율. 이 값이 너무 높으면 과적합할 수 있고 낮으면 학습이 느려질 수 있음. 적절한 값을 찾아야함. default=0.1 - n_estimators: 부스팅 스테이지 수.(랜덤포레스트 트리의 갯수와 비슷한 개념). 높을수록 복잡성 증가함. default=100 - max_depth: 트리의 깊이. 값이 크면 과적합 위험이 있음. default=3. - colsample_bytree: 샘플 사용 비율 (max_features와 비슷한 개념). 과대적합 방지용. default=1.0\n\n\n6. LGBM\n\n#!pip install lightgbm\n\n\nfrom lightgbm import LGBMClassifier\n\nmodel = LGBMClassifier(n_estimator = 3, random_state = 42)\n\nmodel.fit(X_train, y_train)\n\nlg_pred = model.predict(X_test)\n\nprint(confusion_matrix(y_test, lg_pred))\nprint(classification_report(y_test, lg_pred))\n\n[LightGBM] [Warning] Unknown parameter: n_estimator\n[LightGBM] [Warning] Unknown parameter: n_estimator\n[LightGBM] [Info] Number of positive: 1308, number of negative: 3613\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000441 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 629\n[LightGBM] [Info] Number of data points in the train set: 4921, number of used features: 26\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265800 -&gt; initscore=-1.016039\n[LightGBM] [Info] Start training from score -1.016039\n[LightGBM] [Warning] Unknown parameter: n_estimator\n[[1377  172]\n [ 273  287]]\n              precision    recall  f1-score   support\n\n           0       0.83      0.89      0.86      1549\n           1       0.63      0.51      0.56       560\n\n    accuracy                           0.79      2109\n   macro avg       0.73      0.70      0.71      2109\nweighted avg       0.78      0.79      0.78      2109\n\n\n\n\nrecall_eval('LGBM', lg_pred, y_test)\n\n                 model     recall\n0  Logistic Regression  56.071429\n1        Decision Tree  52.678571\n2                  KNN  51.250000\n3                 LGBM  51.250000\n4              XGboost  50.357143\n5        Random Forest  46.428571\n\n\n&lt;Figure size 1200x900 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/EX/2023-10-30-00. Churn.html#데이터-셋-분리",
    "href": "posts/EX/2023-10-30-00. Churn.html#데이터-셋-분리",
    "title": "00. Churn",
    "section": "데이터 셋 분리",
    "text": "데이터 셋 분리\n\ntarget = \"Churn\"\n\nX = df.drop(target, axis = 1)\ny = df[target]\n\n\nfrom sklearn.model_selection import train_test_split\n\n\n# 입력 : X, y\n# Train : Test 비율 = 7:3\n# y Class 비율을 유지하면서 나누기 : stratify=y\n# 여러 번 수행해도 같은 결과 나오게 고정 : random_state=42\n# 결과 : X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state = 42)\n\n\n스케일링\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)"
  },
  {
    "objectID": "posts/EX/2023-10-30-00. Churn.html#dnn-구현-1",
    "href": "posts/EX/2023-10-30-00. Churn.html#dnn-구현-1",
    "title": "00. Churn",
    "section": "DNN 구현 1",
    "text": "DNN 구현 1\n\nimport tensorflow as tf\n\n- 하이퍼 파라미터 설정\n\nbatch_size = 16\nepochs = 20\n\n- 아래의 요구대로 Sequential 모델 만들기\n\n# Sequential() 모델 정의 하고 model로 저장\n# input layer는 input_shape=() 옵션을 사용한다.\n# 39개 input layer\n# unit 4개 hidden layer\n# unit 3개 hidden layer\n# 1개 output layser : 이진분류\n\n\nX_train.shape\n\n(4921, 26)\n\n\n\nnf = X_train.shape[1]\n\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(4, activation = \"relu\", input_shape = (nf,)))\nmodel.add(tf.keras.layers.Dense(3, activation = \"relu\"))\nmodel.add(tf.keras.layers.Dense(1, activation = \"sigmoid\"))\n\n\nmodel.summary()\n\nModel: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_12 (Dense)            (None, 4)                 108       \n                                                                 \n dense_13 (Dense)            (None, 3)                 15        \n                                                                 \n dense_14 (Dense)            (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 127 (508.00 Byte)\nTrainable params: 127 (508.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n- dropout 추가\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(4, activation = \"relu\", input_shape = (nf,)))\nmodel.add(tf.keras.layers.Dropout(0.3))\nmodel.add(tf.keras.layers.Dense(3, activation = \"relu\"))\nmodel.add(tf.keras.layers.Dropout(0.3))\nmodel.add(tf.keras.layers.Dense(1, activation = \"sigmoid\"))\n\n\nmodel.summary()\n\nModel: \"sequential_6\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_15 (Dense)            (None, 4)                 108       \n                                                                 \n dropout (Dropout)           (None, 4)                 0         \n                                                                 \n dense_16 (Dense)            (None, 3)                 15        \n                                                                 \n dropout_1 (Dropout)         (None, 3)                 0         \n                                                                 \n dense_17 (Dense)            (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 127 (508.00 Byte)\nTrainable params: 127 (508.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n- 아래의 요구사항대로 모델을 컴파일\n\n# - 옵티마이저 : 'adam'\n# - 손실 함수 : 'binary_crossentropy'\n# - 평가 지표 : 'accuracy'\n\nmodel.compile(loss = tf.keras.losses.binary_crossentropy,\n                            optimizer = tf.keras.optimizers.Adam(0.001),\n                              metrics = [\"accuracy\"])\n\n\n아래 요구사항대로 모델 학습 시키기\n\n모델 이름 : model\nSequential 모델의 fit() 함수 사용\nX, y : X_train, y_train\nvalidation_data=(X_test, y_test)\nepochs : 10번\nbatch_size : 10번\n\n\nmodel.fit(X_train, y_train,\n                validation_data=(X_test, y_test),\n                epochs = 10,\n                batch_size = 10)\n\nEpoch 1/10\n493/493 [==============================] - 11s 5ms/step - loss: 0.6046 - accuracy: 0.7106 - val_loss: 0.5062 - val_accuracy: 0.7345\nEpoch 2/10\n493/493 [==============================] - 2s 5ms/step - loss: 0.5344 - accuracy: 0.7342 - val_loss: 0.4865 - val_accuracy: 0.7345\nEpoch 3/10\n493/493 [==============================] - 2s 5ms/step - loss: 0.5257 - accuracy: 0.7342 - val_loss: 0.4782 - val_accuracy: 0.7345\nEpoch 4/10\n493/493 [==============================] - 3s 5ms/step - loss: 0.5206 - accuracy: 0.7342 - val_loss: 0.4726 - val_accuracy: 0.7345\nEpoch 5/10\n493/493 [==============================] - 3s 7ms/step - loss: 0.5192 - accuracy: 0.7368 - val_loss: 0.4628 - val_accuracy: 0.7387\nEpoch 6/10\n493/493 [==============================] - 3s 5ms/step - loss: 0.5051 - accuracy: 0.7559 - val_loss: 0.4498 - val_accuracy: 0.7615\nEpoch 7/10\n493/493 [==============================] - 3s 5ms/step - loss: 0.4985 - accuracy: 0.7586 - val_loss: 0.4547 - val_accuracy: 0.7520\nEpoch 8/10\n493/493 [==============================] - 2s 5ms/step - loss: 0.4960 - accuracy: 0.7663 - val_loss: 0.4525 - val_accuracy: 0.7596\nEpoch 9/10\n493/493 [==============================] - 3s 5ms/step - loss: 0.4952 - accuracy: 0.7551 - val_loss: 0.4512 - val_accuracy: 0.7525\nEpoch 10/10\n493/493 [==============================] - 5s 11ms/step - loss: 0.4868 - accuracy: 0.7663 - val_loss: 0.4487 - val_accuracy: 0.7577\n\n\n&lt;keras.src.callbacks.History at 0x780ac638ace0&gt;"
  },
  {
    "objectID": "posts/EX/2023-10-30-00. Churn.html#dnn-구현-2",
    "href": "posts/EX/2023-10-30-00. Churn.html#dnn-구현-2",
    "title": "00. Churn",
    "section": "DNN 구현 2",
    "text": "DNN 구현 2\n- 아래의 요구사항대로 모델을 설계\n\n# 39개 input layer\n# unit 5개 hidden layer\n# dropout\n# unit 4개 hidden layer\n# dropout\n# 2개 output layser : 다중분류\n# epochs : 20번\n# batch_size : 16번\n\nmodel = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.Dense(5, activation = \"relu\", input_shape = (nf,)))\nmodel.add(tf.keras.layers.Dropout(0.3))\nmodel.add(tf.keras.layers.Dense(4, activation  = \"relu\"))\nmodel.add(tf.keras.layers.Dropout(0.3))\nmodel.add(tf.keras.layers.Dense(2, activation = \"softmax\"))\n\n\nmodel.summary()\n\nModel: \"sequential_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_18 (Dense)            (None, 5)                 135       \n                                                                 \n dropout_2 (Dropout)         (None, 5)                 0         \n                                                                 \n dense_19 (Dense)            (None, 4)                 24        \n                                                                 \n dropout_3 (Dropout)         (None, 4)                 0         \n                                                                 \n dense_20 (Dense)            (None, 2)                 10        \n                                                                 \n=================================================================\nTotal params: 169 (676.00 Byte)\nTrainable params: 169 (676.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n아래 요구사항대로 모델 학습 시키기\n\nmodel.compile(loss = tf.keras.losses.sparse_categorical_crossentropy,\n                            optimizer = tf.keras.optimizers.Adam(0.001), metrics = [\"accuracy\"])\n\n\n# 학습데이터 : X_train, y_train\n# 검증데이터 : X_test, y_test\n# epochs : 20, batch_size : 16\n# 'history' 변수에 저장\n\nhistory = model.fit(X_train, y_train,\n                                validation_data = [X_test, y_test],\n                                    epochs = 20, batch_size = 16).history\n\nEpoch 1/20\n308/308 [==============================] - 3s 5ms/step - loss: 0.5928 - accuracy: 0.7055 - val_loss: 0.5120 - val_accuracy: 0.7345\nEpoch 2/20\n308/308 [==============================] - 3s 11ms/step - loss: 0.5300 - accuracy: 0.7476 - val_loss: 0.4789 - val_accuracy: 0.7373\nEpoch 3/20\n308/308 [==============================] - 2s 6ms/step - loss: 0.5085 - accuracy: 0.7535 - val_loss: 0.4659 - val_accuracy: 0.7482\nEpoch 4/20\n308/308 [==============================] - 1s 4ms/step - loss: 0.4919 - accuracy: 0.7606 - val_loss: 0.4555 - val_accuracy: 0.7553\nEpoch 5/20\n308/308 [==============================] - 1s 4ms/step - loss: 0.4909 - accuracy: 0.7588 - val_loss: 0.4540 - val_accuracy: 0.7530\nEpoch 6/20\n308/308 [==============================] - 1s 5ms/step - loss: 0.4769 - accuracy: 0.7635 - val_loss: 0.4469 - val_accuracy: 0.7596\nEpoch 7/20\n308/308 [==============================] - 1s 4ms/step - loss: 0.4734 - accuracy: 0.7702 - val_loss: 0.4440 - val_accuracy: 0.7743\nEpoch 8/20\n308/308 [==============================] - 1s 4ms/step - loss: 0.4767 - accuracy: 0.7669 - val_loss: 0.4434 - val_accuracy: 0.7776\nEpoch 9/20\n308/308 [==============================] - 1s 4ms/step - loss: 0.4803 - accuracy: 0.7610 - val_loss: 0.4478 - val_accuracy: 0.7672\nEpoch 10/20\n308/308 [==============================] - 1s 4ms/step - loss: 0.4787 - accuracy: 0.7629 - val_loss: 0.4457 - val_accuracy: 0.7672\nEpoch 11/20\n308/308 [==============================] - 2s 7ms/step - loss: 0.4748 - accuracy: 0.7641 - val_loss: 0.4414 - val_accuracy: 0.7710\nEpoch 12/20\n308/308 [==============================] - 2s 7ms/step - loss: 0.4741 - accuracy: 0.7692 - val_loss: 0.4396 - val_accuracy: 0.7776\nEpoch 13/20\n308/308 [==============================] - 1s 5ms/step - loss: 0.4752 - accuracy: 0.7653 - val_loss: 0.4382 - val_accuracy: 0.7800\nEpoch 14/20\n308/308 [==============================] - 1s 4ms/step - loss: 0.4637 - accuracy: 0.7647 - val_loss: 0.4379 - val_accuracy: 0.7805\nEpoch 15/20\n308/308 [==============================] - 1s 4ms/step - loss: 0.4571 - accuracy: 0.7704 - val_loss: 0.4369 - val_accuracy: 0.7824\nEpoch 16/20\n308/308 [==============================] - 1s 4ms/step - loss: 0.4623 - accuracy: 0.7667 - val_loss: 0.4388 - val_accuracy: 0.7790\nEpoch 17/20\n308/308 [==============================] - 1s 4ms/step - loss: 0.4656 - accuracy: 0.7655 - val_loss: 0.4400 - val_accuracy: 0.7814\nEpoch 18/20\n308/308 [==============================] - 1s 5ms/step - loss: 0.4638 - accuracy: 0.7635 - val_loss: 0.4393 - val_accuracy: 0.7786\nEpoch 19/20\n308/308 [==============================] - 1s 5ms/step - loss: 0.4618 - accuracy: 0.7659 - val_loss: 0.4384 - val_accuracy: 0.7809\nEpoch 20/20\n308/308 [==============================] - 2s 6ms/step - loss: 0.4616 - accuracy: 0.7673 - val_loss: 0.4386 - val_accuracy: 0.7824\n\n\n\n\n조기종료 설정\n\n# val_loss(검증손실) 모니터링해서 손실이 최소화되는 방향으로 모니터링\n# 성능이 5번 지나도록 좋아지지 않으면 조기 종료\n\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",mode= \"min\",\n                                                                                  verbose = 1, patience = 5)\n\n\n# 최적의 모델을 'best_model.h5' 파일로 저장, 검증 손실이 최소일 때만 저장\n# val_loss 가장 낮은 값을 가질 때마다 모델 저장\n\ncheck_point = tf.keras.callbacks.ModelCheckpoint(\"best_model.h5\", verbose = 1,\n                                                                monitors = \"val_loss\",mode = \"min\", save_best_only = True)\n\n\n\n학습\n\n# early_stop과 check_point Callback 사용하여 모델에 적용시키세요.\n# epochs=50, batch_size=20\n# 검증 데이터로 X_test, y_test 사용\n# verbose 옵션을 1로 설정\n# 'history' 변수에 저장\n\n\nhistory = model.fit(X_train, y_train,\n                                validation_data = (X_test, y_test),\n                                epochs = 50, batch_size = 20,\n                                callbacks = [early_stop, check_point], verbose = 1 ).history\n\nEpoch 1/50\n246/247 [============================&gt;.] - ETA: 0s - loss: 0.4575 - accuracy: 0.7770\nEpoch 1: val_loss improved from inf to 0.43708, saving model to best_model.h5\n247/247 [==============================] - 2s 10ms/step - loss: 0.4579 - accuracy: 0.7769 - val_loss: 0.4371 - val_accuracy: 0.7819\nEpoch 2/50\n 18/247 [=&gt;............................] - ETA: 1s - loss: 0.4827 - accuracy: 0.7278242/247 [============================&gt;.] - ETA: 0s - loss: 0.4659 - accuracy: 0.7616\nEpoch 2: val_loss improved from 0.43708 to 0.43677, saving model to best_model.h5\n247/247 [==============================] - 2s 9ms/step - loss: 0.4652 - accuracy: 0.7620 - val_loss: 0.4368 - val_accuracy: 0.7814\nEpoch 3/50\n240/247 [============================&gt;.] - ETA: 0s - loss: 0.4550 - accuracy: 0.7690\nEpoch 3: val_loss did not improve from 0.43677\n247/247 [==============================] - 2s 10ms/step - loss: 0.4568 - accuracy: 0.7673 - val_loss: 0.4374 - val_accuracy: 0.7881\nEpoch 4/50\n244/247 [============================&gt;.] - ETA: 0s - loss: 0.4572 - accuracy: 0.7736\nEpoch 4: val_loss improved from 0.43677 to 0.43675, saving model to best_model.h5\n247/247 [==============================] - 1s 6ms/step - loss: 0.4576 - accuracy: 0.7726 - val_loss: 0.4367 - val_accuracy: 0.7833\nEpoch 5/50\n243/247 [============================&gt;.] - ETA: 0s - loss: 0.4547 - accuracy: 0.7671\nEpoch 5: val_loss improved from 0.43675 to 0.43665, saving model to best_model.h5\n247/247 [==============================] - 1s 5ms/step - loss: 0.4559 - accuracy: 0.7659 - val_loss: 0.4366 - val_accuracy: 0.7805\nEpoch 6/50\n235/247 [===========================&gt;..] - ETA: 0s - loss: 0.4576 - accuracy: 0.7668\nEpoch 6: val_loss did not improve from 0.43665\n247/247 [==============================] - 1s 5ms/step - loss: 0.4577 - accuracy: 0.7669 - val_loss: 0.4382 - val_accuracy: 0.7795\nEpoch 7/50\n235/247 [===========================&gt;..] - ETA: 0s - loss: 0.4550 - accuracy: 0.7687\nEpoch 7: val_loss did not improve from 0.43665\n247/247 [==============================] - 1s 4ms/step - loss: 0.4531 - accuracy: 0.7700 - val_loss: 0.4376 - val_accuracy: 0.7847\nEpoch 8/50\n237/247 [===========================&gt;..] - ETA: 0s - loss: 0.4614 - accuracy: 0.7696\nEpoch 8: val_loss did not improve from 0.43665\n247/247 [==============================] - 1s 5ms/step - loss: 0.4627 - accuracy: 0.7683 - val_loss: 0.4390 - val_accuracy: 0.7819\nEpoch 9/50\n235/247 [===========================&gt;..] - ETA: 0s - loss: 0.4622 - accuracy: 0.7689\nEpoch 9: val_loss did not improve from 0.43665\n247/247 [==============================] - 1s 4ms/step - loss: 0.4599 - accuracy: 0.7708 - val_loss: 0.4379 - val_accuracy: 0.7881\nEpoch 10/50\n245/247 [============================&gt;.] - ETA: 0s - loss: 0.4555 - accuracy: 0.7684\nEpoch 10: val_loss did not improve from 0.43665\n247/247 [==============================] - 1s 5ms/step - loss: 0.4552 - accuracy: 0.7683 - val_loss: 0.4371 - val_accuracy: 0.7824\nEpoch 10: early stopping\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning:\n\nYou are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n\n\n\n\n\nloss 확인\n\nlosses = pd.DataFrame(history)\n\n\nlosses\n\n\n  \n    \n\n\n\n\n\n\nloss\naccuracy\nval_loss\nval_accuracy\n\n\n\n\n0\n0.457895\n0.776875\n0.437084\n0.781887\n\n\n1\n0.465190\n0.762040\n0.436769\n0.781413\n\n\n2\n0.456798\n0.767324\n0.437379\n0.788051\n\n\n3\n0.457556\n0.772607\n0.436749\n0.783310\n\n\n4\n0.455916\n0.765901\n0.436647\n0.780465\n\n\n5\n0.457705\n0.766917\n0.438182\n0.779516\n\n\n6\n0.453103\n0.769965\n0.437608\n0.784732\n\n\n7\n0.462745\n0.768340\n0.439002\n0.781887\n\n\n8\n0.459855\n0.770778\n0.437930\n0.788051\n\n\n9\n0.455196\n0.768340\n0.437104\n0.782361\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\n성능 시각화\n\nlosses.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n아래 조건에 따라 정확도를 시각화하는 코드작성\n\n‘history’ 객체를 사용\n그래프 제목 : ‘Accuracy’\nx축 레이블 : ‘Epochs’\ny축 레이블 : ‘Acc’\n범례 : ‘acc’, ‘val_acc’\n\n\nplt.plot(losses[\"accuracy\"], label = \"acc\")\nplt.plot(losses[\"val_accuracy\"], label = \"val_acc\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Acc\")\nplt.title(\"Accuracy\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x780ab6edc970&gt;\n\n\n\n\n\n\n\n성능 평가\n\npred = model.predict(X_test).argmax(axis = 1)\n\n66/66 [==============================] - 0s 3ms/step\n\n\n\nprint(confusion_matrix(y_test, pred))\nprint(classification_report(y_test,pred))\n\n[[1447  102]\n [ 357  203]]\n              precision    recall  f1-score   support\n\n           0       0.80      0.93      0.86      1549\n           1       0.67      0.36      0.47       560\n\n    accuracy                           0.78      2109\n   macro avg       0.73      0.65      0.67      2109\nweighted avg       0.77      0.78      0.76      2109\n\n\n\n\n\n(참고) 재현율 성능이 좋지 않다면 어떻게 성능향상 할 수 있나?\n\n성능향상을 할 수 있는 방법은 여러 가지가 있습니다. (시험범위는 아닙니다)\nDNN 하이퍼파라미터를 수정하면서 성능향상이 되는지 확인해 볼 수 있습니다.\n데이터를 줄이거나(UnderSampling) 늘리거나(OverSampling), Feature(컬럼)을 늘리거나 줄이거나 하는 식으로 데이터를 균형하게 조정할 수도 있습니다.c"
  },
  {
    "objectID": "posts/DX/BP/Extra/2024-01-15-00. CA (1).html",
    "href": "posts/DX/BP/Extra/2024-01-15-00. CA (1).html",
    "title": "00. CA (1)",
    "section": "",
    "text": "- DX SUMMIT 2023, KT Enterprise\n- 일시 : 2023. 11. 08"
  },
  {
    "objectID": "posts/DX/BP/Extra/2024-01-15-00. CA (1).html#kt-b2b의-변화",
    "href": "posts/DX/BP/Extra/2024-01-15-00. CA (1).html#kt-b2b의-변화",
    "title": "00. CA (1)",
    "section": "KT B2B의 변화",
    "text": "KT B2B의 변화\n- 통신사업(2000s년대) \\(\\to\\) 통신 + ICT사업 (~2020년)\\(\\to\\) 고객 DX 사업자(2021년 이후)\n- 21년 후에는 통신, ICT, 클라우드, AI 역량을 기반으로 고객들의 DX를 돕는 파트너이자 Enabler로서의 역할을 수행함\n\nKT의 역할\n\n\n1. 고객의 비즈니스 파트너\n\nC레벨 컨설팅 사례\n\nDX에 대한 이해부족과 투자 효과 고민\n\n고객섹터분석 조직을 만들고 컨설팅 조직을 강화\n각 도메인 별로 어떤 고민을 가지고 있는지 분석하기 시작함\n다양한 섹터의 70개 이상 고객 경영진의 Pain point해소(2021년 부터)\n\nC레벨 컨설팅 사례 : 금융, 공공, 운송, 국방, 지자체, 글로벌 등등\n\n기업의 DX 전략\nCloud 기반 IT 최적화 전략\n고객 서비스 혁신전략\n심지어 DX 전략을 컨설팅하는데 돈을 받지 않음\n\n\n\n\n파트너쉽 사례\nKT는 단순히 고객분들이 RFP를 내서 수주하는 방식을 지양함\n그것보다는 파트너쉽을 통해서 같이 고민하고 필요하면 같이 투자를 하는 방시으로 비즈니스를 만드는 것을 선호함\n1 IT Infra DX 협력 체계 구축 (H사)\n\n5G 명품 빌딩 신사옥 구축\n그룹 DR 센터의 KT IDC 전환\n\nH인프라코어 IDC 협력추진(Cloud MSP, IDC, N/W 등)\n\n\n2 디지털 금융상품 파트너십 : S금융지주 (금융 DX)\n\nAICC를 통한 E손보업무 프로세스 혁신\nS은행 홈뱅크 상용화\n\n보험특화형 DX 패키징 개발(AI OCR + Paperless + 공전소)\n\n\n3 의료업무의 DX 전환 파트너십 : B대학병원 (의료 DX)"
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-02. model fit & save.html",
    "href": "posts/DX/BP/2023-12-28-02. model fit & save.html",
    "title": "02. model fit & save",
    "section": "",
    "text": "from google.colab import drive\ndrive._mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True)."
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-02. model fit & save.html#사전-학습-모델-로드",
    "href": "posts/DX/BP/2023-12-28-02. model fit & save.html#사전-학습-모델-로드",
    "title": "02. model fit & save",
    "section": "사전 학습 모델 로드",
    "text": "사전 학습 모델 로드\n\nseed_constant = 70\nnp.random.seed(seed_constant)\nrandom.seed(seed_constant)\ntorch.manual_seed(seed_constant)\ntorch.cuda.manual_seed_all(seed_constant)\n\n\nmodel_ft = torchvision.models.video.mc3_18(pretrained=True, progress=True)\n\nWARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n\nWARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MC3_18_Weights.KINETICS400_V1`. You can also use `weights=MC3_18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\nDownloading: \"https://download.pytorch.org/models/mc3_18-a90a0ba3.pth\" to /root/.cache/torch/hub/checkpoints/mc3_18-a90a0ba3.pth\n100%|██████████| 44.7M/44.7M [00:00&lt;00:00, 70.0MB/s]\n\n\n\nnum_ftrs = model_ft.fc.in_features   #\nmodel_ft.fc = torch.nn.Linear(num_ftrs, 2) #nn.Linear(in_features, out_features)"
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-02. model fit & save.html#데이터셋-로드",
    "href": "posts/DX/BP/2023-12-28-02. model fit & save.html#데이터셋-로드",
    "title": "02. model fit & save",
    "section": "데이터셋 로드",
    "text": "데이터셋 로드\n\nDATASET_DIR = '/content/drive/MyDrive/Colab Notebooks/big project/Fight_Detection_From_Surveillance_Cameras-PyTorch_Project/dataset(bp)' ## 데이터셋 경로 설정\nCLASSES_LIST = ['fall','fight']\nSEQUENCE_LENGTH = 16\nbatch_size= 4\n\n\nx_tensor, y_tensor=Fight_utils.create_dataset(DATASET_DIR,CLASSES_LIST,SEQUENCE_LENGTH)\n\nExtracting Data of Class: fall\nExtracting Data of Class: fight\n\n\n\nx_tensor.shape, y_tensor.shape\n\n(torch.Size([655, 3, 16, 112, 112]), torch.Size([655]))\n\n\n\nnp.unique(y_tensor, return_counts = True)\n\n(array([0, 1]), array([331, 324]))\n\n\n- gpu로 데이터 올리기\n\nx_tensor, y_tensor = x_tensor.to(device), y_tensor.to(device)\n\n\ndataset = torch.utils.data.TensorDataset(x_tensor, y_tensor) # 입력 데이터와 해당하는 레이블을 함께 저장\n\n\n데이터셋 분할\n\nval_size = int(len(dataset)*0.3) ##\n\ntrain_size = len(dataset)- int(len(dataset)*0.3)\ntrain_dataset, test_val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\n\nval_size = int(len(test_val_dataset)*0.5)\ntest_size = len(test_val_dataset)- int(len(test_val_dataset)*0.5)\nval_dataset, test_dataset = torch.utils.data.random_split(test_val_dataset, [val_size, test_size])\n\n\ntrain_lael_lst = np.array([int(train_dataset[i][1]) for i in range(len(train_dataset))])\nval_lael_lst = np.array([int(val_dataset[i][1]) for i in range(len(val_dataset))])\ntest_lael_lst = np.array([int(test_dataset[i][1]) for i in range(len(test_dataset))])\n\n- 데이터 셋은 잘 분리된 것 같음\n\nprint(np.unique(train_lael_lst, return_counts=True))\nprint(np.unique(val_lael_lst, return_counts=True))\nprint(np.unique(test_lael_lst, return_counts=True))\n\n(array([0, 1]), array([225, 234]))\n(array([0, 1]), array([53, 45]))\n(array([0, 1]), array([53, 45]))\n\n\n- 각 데이터셋을 미니매치 형태로 load\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)\n\n\ndataloaders_dict = {'train':train_loader,'val':val_loader }\n\n\n# To empty the Memory\ntorch.cuda.empty_cache()"
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-02. model fit & save.html#원본비디오-재생",
    "href": "posts/DX/BP/2023-12-28-02. model fit & save.html#원본비디오-재생",
    "title": "02. model fit & save",
    "section": "원본비디오 재생",
    "text": "원본비디오 재생\n\npath_output = \"/content/drive/MyDrive/Colab Notebooks/big project/test video/폭행/폭행0.mp4\"\nFight_utils.show_video(path_output, width=960)\n\nOutput hidden; open in https://colab.research.google.com to view."
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-02. model fit & save.html#비디오-클래스-추론",
    "href": "posts/DX/BP/2023-12-28-02. model fit & save.html#비디오-클래스-추론",
    "title": "02. model fit & save",
    "section": "비디오 클래스 추론",
    "text": "비디오 클래스 추론\n\npath_1 = \"/content/drive/MyDrive/Colab Notebooks/big project/test video/실신/실신1.mp4\"\npath_2 = \"/content/drive/MyDrive/Colab Notebooks/big project/test video/실신/실신2.mp4\"\nSEQUENCE_LENGTH=16\n\n\nprint(Fight_utils.FightInference(path_1,model_ft,SEQUENCE_LENGTH))\nprint(Fight_utils.FightInference_Time(path_1, model_ft, SEQUENCE_LENGTH)) ## 추론하는데 걸린시간\n\nfall\n[('fall', 0.56476), ('fight', 0.43524)]\n***********\ntime is: 6.825927972793579\nfall\n\n\n\nprint(Fight_utils.FightInference(path_2,model_ft,SEQUENCE_LENGTH))\nprint(Fight_utils.FightInference_Time(path_2, model_ft, SEQUENCE_LENGTH)) ## 추론하는데 걸린시간\n\nfall\n[('fall', 0.69014), ('fight', 0.30986)]\n***********\ntime is: 6.5522260665893555\nfall"
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-00. model train (1).html",
    "href": "posts/DX/BP/2023-12-28-00. model train (1).html",
    "title": "00. model train (1)",
    "section": "",
    "text": "- 깃허브 파일 로드 후 모델 테스트"
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-00. model train (1).html#작업-폴더-변경",
    "href": "posts/DX/BP/2023-12-28-00. model train (1).html#작업-폴더-변경",
    "title": "00. model train (1)",
    "section": "작업 폴더 변경",
    "text": "작업 폴더 변경\n\n%pwd\n\n'/content'\n\n\n\n%cd /content/drive/MyDrive/Colab Notebooks/big project\n\n/content/drive/MyDrive/Colab Notebooks/big project"
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-00. model train (1).html#깃에서-프로젝트-로드",
    "href": "posts/DX/BP/2023-12-28-00. model train (1).html#깃에서-프로젝트-로드",
    "title": "00. model train (1)",
    "section": "깃에서 프로젝트 로드",
    "text": "깃에서 프로젝트 로드\n\n!git clone ahttps://github.com/MohamedSebaie/Fight_Detection_From_Surveillance_Cameras-PyTorch_Project\n\nCloning into 'Fight_Detection_From_Surveillance_Cameras-PyTorch_Project'...\nremote: Enumerating objects: 709, done.\nremote: Total 709 (delta 0), reused 0 (delta 0), pack-reused 709\nReceiving objects: 100% (709/709), 212.48 MiB | 19.91 MiB/s, done.\nResolving deltas: 100% (231/231), done.\nUpdating files: 100% (329/329), done."
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-00. model train (1).html#gpu-사용가능-여부-확인",
    "href": "posts/DX/BP/2023-12-28-00. model train (1).html#gpu-사용가능-여부-확인",
    "title": "00. model train (1)",
    "section": "gpu 사용가능 여부 확인",
    "text": "gpu 사용가능 여부 확인\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice\n\ndevice(type='cuda', index=0)"
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-00. model train (1).html#사용자-정의-.py-파일-import",
    "href": "posts/DX/BP/2023-12-28-00. model train (1).html#사용자-정의-.py-파일-import",
    "title": "00. model train (1)",
    "section": "사용자 정의 .py 파일 import",
    "text": "사용자 정의 .py 파일 import\n- Fight_utils라는 .py파일을 import 해야함\n\nimport sys\n\n## 시스템 경로 추가\nsys.path.append(\"/content/drive/MyDrive/Colab Notebooks/big project/Fight_Detection_From_Surveillance_Cameras-PyTorch_Project/UtilsFiles\")\n\n\nimport Fight_utils\nfrom Fight_utils import *"
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-00. model train (1).html#사전학습된-모델-로드",
    "href": "posts/DX/BP/2023-12-28-00. model train (1).html#사전학습된-모델-로드",
    "title": "00. model train (1)",
    "section": "사전학습된 모델 로드",
    "text": "사전학습된 모델 로드\n- 참고한 깃허브를 살펴본 결과 mc3_18 모델의 성능이 가장 좋았으므로 해당 모델을 로드\n\nmodel_ft = torchvision.models.video.mc3_18(pretrained=True, progress=True)\n\nWARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n\nWARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MC3_18_Weights.KINETICS400_V1`. You can also use `weights=MC3_18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\nDownloading: \"https://download.pytorch.org/models/mc3_18-a90a0ba3.pth\" to /root/.cache/torch/hub/checkpoints/mc3_18-a90a0ba3.pth\n100%|██████████| 44.7M/44.7M [00:01&lt;00:00, 27.6MB/s]\n\n\n- 인풋, 아웃풋 피처별 선형 레이어 생성\n\nnum_ftrs = model_ft.fc.in_features   #\nmodel_ft.fc = torch.nn.Linear(num_ftrs, 2) #nn.Linear(in_features, out_features)"
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-00. model train (1).html#샘플-데이터셋-로드",
    "href": "posts/DX/BP/2023-12-28-00. model train (1).html#샘플-데이터셋-로드",
    "title": "00. model train (1)",
    "section": "샘플 데이터셋 로드",
    "text": "샘플 데이터셋 로드\n- 일단 깃에서 제공하는 데이터를 이용하고 나중에 우리 데이터 형식에 맞게 변경하기위해 불러오는 방식만 참고하자.\n\nDATASET_DIR = '/content/drive/MyDrive/Colab Notebooks/big project/Fight_Detection_From_Surveillance_Cameras-PyTorch_Project/dataset' ## 데이터셋 경로 설정\nCLASSES_LIST = ['fight','noFight'] ## 폴더이름하고 동일한\nSEQUENCE_LENGTH = 16\nbatch_size= 4\n\n\nx_tensor, y_tensor=Fight_utils.create_dataset(DATASET_DIR,CLASSES_LIST,SEQUENCE_LENGTH)\n\nExtracting Data of Class: fight\nExtracting Data of Class: noFight\n\n\n- shape 확인\n\n총 300개의 데이터를 가져옴\n\n\nx_tensor.shape, y_tensor.shape\n\n(torch.Size([300, 3, 16, 112, 112]), torch.Size([300]))\n\n\n- 클래스 불균형은 일어나지 않음\n\nnp.unique(y_tensor, return_counts = True)\n\n(array([0, 1]), array([150, 150]))\n\n\n- gpu로 데이터 올리기\n\nx_tensor, y_tensor = x_tensor.to(device), y_tensor.to(device)\n\n\ndataset = torch.utils.data.TensorDataset(x_tensor, y_tensor) # 입력 데이터와 해당하는 레이블을 함께 저장\n\n\n#dataset[0][0]\n\n\ndataset[0][1]\n\ntensor(0, device='cuda:0')\n\n\n- dataset 분할\n\n70%를 train, 30%를 test data로 생성\n\n\nval_size = int(len(dataset)*0.3) ##\n\ntrain_size = len(dataset)- int(len(dataset)*0.3)\ntrain_dataset, test_val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\n- test set의 절반은 validation으로 사용\n\nval_size = int(len(test_val_dataset)*0.5)\ntest_size = len(test_val_dataset)- int(len(test_val_dataset)*0.5)\nval_dataset, test_dataset = torch.utils.data.random_split(test_val_dataset, [val_size, test_size])\n\n- 각 데이터셋을 미니배치 형태로 load\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)\n\n\ndataloaders_dict = {'train':train_loader,'val':val_loader }\n\n\n# To empty the Memory\ntorch.cuda.empty_cache()"
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-00. model train (1).html#모델-학습",
    "href": "posts/DX/BP/2023-12-28-00. model train (1).html#모델-학습",
    "title": "00. model train (1)",
    "section": "모델 학습",
    "text": "모델 학습\n\n모델 학습 설계\n\n# 모델 GPU에 올리기\nmodel_ft = model_ft.to(device)\n\n# 에포크 설정\nepochs = 30\n\n# Loss Function 설정\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Optimization Function (SGD-----&gt; Stocastic Gradient Descent)\noptimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n\n\n\n학습 시작\n\nmodel_ft, hist = Fight_utils.train_model(device,model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=epochs)\n\nEpoch 0/29\n----------\ntrain Loss: 0.7539 Acc: 0.5667\nval Loss: 0.7233 Acc: 0.6000\n\nEpoch 1/29\n----------\ntrain Loss: 0.7394 Acc: 0.6810\nval Loss: 0.2258 Acc: 0.9111\n\nEpoch 2/29\n----------\ntrain Loss: 0.3962 Acc: 0.8286\nval Loss: 0.1942 Acc: 0.9333\n\nEpoch 3/29\n----------\ntrain Loss: 0.3335 Acc: 0.8667\nval Loss: 0.2187 Acc: 0.9333\n\nEpoch 4/29\n----------\ntrain Loss: 0.4340 Acc: 0.8286\nval Loss: 0.6510 Acc: 0.8000\n\nEpoch 5/29\n----------\ntrain Loss: 0.3151 Acc: 0.8714\nval Loss: 0.3179 Acc: 0.8889\n\nEpoch 6/29\n----------\ntrain Loss: 0.3075 Acc: 0.8810\nval Loss: 0.2464 Acc: 0.8444\n\nEpoch 7/29\n----------\ntrain Loss: 0.1937 Acc: 0.9286\nval Loss: 0.3592 Acc: 0.9111\n\nEpoch 8/29\n----------\ntrain Loss: 0.1414 Acc: 0.9286\nval Loss: 0.3983 Acc: 0.9111\n\nEpoch 9/29\n----------\ntrain Loss: 0.2672 Acc: 0.8952\nval Loss: 0.2606 Acc: 0.9333\n\nEpoch 10/29\n----------\ntrain Loss: 0.1456 Acc: 0.9333\nval Loss: 0.2169 Acc: 0.8889\n\nEpoch 11/29\n----------\ntrain Loss: 0.1669 Acc: 0.9333\nval Loss: 0.1677 Acc: 0.9111\n\nEpoch 12/29\n----------\ntrain Loss: 0.1950 Acc: 0.9381\nval Loss: 0.1729 Acc: 0.9333\n\nEpoch 13/29\n----------\ntrain Loss: 0.0949 Acc: 0.9619\nval Loss: 0.1852 Acc: 0.9333\n\nEpoch 14/29\n----------\ntrain Loss: 0.0562 Acc: 0.9810\nval Loss: 0.2525 Acc: 0.9333\n\nEpoch 15/29\n----------\ntrain Loss: 0.1724 Acc: 0.9333\nval Loss: 0.2470 Acc: 0.9111\n\nEpoch 16/29\n----------\ntrain Loss: 0.1894 Acc: 0.9238\nval Loss: 0.2414 Acc: 0.8889\n\nEpoch 17/29\n----------\ntrain Loss: 0.2274 Acc: 0.9095\nval Loss: 0.4261 Acc: 0.8667\n\nEpoch 18/29\n----------\ntrain Loss: 0.1400 Acc: 0.9571\nval Loss: 0.3084 Acc: 0.8889\n\nEpoch 19/29\n----------\ntrain Loss: 0.1318 Acc: 0.9429\nval Loss: 0.1799 Acc: 0.9333\n\nEpoch 20/29\n----------\ntrain Loss: 0.0647 Acc: 0.9857\nval Loss: 0.1865 Acc: 0.9333\n\nEpoch 21/29\n----------\ntrain Loss: 0.1398 Acc: 0.9381\nval Loss: 0.1307 Acc: 0.9333\n\nEpoch 22/29\n----------\ntrain Loss: 0.0524 Acc: 0.9857\nval Loss: 0.1070 Acc: 0.9333\n\nEpoch 23/29\n----------\ntrain Loss: 0.0680 Acc: 0.9667\nval Loss: 0.1617 Acc: 0.9111\n\nEpoch 24/29\n----------\ntrain Loss: 0.1251 Acc: 0.9429\nval Loss: 0.3618 Acc: 0.9111\n\nEpoch 25/29\n----------\ntrain Loss: 0.0739 Acc: 0.9762\nval Loss: 0.1279 Acc: 0.9333\n\nEpoch 26/29\n----------\ntrain Loss: 0.0703 Acc: 0.9667\nval Loss: 0.1882 Acc: 0.9333\n\nEpoch 27/29\n----------\ntrain Loss: 0.0583 Acc: 0.9714\nval Loss: 0.2303 Acc: 0.9111\n\nEpoch 28/29\n----------\ntrain Loss: 0.1062 Acc: 0.9667\nval Loss: 0.1782 Acc: 0.8889\n\nEpoch 29/29\n----------\ntrain Loss: 0.0651 Acc: 0.9762\nval Loss: 0.1442 Acc: 0.9111\n\nTraining complete in 9m 41s\nBest val Acc: 0.933333"
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-00. model train (1).html#test-데이터-예측",
    "href": "posts/DX/BP/2023-12-28-00. model train (1).html#test-데이터-예측",
    "title": "00. model train (1)",
    "section": "test 데이터 예측",
    "text": "test 데이터 예측\n\nsince = time.time()\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True) ## test 데이터를 배치단위로 쪼갬\n\nmodel_ft.eval() ##  평가모드로 전환\n\nrunning_loss = 0.0 ## loss값 초기\nrunning_corrects = 0\ny_test = []\ny_pred = []\nfor inputs, labels in test_loader:\n  inputs = inputs.to(device)\n  labels = labels.to(device)\n  optimizer_ft.zero_grad()\n  with torch.set_grad_enabled(False):\n    outputs = model_ft(inputs)\n    loss = criterion(outputs, labels)\n    _, preds = torch.max(outputs, 1)\n  running_loss += loss.item() * inputs.size(0)\n  running_corrects += torch.sum(preds == labels.data)\n  y_test += labels.data.tolist()\n  y_pred += preds.data.tolist()\n\nepoch_loss = running_loss / len(test_loader.dataset)\nepoch_acc = running_corrects.double() / len(test_loader.dataset)\ntime_elapsed = time.time() - since\nprint('완료 시간 : {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\nprint('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n\n완료 시간 : 0m 1s\nLoss: 0.3580 Acc: 0.8222\n\n\n- 오 loss가 깃에서 사용한 모델보다 적음\n\n결과 확인\n\nfrom sklearn.metrics import *\n\n\ncf_matrix = confusion_matrix(y_test, y_pred)\nprint(cf_matrix)\n\n[[21  6]\n [ 2 16]]\n\n\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.91      0.78      0.84        27\n           1       0.73      0.89      0.80        18\n\n    accuracy                           0.82        45\n   macro avg       0.82      0.83      0.82        45\nweighted avg       0.84      0.82      0.82        45\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\nax.set_title('Confusion Matrix\\n\\n')\n\nax.set_xlabel('\\nPredicted Values')\nax.set_ylabel('Actual Values ')\n\nax.xaxis.set_ticklabels(['fight','noFight'])\nax.yaxis.set_ticklabels(['fight','noFight'])\n\nplt.show()"
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-00. model train (1).html#save",
    "href": "posts/DX/BP/2023-12-28-00. model train (1).html#save",
    "title": "00. model train (1)",
    "section": "save",
    "text": "save\n\nPATH = \"/content/drive/MyDrive/Colab Notebooks/big project/model_save/test_model1.pth\" ## 경로 설정\n\n\ntorch.save(model_ft.state_dict(), PATH)"
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-00. model train (1).html#load",
    "href": "posts/DX/BP/2023-12-28-00. model train (1).html#load",
    "title": "00. model train (1)",
    "section": "load",
    "text": "load\n\nmodel_ft = torchvision.models.video.mc3_18(pretrained=True, progress=True)\n\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = torch.nn.Linear(num_ftrs, 2)\nmodel_ft.load_state_dict(torch.load(PATH))\nmodel_ft = model_ft.to(device)\n#model_ft.eval()"
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-00. model train (1).html#predict",
    "href": "posts/DX/BP/2023-12-28-00. model train (1).html#predict",
    "title": "00. model train (1)",
    "section": "predict",
    "text": "predict\n- test할 유튭 비디오 다운로드\n\n# Download Videos From Youtube for Testing\nFight_utils.downloadYouTube(\"https://www.youtube.com/watch?v=dxOpYEFf9zs&ab_channel=PluggingDaily\", \"/content/\")\nFight_utils.downloadYouTube(\"https://www.youtube.com/watch?v=rEiEJT6B7Yo&ab_channel=TelegramVideo\",\"/content/\")\n\nAgeRestrictedError: ignored\n\n\n- 슈발 연령제한 걸려있다고 안되네\n\n걍 내꺼 데이터로 하자…\n\n\n원본 비디오 재생\n\npath_output = \"/content/drive/MyDrive/Colab Notebooks/big project/test video/폭행/폭행0.mp4\"\nFight_utils.show_video(path_output, width=960)\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n\n비디오 클래스 추론\n\npath_1 = \"/content/drive/MyDrive/Colab Notebooks/big project/test video/폭행/폭행0.mp4\"\npath_2 = \"/content/drive/MyDrive/Colab Notebooks/big project/test video/폭행/폭행59.mp4\"\nSEQUENCE_LENGTH=16\n\n\ndevice\n\ndevice(type='cuda', index=0)\n\n\n- 우리 데이터로 학습하지 않아서 그런가 예측을 잘하지 못한다.\n\n그리고 화질의 차이도 있고\n실제 학습한 데이터는 살벌하게 싸우기 때문에 아래와 같이 잘 예측하지 못하는 것 같음\n\n\nprint(Fight_utils.FightInference(path_1,model_ft,SEQUENCE_LENGTH))\nprint(Fight_utils.FightInference_Time(path_1, model_ft, SEQUENCE_LENGTH)) ## 추론하는데 걸린시간\n\nnoFight\n[('noFight', 0.60027), ('fight', 0.39973)]\n***********\ntime is: 10.565218925476074\nnoFight\n\n\n\noutput_path = \"/content/drive/MyDrive/Colab Notebooks/big project/test video/output/result1.mp4\" ## 임의의 경로 설정\n\n\nstart_time = time.time()\noutVideo_1=Fight_utils.showIference(model_ft, 16, 2, path_1, output_path, showInfo = False)\nelapsed = time.time() - start_time\nprint(\"time is:\",elapsed)\n\ntime is: 9.130757331848145\n\n\n\nVideoFileClip(outVideo_1, audio=False, target_resolution=(300,None)).ipython_display()\n\nMoviepy - Building video __temp__.mp4.\nMoviepy - Writing video __temp__.mp4\n\nMoviepy - Done !\nMoviepy - video ready __temp__.mp4\n\n\n                                                              \n\n\nSorry, seems like your browser doesn't support HTML5 audio/video"
  },
  {
    "objectID": "posts/DX/2023-08-19-01. Plotly test.html",
    "href": "posts/DX/2023-08-19-01. Plotly test.html",
    "title": "01. Plotly test",
    "section": "",
    "text": "import\n\nimport plotly.express as ex\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nimport numpy as np\nimport pandas as pd\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/kalilurrahman/datasets/main/mobilephonemktshare2020.csv')\ndf.head()\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n31.49\n22.09\n10.02\n7.79\n4.10\n3.15\n2.41\n2.40\n9.51\n0.54\n2.35\n0.95\n0.96\n0.70\n0.84\n0.74\n\n\n1\n2019-11\n31.36\n22.90\n10.18\n8.16\n4.42\n3.41\n2.40\n2.40\n9.10\n0.78\n0.66\n0.97\n0.97\n0.73\n0.83\n0.75\n\n\n2\n2019-12\n31.37\n24.79\n9.95\n7.73\n4.23\n3.19\n2.50\n2.54\n8.13\n0.84\n0.75\n0.90\n0.87\n0.74\n0.77\n0.70\n\n\n3\n2020-01\n31.29\n24.76\n10.61\n8.10\n4.25\n3.02\n2.42\n2.40\n7.55\n0.88\n0.69\n0.88\n0.86\n0.79\n0.80\n0.69\n\n\n4\n2020-02\n30.91\n25.89\n10.98\n7.80\n4.31\n2.89\n2.36\n2.34\n7.06\n0.89\n0.70\n0.81\n0.77\n0.78\n0.80\n0.69\n\n\n\n\n\n\n\n\ndf.set_index(\"Date\").diff().\\\n  dropna().boxplot(backend = \"plotly\")"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-30-05. 제안서 작성 (1).html#제안-구성",
    "href": "posts/DX/09. 제안전략수립/2023-11-30-05. 제안서 작성 (1).html#제안-구성",
    "title": "05. 제안서 작성 (1)",
    "section": "제안 구성",
    "text": "제안 구성\n보고서 - 발표자료 - 청자\n\n발표자료에는 청자가 궁금해 하는 내용이 포함되어 있어야함\n\n문제정의 (point, reason)\n해결책 (example)\n실행방안 (인력, 방법론, 비용, 기간 등) (point)\n\n\n- 역사적으로 중심 보고서 작성을 위한 시나리오 구조는 다음과 같다.\n\nPoint\nReason\nExample\nPoint"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-30-05. 제안서 작성 (1).html#문제정의",
    "href": "posts/DX/09. 제안전략수립/2023-11-30-05. 제안서 작성 (1).html#문제정의",
    "title": "05. 제안서 작성 (1)",
    "section": "문제정의",
    "text": "문제정의\n- GAP 분석\n\nAs-IS : 고정 되어있는 상황, 변화를 가지고 가기가 어려움…\nTo-Be : to-be설정을 어떻게 하느냐에 따라서 발생/탐색/설정형의 여부가 정해짐 \\(\\to\\) 문제유형 예시\n\n- 공고나 제안사의 문제정의에서 숨어있는 요구사항을 파악할 줄 알아야한다.\n- 제안서 작성 시, 사업의 범위를 정한 후 담당 실무자에게 확인을 거치는 작업을 해야함.\n- 문서의 내용 외 고객이 말하지 않은 이슈\n\n개인적 편견 및 이슈, 내부 의사결정구조\n\n- 핵심 차별화 요소 = 고객의 중요니즈 + 경쟁자 대비 우위점\n\n고객의 중요 니즈 : 구매센터(이해관계자)별 포지션 파워 x 핵심이슈\n경쟁자 대비 우위점 : 고객 중요 니즈 기반에서 찾아야함"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-30-05. 제안서 작성 (1).html#ex-1.-이슈발굴-및-차별화-요소-도출",
    "href": "posts/DX/09. 제안전략수립/2023-11-30-05. 제안서 작성 (1).html#ex-1.-이슈발굴-및-차별화-요소-도출",
    "title": "05. 제안서 작성 (1)",
    "section": "ex 1. 이슈발굴 및 차별화 요소 도출",
    "text": "ex 1. 이슈발굴 및 차별화 요소 도출"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-28-03. 실행계획 수립.html",
    "href": "posts/DX/09. 제안전략수립/2023-11-28-03. 실행계획 수립.html",
    "title": "03. 실행계획 수립",
    "section": "",
    "text": "3c 분석을 토대로 포지셔닝을 통해 고객이 필요성과 차별성을 인지하도록 솔루션을 만들어야함 (ppt 157)\n\n\n\n1 제품 : 제품 개발 or 제품 수명관리\n2 가격 : 가격책정 or 가격 시나리오 정책\n3 유통 : 유통설계, 유통갈등관리\n4 프로모션\n5 people\n6 Process\n\n\n\ncon- : 함께, -cept : 잡다\n- 솔루션 컨셉을 정의하기 위해서는 고객과 함께 감각화와 구현화를 진행해야함\n\n고객가치를 반영한 모형 설계\n프로토 타입 개발\n현장 테스트 등\n\n- 전략도출을 위한 4가지 질문\n\nWhere : ST\nWhat : Positioning(필요성과 차별성, concept과는 같은 이야기를 하지만 concept은 감각화와 구현화로 구체화하는 것임)\nhow : 4p mix(product, promotion 집중)\nwhen\n\n- 전략은 일반적으로 일관성과 상호보완성을 갖추어야함.\n\n일관성 : 상위 전략(포지셔닝)과 하위 전략(6p)간 일관성이 있어야 함(상급자와 그 밑에 직원들의 전략이 일관성이 있어야함)\n\n\n\n\n\nRTB (Reason to Believe) : 자사가 이야기하는 믿음에 대한 근거(연구소의 실험결과 같은거)"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-28-03. 실행계획 수립.html#구조",
    "href": "posts/DX/09. 제안전략수립/2023-11-28-03. 실행계획 수립.html#구조",
    "title": "03. 실행계획 수립",
    "section": "",
    "text": "3c 분석을 토대로 포지셔닝을 통해 고객이 필요성과 차별성을 인지하도록 솔루션을 만들어야함 (ppt 157)"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-28-03. 실행계획 수립.html#마케팅-6p",
    "href": "posts/DX/09. 제안전략수립/2023-11-28-03. 실행계획 수립.html#마케팅-6p",
    "title": "03. 실행계획 수립",
    "section": "",
    "text": "1 제품 : 제품 개발 or 제품 수명관리\n2 가격 : 가격책정 or 가격 시나리오 정책\n3 유통 : 유통설계, 유통갈등관리\n4 프로모션\n5 people\n6 Process"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-28-03. 실행계획 수립.html#컨셉",
    "href": "posts/DX/09. 제안전략수립/2023-11-28-03. 실행계획 수립.html#컨셉",
    "title": "03. 실행계획 수립",
    "section": "",
    "text": "con- : 함께, -cept : 잡다\n- 솔루션 컨셉을 정의하기 위해서는 고객과 함께 감각화와 구현화를 진행해야함\n\n고객가치를 반영한 모형 설계\n프로토 타입 개발\n현장 테스트 등\n\n- 전략도출을 위한 4가지 질문\n\nWhere : ST\nWhat : Positioning(필요성과 차별성, concept과는 같은 이야기를 하지만 concept은 감각화와 구현화로 구체화하는 것임)\nhow : 4p mix(product, promotion 집중)\nwhen\n\n- 전략은 일반적으로 일관성과 상호보완성을 갖추어야함.\n\n일관성 : 상위 전략(포지셔닝)과 하위 전략(6p)간 일관성이 있어야 함(상급자와 그 밑에 직원들의 전략이 일관성이 있어야함)\n\n\n\n\n\nRTB (Reason to Believe) : 자사가 이야기하는 믿음에 대한 근거(연구소의 실험결과 같은거)"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-28-03. 실행계획 수립.html#감각화와-스토리텔링",
    "href": "posts/DX/09. 제안전략수립/2023-11-28-03. 실행계획 수립.html#감각화와-스토리텔링",
    "title": "03. 실행계획 수립",
    "section": "감각화와 스토리텔링",
    "text": "감각화와 스토리텔링\n1 브랜트 컨셉으로서의 스토리텔링\n\n전략 측면 : 기업의 핵심 스토리는 내부와 외부를 포함한 기업의 모든 커뮤니케이션 활동에 일관성을 부여 (기업 브랜드와 제품 브랜드 모두에 적용 가능)\n\n2 커뮤니케이션 도구로서의 스토리텔링\n\n기업 운영 측면 : 스토리와 일화는 기업의 메시지를 기업 내부와 외부로 전달할 떄 사용 가능. (PT와 광고 등에 적용 가능)\n\n3 스토리텔링 4가지 구성요소\n\n메시지 : 주제, 교훈\n갈등 : 도전, 역경\n등장인물 : 주연, 조연, 적대 세력\n플롯 : 시작, 중간, 끝\n\n4 스토리텔링 맞춤화\n보수적 구매와 실용적 구매로 크게 구분하여 설득 메시지를 작성\n\n보수적 구매 : 유지 관리\n실용적 구매 : 성취, 신제품 개발 등"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-28-03. 실행계획 수립.html#세일즈-툴킷",
    "href": "posts/DX/09. 제안전략수립/2023-11-28-03. 실행계획 수립.html#세일즈-툴킷",
    "title": "03. 실행계획 수립",
    "section": "세일즈 툴킷",
    "text": "세일즈 툴킷\n- 정의 : 고객의 구매프로세스 및 각 단계별로 필요한 정보를 정의하여 고객접점의 담당자들이 유사한 수준의 고객커뮤니케이션 활동을 수행할 수 있도록 디자인된 도구의 집합\n\n설득을 위한 세일즈 툴킷 내용\n1 로고스 : 이성적 설득, 명료하고 보편 타당한 주장하기\n2 파토스 : 감성적 설득, 인간과 인간의 만남, 자신의 이야기와 비언어적 요소 많이 쓰기(고객과 관련된 사례 서술)\n3 에토스 : 진정성, 전문적 식견을 가지고 상대를 배려하며 언행일치하기\n\n\n설득력을 높이는 방법(3요소)\n1 전문성 : 영업사원의 전문성을 증명할 수 있는 구체적인 증거를 제시\n2 논리성 : 기승전결의 구조를 갖고 말하기\n3 타 산업의 성공 사레를 보여주며 당신의 문제를 해결할 수 있다는 확신 전달\n\n\n세일즈 툴킷 주요 유형\n(ppt 186, 187 참고)"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-28-03. 실행계획 수립.html#조사현장에서의-질문",
    "href": "posts/DX/09. 제안전략수립/2023-11-28-03. 실행계획 수립.html#조사현장에서의-질문",
    "title": "03. 실행계획 수립",
    "section": "조사현장에서의 질문",
    "text": "조사현장에서의 질문\n- 모든 대화는 질문과 답변으로 구성되어 있음\n- 대화를 잘하는 것은 질문을 잘하는 것임\n\n좋은 질문\n1 문제의 본질을 파악하게 해주는 질문\n2 상황을 새로운 눈으로 보게 해주는 질문\n3 생각의 수준을 높여주는 질문\n4 생각 확장 및 상상력을 높여주는 질문\n5 미래에 대한 의견, 조직의 강점 및 긍정 요소\n6 구체적으로 묻기\n\n\nSPNI\n1 상황질문(situation) : 고객사/사용자의 현재 상황에 대한 사실 정보와 고객사 내에서 일어나고 있는 특정 현상과 활동 또는 사용자의 사용현황에 대한 데이터를 모으기 위한 질문\n\n문제를 일으킨 영향요소는 무엇인가?\n\n2 문제질문(problem) : 고객사/사용자가 겪고 있는 문제점, 어려움, 불만족을 탐색하고 고객/사용자에게 잠재 니즈를 스스로 말하도록 유도하는 질문\n\n가치제안이 필요한 문제는 무엇인가?\n\n3 시사질문(implication) : 고객/사용자가 대수롭지 않게 여기고 작게 인식하고 있는 문제들을 확대하고 발전시키는 질문\n\n고객이 추구하는 가치를 올바르게 공략하고 있는가?(물리적/경제적/정서적 가치)\n\n4 해결질문(Need-off) : 고객/사용자가 가진 문제를 해결하는 솔루션에 대해 유용성과 가치를 키워주는 질문\n\n내가 제시하는 또는 고객과 협의한 혜택이 올바른가?"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html",
    "href": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html",
    "title": "01. 환경분석 기법의 이해",
    "section": "",
    "text": "1 무엇을 분석할 것인가?\n2 분석의 범위는 어느 정도가지 봐야 하는가?\n3 어떻게 분석할 것인가?\n\n\n- 우리가 공급업체라면 고객과 고객의 기업까지 보아야한다.\n\nexample : 공급기업(KT) \\(\\to\\) 고객사/정부 \\(\\to\\) 소비자/이용자\n\n\n\n\n\n\n- 최종재의수요의 결과로 생겨나는 간접적인 요소, 즉 생산요소(산업재)에 대한 수요\n\n산업재(원자재) \\(\\to\\) 산업재(부품1) \\(\\to\\) 산업재(부품2) \\(\\to\\) 최종재 \\(\\to\\) 고객\n일반적으로 생산 연쇄 과정이 길수록 가속효과는 두드러짐\n\n- 발생하는 현상\n\n최종고객에 비하여 산업재의 수요 변동폭이 크게 나타남 \\(\\to\\) 이러한 가속효과는 수요예측의 어려음을 가속시킴\n\n- 그래서!! 최종고객 니즈 변화에 대한 예측이 필요하고, 최종고객에 대한 마케팅활동의 수정이 필요하다!!\n\n즉, B2B 뿐만 아니라 B2C 모니터링도 지속적으로 이루어져야 한다.\n\n\n\n\n- 구매자의 필요나 기호에 의하여 함께 사용되는 둘 이상의 생산물에 대한 수요\n\n어떤 상품을 구입할 때 그에 따른 다른 상품도 반드시 구입해야 하는 경우에 나타남\n\n\n\n\n- 기업 내에서 제품이나 서비스를 구매하기 위한 의사 결정 과정에서 관여하는 조직 내의 직원들의 그룹\n\n구성원 : 구매 관리자, 기술 전문가, 운영 전문가 등"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#분석-프레임",
    "href": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#분석-프레임",
    "title": "01. 환경분석 기법의 이해",
    "section": "",
    "text": "1 무엇을 분석할 것인가?\n2 분석의 범위는 어느 정도가지 봐야 하는가?\n3 어떻게 분석할 것인가?\n\n\n- 우리가 공급업체라면 고객과 고객의 기업까지 보아야한다.\n\nexample : 공급기업(KT) \\(\\to\\) 고객사/정부 \\(\\to\\) 소비자/이용자\n\n\n\n\n\n\n- 최종재의수요의 결과로 생겨나는 간접적인 요소, 즉 생산요소(산업재)에 대한 수요\n\n산업재(원자재) \\(\\to\\) 산업재(부품1) \\(\\to\\) 산업재(부품2) \\(\\to\\) 최종재 \\(\\to\\) 고객\n일반적으로 생산 연쇄 과정이 길수록 가속효과는 두드러짐\n\n- 발생하는 현상\n\n최종고객에 비하여 산업재의 수요 변동폭이 크게 나타남 \\(\\to\\) 이러한 가속효과는 수요예측의 어려음을 가속시킴\n\n- 그래서!! 최종고객 니즈 변화에 대한 예측이 필요하고, 최종고객에 대한 마케팅활동의 수정이 필요하다!!\n\n즉, B2B 뿐만 아니라 B2C 모니터링도 지속적으로 이루어져야 한다.\n\n\n\n\n- 구매자의 필요나 기호에 의하여 함께 사용되는 둘 이상의 생산물에 대한 수요\n\n어떤 상품을 구입할 때 그에 따른 다른 상품도 반드시 구입해야 하는 경우에 나타남\n\n\n\n\n- 기업 내에서 제품이나 서비스를 구매하기 위한 의사 결정 과정에서 관여하는 조직 내의 직원들의 그룹\n\n구성원 : 구매 관리자, 기술 전문가, 운영 전문가 등"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#가설지향-프로세스",
    "href": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#가설지향-프로세스",
    "title": "01. 환경분석 기법의 이해",
    "section": "가설지향 프로세스",
    "text": "가설지향 프로세스\n- 설정한 가설과 관련된 Data 분석 등을 통해 수정하는 활동을 거쳐 정확한 가설로서 결론에 도달함\n\n핵심이슈 \\(\\to\\) 가설설정 \\(\\to\\) 가설수정 \\(\\to\\) 가설설정 \\(\\to\\) 가설수정 \\(\\dots\\) \\(\\to\\) 가설수정 \\(\\to\\) 결론"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#전략요소",
    "href": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#전략요소",
    "title": "01. 환경분석 기법의 이해",
    "section": "전략요소",
    "text": "전략요소\n1 전략적 의사결정을 수행하기 위해 파악해야 하는 요소들\n2 그 요소들의 변화에 큰 영향을 주는 핵심 환경 요소를 의미\n3 산업/고객의 변화에 따른 전략요소 정의\n\nexample : 전통시장 활성화 전략\n\n\n전통시장의 매출감소량 및 매출이 유지되는 곳 파악\n고객과 관련된 내용 파악 \\(\\to\\) 니즈 파악\n경쟁사의 전략을 파악 \\(\\to\\) 대형마트에서 파는 상품, 전략 등"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#자료-정리-및-추정",
    "href": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#자료-정리-및-추정",
    "title": "01. 환경분석 기법의 이해",
    "section": "자료 정리 및 추정",
    "text": "자료 정리 및 추정\n1 연역법 : 일반적인 전제로부터 특정한 결론을 도출하는 방법\n\n모든 사람은 죽는다. 존은 사람이다. 존은 죽는다.\n\n2 귀납법 : 특별한 사례나 관찰로부터 일반적인 규칙이나 법칙을 도출\n\n지금까지의 관측에서 모든 사람은 물을 마시고 있다. 따라서 미래에도 모든 사람은 물을 마실 것이다.\n\n3 가추법 : 주어진 사실이나 현상을 설명할 수 있는 여러 가능성 중에서 가장 타당한 설명을 찾는 추론 방법\n\n창문이 열려 있고 비가 온다. 지면이 젖어 있으며 물기가 많다. 따라서 비가 내린 것이다.\n\n4 규모추정 : 특정 프로젝트, 작업 도는 일련의 활동에 대한 크기 또는 양을 평가\n\n페르미 추정 : 대략적인 수량을 어림잡아 추정 \\(\\to\\) 오래된 자료와 시장에 영향을 미치는 요소들을 중심으로 페르미 추정 실시\n\n5 전략추정 : 조직이나 기업이 비즈니스 전략을 수립하고 실행하기 위해 미래의 상황이나 변수에 대해 평가하고 예측하는 과정\n\nWhy 추정: 전략 추정에서 “Why” 추정은 특정 전략이나 목표를 설정한 이유, 그 목표를 달성하기 위한 비전 및 이유를 설명하는 단계.\n\n전략의 근본적인 목적을 이해하고, 그 목적이 조직의 장기적인 비전과 어떻게 연결되는지 이해하는 것이 중요\n\nHow 추정: 전략이나 목표를 달성하는 데 필요한 구체적인 행동 계획을 설계하는 것.\n\n이는 전략의 실행 방법, 필요한 리소스, 프로젝트 일정 등을 정의하는 과정을 포함"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#환경분석과-정보원",
    "href": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#환경분석과-정보원",
    "title": "01. 환경분석 기법의 이해",
    "section": "환경분석과 정보원",
    "text": "환경분석과 정보원\n\n\n\n환경분석\n정보원\n\n\n\n\n소비자의 소비 변화\n카드사 발표자료 등\n\n\n소비자의 소비 변화와 관련된 파생공급자\n파생공급자 협회 등\n\n\n정부의 지원 정책 변화\n주요 부처의 홍보 자료\n\n\n산업 변화\n경제기관 애널리스트\n\n\n고객사내 변화\n논문/전문잡지 검색"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#example.-전략요소-정의",
    "href": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#example.-전략요소-정의",
    "title": "01. 환경분석 기법의 이해",
    "section": "example. 전략요소 정의",
    "text": "example. 전략요소 정의"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#인터뷰시-유의점",
    "href": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#인터뷰시-유의점",
    "title": "01. 환경분석 기법의 이해",
    "section": "인터뷰시 유의점",
    "text": "인터뷰시 유의점\n1 확인 질문에서 시작해서 새로운 정보 파악을 위한 질문으로 넘기기\n2 Yes/No로 답할 수 있는 질문은 삼가\n3 왜 why를 묻지 않고 속성 attribute를 중심으로 질문"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#조사-자료에-대한-평가",
    "href": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#조사-자료에-대한-평가",
    "title": "01. 환경분석 기법의 이해",
    "section": "조사 자료에 대한 평가",
    "text": "조사 자료에 대한 평가\n1 이미 알고 있던 정보는 무엇인가?\n2 모호하게 알고 있었던 정보는 무엇인가?\n3 전혀 몰랐던 정보는 무엇인가?\n4 전략요소가 명확하게 정의 되었는가?\n5 정보원이 믿을만 한가?\n6 선입관은 배제 되었는가?"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#고객-니즈-구조",
    "href": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#고객-니즈-구조",
    "title": "01. 환경분석 기법의 이해",
    "section": "고객 니즈 구조",
    "text": "고객 니즈 구조\n1 고객이 표현하는 니즈\n2 고객이 표현하지 않는 니즈\n\n고객이 아는 니즈\n고객도 모르는 니즈"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#b2bg-구매의-특성",
    "href": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#b2bg-구매의-특성",
    "title": "01. 환경분석 기법의 이해",
    "section": "B2B(G) 구매의 특성",
    "text": "B2B(G) 구매의 특성\n- 조직 구매자는 구매를 위해 집단 의사결정 또는 의사결정을 조율함 (B2C와의 차이점)\n- 즉, 구매센터의 경계가 없음 (B2C와의 차이점)\n- 구매센터의 규모와 구조(ppt101, 102 참고)"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#고객-니즈-정의-요소",
    "href": "posts/DX/09. 제안전략수립/2023-11-24-01. 환경분석 기법의 이해.html#고객-니즈-정의-요소",
    "title": "01. 환경분석 기법의 이해",
    "section": "고객 니즈 정의 요소",
    "text": "고객 니즈 정의 요소\n1 Quality (품질):\n\n품질은 제품이나 서비스가 기대치를 충족하거나 초과하는 정도를 의미하며, 신뢰성, 내구성, 성능 등이 중요한 평가 요소\n\n2 Cost (비용):\n\n고객은 합리적인 가격대에서 원하는 가치를 얻을 수 있는지에 대해 관심을 가짐\n\n3 Delivery (납품):\n\n고객은 빠르고 정확한 납품이 기업과의 협력을 강화하는 중요한 요소로 인식\n\n4 Technology (기술):\n\n고객은 혁신적이고 최신 기술을 활용한 제품이나 서비스를 원함. 기술적인 우위는 경쟁 우위로 이어질 수 있습니다."
  },
  {
    "objectID": "posts/DX/07. IT 인프라/2023-11-06-00. IT 인프라 (1).html",
    "href": "posts/DX/07. IT 인프라/2023-11-06-00. IT 인프라 (1).html",
    "title": "00. IT 인프라 (1)",
    "section": "",
    "text": "- 인프라 : 기반 시설, 또는 인프라스트럭처는 경제 활동의 기반을 형성하는 기초적인 시설과 시스템을 말하며, 도로나 하천, 항만, 공항 등과 같이 경제 활동에 밀접한 사회 자본\n- IT 인프라란?\n\n네트워크, 서버, 데이터베이스, 정보보안, 시스템 소프트웨어 및 기반시설 등 IT 서비스의 기반이 되는 시스템 및 구조\n\n- 우리가 사용하는 개발언어(python, R)들을 컴퓨터 서버에 설치하고 분석을 하게끔 도와주고, 활용 및 공유하고 이점을 가질 수 있도록 해주는 것이 전부 IT인프라 구성도에 포함되어 있다.\n- 이번 시간에 스토리지, 네트워크 등 인프라 구성도에 관한 요소들을 학습한다.\n\n\n1 하드웨어 : 눈에 보이는 물리적 장비\n\nPC, 모니터, 하드디스크, CPU, MEMORY, 서버, 스토리지, 라우터, 스위치 등\n\n2 소프트웨어 : 하드웨어를 움직이게 하는 눈에 보이지 않는 프로그램\n\nOS, Database, Microsofrt Office, Photoshop 등\n\n3 네트워크 : 컴퓨터 같은 장비들이 그물망처럼 연결된 형태 또는 장비\n\n공유기(Router), 스위치, 방화벽, 케이블 등\n\n\n\n\n- 서버, 네트워크, 스토리지와 같은 기업의 IT인프라 구성 요소들이 모여 있는 시설\n\n\n\n-기존 인프라(On-Premise)와 클라우드 인프라로 구분\n\n\n1 서버, 스토리지, 네트워크 등을 기업 자체적으로 보유하여 운영하는 방식\n2 클라우드 컴퓨팅의 발전 이전까지 사용하던 일반적이고 전통적인 인프라 유형\n3 기업이 데이터 센터를 직접 만들어 운영하는 방식\n\n\n\n1 퍼블릭 클라우드\n\n클라우드 컴퓨팅 서비스를 제공해주는 업체(CSP, Cloud Service Provider)에게 인프라에 필요한 자원들을 대여하여 사용하는 방식\nex : AWS, Azure, GCP, KT 클라우드, 네이버 클라우드\n\n2 프라이빗 클라우드\n\n기업이 직접 클라우드 환경을 구축, 이를 기업내부에서 활용, 계열사에 공개\n특정 기업, 특정 사용자만 사용하는 방식\n서비스 자원과 데이터는 기업의 데이터 센터에 저장\n\n3 하이브리드 유형\n\n기존 On-premise에 구성되어 있는 인프라와 Public Cloud를 혼용하여 함께 사용하는 방식\n또는, 프라이빗 클라우드 + 퍼블릭 클라우드\n\n4 멀티 클라우드\n\n2개 이상의 서로 다른 퍼블릭 클라우드를 함께 사용하는 방식\nAWS + Azure, AWS + KT\n하나의 CSP에 종속되지 않게 하여, 기능 이상 시 하나의 클라우드를 통해 가동하는 방식\n\n\n\n\n\n- 온프레미스 : 서버, 스토리지, 네트워크, 데이터 센터 등을 기업 자체적으로 보유하여 운영하는 방식\n- 하이브리드 클라우드 : 기존 On-premise에 구성되어 있는 인프라와 Public Cloud를 혼용하여 함께 사용하는 방식\n\n또는, 프라이빗 클라우드 + 퍼블릭 클라우\n\n- IT 인프라 구성도 : IT 인프라 구성 등을 한 눈에 알아볼 수 있는 다이어그램 또는 구성도\n\n시스템의 배치 연결관계를 아이콘, 선으로 구조화"
  },
  {
    "objectID": "posts/DX/07. IT 인프라/2023-11-06-00. IT 인프라 (1).html#주요-구성요소",
    "href": "posts/DX/07. IT 인프라/2023-11-06-00. IT 인프라 (1).html#주요-구성요소",
    "title": "00. IT 인프라 (1)",
    "section": "",
    "text": "1 하드웨어 : 눈에 보이는 물리적 장비\n\nPC, 모니터, 하드디스크, CPU, MEMORY, 서버, 스토리지, 라우터, 스위치 등\n\n2 소프트웨어 : 하드웨어를 움직이게 하는 눈에 보이지 않는 프로그램\n\nOS, Database, Microsofrt Office, Photoshop 등\n\n3 네트워크 : 컴퓨터 같은 장비들이 그물망처럼 연결된 형태 또는 장비\n\n공유기(Router), 스위치, 방화벽, 케이블 등"
  },
  {
    "objectID": "posts/DX/07. IT 인프라/2023-11-06-00. IT 인프라 (1).html#데이터-센터",
    "href": "posts/DX/07. IT 인프라/2023-11-06-00. IT 인프라 (1).html#데이터-센터",
    "title": "00. IT 인프라 (1)",
    "section": "",
    "text": "- 서버, 네트워크, 스토리지와 같은 기업의 IT인프라 구성 요소들이 모여 있는 시설"
  },
  {
    "objectID": "posts/DX/07. IT 인프라/2023-11-06-00. IT 인프라 (1).html#it-인프라-유형",
    "href": "posts/DX/07. IT 인프라/2023-11-06-00. IT 인프라 (1).html#it-인프라-유형",
    "title": "00. IT 인프라 (1)",
    "section": "",
    "text": "-기존 인프라(On-Premise)와 클라우드 인프라로 구분\n\n\n1 서버, 스토리지, 네트워크 등을 기업 자체적으로 보유하여 운영하는 방식\n2 클라우드 컴퓨팅의 발전 이전까지 사용하던 일반적이고 전통적인 인프라 유형\n3 기업이 데이터 센터를 직접 만들어 운영하는 방식\n\n\n\n1 퍼블릭 클라우드\n\n클라우드 컴퓨팅 서비스를 제공해주는 업체(CSP, Cloud Service Provider)에게 인프라에 필요한 자원들을 대여하여 사용하는 방식\nex : AWS, Azure, GCP, KT 클라우드, 네이버 클라우드\n\n2 프라이빗 클라우드\n\n기업이 직접 클라우드 환경을 구축, 이를 기업내부에서 활용, 계열사에 공개\n특정 기업, 특정 사용자만 사용하는 방식\n서비스 자원과 데이터는 기업의 데이터 센터에 저장\n\n3 하이브리드 유형\n\n기존 On-premise에 구성되어 있는 인프라와 Public Cloud를 혼용하여 함께 사용하는 방식\n또는, 프라이빗 클라우드 + 퍼블릭 클라우드\n\n4 멀티 클라우드\n\n2개 이상의 서로 다른 퍼블릭 클라우드를 함께 사용하는 방식\nAWS + Azure, AWS + KT\n하나의 CSP에 종속되지 않게 하여, 기능 이상 시 하나의 클라우드를 통해 가동하는 방식"
  },
  {
    "objectID": "posts/DX/07. IT 인프라/2023-11-06-00. IT 인프라 (1).html#summary-1",
    "href": "posts/DX/07. IT 인프라/2023-11-06-00. IT 인프라 (1).html#summary-1",
    "title": "00. IT 인프라 (1)",
    "section": "",
    "text": "- 온프레미스 : 서버, 스토리지, 네트워크, 데이터 센터 등을 기업 자체적으로 보유하여 운영하는 방식\n- 하이브리드 클라우드 : 기존 On-premise에 구성되어 있는 인프라와 Public Cloud를 혼용하여 함께 사용하는 방식\n\n또는, 프라이빗 클라우드 + 퍼블릭 클라우\n\n- IT 인프라 구성도 : IT 인프라 구성 등을 한 눈에 알아볼 수 있는 다이어그램 또는 구성도\n\n시스템의 배치 연결관계를 아이콘, 선으로 구조화"
  },
  {
    "objectID": "posts/DX/07. IT 인프라/2023-11-06-00. IT 인프라 (1).html#네트워크",
    "href": "posts/DX/07. IT 인프라/2023-11-06-00. IT 인프라 (1).html#네트워크",
    "title": "00. IT 인프라 (1)",
    "section": "(1) 네트워크",
    "text": "(1) 네트워크\n- Net + Work의 합성어\n- 컴퓨터 같은 노드들이 통신 기술을 이용하여 그물망처럼 연결된 통신형태\n- 종류\n\nPAN(Personal Area Network) : 가장 작은 규모\nLAN(Local Area Network) : 근거리 영역\nMAN(Metropolitan Area Network) : 대도시 영역\nWAN(Wide Area Network) : 광대역 등등\n\n- 네트워크 사용시 장점\n\n통신의 유용성\n효율적이고 데이터 전송이 쉬움\n\n- 단점\n\n정보 보안을 많이 신경써야한다.\n디도스, 랜섬웨어 등에 대한 방안을 면밀하게 세워야함"
  },
  {
    "objectID": "posts/DX/07. IT 인프라/2023-11-06-00. IT 인프라 (1).html#구성요소",
    "href": "posts/DX/07. IT 인프라/2023-11-06-00. IT 인프라 (1).html#구성요소",
    "title": "00. IT 인프라 (1)",
    "section": "(2) 구성요소",
    "text": "(2) 구성요소\n\n전송 단말기\n- 정보를 주고 받는 장치 : PC, 휴대폰, 노트북, 서버 등\n\n\n전송 매체\n- 정보를 전달할 수 있는 수단\n- 유선케이블 : 동축케이블, 광케이블, LAN 케이블 등\n- 무선기술 : Wifi, 5G, LTE, 블루투스 등\n\n\n전송 프로토콜\n- 전송 단말기간 정보를 주고받기 위한 일종의 규칙, 약속, 규약\n\n\n네트워크 장비\n- 정보를 전달하기 위해 목적지를 찾고 길을 결정하는 장비(허브, 스위치, 라우터 등)"
  },
  {
    "objectID": "posts/DX/07. IT 인프라/2023-11-06-00. IT 인프라 (1).html#osi-7계층-모델",
    "href": "posts/DX/07. IT 인프라/2023-11-06-00. IT 인프라 (1).html#osi-7계층-모델",
    "title": "00. IT 인프라 (1)",
    "section": "(3) OSI 7계층 모델",
    "text": "(3) OSI 7계층 모델\n\n각 계층 설명\n- 7계층 : 응용(Application)\n\n사용자에게 다양한 Network 서비스를 제공(email-SMTP, FTP, 가상터미널…)\n사용자가 크롬같은 브라우저 등을 켰을 때 가장 먼저 보이는 화면 (https)\n사용자가 눈으로 볼 수 있는 화면\n\n- 6계층 : 표현(Presentation)\n\nApp, Layer에 대한 Date 전송, Syntax 협의\nData Format 결정, 구성 : jpg, png, mp4, mp3 등등\nCode 변환\n구축변환\n압축 및 압축 해제\n7계층에서 사용되는 데이터를 암호화, 5계층에 보내기 위한 데이터를 번역해줌\n\n- 5계층 : 세션(Session)\n\nInterhost Communication\nApplication 사이의 대화를 시작, 유지, 끝을 맺는 계층\n\n- 4계층 : 전송(Transport)\n\n데이터를 어디로 보낼지 결정하는 단계 (ex, 아파트 동 호수)\nApplication 사이의 논리적인 통로 제공\n전송 문제 관리 및 신뢰성 있는 전송 보장\n\n- 3계층 : 네트워크(Network)\n\n논리적인 주소를 사용(IP주소)\n경로 관리, 최적 경로 선정\n사용자 Date(Packet) 전송\n\n- 2계층 : 데이터 링크(Date Link)\n\nIP주소를 읽어 하드웨어 주소인 MAC을 사용\n3계층에서 IP주소를 받아서 하드웨어 주소인 MAC주소와 연결\n\n- 1계층 : 물리(Physical)\n\n물리적인 연결\n전기적, 기계적, 기능적 절차적인 수단을 제공"
  },
  {
    "objectID": "posts/DX/07. IT 인프라/2023-11-06-00. IT 인프라 (1).html#summary2",
    "href": "posts/DX/07. IT 인프라/2023-11-06-00. IT 인프라 (1).html#summary2",
    "title": "00. IT 인프라 (1)",
    "section": "summary2",
    "text": "summary2\n- 스위치 :\n- MAC주소 :\n- 캡슐화 :"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 03. transformer.html",
    "href": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 03. transformer.html",
    "title": "summary 03. Transformer",
    "section": "",
    "text": "import tensorflow as tf\nimport os\n\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n\ntf.config.experimental_connect_to_cluster(resolver)\ntf.tpu.experimental.initialize_tpu_system(resolver)\n\n&lt;tensorflow.python.tpu.topology.Topology at 0x7a04d3af47f0&gt;\n\n\n\nstrategy = tf.distribute.TPUStrategy(resolver)"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 03. transformer.html#구조",
    "href": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 03. transformer.html#구조",
    "title": "summary 03. Transformer",
    "section": "구조",
    "text": "구조\n- 쉽게 풀어보자면 Query, key, value라는 변수를 설정 후 학습하는 것이다.\n- step1. Query, key, value\n\n\n\n\n3가지 변수의 초기 시작 값은 동일하다. 그러나 위의 학습과정을 통해 최종 값은 달라진다.\n\n- step 2. Attention Layer\n\n주어진 text가 “I am a student”이고 “i”라는 단어에 대한 \\(Q, K, V\\)\n\n\n\n\n\\[\\text{Attention}(Q,K,V) = \\text{softmax}\\left (\\frac {QK^{\\top}}{\\sqrt{d_k}}V \\right )= \\frac {3}{2} = 1.5\\]\n- step3. 이를 모든 단어에 대해 수행하면?\n\n&lt;img src = “https://blog.kakaocdn.net/dn/yt8W5/btrTzCenX8v/n1gjS3MTuhktj0xDDsox20/img.png”, width = 500&gt;"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 03. transformer.html#중간-정리",
    "href": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 03. transformer.html#중간-정리",
    "title": "summary 03. Transformer",
    "section": "중간 정리",
    "text": "중간 정리\n1 기존의 RNN 모델을 은닉노두 \\(h_t\\)가 이전 시점의 결과를 순차적으로 학습하였음\n2 이는 연산 효율이 떨어지는 병목현상을 유발함\n3 attention 매커니즘은 주목해야할 정보에만 집중(softmax)하면서 모델의 성능 향상을 야기함.\n4 그 중, self-attettion은 자기 자신에게 Attention 매커니즘을 행하는 방식임\n\n이를 하는 이유는 단어들의 연관성을 파악하기 위함"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 03. transformer.html#비교",
    "href": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 03. transformer.html#비교",
    "title": "summary 03. Transformer",
    "section": "비교",
    "text": "비교\n\n일반적인 Attention 메커니즘\n\n\n\n\n\nMulti-head Attention 메커니즘\n\n\n\n\n\n자! 멀티헤드 어텐션을 이용하면 기존의 4 x 8 이었던 \\(Q, K, V\\)를 4등분하여 병렬처리 하는 것을 볼 수 있다!!\n앞서 설명했듯이 사람들이 회의를 하듯이 여러 부분에서 도출된 결과를 통해 서로 정보를 상호보완하여 더 좋은 성능을 야기할 수 있다!"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 03. transformer.html#positional-embedding의-조건",
    "href": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 03. transformer.html#positional-embedding의-조건",
    "title": "summary 03. Transformer",
    "section": "positional embedding의 조건",
    "text": "positional embedding의 조건\n1 모델의 효율적인 학습을 위해 스케일이 어느 정도 범위 안에 있어야 한다.\n\n만약, positional embedding의 스케일 범위가 너무 크면 이 자체 값의 영향력이 너무 커져 다른 값이 무시되어 학습되지 않음,.\n\n2 input data의 크기에 상관없이 output을 도출해야된다.\n\n만약, input data의 크기가 10일 때에 대해서만 output을 도출할 수 있다면, 다른 사이즈의 input data에 대해서는 output을 도출할 수 없다..\n위 조건을 모두 만족하는 함수가 삼각함수인데 주기적으로 반복돼서 정보가 겹친다는 점을 해결하위해 임베딩 차원이 짝수, 홀수에 따라 다음과 같이 정의한다.\n\n\\[PE_{pos, 2i} =  \\text{sin}\\left(\\frac {pos}{10000^{2i/d_{model}}}\\right )\\]\n\\[PE_{pos, 2i+1} =  \\text{cos}\\left(\\frac {pos}{10000^{2i/d_{model}}}\\right )\\]"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 03. transformer.html#실습.-imdb-데이터",
    "href": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 03. transformer.html#실습.-imdb-데이터",
    "title": "summary 03. Transformer",
    "section": "실습. imdb 데이터",
    "text": "실습. imdb 데이터\n\n(1) 데이터 로드 및 전처리\n\nimport tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n17464789/17464789 [==============================] - 0s 0us/step\n\n\n\nmax_len = 500\nvocab_size = len(set([j for i in x_train for j in i]))\n\n- 패딩\n\nx_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen = 500)\nx_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen = 500)\n\n\n\n(2) 멀티 헤드 엍켄션 클래스 구현\n- num_heads = 8로 설정\n\n\nCode\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, embedding_dim, num_heads=8):\n        super(MultiHeadAttention, self).__init__()\n        self.embedding_dim = embedding_dim # d_model\n        self.num_heads = num_heads\n\n        assert embedding_dim % self.num_heads == 0\n\n        self.projection_dim = embedding_dim // num_heads\n        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n        self.dense = tf.keras.layers.Dense(embedding_dim)\n\n    def scaled_dot_product_attention(self, query, key, value):\n        matmul_qk = tf.matmul(query, key, transpose_b=True)\n        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n        logits = matmul_qk / tf.math.sqrt(depth)\n        attention_weights = tf.nn.softmax(logits, axis=-1)\n        output = tf.matmul(attention_weights, value)\n        return output, attention_weights\n\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, inputs):\n        # x.shape = [batch_size, seq_len, embedding_dim]\n        batch_size = tf.shape(inputs)[0]\n\n        # (batch_size, seq_len, embedding_dim)\n        query = self.query_dense(inputs)\n        key = self.key_dense(inputs)\n        value = self.value_dense(inputs)\n\n        # (batch_size, num_heads, seq_len, projection_dim)\n        query = self.split_heads(query, batch_size)\n        key = self.split_heads(key, batch_size)\n        value = self.split_heads(value, batch_size)\n\n        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n        # (batch_size, seq_len, num_heads, projection_dim)\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n\n        # (batch_size, seq_len, embedding_dim)\n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n        outputs = self.dense(concat_attention)\n        return outputs\n\n\n\n\n(3) 인코더 설계\n- 멀티 헤드 어텐션에 두 번쨰 서브층, 포지션 와이즈 피드 포워드 신경망을 추가하여 인코더 클래스를 설계\n\nclass TransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, embedding_dim, num_heads, dff, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = MultiHeadAttention(embedding_dim, num_heads)\n        self.ffn = tf.keras.Sequential(\n            [tf.keras.layers.Dense(dff, activation=\"relu\"),\n             tf.keras.layers.Dense(embedding_dim),]\n        )\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs) # 첫번째 서브층 : 멀티 헤드 어텐션\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output) # Add & Norm\n        ffn_output = self.ffn(out1) # 두번째 서브층 : 포지션 와이즈 피드 포워드 신경망\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output) # Add & Norm\n\n\n\n(4) 포지션 임베딩 설계\n\nclass TokenAndPositionEmbedding(tf.keras.layers.Layer):\n    def __init__(self, max_len, vocab_size, embedding_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.pos_emb = tf.keras.layers.Embedding(max_len, embedding_dim)\n\n    def call(self, x):\n        max_len = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=max_len, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions\n\n\n\n(5) 모델링\n\nembedding_dim = 120  # 각 단어의 임베딩 벡터의 차원\nnum_heads = 8  # 어텐션 헤드의 수\ndff = 120  # 포지션 와이즈 피드 포워드 신경망의 은닉층의 크기\n\nwith strategy.scope() :\n          inputs = tf.keras.layers.Input(shape=(max_len,))\n          embedding_layer = TokenAndPositionEmbedding(max_len, vocab_size, embedding_dim)\n          x = embedding_layer(inputs)\n          transformer_block = TransformerBlock(embedding_dim, num_heads, dff)\n          x = transformer_block(x)\n          x = tf.keras.layers.GlobalAveragePooling1D()(x)\n          x = tf.keras.layers.Dropout(0.1)(x)\n          x = tf.keras.layers.Dense(20, activation=\"relu\")(x)\n          x = tf.keras.layers.Dropout(0.1)(x)\n          outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n\n          model = tf.keras.Model(inputs=inputs, outputs=outputs)\n          model.compile(optimizer = tf.keras.optimizers.Adam(0.001),\n                                      loss = tf.keras.losses.binary_crossentropy)\n\n\n\n(6) 모델 학습\n\nh = model.fit(x_train, y_train, epochs = 10,\n                          validation_split = 0.2).history\n\nEpoch 1/10\n625/625 [==============================] - 23s 23ms/step - loss: 0.6950 - val_loss: 0.6932\nEpoch 2/10\n625/625 [==============================] - 12s 19ms/step - loss: 0.6932 - val_loss: 0.6931\nEpoch 3/10\n625/625 [==============================] - 12s 19ms/step - loss: 0.6932 - val_loss: 0.6932\nEpoch 4/10\n625/625 [==============================] - 12s 19ms/step - loss: 0.4739 - val_loss: 0.2822\nEpoch 5/10\n625/625 [==============================] - 12s 19ms/step - loss: 0.1884 - val_loss: 0.2648\nEpoch 6/10\n625/625 [==============================] - 12s 19ms/step - loss: 0.0955 - val_loss: 0.3670\nEpoch 7/10\n625/625 [==============================] - 12s 19ms/step - loss: 0.0428 - val_loss: 0.5349\nEpoch 8/10\n625/625 [==============================] - 12s 19ms/step - loss: 0.0252 - val_loss: 0.6099\nEpoch 9/10\n625/625 [==============================] - 12s 20ms/step - loss: 0.0134 - val_loss: 0.6918\nEpoch 10/10\n625/625 [==============================] - 12s 19ms/step - loss: 0.0103 - val_loss: 0.7428\n\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize = (10,4))\nplt.plot(h[\"loss\"], label = \"train\")\nplt.plot(h[\"val_loss\"], label = \"val\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n(7) 예측\n\nimport numpy as np\nfrom sklearn.metrics import *\n\n\npred = model.predict(x_test)\ny_pred = np.where(pred&gt;=0.5, 1, 0)\n\n782/782 [==============================] - 8s 10ms/step\n\n\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.85      0.86      0.86     12500\n           1       0.86      0.85      0.86     12500\n\n    accuracy                           0.86     25000\n   macro avg       0.86      0.86      0.86     25000\nweighted avg       0.86      0.86      0.86     25000\n\n\n\n- 방금전 LSTM으로 0.82의 test 정확도가 나왔는데…\n- 오, 0.04이나 개선되었다!"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 03. transformer.html#ref",
    "href": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 03. transformer.html#ref",
    "title": "summary 03. Transformer",
    "section": "ref",
    "text": "ref\n\nratsgo’s NLPBOOK : Copyright © 2020 Gichang LEE.\n코딩 오페라\n딥러닝을 이용한 자연어 처리 입문 (안상준, 유원준)"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 01. RNN.html",
    "href": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 01. RNN.html",
    "title": "summary 01. RNN",
    "section": "",
    "text": "- RNN은 이전 시점의 정보들과 현 시점의 입력데이터의 정보를 이용하여 다음 시점의 값을 예측하는 것이다.\n- RNN과 더불어 CNN은 텍스트와 주가 같은 시계열 데이터에 사용되나 CNN의 경우 순차적인 정보와 장기 의존성에 대한 문제 때문에 적합하지않다.\n\n항상 그런 것은 아니지만 대부분 그럼(maxpooling만 생각해봐도 정보를 압축해서 대표값으로 feature를 표현하기 때문에 적합하지않음)\n\n- 뭐 여튼, 결국 RNN은 이점시점의 정보들을 이용하여 다음 시점의 정보를 에측하는 것이다!\n- RNN의 기본구조\n\\[h_t = \\text {tanh}(w_{hh}h_{t-1} + W_{xh}x_t + b_h)\\]\n\\[y_t  =  W_{hy}h_t + b_y\\]\n\n\n\n\n\n- 4주전의 온도 데이터를 가지고 그 다음주의 온도를 예측해보자.\n\n\n\nimport pandas as pd\n\n\ndata = pd.read_csv('https://raw.githubusercontent.com/DA4BAM/dataset/master/temperature.csv')\ndata.head(10)\n\n\n  \n    \n\n\n\n\n\n\nyear\nweek\nAvgTemp\n\n\n\n\n0\n2010\n1\n-3.000000\n\n\n1\n2010\n2\n-7.500000\n\n\n2\n2010\n3\n-7.900000\n\n\n3\n2010\n4\n-2.357143\n\n\n4\n2010\n5\n-3.342857\n\n\n5\n2010\n6\n-1.800000\n\n\n6\n2010\n7\n-0.314286\n\n\n7\n2010\n8\n-2.142857\n\n\n8\n2010\n9\n4.400000\n\n\n9\n2010\n10\n7.057143\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- y만들기\n\ndata[\"y\"] = data[\"AvgTemp\"].shift(-1)\ndata.dropna(axis = 0, inplace = True)\n\n\ndata.head()\n\n\n  \n    \n\n\n\n\n\n\nyear\nweek\nAvgTemp\ny\n\n\n\n\n0\n2010\n1\n-3.000000\n-7.500000\n\n\n1\n2010\n2\n-7.500000\n-7.900000\n\n\n2\n2010\n3\n-7.900000\n-2.357143\n\n\n3\n2010\n4\n-2.357143\n-3.342857\n\n\n4\n2010\n5\n-3.342857\n-1.800000\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndata.shape\n\n(263, 4)\n\n\n- x, y 분리\n\nx = data.loc[:, [\"AvgTemp\"]]\ny = data[\"y\"]\n\n- 스케일링\n\nfrom sklearn.preprocessing import MinMaxScaler ## 텍스트 데이터에서는 다른 방법 사용\n\nscaler = MinMaxScaler()\nx = scaler.fit_transform(x)\n\n- 3차원 구조 만들기\n\nimport numpy as np\n\n\n# 시계열 데이터 전처리 2차원 --&gt; 3차원으로 변환\ndef temporalize(x, y, timesteps):\n    nfeature = x.shape[1]\n    output_x = []\n    output_y = []\n    for i in range(len(x) - timesteps + 1):\n        t = []\n        for j in range(timesteps):\n            t.append(x[[(i + j)], :])\n        output_x.append(t)\n        output_y.append(y[i + timesteps - 1])\n    return np.array(output_x).reshape(-1,timesteps, nfeature), np.array(output_y)\n\n\nx2, y2 =  temporalize(x, y, 4)\nx2.shape, y2.shape\n\n((260, 4, 1), (260,))\n\n\n- 데이터셋 분할\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_val, y_train, y_val = train_test_split(x2, y2, test_size= 53, shuffle = False) ## test_size를 53개로 지정\n\n\n\n\n\nx_train.shape[1], x_train.shape[]\n\n(4, 1)\n\n\n\ntimestep = x_train.shape[1] ## 반영할 시점의 수\nnf = x_train.shape[2] ##  feature\n\n\nimport tensorflow as tf\n\n\ntf.keras.backend.clear_session()\n\nX = tf.keras.layers.Input(shape = [timestep, nf])\nhl = tf.keras.layers.SimpleRNN(8)(X)\nY = tf.keras.layers.Dense(1)(hl)\n\nmodel = tf.keras.models.Model(X, Y)\nmodel.compile(loss = \"mse\", optimizer = tf.keras.optimizers.Adam(0.01))\n\n\nmodel.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 4, 1)]            0         \n                                                                 \n simple_rnn (SimpleRNN)      (None, 8)                 80        \n                                                                 \n dense (Dense)               (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 89\nTrainable params: 89\nNon-trainable params: 0\n_________________________________________________________________\n\n\n- 은닉층 파라미터 개수 세보기 (hidden layer)\n\n8*(8+1+1) ## (hidden output)x (hidden output + input feature + bias)\n\n80\n\n\n\n\n\n\nhistory = model.fit(x_train, y_train, epochs = 100, validation_split = 0.2, verbose = 0).history\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize = (10,4))\nplt.plot(history[\"loss\"], label = \"train\")\nplt.plot(history[\"val_loss\"], label = \"val\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7e11b4c42890&gt;\n\n\n\n\n\n\n\n\n\npred = model.predict(x_val)\n\n2/2 [==============================] - 0s 10ms/step\n\n\n\nfrom sklearn.metrics import *\n\nmean_absolute_error(y_val, pred)\n\n2.579951970680879\n\n\n\nplt.figure(figsize = (10,4))\nplt.plot(y_val, label = \"true\")\nplt.plot(pred, label = \"pred\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7e11b4ad9f00&gt;\n\n\n\n\n\n- 오 너무나도 잘 적합되었음.\n\n\n\n\n\nfrom tensorflow.keras.backend import clear_session\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n\nclear_session()\n\nX = layers.Input(shape = [timestep, nf])\n\nh1 = layers.SimpleRNN(8, return_sequences = True)(X)\nh2 = layers.SimpleRNN(4)(h1)\n\nY = layers.Dense(1)(h2)\n\nmodel = Model(X, Y)\nmodel.compile(loss = \"mse\", optimizer = Adam(0.01))\n\n\nh = model.fit(x_train, y_train, validation_split = 0.2, epochs = 100,\n                        verbose = 0).history\n\n\nplt.plot(h[\"loss\"], label = \"train\")\nplt.plot(h[\"val_loss\"], label = \"val\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7e11a7f5feb0&gt;\n\n\n\n\n\n\npred = model.predict(x_val)\n\n2/2 [==============================] - 0s 6ms/step\n\n\n\nplt.plot(y_val, label = \"true\")\nplt.plot(pred, label = \"pred\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7e11b4ce21a0&gt;\n\n\n\n\n\n- 특정 구간에서 처음에 설정한 모델보다 잘 예측하지 못함..\n\n어떻게 보면 당연한 거임. RNN layer의 각 은닉 노드는 이전 시점의 값을 계속 반영하는데 당연히 2개나 쌓으니 overfitting이 날 수 빆에 없음..\n\n\n\n\n\n\n\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n17464789/17464789 [==============================] - 0s 0us/step\n\n\n\nprint(x_train.shape, y_train.shape)\n\n(25000,) (25000,)\n\n\n- 이미 주어진 데이터는 인코딩 되어 있음\n\nprint(x_train[0])\n\n[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n\n\n- 각 target 변수들은 동일한 비율로 구성되어 있음.\n\nnp.unique(y_train,return_counts=True)\n\n(array([0, 1]), array([12500, 12500]))\n\n\n- padding\n\nx_train  = tf.keras.utils.pad_sequences(x_train, maxlen = 20)\nx_test  = tf.keras.utils.pad_sequences(x_test, maxlen = 20)\n\nprint(x_train.shape, x_test.shape)\n\n(25000, 20) (25000, 20)\n\n\n\n\n\n\n\n\nimport tensorflow as tf\nimport os\n\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n\ntf.config.experimental_connect_to_cluster(resolver)\ntf.tpu.experimental.initialize_tpu_system(resolver)\n\n&lt;tensorflow.python.tpu.topology.Topology at 0x7e11b4e43580&gt;\n\n\n\nstrategy = tf.distribute.TPUStrategy(resolver)\n\n\nlayers.Embedding?\n\n\nclear_session()\nwith strategy.scope() :\n          X = layers.Input(shape = [20]) ## max_len = 20 이므로\n          H = layers.Embedding(input_dim = 88585, output_dim = 10, input_length = 20)(X) ## 총 단어개수 88585, 28은\n          H = layers.SimpleRNN(32)(H)\n          Y =  layers.Dense(1, activation = \"sigmoid\")(H)\n\n          model = Model(X,Y)\n          model.compile(loss = tf.keras.losses.binary_crossentropy,\n                                      optimizer = Adam(0.01))\n\n\nmodel.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 20)]              0         \n                                                                 \n embedding (Embedding)       (None, 20, 10)            885850    \n                                                                 \n simple_rnn (SimpleRNN)      (None, 32)                1376      \n                                                                 \n dense (Dense)               (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 887,259\nTrainable params: 887,259\nNon-trainable params: 0\n_________________________________________________________________\n\n\n- 임베딩 레이어 파라미터 수\n\n88585 * 10\n\n885850\n\n\n- rnn 레이어 파라미터 수\n\n앞서 배운 시계열 데이터라고 생각하면, 20일치 데이터를 28개의 차원으로 전달해서 32개의 은닉노드를 가지는 RNN레이어에 태운 것임\n\n\n(10+32+1)*32\n\n430\n\n\n\n\n\n\n\nh = model.fit(x_train, y_train, epochs = 10, validation_split = 0.2,\n                verbose = 1).history\n\nEpoch 1/10\n625/625 [==============================] - 16s 21ms/step - loss: 0.6116 - val_loss: 0.5674\nEpoch 2/10\n625/625 [==============================] - 11s 18ms/step - loss: 0.4921 - val_loss: 0.6649\nEpoch 3/10\n625/625 [==============================] - 11s 18ms/step - loss: 0.5216 - val_loss: 0.6466\nEpoch 4/10\n625/625 [==============================] - 11s 18ms/step - loss: 0.4466 - val_loss: 0.6317\nEpoch 5/10\n625/625 [==============================] - 10s 16ms/step - loss: 0.4363 - val_loss: 0.6761\nEpoch 6/10\n625/625 [==============================] - 11s 18ms/step - loss: 0.4677 - val_loss: 0.6652\nEpoch 7/10\n625/625 [==============================] - 10s 17ms/step - loss: 0.4385 - val_loss: 0.6793\nEpoch 8/10\n625/625 [==============================] - 10s 15ms/step - loss: 0.4090 - val_loss: 0.6845\nEpoch 9/10\n625/625 [==============================] - 12s 19ms/step - loss: 0.4148 - val_loss: 0.6996\nEpoch 10/10\n625/625 [==============================] - 11s 17ms/step - loss: 0.4816 - val_loss: 0.7072\n\n\n\nplt.figure(figsize = (10, 4))\nplt.plot(h[\"loss\"], label = \"train\")\nplt.plot(h[\"val_loss\"], label = \"val\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\npred = model.predict(x_test)\n\n782/782 [==============================] - 10s 11ms/step\n\n\n- 흠 모델 성능이 쓰레기다..\n\n\n\n\nclear_session()\nwith strategy.scope() :\n          X = layers.Input(shape = [20]) ## max_len = 20 이므로\n          H = layers.Embedding(input_dim = 88585, output_dim = 10, input_length = 20)(X) ## 총 단어개수 88585, 28은\n          H = layers.SimpleRNN(32)(H)\n          H = layers.BatchNormalization()(H)\n          Y =  layers.Dense(1, activation = \"sigmoid\")(H)\n\n          model = Model(X,Y)\n          model.compile(loss = tf.keras.losses.binary_crossentropy,\n                                      optimizer = Adam(0.001))\n\n\nh = model.fit(x_train, y_train, epochs = 10, validation_split = 0.2,\n                verbose = 1).history\n\nEpoch 1/10\n625/625 [==============================] - 16s 19ms/step - loss: 0.6240 - val_loss: 0.5372\nEpoch 2/10\n625/625 [==============================] - 10s 16ms/step - loss: 0.4808 - val_loss: 0.7613\nEpoch 3/10\n625/625 [==============================] - 11s 17ms/step - loss: 0.3965 - val_loss: 0.5965\nEpoch 4/10\n625/625 [==============================] - 10s 16ms/step - loss: 0.3379 - val_loss: 1.9309\nEpoch 5/10\n625/625 [==============================] - 10s 17ms/step - loss: 0.3052 - val_loss: 0.9143\nEpoch 6/10\n625/625 [==============================] - 11s 17ms/step - loss: 0.2626 - val_loss: 0.8126\nEpoch 7/10\n625/625 [==============================] - 11s 17ms/step - loss: 0.2518 - val_loss: 1.0190\nEpoch 8/10\n625/625 [==============================] - 12s 19ms/step - loss: 0.2296 - val_loss: 1.2424\nEpoch 9/10\n625/625 [==============================] - 10s 16ms/step - loss: 0.2284 - val_loss: 0.8976\nEpoch 10/10\n625/625 [==============================] - 10s 16ms/step - loss: 0.2154 - val_loss: 1.0790\n\n\n\nplt.figure(figsize = (10, 4))\nplt.plot(h[\"loss\"], label = \"train\")\nplt.plot(h[\"val_loss\"], label = \"val\")\nplt.legend()\nplt.show()\n\n\n\n\n\npred = model.predict(x_test)\npred\n\n782/782 [==============================] - 14s 14ms/step\n\n\narray([[0.973527  ],\n       [0.9925418 ],\n       [0.91735345],\n       ...,\n       [0.41336167],\n       [0.5320429 ],\n       [0.9485291 ]], dtype=float32)\n\n\n\ny_pred = np.where(pred&gt;=0.5,1,0)\n\n\nprint(confusion_matrix(y_test,y_pred))\n\n[[ 7156  5344]\n [ 1969 10531]]\n\n\n- 성능이 그닥…..\n\nprint(classification_report(y_test,y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.78      0.57      0.66     12500\n           1       0.66      0.84      0.74     12500\n\n    accuracy                           0.71     25000\n   macro avg       0.72      0.71      0.70     25000\nweighted avg       0.72      0.71      0.70     25000\n\n\n\n\n\n\n\n\n- RNN 모델은 시계열, 텍스트 데이터와 같이 시간의 흐름에 따른 시퀀셜 데이터에 대한 모형 적합시 사용된다.\n- 그러나 앞서 살펴보았듯이 높은 예측성을 기대하는 것은 좀 힘듬…\n- 그래디언트 소실문제\n\n긴 시퀀스를 처리할 때 발생\n역전파 시에 gradient가 작아져 학습이 어려워지는 현상\ntanh함수는 비선형성을 유지시켜주면서, 시그모이드에 비해 gradient를 더 크게 유지시켜주나 그래도 기울기 소실문제를 완전히 해결한 것이 아님\n이러한 그래디언트 소실 문제를 장기 의존성 문제라고 하며 LSTM에서 이를 어느정도 해결하는 법을 배운다!\n\n\n\n\nratsgo’s NLPBOOK : Copyright © 2020 Gichang LEE."
  },
  {
    "objectID": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 01. RNN.html#실습-1.-주가-데이터",
    "href": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 01. RNN.html#실습-1.-주가-데이터",
    "title": "summary 01. RNN",
    "section": "",
    "text": "- 4주전의 온도 데이터를 가지고 그 다음주의 온도를 예측해보자.\n\n\n\nimport pandas as pd\n\n\ndata = pd.read_csv('https://raw.githubusercontent.com/DA4BAM/dataset/master/temperature.csv')\ndata.head(10)\n\n\n  \n    \n\n\n\n\n\n\nyear\nweek\nAvgTemp\n\n\n\n\n0\n2010\n1\n-3.000000\n\n\n1\n2010\n2\n-7.500000\n\n\n2\n2010\n3\n-7.900000\n\n\n3\n2010\n4\n-2.357143\n\n\n4\n2010\n5\n-3.342857\n\n\n5\n2010\n6\n-1.800000\n\n\n6\n2010\n7\n-0.314286\n\n\n7\n2010\n8\n-2.142857\n\n\n8\n2010\n9\n4.400000\n\n\n9\n2010\n10\n7.057143\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- y만들기\n\ndata[\"y\"] = data[\"AvgTemp\"].shift(-1)\ndata.dropna(axis = 0, inplace = True)\n\n\ndata.head()\n\n\n  \n    \n\n\n\n\n\n\nyear\nweek\nAvgTemp\ny\n\n\n\n\n0\n2010\n1\n-3.000000\n-7.500000\n\n\n1\n2010\n2\n-7.500000\n-7.900000\n\n\n2\n2010\n3\n-7.900000\n-2.357143\n\n\n3\n2010\n4\n-2.357143\n-3.342857\n\n\n4\n2010\n5\n-3.342857\n-1.800000\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndata.shape\n\n(263, 4)\n\n\n- x, y 분리\n\nx = data.loc[:, [\"AvgTemp\"]]\ny = data[\"y\"]\n\n- 스케일링\n\nfrom sklearn.preprocessing import MinMaxScaler ## 텍스트 데이터에서는 다른 방법 사용\n\nscaler = MinMaxScaler()\nx = scaler.fit_transform(x)\n\n- 3차원 구조 만들기\n\nimport numpy as np\n\n\n# 시계열 데이터 전처리 2차원 --&gt; 3차원으로 변환\ndef temporalize(x, y, timesteps):\n    nfeature = x.shape[1]\n    output_x = []\n    output_y = []\n    for i in range(len(x) - timesteps + 1):\n        t = []\n        for j in range(timesteps):\n            t.append(x[[(i + j)], :])\n        output_x.append(t)\n        output_y.append(y[i + timesteps - 1])\n    return np.array(output_x).reshape(-1,timesteps, nfeature), np.array(output_y)\n\n\nx2, y2 =  temporalize(x, y, 4)\nx2.shape, y2.shape\n\n((260, 4, 1), (260,))\n\n\n- 데이터셋 분할\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_val, y_train, y_val = train_test_split(x2, y2, test_size= 53, shuffle = False) ## test_size를 53개로 지정\n\n\n\n\n\nx_train.shape[1], x_train.shape[]\n\n(4, 1)\n\n\n\ntimestep = x_train.shape[1] ## 반영할 시점의 수\nnf = x_train.shape[2] ##  feature\n\n\nimport tensorflow as tf\n\n\ntf.keras.backend.clear_session()\n\nX = tf.keras.layers.Input(shape = [timestep, nf])\nhl = tf.keras.layers.SimpleRNN(8)(X)\nY = tf.keras.layers.Dense(1)(hl)\n\nmodel = tf.keras.models.Model(X, Y)\nmodel.compile(loss = \"mse\", optimizer = tf.keras.optimizers.Adam(0.01))\n\n\nmodel.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 4, 1)]            0         \n                                                                 \n simple_rnn (SimpleRNN)      (None, 8)                 80        \n                                                                 \n dense (Dense)               (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 89\nTrainable params: 89\nNon-trainable params: 0\n_________________________________________________________________\n\n\n- 은닉층 파라미터 개수 세보기 (hidden layer)\n\n8*(8+1+1) ## (hidden output)x (hidden output + input feature + bias)\n\n80\n\n\n\n\n\n\nhistory = model.fit(x_train, y_train, epochs = 100, validation_split = 0.2, verbose = 0).history\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize = (10,4))\nplt.plot(history[\"loss\"], label = \"train\")\nplt.plot(history[\"val_loss\"], label = \"val\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7e11b4c42890&gt;\n\n\n\n\n\n\n\n\n\npred = model.predict(x_val)\n\n2/2 [==============================] - 0s 10ms/step\n\n\n\nfrom sklearn.metrics import *\n\nmean_absolute_error(y_val, pred)\n\n2.579951970680879\n\n\n\nplt.figure(figsize = (10,4))\nplt.plot(y_val, label = \"true\")\nplt.plot(pred, label = \"pred\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7e11b4ad9f00&gt;\n\n\n\n\n\n- 오 너무나도 잘 적합되었음."
  },
  {
    "objectID": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 01. RNN.html#실습-2.-주가-데이터-rnn-layer-2개",
    "href": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 01. RNN.html#실습-2.-주가-데이터-rnn-layer-2개",
    "title": "summary 01. RNN",
    "section": "",
    "text": "from tensorflow.keras.backend import clear_session\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n\nclear_session()\n\nX = layers.Input(shape = [timestep, nf])\n\nh1 = layers.SimpleRNN(8, return_sequences = True)(X)\nh2 = layers.SimpleRNN(4)(h1)\n\nY = layers.Dense(1)(h2)\n\nmodel = Model(X, Y)\nmodel.compile(loss = \"mse\", optimizer = Adam(0.01))\n\n\nh = model.fit(x_train, y_train, validation_split = 0.2, epochs = 100,\n                        verbose = 0).history\n\n\nplt.plot(h[\"loss\"], label = \"train\")\nplt.plot(h[\"val_loss\"], label = \"val\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7e11a7f5feb0&gt;\n\n\n\n\n\n\npred = model.predict(x_val)\n\n2/2 [==============================] - 0s 6ms/step\n\n\n\nplt.plot(y_val, label = \"true\")\nplt.plot(pred, label = \"pred\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7e11b4ce21a0&gt;\n\n\n\n\n\n- 특정 구간에서 처음에 설정한 모델보다 잘 예측하지 못함..\n\n어떻게 보면 당연한 거임. RNN layer의 각 은닉 노드는 이전 시점의 값을 계속 반영하는데 당연히 2개나 쌓으니 overfitting이 날 수 빆에 없음.."
  },
  {
    "objectID": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 01. RNN.html#실습-3.-imdb-영화-리뷰-데이터",
    "href": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 01. RNN.html#실습-3.-imdb-영화-리뷰-데이터",
    "title": "summary 01. RNN",
    "section": "",
    "text": "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n17464789/17464789 [==============================] - 0s 0us/step\n\n\n\nprint(x_train.shape, y_train.shape)\n\n(25000,) (25000,)\n\n\n- 이미 주어진 데이터는 인코딩 되어 있음\n\nprint(x_train[0])\n\n[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n\n\n- 각 target 변수들은 동일한 비율로 구성되어 있음.\n\nnp.unique(y_train,return_counts=True)\n\n(array([0, 1]), array([12500, 12500]))\n\n\n- padding\n\nx_train  = tf.keras.utils.pad_sequences(x_train, maxlen = 20)\nx_test  = tf.keras.utils.pad_sequences(x_test, maxlen = 20)\n\nprint(x_train.shape, x_test.shape)\n\n(25000, 20) (25000, 20)\n\n\n\n\n\n\n\n\nimport tensorflow as tf\nimport os\n\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n\ntf.config.experimental_connect_to_cluster(resolver)\ntf.tpu.experimental.initialize_tpu_system(resolver)\n\n&lt;tensorflow.python.tpu.topology.Topology at 0x7e11b4e43580&gt;\n\n\n\nstrategy = tf.distribute.TPUStrategy(resolver)\n\n\nlayers.Embedding?\n\n\nclear_session()\nwith strategy.scope() :\n          X = layers.Input(shape = [20]) ## max_len = 20 이므로\n          H = layers.Embedding(input_dim = 88585, output_dim = 10, input_length = 20)(X) ## 총 단어개수 88585, 28은\n          H = layers.SimpleRNN(32)(H)\n          Y =  layers.Dense(1, activation = \"sigmoid\")(H)\n\n          model = Model(X,Y)\n          model.compile(loss = tf.keras.losses.binary_crossentropy,\n                                      optimizer = Adam(0.01))\n\n\nmodel.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 20)]              0         \n                                                                 \n embedding (Embedding)       (None, 20, 10)            885850    \n                                                                 \n simple_rnn (SimpleRNN)      (None, 32)                1376      \n                                                                 \n dense (Dense)               (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 887,259\nTrainable params: 887,259\nNon-trainable params: 0\n_________________________________________________________________\n\n\n- 임베딩 레이어 파라미터 수\n\n88585 * 10\n\n885850\n\n\n- rnn 레이어 파라미터 수\n\n앞서 배운 시계열 데이터라고 생각하면, 20일치 데이터를 28개의 차원으로 전달해서 32개의 은닉노드를 가지는 RNN레이어에 태운 것임\n\n\n(10+32+1)*32\n\n430\n\n\n\n\n\n\n\nh = model.fit(x_train, y_train, epochs = 10, validation_split = 0.2,\n                verbose = 1).history\n\nEpoch 1/10\n625/625 [==============================] - 16s 21ms/step - loss: 0.6116 - val_loss: 0.5674\nEpoch 2/10\n625/625 [==============================] - 11s 18ms/step - loss: 0.4921 - val_loss: 0.6649\nEpoch 3/10\n625/625 [==============================] - 11s 18ms/step - loss: 0.5216 - val_loss: 0.6466\nEpoch 4/10\n625/625 [==============================] - 11s 18ms/step - loss: 0.4466 - val_loss: 0.6317\nEpoch 5/10\n625/625 [==============================] - 10s 16ms/step - loss: 0.4363 - val_loss: 0.6761\nEpoch 6/10\n625/625 [==============================] - 11s 18ms/step - loss: 0.4677 - val_loss: 0.6652\nEpoch 7/10\n625/625 [==============================] - 10s 17ms/step - loss: 0.4385 - val_loss: 0.6793\nEpoch 8/10\n625/625 [==============================] - 10s 15ms/step - loss: 0.4090 - val_loss: 0.6845\nEpoch 9/10\n625/625 [==============================] - 12s 19ms/step - loss: 0.4148 - val_loss: 0.6996\nEpoch 10/10\n625/625 [==============================] - 11s 17ms/step - loss: 0.4816 - val_loss: 0.7072\n\n\n\nplt.figure(figsize = (10, 4))\nplt.plot(h[\"loss\"], label = \"train\")\nplt.plot(h[\"val_loss\"], label = \"val\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\npred = model.predict(x_test)\n\n782/782 [==============================] - 10s 11ms/step\n\n\n- 흠 모델 성능이 쓰레기다..\n\n\n\n\nclear_session()\nwith strategy.scope() :\n          X = layers.Input(shape = [20]) ## max_len = 20 이므로\n          H = layers.Embedding(input_dim = 88585, output_dim = 10, input_length = 20)(X) ## 총 단어개수 88585, 28은\n          H = layers.SimpleRNN(32)(H)\n          H = layers.BatchNormalization()(H)\n          Y =  layers.Dense(1, activation = \"sigmoid\")(H)\n\n          model = Model(X,Y)\n          model.compile(loss = tf.keras.losses.binary_crossentropy,\n                                      optimizer = Adam(0.001))\n\n\nh = model.fit(x_train, y_train, epochs = 10, validation_split = 0.2,\n                verbose = 1).history\n\nEpoch 1/10\n625/625 [==============================] - 16s 19ms/step - loss: 0.6240 - val_loss: 0.5372\nEpoch 2/10\n625/625 [==============================] - 10s 16ms/step - loss: 0.4808 - val_loss: 0.7613\nEpoch 3/10\n625/625 [==============================] - 11s 17ms/step - loss: 0.3965 - val_loss: 0.5965\nEpoch 4/10\n625/625 [==============================] - 10s 16ms/step - loss: 0.3379 - val_loss: 1.9309\nEpoch 5/10\n625/625 [==============================] - 10s 17ms/step - loss: 0.3052 - val_loss: 0.9143\nEpoch 6/10\n625/625 [==============================] - 11s 17ms/step - loss: 0.2626 - val_loss: 0.8126\nEpoch 7/10\n625/625 [==============================] - 11s 17ms/step - loss: 0.2518 - val_loss: 1.0190\nEpoch 8/10\n625/625 [==============================] - 12s 19ms/step - loss: 0.2296 - val_loss: 1.2424\nEpoch 9/10\n625/625 [==============================] - 10s 16ms/step - loss: 0.2284 - val_loss: 0.8976\nEpoch 10/10\n625/625 [==============================] - 10s 16ms/step - loss: 0.2154 - val_loss: 1.0790\n\n\n\nplt.figure(figsize = (10, 4))\nplt.plot(h[\"loss\"], label = \"train\")\nplt.plot(h[\"val_loss\"], label = \"val\")\nplt.legend()\nplt.show()\n\n\n\n\n\npred = model.predict(x_test)\npred\n\n782/782 [==============================] - 14s 14ms/step\n\n\narray([[0.973527  ],\n       [0.9925418 ],\n       [0.91735345],\n       ...,\n       [0.41336167],\n       [0.5320429 ],\n       [0.9485291 ]], dtype=float32)\n\n\n\ny_pred = np.where(pred&gt;=0.5,1,0)\n\n\nprint(confusion_matrix(y_test,y_pred))\n\n[[ 7156  5344]\n [ 1969 10531]]\n\n\n- 성능이 그닥…..\n\nprint(classification_report(y_test,y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.78      0.57      0.66     12500\n           1       0.66      0.84      0.74     12500\n\n    accuracy                           0.71     25000\n   macro avg       0.72      0.71      0.70     25000\nweighted avg       0.72      0.71      0.70     25000"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 01. RNN.html#summary",
    "href": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 01. RNN.html#summary",
    "title": "summary 01. RNN",
    "section": "",
    "text": "- RNN 모델은 시계열, 텍스트 데이터와 같이 시간의 흐름에 따른 시퀀셜 데이터에 대한 모형 적합시 사용된다.\n- 그러나 앞서 살펴보았듯이 높은 예측성을 기대하는 것은 좀 힘듬…\n- 그래디언트 소실문제\n\n긴 시퀀스를 처리할 때 발생\n역전파 시에 gradient가 작아져 학습이 어려워지는 현상\ntanh함수는 비선형성을 유지시켜주면서, 시그모이드에 비해 gradient를 더 크게 유지시켜주나 그래도 기울기 소실문제를 완전히 해결한 것이 아님\n이러한 그래디언트 소실 문제를 장기 의존성 문제라고 하며 LSTM에서 이를 어느정도 해결하는 법을 배운다!"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 01. RNN.html#ref",
    "href": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 01. RNN.html#ref",
    "title": "summary 01. RNN",
    "section": "",
    "text": "ratsgo’s NLPBOOK : Copyright © 2020 Gichang LEE."
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-10-Extra 01. 언어지능 (1).html",
    "href": "posts/DX/05. 딥러닝/2023-10-10-Extra 01. 언어지능 (1).html",
    "title": "Extra 01. 언어지능",
    "section": "",
    "text": "cd /content/drive/MyDrive/Colab Notebooks/ISLP/\n\n/content/drive/MyDrive/Colab Notebooks/ISLP"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-10-Extra 01. 언어지능 (1).html#ex1.레모네이드",
    "href": "posts/DX/05. 딥러닝/2023-10-10-Extra 01. 언어지능 (1).html#ex1.레모네이드",
    "title": "Extra 01. 언어지능",
    "section": "ex1.레모네이드",
    "text": "ex1.레모네이드\n0 import\n\nimport pandas as pd\nimport tensorflow as tf\n\n1 데이터 준비\n\n레모네이드 = pd.read_csv(\"https://raw.githubusercontent.com/blackdew/tensorflow1/master/csv/lemonade.csv\")\n독립 = 레모네이드[[\"온도\"]]\n종속 = 레모네이드[[\"판매량\"]]\n\n2 모델 구조 생성\n\nX = tf.keras.Input(shape = [1])\nY = tf.keras.layers.Dense(1)(X)\n\nmodel = tf.keras.Model(X,Y)\nmodel.compile(loss = \"mse\")\n\n3 모델학습\n\nmodel.fit(독립, 종속, epochs  = 10)\n\nEpoch 1/10\n1/1 [==============================] - 5s 5s/step - loss: 2335.1794\nEpoch 2/10\n1/1 [==============================] - 0s 11ms/step - loss: 2327.9836\nEpoch 3/10\n1/1 [==============================] - 0s 9ms/step - loss: 2322.7744\nEpoch 4/10\n1/1 [==============================] - 0s 10ms/step - loss: 2318.4187\nEpoch 5/10\n1/1 [==============================] - 0s 8ms/step - loss: 2314.5571\nEpoch 6/10\n1/1 [==============================] - 0s 10ms/step - loss: 2311.0217\nEpoch 7/10\n1/1 [==============================] - 0s 8ms/step - loss: 2307.7200\nEpoch 8/10\n1/1 [==============================] - 0s 8ms/step - loss: 2304.5938\nEpoch 9/10\n1/1 [==============================] - 0s 7ms/step - loss: 2301.6042\nEpoch 10/10\n1/1 [==============================] - 0s 10ms/step - loss: 2298.7239\n\n\n&lt;keras.src.callbacks.History at 0x7870c810c700&gt;\n\n\n4 모델학습2. epochs를 많이 줘보자\n\nmodel.fit(독립, 종속, epochs  = 1000, verbose = 0)\n\n&lt;keras.src.callbacks.History at 0x7870b7fc15a0&gt;\n\n\n\nmodel.history.history[\"loss\"][-10:]\n\n[609.4378051757812,\n 608.2850341796875,\n 607.1331787109375,\n 605.982421875,\n 604.8328247070312,\n 603.6843872070312,\n 602.5370483398438,\n 601.3908081054688,\n 600.2455444335938,\n 599.1014404296875]\n\n\n5 predict\n\n레모네이드\n\n\n  \n    \n\n\n\n\n\n\n온도\n판매량\n\n\n\n\n0\n20\n40\n\n\n1\n21\n42\n\n\n2\n22\n44\n\n\n3\n23\n46\n\n\n4\n24\n48\n\n\n5\n25\n50\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nmodel.predict([20])\n\n1/1 [==============================] - 0s 133ms/step\n\n\narray([[18.444012]], dtype=float32)\n\n\n- 음 대충 엇비슷한 결과 나온다.\n- weights 확인\n\nmodel.weights\n\n[&lt;tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[0.8715454]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([1.0131029], dtype=float32)&gt;]"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-10-Extra 01. 언어지능 (1).html#ex2.-iris",
    "href": "posts/DX/05. 딥러닝/2023-10-10-Extra 01. 언어지능 (1).html#ex2.-iris",
    "title": "Extra 01. 언어지능",
    "section": "ex2. iris",
    "text": "ex2. iris\n- 아이리스 품종분류\n1 데이터 이해 및 전처리\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/blackdew/tensorflow1/master/csv/iris.csv\")\n\ndf = pd.get_dummies(df)\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\n꽃잎길이\n꽃잎폭\n꽃받침길이\n꽃받침폭\n품종_setosa\n품종_versicolor\n품종_virginica\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n1\n0\n0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n1\n0\n0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n1\n0\n0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n1\n0\n0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n1\n0\n0\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nx_input = df[df.columns.values[:4]]\ny_label = df[df.columns.values[4:]]\n\nprint(x_input.shape, y_label.shape)\n\n(150, 4) (150, 3)\n\n\n2 모델 설계\n\nX = tf.keras.Input(shape = [4])\nY = tf.keras.layers.Dense(3, activation = \"softmax\")(X)\n\nmodel = tf.keras.Model(X, Y)\nmodel.compile(loss = tf.losses.categorical_crossentropy, metrics = \"accuracy\")\n\n\n컴퓨터는 loss만 보고 loss가 낮아지는 방향으로 weight를 조절해 간다.\nmetrics는 사람이 보려고 만들어준거 컴퓨터는 상관안함.\n\n3 모델 학습\n\nmodel.fit(x_input, y_label, epochs = 100, verbose = 0)\n\n&lt;keras.src.callbacks.History at 0x78707b9b3f40&gt;\n\n\n\nhistory의 keys값 확인\n\n\nmodel.history.history.keys()\n\ndict_keys(['loss', 'accuracy'])\n\n\n\nh1 = pd.DataFrame(model.history.history)\nh1.head()\n\n\n  \n    \n\n\n\n\n\n\nloss\naccuracy\n\n\n\n\n0\n1.739628\n0.333333\n\n\n1\n1.619714\n0.333333\n\n\n2\n1.542850\n0.333333\n\n\n3\n1.476537\n0.333333\n\n\n4\n1.420134\n0.340000\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n4 loss, accuracy 시각화\n\nimport matplotlib.pyplot  as plt\n\n\n\nCode\nplt.figure(figsize = (8, 4))\nplt.plot(h1[\"loss\"],\".-b\", label = \"loss\", alpha = 0.5)\nplt.plot(h1[\"accuracy\"],\".-r\", label = \"accuracy\", alpha = 0.5)\nplt.title(\"train loss & accuracy\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-10-Extra 01. 언어지능 (1).html#주식예측",
    "href": "posts/DX/05. 딥러닝/2023-10-10-Extra 01. 언어지능 (1).html#주식예측",
    "title": "Extra 01. 언어지능",
    "section": "주식예측",
    "text": "주식예측\n\n!pip uninstall bokeh -y\n!pip install bokeh==2.4.3\n!pip install finance-datareader\n\nFound existing installation: bokeh 3.2.2\nUninstalling bokeh-3.2.2:\n  Successfully uninstalled bokeh-3.2.2\nCollecting bokeh==2.4.3\n  Downloading bokeh-2.4.3-py3-none-any.whl (18.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.5/18.5 MB 45.7 MB/s eta 0:00:00\nRequirement already satisfied: Jinja2&gt;=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh==2.4.3) (3.1.2)\nRequirement already satisfied: numpy&gt;=1.11.3 in /usr/local/lib/python3.10/dist-packages (from bokeh==2.4.3) (1.23.5)\nRequirement already satisfied: packaging&gt;=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh==2.4.3) (23.2)\nRequirement already satisfied: pillow&gt;=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh==2.4.3) (9.4.0)\nRequirement already satisfied: PyYAML&gt;=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh==2.4.3) (6.0.1)\nRequirement already satisfied: tornado&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from bokeh==2.4.3) (6.3.2)\nRequirement already satisfied: typing-extensions&gt;=3.10.0 in /usr/local/lib/python3.10/dist-packages (from bokeh==2.4.3) (4.5.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2&gt;=2.9-&gt;bokeh==2.4.3) (2.1.3)\nInstalling collected packages: bokeh\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npanel 1.2.3 requires bokeh&lt;3.3.0,&gt;=3.1.1, but you have bokeh 2.4.3 which is incompatible.\nSuccessfully installed bokeh-2.4.3\nCollecting finance-datareader\n  Downloading finance_datareader-0.9.50-py3-none-any.whl (19 kB)\nRequirement already satisfied: pandas&gt;=0.19.2 in /usr/local/lib/python3.10/dist-packages (from finance-datareader) (1.5.3)\nRequirement already satisfied: requests&gt;=2.3.0 in /usr/local/lib/python3.10/dist-packages (from finance-datareader) (2.31.0)\nCollecting requests-file (from finance-datareader)\n  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from finance-datareader) (4.9.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from finance-datareader) (4.66.1)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=0.19.2-&gt;finance-datareader) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=0.19.2-&gt;finance-datareader) (2023.3.post1)\nRequirement already satisfied: numpy&gt;=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=0.19.2-&gt;finance-datareader) (1.23.5)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.3.0-&gt;finance-datareader) (3.3.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.3.0-&gt;finance-datareader) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.3.0-&gt;finance-datareader) (2.0.6)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.3.0-&gt;finance-datareader) (2023.7.22)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from requests-file-&gt;finance-datareader) (1.16.0)\nInstalling collected packages: requests-file, finance-datareader\nSuccessfully installed finance-datareader-0.9.50 requests-file-1.5.1\n\n\nUnable to display output for mime type(s): application/vnd.colab-display-data+json\n\n\n\nimport bokeh\nprint(bokeh.__version__)\n\n3.2.2\n\n\n\nimport FinanceDataReader as fdr\nimport numpy as np\n\n\n데이터 로드 및 준비\n- 데이터 상세정보\n\n# KT(030200) 전체 (1999-05-31 ~ 현재)\ndf = fdr.DataReader('030200')\ndata = df.values[ : ,  : -1]\nprint(data.shape)\n\n# train, val, test 분리\ntrain, val, test = data[:-900], data[-900:-600], data[-600:]\nprint(train.shape, val.shape, test.shape)\n\n# normailze - 표준화\nnorm = {'std': train.std(axis=0), 'mean': train.mean(axis=0)}\ntrain = (train - norm['mean']) / norm['std']\nval = (val - norm['mean']) / norm['std']\ntest = (test - norm['mean']) / norm['std']\n\n(6000, 5)\n(5100, 5) (300, 5) (600, 5)\n\n\n- 잎에 데이터 10개의 흐름을 보고 다음날의 종가를 예측\n\nx_train = np.array([train[i:i+10] for i in range(len(train) - 10)])\ny_train = np.array([train[i+0, 3] for i in range(len(train) - 10)])\nprint(x_train.shape, y_train.shape)\n\nx_val = np.array([val[i:i+10] for i in range(len(val) - 10)])\ny_val = np.array([val[i+10, 3] for i in range(len(val) - 10)])\nprint(x_val.shape, y_val.shape)\n\nx_test = np.array([test[i:i+10] for i in range(len(test) - 10)])\ny_test = np.array([test[i+10, 3] for i in range(len(test) - 10)])\nprint(x_test.shape, y_test.shape)\n\n(5090, 10, 5) (5090,)\n(290, 10, 5) (290,)\n(590, 10, 5) (590,)\n\n\n\n\nVanilla RNN 모델\n\nimport tensorflow as tf\n\n1 모델 설계\n\nX = tf.keras.layers.Input(shape=[10, 5])\nH = tf.keras.layers.SimpleRNN(8)(X)\nY = tf.keras.layers.Dense(1)(H)\n\nmodel = tf.keras.models.Model(X, Y)\nmodel.compile(loss='mse')\n\n\nmodel.summary()\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 10, 5)]           0         \n                                                                 \n simple_rnn (SimpleRNN)      (None, 8)                 112       \n                                                                 \n dense_2 (Dense)             (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 121 (484.00 Byte)\nTrainable params: 121 (484.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n2 모델 학습\n\nmodel.fit(x_train, y_train, epochs = 10, validation_data = (x_val, y_val), verbose = 0)\n\n&lt;keras.src.callbacks.History at 0x7870c947c7f0&gt;\n\n\n3 예측\n\ny_pred = model.predict(x_test)\n\n19/19 [==============================] - 0s 5ms/step\n\n\n\nplt.figure(figsize = (12, 4))\nplt.plot(y_pred,label = r\"$\\hat y$\", alpha = 0.5)\nplt.plot(y_test ,label = r\"$y$\", alpha = 0.5)\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7870781f2d40&gt;"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-10-Extra 01. 언어지능 (1).html#이미지-분류-모델",
    "href": "posts/DX/05. 딥러닝/2023-10-10-Extra 01. 언어지능 (1).html#이미지-분류-모델",
    "title": "Extra 01. 언어지능",
    "section": "이미지 분류 모델",
    "text": "이미지 분류 모델\n\nimport tensorflow as tf\n\n# MNIST\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nprint(x_train.shape, y_train.shape)\n# (60000, 28, 28) (60000,)\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11490434/11490434 [==============================] - 0s 0us/step\n(60000, 28, 28) (60000,)\n\n\n\n모델 1. RNN\n\n# 모델 생성\nX = tf.keras.layers.Input(shape=[28, 28])\nH = tf.keras.layers.SimpleRNN(32)(X)\nY = tf.keras.layers.Dense(10, activation=\"softmax\")(H)\n\nmodel = tf.keras.Model(X, Y)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              metrics=\"accuracy\")\nmodel.summary()\n\nModel: \"model_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, 28, 28)]          0         \n                                                                 \n simple_rnn_1 (SimpleRNN)    (None, 32)                1952      \n                                                                 \n dense_3 (Dense)             (None, 10)                330       \n                                                                 \n=================================================================\nTotal params: 2282 (8.91 KB)\nTrainable params: 2282 (8.91 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n- 은닉층에서 가중치 개수\n\n32*((28+32)+1)\n\n1952\n\n\n\n\n모델 2. RNN(return_sqeuences = True)\n\nmany to many : ppt 60 pages\n\n\n# 모델 생성\nX = tf.keras.layers.Input(shape=[28, 28])\nH = tf.keras.layers.SimpleRNN(32, return_sequences = True)(X) ## 마지막 32번쩨가 아닌 전체 1~32번쨰까지  모두 꺼냄\nY = tf.keras.layers.Dense(10, activation=\"softmax\")(H)\n\nmodel = tf.keras.Model(X, Y)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              metrics=\"accuracy\")\nmodel.summary()\n\nModel: \"model_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_5 (InputLayer)        [(None, 28, 28)]          0         \n                                                                 \n simple_rnn_2 (SimpleRNN)    (None, 28, 32)            1952      \n                                                                 \n dense_4 (Dense)             (None, 28, 10)            330       \n                                                                 \n=================================================================\nTotal params: 2282 (8.91 KB)\nTrainable params: 2282 (8.91 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n# 학습\nmodel.fit(x_train, y_train, epochs=10,\n          batch_size=128, validation_split=0.2)\n\n# 평가\nmodel.evaluate(x_test, y_test)\n\nEpoch 1/10\n\n\nValueError: ignored\n\n\n- 학습시 에러가 나는 이유는 현재 many to many 형태이기 때문임. 따라서 다음과 같은 은닉층을 추가\n\nx_train.shape\n\n(60000, 28, 28)\n\n\n\n# 모델 생성\nX = tf.keras.layers.Input(shape=[28, 28])\nH = tf.keras.layers.SimpleRNN(32, return_sequences = True)(X) ## 마지막 28일 차가 아닌 전체 1~28일 을 모두 꺼냄\nH = tf.keras.layers.GlobalAveragePooling1D()(H) ## 꺼낸 28일차 데이터를 평균내줌\nY = tf.keras.layers.Dense(10, activation=\"softmax\")(H)\n\nmodel = tf.keras.Model(X, Y)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              metrics=\"accuracy\")\nmodel.summary()\n\nModel: \"model_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_6 (InputLayer)        [(None, 28, 28)]          0         \n                                                                 \n simple_rnn_3 (SimpleRNN)    (None, 28, 32)            1952      \n                                                                 \n global_average_pooling1d (  (None, 32)                0         \n GlobalAveragePooling1D)                                         \n                                                                 \n dense_5 (Dense)             (None, 10)                330       \n                                                                 \n=================================================================\nTotal params: 2282 (8.91 KB)\nTrainable params: 2282 (8.91 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n# 학습\nmodel.fit(x_train, y_train, epochs=10,\n          batch_size=128, validation_split=0.2)\n\n# 평가\nmodel.evaluate(x_test, y_test)\n\nEpoch 1/10\n375/375 [==============================] - 9s 20ms/step - loss: 1.7922 - accuracy: 0.4506 - val_loss: 1.4524 - val_accuracy: 0.5940\nEpoch 2/10\n375/375 [==============================] - 6s 17ms/step - loss: 1.3072 - accuracy: 0.6311 - val_loss: 1.1465 - val_accuracy: 0.6837\nEpoch 3/10\n375/375 [==============================] - 6s 15ms/step - loss: 1.0861 - accuracy: 0.6901 - val_loss: 0.9764 - val_accuracy: 0.7306\nEpoch 4/10\n375/375 [==============================] - 7s 18ms/step - loss: 0.9554 - accuracy: 0.7231 - val_loss: 0.8709 - val_accuracy: 0.7572\nEpoch 5/10\n375/375 [==============================] - 6s 16ms/step - loss: 0.8694 - accuracy: 0.7439 - val_loss: 0.8000 - val_accuracy: 0.7725\nEpoch 6/10\n375/375 [==============================] - 6s 15ms/step - loss: 0.8092 - accuracy: 0.7583 - val_loss: 0.7491 - val_accuracy: 0.7807\nEpoch 7/10\n375/375 [==============================] - 7s 19ms/step - loss: 0.7634 - accuracy: 0.7705 - val_loss: 0.7104 - val_accuracy: 0.7936\nEpoch 8/10\n375/375 [==============================] - 6s 15ms/step - loss: 0.7286 - accuracy: 0.7811 - val_loss: 0.6843 - val_accuracy: 0.7949\nEpoch 9/10\n375/375 [==============================] - 7s 18ms/step - loss: 0.7004 - accuracy: 0.7873 - val_loss: 0.6561 - val_accuracy: 0.8032\nEpoch 10/10\n375/375 [==============================] - 6s 16ms/step - loss: 0.6782 - accuracy: 0.7938 - val_loss: 0.6352 - val_accuracy: 0.8084\n313/313 [==============================] - 2s 6ms/step - loss: 0.6648 - accuracy: 0.7976\n\n\n[0.6647534370422363, 0.7975999712944031]\n\n\n- return_sequences = True\n\nsimple_rnn_6 (SimpleRNN)    (None, 28, 32)\n\n- return_sequences = False\n\nsimple_rnn_3 (SimpleRNN)    (None, 32)\n\n\n\n모델3. LSTM\n\n# 모델 생성\nX = tf.keras.layers.Input(shape=[28, 28])\nH = tf.keras.layers.LSTM(32)(X)\nY = tf.keras.layers.Dense(10, activation=\"softmax\")(H)\n\nmodel = tf.keras.Model(X, Y)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              metrics=\"accuracy\")\nmodel.summary()\n\nModel: \"model_6\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_7 (InputLayer)        [(None, 28, 28)]          0         \n                                                                 \n lstm (LSTM)                 (None, 32)                7808      \n                                                                 \n dense_6 (Dense)             (None, 10)                330       \n                                                                 \n=================================================================\nTotal params: 8138 (31.79 KB)\nTrainable params: 8138 (31.79 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n# 학습\nx_train = x_train / 255\nx_test = x_test / 255\n\nmodel.fit(x_train, y_train, epochs=10,\n          batch_size=128, validation_split=0.2)\n\nEpoch 1/10\n375/375 [==============================] - 5s 6ms/step - loss: 1.2481 - accuracy: 0.6010 - val_loss: 0.6712 - val_accuracy: 0.7987\nEpoch 2/10\n375/375 [==============================] - 2s 5ms/step - loss: 0.5351 - accuracy: 0.8384 - val_loss: 0.3981 - val_accuracy: 0.8808\nEpoch 3/10\n375/375 [==============================] - 2s 6ms/step - loss: 0.3515 - accuracy: 0.8941 - val_loss: 0.2863 - val_accuracy: 0.9165\nEpoch 4/10\n375/375 [==============================] - 2s 6ms/step - loss: 0.2719 - accuracy: 0.9174 - val_loss: 0.2208 - val_accuracy: 0.9338\nEpoch 5/10\n375/375 [==============================] - 2s 5ms/step - loss: 0.2232 - accuracy: 0.9331 - val_loss: 0.1885 - val_accuracy: 0.9453\nEpoch 6/10\n375/375 [==============================] - 3s 7ms/step - loss: 0.1875 - accuracy: 0.9447 - val_loss: 0.1703 - val_accuracy: 0.9510\nEpoch 7/10\n375/375 [==============================] - 2s 6ms/step - loss: 0.1632 - accuracy: 0.9518 - val_loss: 0.2068 - val_accuracy: 0.9398\nEpoch 8/10\n375/375 [==============================] - 3s 8ms/step - loss: 0.1455 - accuracy: 0.9561 - val_loss: 0.1381 - val_accuracy: 0.9596\nEpoch 9/10\n375/375 [==============================] - 4s 11ms/step - loss: 0.1310 - accuracy: 0.9616 - val_loss: 0.1301 - val_accuracy: 0.9607\nEpoch 10/10\n375/375 [==============================] - 2s 6ms/step - loss: 0.1178 - accuracy: 0.9651 - val_loss: 0.1160 - val_accuracy: 0.9662\n\n\n&lt;keras.src.callbacks.History at 0x7870280f2350&gt;\n\n\n\nmodel.evaluate(x_test, y_test)\n\n313/313 [==============================] - 1s 3ms/step - loss: 0.1152 - accuracy: 0.9651\n\n\n[0.11523875594139099, 0.9650999903678894]"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-10-Extra 01. 언어지능 (1).html#텍스트-분류",
    "href": "posts/DX/05. 딥러닝/2023-10-10-Extra 01. 언어지능 (1).html#텍스트-분류",
    "title": "Extra 01. 언어지능",
    "section": "텍스트 분류",
    "text": "텍스트 분류\n\n데이터 이해 및 정리\n- IMDB 영화 리뷰 데이터\n\nimport tensorflow as tf\nimport numpy as np\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()\n\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n\nx_train = tf.keras.utils.pad_sequences(x_train, maxlen=20)\nx_test = tf.keras.utils.pad_sequences(x_test, maxlen=20)\n\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n\n(25000,) (25000,)\n(25000,) (25000,)\n(25000, 20) (25000,)\n(25000, 20) (25000,)\n\n\n\nnp.unique(y_train, return_counts = True)\n\n(array([0, 1]), array([12500, 12500]))\n\n\n- 데이터는 이미 정수 인코딩 되어있음.\n\nx_train[0]\n\narray([  65,   16,   38, 1334,   88,   12,   16,  283,    5,   16, 4472,\n        113,  103,   32,   15,   16, 5345,   19,  178,   32], dtype=int32)\n\n\n- word2index \\(\\to\\) index2word\n\nword2index =  tf.keras.datasets.imdb.get_word_index() ## tensorflow에 내장된 단어 사전 이용\nindex2word = { i  : w  for w, i in  word2index.items()}\nindex2word[5]\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n1641221/1641221 [==============================] - 0s 0us/step\n\n\n'to'\n\n\n- 리뷰글 출력 (그냥 확인을 위해)\n\nwords = [index2word[i] for i in x_train[0]]\nprint(words) ## 단어 확인\nprint(\" \".join(words)) ## 단어를 공백 기준으로 붙임\n\n['their', 'with', 'her', 'nobody', 'most', 'that', 'with', \"wasn't\", 'to', 'with', 'armed', 'acting', 'watch', 'an', 'for', 'with', 'heartfelt', 'film', 'want', 'an']\ntheir with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\n\n\n\n\n모델링 1. SimpleRNN + Embedding\n\nimport tensorflow as tf\n\nX = tf.keras.Input(shape = [20])\nH = tf.keras.layers.Embedding(88585, 28)(X) ## 88,584개의 단어인데 1개 더 추가한 이유는 패딩에서 비어있는 값을 0으로 표현하기 때문\nH = tf.keras.layers.SimpleRNN(32)(H)\nY = tf.keras.layers.Dense(1, activation=\"sigmoid\")(H)\n\nmodel = tf.keras.Model(X, Y)\nmodel.compile(loss=\"binary_crossentropy\", metrics=\"accuracy\")\n\nmodel.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 20)]              0         \n                                                                 \n embedding (Embedding)       (None, 20, 28)            2480380   \n                                                                 \n simple_rnn (SimpleRNN)      (None, 32)                1952      \n                                                                 \n dense (Dense)               (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 2482365 (9.47 MB)\nTrainable params: 2482365 (9.47 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n- 잠깐 임베딩 파라미터 계산\n\n88585*28 # 아걍 고유 단어 수 x 임베딩 차원 수\n\n2480380\n\n\n- RNN 파라미터 계산\n\n시계열 데이터라고 생각하면 20일치 데이터를 28개의 차원으로 전달하는 것임\n\n\n32*((28+32)+1)\n\n1952\n\n\n\nh1 = model.fit(x_train, y_train, epochs = 10,  validation_split = 0.2)\n\nEpoch 1/10\n625/625 [==============================] - 30s 38ms/step - loss: 0.5854 - accuracy: 0.6722 - val_loss: 0.5137 - val_accuracy: 0.7438\nEpoch 2/10\n625/625 [==============================] - 18s 29ms/step - loss: 0.4192 - accuracy: 0.8105 - val_loss: 0.5079 - val_accuracy: 0.7494\nEpoch 3/10\n625/625 [==============================] - 13s 21ms/step - loss: 0.3187 - accuracy: 0.8665 - val_loss: 0.5537 - val_accuracy: 0.7392\nEpoch 4/10\n625/625 [==============================] - 11s 18ms/step - loss: 0.2163 - accuracy: 0.9176 - val_loss: 0.6442 - val_accuracy: 0.7280\nEpoch 5/10\n625/625 [==============================] - 13s 21ms/step - loss: 0.1323 - accuracy: 0.9513 - val_loss: 0.8045 - val_accuracy: 0.7280\nEpoch 6/10\n625/625 [==============================] - 15s 23ms/step - loss: 0.0766 - accuracy: 0.9726 - val_loss: 0.9050 - val_accuracy: 0.7040\nEpoch 7/10\n625/625 [==============================] - 12s 20ms/step - loss: 0.0431 - accuracy: 0.9858 - val_loss: 1.1525 - val_accuracy: 0.6764\nEpoch 8/10\n625/625 [==============================] - 12s 20ms/step - loss: 0.0217 - accuracy: 0.9934 - val_loss: 1.3006 - val_accuracy: 0.6826\nEpoch 9/10\n625/625 [==============================] - 12s 19ms/step - loss: 0.0103 - accuracy: 0.9969 - val_loss: 1.4636 - val_accuracy: 0.6828\nEpoch 10/10\n625/625 [==============================] - 11s 18ms/step - loss: 0.0064 - accuracy: 0.9982 - val_loss: 1.7729 - val_accuracy: 0.6620\n\n\n\n\n\n잡담. batchNormalization\n\n(1) 데이터 이해\n\nimport tensorflow as tf\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n29515/29515 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26421880/26421880 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n5148/5148 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4422102/4422102 [==============================] - 0s 0us/step\n\n\n\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n\n(60000, 28, 28) (60000,)\n(10000, 28, 28) (10000,)\n\n\n- 각각의 이미지 데이터가 인코딩 되어있음.\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize = (4,4))\nplt.imshow(x_train[0], cmap = \"gray\")\nplt.show()\nprint(y_train[0])\n\n\n\n\n9\n\n\n\n\n(2) 모델학습\n\nimport tensorflow as tf\n\nX = tf.keras.Input(shape = [28, 28])\nH = tf.keras.layers.Flatten()(X)\nH = tf.keras.layers.Dense(64, activation = \"swish\")(H)\nH = tf.keras.layers.Dense(32, activation = \"swish\")(H)\nY = tf.keras.layers.Dense(10, activation = \"softmax\")(H)\n\nmodel = tf.keras.Model(X, Y)\nmodel.compile(loss = tf.keras.losses.sparse_categorical_crossentropy, metrics = \"accuracy\")\nmodel.summary()\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 28, 28)]          0         \n                                                                 \n flatten_1 (Flatten)         (None, 784)               0         \n                                                                 \n dense_4 (Dense)             (None, 64)                50240     \n                                                                 \n dense_5 (Dense)             (None, 32)                2080      \n                                                                 \n dense_6 (Dense)             (None, 10)                330       \n                                                                 \n=================================================================\nTotal params: 52650 (205.66 KB)\nTrainable params: 52650 (205.66 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nmodel.fit(x_train, y_train, validation_split = 0.2, epochs = 10)\n\nEpoch 1/10\n1500/1500 [==============================] - 9s 4ms/step - loss: 2.4305 - accuracy: 0.4121 - val_loss: 1.4045 - val_accuracy: 0.4848\nEpoch 2/10\n1500/1500 [==============================] - 6s 4ms/step - loss: 1.3273 - accuracy: 0.5452 - val_loss: 1.0820 - val_accuracy: 0.5848\nEpoch 3/10\n1500/1500 [==============================] - 5s 3ms/step - loss: 1.0889 - accuracy: 0.6066 - val_loss: 1.0017 - val_accuracy: 0.6120\nEpoch 4/10\n1500/1500 [==============================] - 5s 3ms/step - loss: 1.0031 - accuracy: 0.6505 - val_loss: 1.0787 - val_accuracy: 0.5946\nEpoch 5/10\n1500/1500 [==============================] - 6s 4ms/step - loss: 0.9876 - accuracy: 0.6540 - val_loss: 0.9953 - val_accuracy: 0.6647\nEpoch 6/10\n1500/1500 [==============================] - 5s 4ms/step - loss: 0.9904 - accuracy: 0.6639 - val_loss: 0.9663 - val_accuracy: 0.6608\nEpoch 7/10\n1500/1500 [==============================] - 5s 3ms/step - loss: 1.0046 - accuracy: 0.6620 - val_loss: 0.9973 - val_accuracy: 0.6436\nEpoch 8/10\n1500/1500 [==============================] - 6s 4ms/step - loss: 1.0068 - accuracy: 0.6617 - val_loss: 0.9530 - val_accuracy: 0.6842\nEpoch 9/10\n1500/1500 [==============================] - 5s 3ms/step - loss: 1.0003 - accuracy: 0.6595 - val_loss: 1.0946 - val_accuracy: 0.6669\nEpoch 10/10\n1500/1500 [==============================] - 6s 4ms/step - loss: 0.9964 - accuracy: 0.6655 - val_loss: 1.0111 - val_accuracy: 0.6254\n\n\n&lt;keras.src.callbacks.History at 0x7e1db80a81c0&gt;\n\n\n\nh1 = model.history.history\n\n\npred1 = model.predict(x_test).argmax(axis = 1)\n\n313/313 [==============================] - 1s 4ms/step\n\n\n\nimport pandas as pd\nimport seaborn as sns\nclass_table = pd.DataFrame({\"y_true\" : y_test, \"y_pred\" : pred1}).melt(var_name = \"y_?\", value_name = \"label\")\nsns.countplot(x= class_table[\"label\"], hue = class_table[\"y_?\"])\nplt.show()\n\n\n\n\n- 학습이 잘안된다..\n\npred2 = model.predict(x_train).argmax(axis = 1)\n\nclass_table2 = pd.DataFrame({\"y_train\" : y_train, \"y_pred\" : pred2}).melt(var_name = \"y_?\", value_name = \"label\")\nsns.countplot(x= class_table2[\"label\"], hue = class_table2[\"y_?\"])\nplt.show()\n\n1875/1875 [==============================] - 4s 2ms/step\n\n\n\n\n\n- 그렇다고 오버피팅 된것도 아님\n- 일단 train data에 대한 학습이 더 필요함, 근데 깊이가 깊어질 수록 학습하는게 당연히 어려워짐\n\n이를 위해서 Nomalize layer을 추가 \\(\\to\\) 요약하자면 학습이 잘 되도록 도와줌\n\n\n\n(3) +batchNormalization\n\nimport tensorflow as tf\n\nX = tf.keras.Input(shape = [28, 28])\nH = tf.keras.layers.Flatten()(X)\nH = tf.keras.layers.Dense(64, activation = \"swish\")(H)\nH = tf.keras.layers.BatchNormalization()(H) ##\nH = tf.keras.layers.Dense(32, activation = \"swish\")(H)\nY = tf.keras.layers.Dense(10, activation = \"softmax\")(H)\n\nmodel = tf.keras.Model(X, Y)\nmodel.compile(loss = tf.keras.losses.sparse_categorical_crossentropy, metrics = \"accuracy\")\nmodel.summary()\n\nModel: \"model_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_5 (InputLayer)        [(None, 28, 28)]          0         \n                                                                 \n flatten_3 (Flatten)         (None, 784)               0         \n                                                                 \n dense_10 (Dense)            (None, 64)                50240     \n                                                                 \n batch_normalization_1 (Bat  (None, 64)                256       \n chNormalization)                                                \n                                                                 \n dense_11 (Dense)            (None, 32)                2080      \n                                                                 \n dense_12 (Dense)            (None, 10)                330       \n                                                                 \n=================================================================\nTotal params: 52906 (206.66 KB)\nTrainable params: 52778 (206.16 KB)\nNon-trainable params: 128 (512.00 Byte)\n_________________________________________________________________\n\n\n\nmodel.fit(x_train, y_train, validation_split = 0.2, epochs = 10)\n\nEpoch 1/10\n1500/1500 [==============================] - 13s 8ms/step - loss: 0.5162 - accuracy: 0.8203 - val_loss: 0.5449 - val_accuracy: 0.8217\nEpoch 2/10\n1500/1500 [==============================] - 8s 5ms/step - loss: 0.4035 - accuracy: 0.8566 - val_loss: 0.3995 - val_accuracy: 0.8611\nEpoch 3/10\n1500/1500 [==============================] - 8s 5ms/step - loss: 0.3682 - accuracy: 0.8661 - val_loss: 0.4471 - val_accuracy: 0.8572\nEpoch 4/10\n1500/1500 [==============================] - 10s 7ms/step - loss: 0.3505 - accuracy: 0.8719 - val_loss: 0.4714 - val_accuracy: 0.8605\nEpoch 5/10\n1500/1500 [==============================] - 7s 5ms/step - loss: 0.3383 - accuracy: 0.8780 - val_loss: 0.3993 - val_accuracy: 0.8587\nEpoch 6/10\n1500/1500 [==============================] - 11s 7ms/step - loss: 0.3239 - accuracy: 0.8827 - val_loss: 0.5136 - val_accuracy: 0.8654\nEpoch 7/10\n1500/1500 [==============================] - 8s 6ms/step - loss: 0.3153 - accuracy: 0.8842 - val_loss: 0.4233 - val_accuracy: 0.8583\nEpoch 8/10\n1500/1500 [==============================] - 9s 6ms/step - loss: 0.3085 - accuracy: 0.8889 - val_loss: 0.4091 - val_accuracy: 0.8754\nEpoch 9/10\n1500/1500 [==============================] - 10s 7ms/step - loss: 0.3017 - accuracy: 0.8897 - val_loss: 0.4520 - val_accuracy: 0.8712\nEpoch 10/10\n1500/1500 [==============================] - 9s 6ms/step - loss: 0.2932 - accuracy: 0.8944 - val_loss: 0.4376 - val_accuracy: 0.8745\n\n\n&lt;keras.src.callbacks.History at 0x7e1db8075c60&gt;\n\n\n\nh2 = model.history.history\n\n- loss가 확실히 줄어들었음을 확인\n\nplt.figure(figsize = (12,4))\nplt.plot(h1[\"loss\"], label = \"train_loss\")\nplt.plot(h1[\"val_loss\"],label = \"val_loss\")\nplt.plot(h2[\"loss\"],label = \"+batch_train_loss\")\nplt.plot(h2[\"val_loss\"],label = \"+batch_val_loss\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7e1db652fdf0&gt;\n\n\n\n\n\n\npred3 = model.predict(x_test).argmax(axis=1)\n\n313/313 [==============================] - 2s 5ms/step\n\n\n- 오 확실히 아까보다 예측성능이 훨씬 낫다!!\n\nclass_table3 = pd.DataFrame({\"y_train\" : y_test, \"y_pred\" : pred3}).melt(var_name = \"y_?\", value_name = \"label\")\nsns.countplot(x= class_table3[\"label\"], hue = class_table3[\"y_?\"])\nplt.show()\n\n\n\n\n\n\n(4) summary\n- 딥러닝 모델에서 학습이 안되는 이유\n\n입력값에 대한 weight값이 랜덤하게 잡힘\n각각의 은닉층에서도 랜덤하게 잡힘\n이런식이면 은닉층이 깊어질 수록 scale이 각 은닉층에서 계속 깨질수 밖에 없음\n당연히 scale이 깨지니 학습이 안됨\n\n- 그래서 은닉층에 입력전에 스케일을 맞춰주기 위해서 사용하는 것이 배치 정규화임\n\nKNN처럼 각 컬럼에 스케일을 맞춘다고 생각하자.\n이제 앞으로 은닉층과 은닉층 사이에 Nomalize layer을 무조건 추가하자…\n심지어 정규화 레이어를 여러 개 쌓을수 도 있음\n\nimport tensorflow as tf\n\nX = tf.keras.Input(shape = [28, 28])\nH = tf.keras.layers.Flatten()(X)\nH = tf.keras.layers.Dense(64, activation = \"swish\")(H)\nH = tf.keras.layers.BatchNormalization()(H) ## 첫 번째 정규레이어\nH = tf.keras.layers.Dense(32, activation = \"swish\")(H)\nH = tf.keras.layers.BatchNormalization()(H) ## 두 번째 정규레이어\nY = tf.keras.layers.Dense(10, activation = \"softmax\")(H)\n\nmodel = tf.keras.Model(X, Y)\nmodel.compile(loss = tf.keras.losses.sparse_categorical_crossentropy, metrics = \"accuracy\")\nmodel.summary()\n\n\n\n모델링 2. Transformer\n\nimport tensorflow as tf\nimport numpy as np\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()\n\nx_train = tf.keras.utils.pad_sequences(x_train, maxlen=20)\nx_test = tf.keras.utils.pad_sequences(x_test, maxlen=20)\n\n\nimport tensorflow as tf\n\nX = tf.keras.Input(shape=[20])\n\nH = tf.keras.layers.Embedding(88585, 28)(X)\nH = tf.keras.layers.SimpleRNN(32, return_sequences=True)(H)\n\n# Transformer::self-attentions\nH1 = tf.keras.layers.MultiHeadAttention(2, 16)(H, H)\nH = tf.keras.layers.BatchNormalization()(H + H1)\n\n# Transformer::feed-forward\nH1 = tf.keras.layers.Dense(32, activation='swish')(H)\nH = tf.keras.layers.BatchNormalization()(H + H1)\n\nH = tf.keras.layers.GlobalAveragePooling1D()(H)\nY = tf.keras.layers.Dense(1, activation=\"sigmoid\")(H)\n\nmodel = tf.keras.Model(X, Y)\nmodel.compile(loss=\"binary_crossentropy\", metrics=\"accuracy\")\n\nmodel.summary()\n\nModel: \"model_6\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_7 (InputLayer)        [(None, 20)]                 0         []                            \n                                                                                                  \n embedding_2 (Embedding)     (None, 20, 28)               2480380   ['input_7[0][0]']             \n                                                                                                  \n simple_rnn_2 (SimpleRNN)    (None, 20, 32)               1952      ['embedding_2[0][0]']         \n                                                                                                  \n multi_head_attention_1 (Mu  (None, 20, 32)               4224      ['simple_rnn_2[0][0]',        \n ltiHeadAttention)                                                   'simple_rnn_2[0][0]']        \n                                                                                                  \n tf.__operators__.add_2 (TF  (None, 20, 32)               0         ['simple_rnn_2[0][0]',        \n OpLambda)                                                           'multi_head_attention_1[0][0]\n                                                                    ']                            \n                                                                                                  \n batch_normalization_4 (Bat  (None, 20, 32)               128       ['tf.__operators__.add_2[0][0]\n chNormalization)                                                   ']                            \n                                                                                                  \n dense_15 (Dense)            (None, 20, 32)               1056      ['batch_normalization_4[0][0]'\n                                                                    ]                             \n                                                                                                  \n tf.__operators__.add_3 (TF  (None, 20, 32)               0         ['batch_normalization_4[0][0]'\n OpLambda)                                                          , 'dense_15[0][0]']           \n                                                                                                  \n batch_normalization_5 (Bat  (None, 20, 32)               128       ['tf.__operators__.add_3[0][0]\n chNormalization)                                                   ']                            \n                                                                                                  \n global_average_pooling1d_1  (None, 32)                   0         ['batch_normalization_5[0][0]'\n  (GlobalAveragePooling1D)                                          ]                             \n                                                                                                  \n dense_16 (Dense)            (None, 1)                    33        ['global_average_pooling1d_1[0\n                                                                    ][0]']                        \n                                                                                                  \n==================================================================================================\nTotal params: 2487901 (9.49 MB)\nTrainable params: 2487773 (9.49 MB)\nNon-trainable params: 128 (512.00 Byte)\n__________________________________________________________________________________________________\n\n\n\nmodel.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n\nEpoch 1/10\n157/157 [==============================] - 40s 232ms/step - loss: 0.5601 - accuracy: 0.7021 - val_loss: 0.6164 - val_accuracy: 0.7194\nEpoch 2/10\n157/157 [==============================] - 8s 51ms/step - loss: 0.4087 - accuracy: 0.8120 - val_loss: 0.5232 - val_accuracy: 0.7470\nEpoch 3/10\n157/157 [==============================] - 8s 49ms/step - loss: 0.3237 - accuracy: 0.8620 - val_loss: 0.5408 - val_accuracy: 0.7218\nEpoch 4/10\n157/157 [==============================] - 5s 33ms/step - loss: 0.2433 - accuracy: 0.9029 - val_loss: 0.5935 - val_accuracy: 0.7378\nEpoch 5/10\n157/157 [==============================] - 7s 47ms/step - loss: 0.1615 - accuracy: 0.9401 - val_loss: 0.7722 - val_accuracy: 0.7094\nEpoch 6/10\n157/157 [==============================] - 5s 33ms/step - loss: 0.0965 - accuracy: 0.9655 - val_loss: 1.1340 - val_accuracy: 0.6932\nEpoch 7/10\n157/157 [==============================] - 6s 38ms/step - loss: 0.0556 - accuracy: 0.9818 - val_loss: 3.5060 - val_accuracy: 0.5690\nEpoch 8/10\n157/157 [==============================] - 6s 39ms/step - loss: 0.0289 - accuracy: 0.9914 - val_loss: 1.7923 - val_accuracy: 0.6898\nEpoch 9/10\n157/157 [==============================] - 4s 28ms/step - loss: 0.0169 - accuracy: 0.9949 - val_loss: 2.3478 - val_accuracy: 0.6732\nEpoch 10/10\n157/157 [==============================] - 7s 43ms/step - loss: 0.0144 - accuracy: 0.9956 - val_loss: 1.9380 - val_accuracy: 0.7124\n\n\n&lt;keras.src.callbacks.History at 0x7e1db802c1c0&gt;\n\n\n\npred1 = model.predict(x_test).argmax(axis = 1)\npred2 = model.predict(x_train).argmax(axis = 1)\n\n782/782 [==============================] - 6s 7ms/step\n782/782 [==============================] - 4s 5ms/step\n\n\n\npred1.shape, pred2.shape\n\n((25000,), (25000,))\n\n\n\nimport pandas as pd\nimport seaborn as sns\n\nclass_table = pd.DataFrame({\"y_true\" : y_test, \"y_pred\" : pred1}).melt(var_name = \"y_?\", value_name = \"label\")\nclass_table2 = pd.DataFrame({\"y_train\" : y_train, \"y_pred\" : pred2}).melt(var_name = \"y_?\", value_name = \"label\")\n\nfig, axes = plt.subplots(1,2, figsize = (12,4))\nax1, ax2 = axes\n\nsns.countplot(x = class_table[\"label\"], hue = class_table[\"y_?\"],ax = ax1)\nsns.countplot(x = class_table2[\"label\"], hue = class_table2[\"y_?\"], ax = ax2)\n\nplt.show()\n\n\n\n\n- 딱히 잘 적합된 것 같지 않음..\n\n\n잡담. 텍스트 전처리\n\n(1) 코랩 한글 이용\n- 라이브러리 및 폰트 설치\n\n!pip install konlpy\n!sudo apt-get install -y fonts-nanum\n!sudo fc-cache -fv\n!rm ~/.cache/matplotlib -rf\n\nCollecting konlpy\n  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.4/19.4 MB 42.2 MB/s eta 0:00:00\nCollecting JPype1&gt;=0.7.0 (from konlpy)\n  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 465.3/465.3 kB 45.9 MB/s eta 0:00:00\nRequirement already satisfied: lxml&gt;=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\nRequirement already satisfied: numpy&gt;=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1&gt;=0.7.0-&gt;konlpy) (23.2)\nInstalling collected packages: JPype1, konlpy\nSuccessfully installed JPype1-1.4.1 konlpy-0.6.0\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  fonts-nanum\n0 upgraded, 1 newly installed, 0 to remove and 18 not upgraded.\nNeed to get 10.3 MB of archives.\nAfter this operation, 34.1 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-nanum all 20200506-1 [10.3 MB]\nFetched 10.3 MB in 2s (5,802 kB/s)\ndebconf: unable to initialize frontend: Dialog\ndebconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, &lt;&gt; line 1.)\ndebconf: falling back to frontend: Readline\ndebconf: unable to initialize frontend: Readline\ndebconf: (This frontend requires a controlling tty.)\ndebconf: falling back to frontend: Teletype\ndpkg-preconfigure: unable to re-open stdin: \nSelecting previously unselected package fonts-nanum.\n(Reading database ... 120875 files and directories currently installed.)\nPreparing to unpack .../fonts-nanum_20200506-1_all.deb ...\nUnpacking fonts-nanum (20200506-1) ...\nSetting up fonts-nanum (20200506-1) ...\nProcessing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n/root/.local/share/fonts: skipping, no such directory\n/root/.fonts: skipping, no such directory\n/usr/share/fonts/truetype: skipping, looped directory detected\n/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n/var/cache/fontconfig: cleaning cache directory\n/root/.cache/fontconfig: not cleaning non-existent cache directory\n/root/.fontconfig: not cleaning non-existent cache directory\nfc-cache: succeeded\n\n\n\n!ls /usr/share/fonts/truetype/nanum/\n\nNanumBarunGothicBold.ttf   NanumGothicCoding.ttf  NanumSquareB.ttf\nNanumBarunGothic.ttf       NanumGothic.ttf    NanumSquareRoundB.ttf\nNanumGothicBold.ttf    NanumMyeongjoBold.ttf  NanumSquareRoundR.ttf\nNanumGothicCodingBold.ttf  NanumMyeongjo.ttf      NanumSquareR.ttf\n\n\n- 이후 런타임 다시 시작\n\nimport nltk\n\n\n\n(2) 토큰화\n\n# 말뭉치 다운로드\nnltk.download('punkt')\n# 텍스트 토큰화 (Tokenization)\n\ntext1 = \"\"\"Natural language processing (NLP)\nis a field of computer science, artificial\nintelligence, and computational linguistics\nconcerned with the interactions between\ncomputers and human (natural) languages.\"\"\"\n\ntokens1 = nltk.tokenize.word_tokenize(text1)\nprint(tokens)\n\n['자연어', '처리', '(', 'Natural', 'Language', '\\n', 'Processing', ',', 'NLP', ')', '늘다', '인간', '의', '언어', '현상', '을', '\\n', '컴퓨터', '와', '같다', '기계', '를', '이용', '하다', '모사', '하다', '수', '있다', '\\n', '하다', '인공', '지능', '의', '하위', '분야', '중', '하나', '이다', '.']\n\n\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\ntext2 = \"\"\"자연어 처리(Natural Language\nProcessing, NLP)는 인간의 언어 현상을\n컴퓨터와 같은 기계를 이용하여 모사할 수 있도록\n하는 인공 지능의 하위 분야 중 하나입니다.\"\"\"\n\n\nfrom konlpy.tag import Okt\n# KoNLPy에서 Okt 형태소 분석기를 사용\n\nokt = Okt()\n\n# 텍스트 토큰화 (Tokenization)\ntokens2 = okt.morphs(text2, stem=True)\nprint(tokens)\n\n['자연어', '처리', '(', 'Natural', 'Language', '\\n', 'Processing', ',', 'NLP', ')', '늘다', '인간', '의', '언어', '현상', '을', '\\n', '컴퓨터', '와', '같다', '기계', '를', '이용', '하다', '모사', '하다', '수', '있다', '\\n', '하다', '인공', '지능', '의', '하위', '분야', '중', '하나', '이다', '.']\n\n\n\n\n(3) 데이터 전처리\n\n# 정제 (Cleaning) 및 정규화 (Normalization) : 모든 단어 소문자 변환, 어간추출 ,원형 복원\ncleaned_tokens1 = [token.lower() for token in tokens1 if token.isalnum()]\nprint(cleaned_tokens1)\n\ncleaned_tokens2 = [token for token in tokens2 if token.isalnum()]\nprint(cleaned_tokens2)\n\n['natural', 'language', 'processing', 'nlp', 'is', 'a', 'field', 'of', 'computer', 'science', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'natural', 'languages']\n['자연어', '처리', 'Natural', 'Language', 'Processing', 'NLP', '늘다', '인간', '의', '언어', '현상', '을', '컴퓨터', '와', '같다', '기계', '를', '이용', '하다', '모사', '하다', '수', '있다', '하다', '인공', '지능', '의', '하위', '분야', '중', '하나', '이다']\n\n\n\n# 어간 추출 (Stemming)\nstemmer1 = nltk.stem.PorterStemmer()\nstemmed_tokens1 = [stemmer1.stem(token) for token in cleaned_tokens1]\nprint(stemmed_tokens1)\n\n# 표제어 추출 (Lemmatization)\nnltk.download('wordnet')\nlemmatizer1 = nltk.stem.WordNetLemmatizer()\nlemmatized_tokens1 = [lemmatizer1.lemmatize(token) for token in cleaned_tokens1]\nprint(lemmatizer1)\n\n['natur', 'languag', 'process', 'nlp', 'is', 'a', 'field', 'of', 'comput', 'scienc', 'artifici', 'intellig', 'and', 'comput', 'linguist', 'concern', 'with', 'the', 'interact', 'between', 'comput', 'and', 'human', 'natur', 'languag']\n&lt;WordNetLemmatizer&gt;\n\n\n[nltk_data] Downloading package wordnet to /root/nltk_data...\n\n\n\n# 불용어 (Stopword) 제거\n\nnltk.download('stopwords')\n\nstop_words1 = set(nltk.corpus.stopwords.words('english'))\nfiltered_tokens1 = [token for token in lemmatized_tokens1 if token not in stop_words1]\nprint(filtered_tokens1)\n\nstop_words2 = set([\"은\", \"는\", \"이\", \"가\", \"을\", \"를\"])\nfiltered_tokens2 = [token for token in cleaned_tokens2 if token not in stop_words2]\nprint(filtered_tokens2)\n\n['natural', 'language', 'processing', 'nlp', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concerned', 'interaction', 'computer', 'human', 'natural', 'language']\n['자연어', '처리', 'Natural', 'Language', 'Processing', 'NLP', '늘다', '인간', '의', '언어', '현상', '컴퓨터', '와', '같다', '기계', '이용', '하다', '모사', '하다', '수', '있다', '하다', '인공', '지능', '의', '하위', '분야', '중', '하나', '이다']\n\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n\n\n\n\n(4) 워드클라우드 생성\n\nfrom wordcloud import WordCloud\n\n# 워드클라우드 생성\nwordcloud = WordCloud(width=800, height=400, background_color='white')\n\n# 폰트 변경\nwordcloud.font_path = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\nwordcloud.generate(' '.join(filtered_tokens1))\nwordcloud.to_file(\"wordcloud1.png\")\n\nwordcloud.generate(' '.join(filtered_tokens2))\nwordcloud.to_file(\"wordcloud2.png\")\n\n&lt;wordcloud.wordcloud.WordCloud at 0x7ce6ca897460&gt;\n\n\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n# Colab 의 한글 폰트 설정\nplt.rc('font', family='NanumBarunGothic')\nmpl.rc('axes', unicode_minus=False)\n# 워드클라우드 출력\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()\n\n\n\n\n1 텍스트자체의전처리\n\n토큰화(Tokenization)\n정제(Cleaning)and정규화(Normalization)\n어간추출(Stemming)and원형복원(Lemmatization)\n불용어(Stopword)\n\n2 학습을위한전처리\n\n라벨인코딩&원핫인코딩\n시퀀스패딩\n워드임베딩(WordEmbedding)\n학습/검증/테스트데이터분리"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html",
    "title": "02. 딥러닝 (3)",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\n\nimport plotly.express as px\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\ntnp.experimental_enable_numpy_behavior()\n\n\n\ntf.config.experimental.list_physical_devices(\"GPU\")\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#데이터-이해-및-정리",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#데이터-이해-및-정리",
    "title": "02. 딥러닝 (3)",
    "section": "(1) 데이터 이해 및 정리",
    "text": "(1) 데이터 이해 및 정리\n- 해당 데이터는 iris 붓꽃데이터이다.\n\npath = \"https://raw.githubusercontent.com/DA4BAM/dataset/master/iris.csv\"\ndata = pd.read_csv(path)\ndata.head()\n\n\n  \n    \n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- target변수를 정수 인코딩\n\ndata['Species'] = data['Species'].map({'setosa':0, 'versicolor':1, 'virginica':2})\ndata.head()\n\n\n  \n    \n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- x, y분리\n\ntarget = \"Species\"\nx = data.drop(target, axis = 1)\ny = data[target]\n\n- 데이터셋 분할\n\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size = .3, random_state = 20)\n\n- scaling\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nx_train = scaler.fit_transform(x_train)\nx_val = scaler.transform(x_val)"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#모델링-1-은닉층-x",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#모델링-1-은닉층-x",
    "title": "02. 딥러닝 (3)",
    "section": "(2) 모델링 1 : 은닉층 x",
    "text": "(2) 모델링 1 : 은닉층 x\n\nnf = x_train.shape[1]\n\n\nmodel = Sequential()\n\nmodel.add(Dense(3, input_shape = (nf,), activation = \"softmax\"))\n\n## 정수 인코딩 - &gt; loss : sparse_categorical_crossentropy\n## 원핫 인코딩 - &gt; loss : categorical_crossentropy\nmodel.compile(optimizer = Adam(0.1), loss  = tf.keras.losses.sparse_categorical_crossentropy)\n\nhistory = model.fit(x_train, y_train,\n                                  validation_split = 0.2, epochs = 50, verbose = 0).history\n\n- train, val loss 시각화\n\nplt.figure(figsize = (12, 4))\nplt.plot(history[\"loss\"], \"--.\", label = \"train_loss\", alpha = 0.3)\nplt.plot(history[\"val_loss\"], \"--.\", label = \"val_loss\",alpha = 0.5)\nplt.legend()\nplt.show()\n\n\n\n\n- 예측\n\npred =  model.predict(x_val)\npred[:5]\n\nWARNING:tensorflow:5 out of the last 13 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7fb0e84ad6c0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\n2/2 [==============================] - 0s 5ms/step\n\n\narray([[9.79680717e-01, 2.02296004e-02, 8.96496858e-05],\n       [1.11060282e-02, 7.97759116e-01, 1.91134900e-01],\n       [1.30379135e-02, 7.17990279e-01, 2.68971771e-01],\n       [4.22110228e-04, 3.48259479e-01, 6.51318491e-01],\n       [3.11934222e-02, 8.82108092e-01, 8.66985321e-02]], dtype=float32)\n\n\n\ny_pred = pred.argmax(axis = 1)"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#예측결과-report",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#예측결과-report",
    "title": "02. 딥러닝 (3)",
    "section": "(3) 예측결과 report",
    "text": "(3) 예측결과 report\n\nprint(confusion_matrix(y_val, y_pred))\nprint(\"\\n\")\nprint(classification_report(y_val, y_pred))\n\n[[13  0  0]\n [ 0 17  1]\n [ 0  4 10]]\n\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        13\n           1       0.81      0.94      0.87        18\n           2       0.91      0.71      0.80        14\n\n    accuracy                           0.89        45\n   macro avg       0.91      0.89      0.89        45\nweighted avg       0.90      0.89      0.89        45"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#import-losses",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#import-losses",
    "title": "02. 딥러닝 (3)",
    "section": "(1) import losses",
    "text": "(1) import losses\n\nfrom keras import losses"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#모델-설계",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#모델-설계",
    "title": "02. 딥러닝 (3)",
    "section": "(2) 모델 설계",
    "text": "(2) 모델 설계\n\nmodel = Sequential()\nmodel.add(Dense(8, input_shape= (nf,), activation = \"relu\"))\nmodel.add(Dense(3, activation = \"softmax\"))\nmodel.compile(optimizer = Adam(0.01), loss = losses.sparse_categorical_crossentropy)\n\nhistoty = model.fit(x_train, y_train, epochs = 50,\n                                validation_split = 0.2, verbose = 0).history\n\n\nmodel.summary()\n\nModel: \"sequential_9\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_12 (Dense)            (None, 8)                 40        \n                                                                 \n dense_13 (Dense)            (None, 3)                 27        \n                                                                 \n=================================================================\nTotal params: 67 (268.00 Byte)\nTrainable params: 67 (268.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#train-val-loss-시각화",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#train-val-loss-시각화",
    "title": "02. 딥러닝 (3)",
    "section": "(3) train, val loss 시각화",
    "text": "(3) train, val loss 시각화\n\ndef loss_v(history) :\n      plt.figure(figsize = (12, 4))\n      plt.plot(history[\"loss\"], \"--.\", label = \"train_loss\", alpha = 0.3)\n      plt.plot(history[\"val_loss\"], \"--.\", label = \"val_loss\",alpha = 0.5)\n      plt.legend()\n      plt.show()\n\n\nloss_v(history)"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#예측",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#예측",
    "title": "02. 딥러닝 (3)",
    "section": "(4) 예측",
    "text": "(4) 예측\n\npred = model.predict(x_val).argmax(axis =1)\n\nWARNING:tensorflow:5 out of the last 11 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7fb0e84aec20&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\n2/2 [==============================] - 0s 5ms/step\n\n\n\nprint(confusion_matrix(y_val, pred))\nprint(classification_report(y_val, pred))\n\n[[13  0  0]\n [ 0 18  0]\n [ 0  3 11]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        13\n           1       0.86      1.00      0.92        18\n           2       1.00      0.79      0.88        14\n\n    accuracy                           0.93        45\n   macro avg       0.95      0.93      0.93        45\nweighted avg       0.94      0.93      0.93        45"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#모델-설계-1",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#모델-설계-1",
    "title": "02. 딥러닝 (3)",
    "section": "(1) 모델 설계",
    "text": "(1) 모델 설계\n\nmodel = Sequential()\nmodel.add(Dense(8, input_shape= (nf,), activation = \"relu\"))\nmodel.add(Dense(8, activation = \"relu\"))\nmodel.add(Dense(3, activation = \"softmax\"))\nmodel.compile(optimizer = Adam(0.01), loss = losses.sparse_categorical_crossentropy)\n\nhistoty = model.fit(x_train, y_train, epochs = 50,\n                                validation_split = 0.2, verbose = 0).history\n\n\nmodel.summary()\n\nModel: \"sequential_10\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_14 (Dense)            (None, 8)                 40        \n                                                                 \n dense_15 (Dense)            (None, 8)                 72        \n                                                                 \n dense_16 (Dense)            (None, 3)                 27        \n                                                                 \n=================================================================\nTotal params: 139 (556.00 Byte)\nTrainable params: 139 (556.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#train-val-loss-확인",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#train-val-loss-확인",
    "title": "02. 딥러닝 (3)",
    "section": "(2) train, val loss 확인",
    "text": "(2) train, val loss 확인\n\nloss_v(history)"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#예측-1",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#예측-1",
    "title": "02. 딥러닝 (3)",
    "section": "(3) 예측",
    "text": "(3) 예측\n\npred = model.predict(x_val).argmax(axis = 1)\n\nprint(confusion_matrix(y_val, pred))\nprint(classification_report(y_val, pred))\n\n2/2 [==============================] - 0s 4ms/step\n[[13  0  0]\n [ 0 18  0]\n [ 0  2 12]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        13\n           1       0.90      1.00      0.95        18\n           2       1.00      0.86      0.92        14\n\n    accuracy                           0.96        45\n   macro avg       0.97      0.95      0.96        45\nweighted avg       0.96      0.96      0.96        45\n\n\n\n- 결과를 보니 은닉층 1개를 추가할 때와 비교했을 떄 성능 차이가 없음\n\n모델을 충분히 복잡하게 하지 않아도 될 것 같다!\n\n\n# 다중 분류 4. 원핫 인코딩\n- 이전까지는 원핫 인코딩이 아닌 각 타겟들의 고유 범주를 정수 인코딩으로 변환했음\n\n이번엔 원핫 인코딩을 이용하여 모델을 fit하는 과정을 끄적끄적~~"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#import-1",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#import-1",
    "title": "02. 딥러닝 (3)",
    "section": "(1) import",
    "text": "(1) import\n\nfrom keras.utils import to_categorical\n\n\ny_c = to_categorical(y.values, 3)\n\n\ny_c[:5]\n\narray([[1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.]], dtype=float32)"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#데이터-전처리",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#데이터-전처리",
    "title": "02. 딥러닝 (3)",
    "section": "(2) 데이터 전처리",
    "text": "(2) 데이터 전처리\n- 데이터셋 분할\n\nx_train, x_val, y_train, y_val = train_test_split(x, y_c, test_size = .3, random_state = 2022)\n\n- 스케일링\n\nscaler = MinMaxScaler()\nx_train = scaler.fit_transform(x_train)\nx_val = scaler.transform(x_val)"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#모델-설계-2",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#모델-설계-2",
    "title": "02. 딥러닝 (3)",
    "section": "(3) 모델 설계",
    "text": "(3) 모델 설계\n- 모델 설계\n\nmodel = Sequential([Dense(3, input_shape = (nf,), activation = 'softmax')])\n\nmodel.compile(optimizer=Adam(learning_rate=0.1), loss='categorical_crossentropy')\n\nhistory = model.fit(x_train, y_train, epochs = 100,\n                    validation_split=0.2,verbose=0).history"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#loss-시각화",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#loss-시각화",
    "title": "02. 딥러닝 (3)",
    "section": "(4) loss 시각화",
    "text": "(4) loss 시각화\n\nloss_v(history)"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#예측-2",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#예측-2",
    "title": "02. 딥러닝 (3)",
    "section": "(5) 예측",
    "text": "(5) 예측\n\npred = model.predict(x_val).argmax(axis=1)\npred[:5]\n\n2/2 [==============================] - 0s 5ms/step\n\n\narray([2, 2, 0, 2, 0])\n\n\n- y_val 도 원래대로 돌려놈 (decoding)\n\ny_val_1 = y_val.argmax(axis=1)\ny_val_1\n\narray([2, 2, 0, 2, 0, 0, 1, 1, 0, 1, 1, 2, 1, 2, 2, 0, 1, 2, 2, 1, 0, 0,\n       2, 0, 2, 2, 2, 0, 1, 2, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 2,\n       2])\n\n\n- 결과 report 작성\n\nprint(confusion_matrix(y_val_1, pred))\nprint(classification_report(y_val_1, pred))\n\n[[14  0  0]\n [ 0 14  1]\n [ 0  2 14]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        14\n           1       0.88      0.93      0.90        15\n           2       0.93      0.88      0.90        16\n\n    accuracy                           0.93        45\n   macro avg       0.94      0.94      0.94        45\nweighted avg       0.93      0.93      0.93        45"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#데이터-이해-및-정리-1",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#데이터-이해-및-정리-1",
    "title": "02. 딥러닝 (3)",
    "section": "(1) 데이터 이해 및 정리",
    "text": "(1) 데이터 이해 및 정리\n\npath = \"https://raw.githubusercontent.com/DA4BAM/dataset/master/winequality-white.csv\"\ndata = pd.read_csv(path)\ndata['quality'] = np.where(data['quality'] == 3, 4, np.where(data['quality'] == 9, 8, data['quality']))\ndata['quality'] = data['quality'] - 4\ndata.head()\n\n\n  \n    \n\n\n\n\n\n\nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\n0\n7.0\n0.27\n0.36\n20.7\n0.045\n45.0\n170.0\n1.0010\n3.00\n0.45\n8.8\n2\n\n\n1\n6.3\n0.30\n0.34\n1.6\n0.049\n14.0\n132.0\n0.9940\n3.30\n0.49\n9.5\n2\n\n\n2\n8.1\n0.28\n0.40\n6.9\n0.050\n30.0\n97.0\n0.9951\n3.26\n0.44\n10.1\n2\n\n\n3\n7.2\n0.23\n0.32\n8.5\n0.058\n47.0\n186.0\n0.9956\n3.19\n0.40\n9.9\n2\n\n\n4\n7.2\n0.23\n0.32\n8.5\n0.058\n47.0\n186.0\n0.9956\n3.19\n0.40\n9.9\n2\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4898 entries, 0 to 4897\nData columns (total 12 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   fixed acidity         4898 non-null   float64\n 1   volatile acidity      4898 non-null   float64\n 2   citric acid           4898 non-null   float64\n 3   residual sugar        4898 non-null   float64\n 4   chlorides             4898 non-null   float64\n 5   free sulfur dioxide   4898 non-null   float64\n 6   total sulfur dioxide  4898 non-null   float64\n 7   density               4898 non-null   float64\n 8   pH                    4898 non-null   float64\n 9   sulphates             4898 non-null   float64\n 10  alcohol               4898 non-null   float64\n 11  quality               4898 non-null   int64  \ndtypes: float64(11), int64(1)\nmemory usage: 459.3 KB\n\n\n- x, y 분리\n\ntarget = 'quality'\nx = data.drop(target, axis = 1)\ny = data.loc[:, target]\n\n\ny.value_counts()\n\n2    2198\n1    1457\n3     880\n0     183\n4     180\nName: quality, dtype: int64\n\n\n- 데이터셋 분할\n\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size= .3, random_state = 20)\n\n- 스케일링\n\nscaler = MinMaxScaler()\nx_train = scaler.fit_transform(x_train)\nx_val = scaler.transform(x_val)"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#모델링-3개의-모델을-생성-후-비교",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#모델링-3개의-모델을-생성-후-비교",
    "title": "02. 딥러닝 (3)",
    "section": "(2) 모델링 : 3개의 모델을 생성 후 비교",
    "text": "(2) 모델링 : 3개의 모델을 생성 후 비교\n\n\n\n\noutput_shape\nactivation\n\n\n\n\nmodel1\n(8, 4, 5)\n(relu, relu, softmax)\n\n\nmodel2\n(16, 8, 4, 5)\n(relu, relu, relu, softmax)\n\n\nmodel3\n(32, 16, 8, 4, 5)\n(relu, relu, relu, relu, softmax)\n\n\n\n\n1) 모델 설계\n\nnf = x_train.shape[1]\nnf\n\n11\n\n\n\n# model1 설계\nmodel1 = Sequential()\nmodel1.add(Dense(8, input_shape = (nf,), activation = \"relu\"))\nmodel1.add(Dense(4, activation = \"relu\"))\nmodel1.add(Dense(5, activation = \"softmax\"))\nmodel1.compile(optimizer = Adam(0.01), loss = losses.sparse_categorical_crossentropy)\n\n# model2 설계\nmodel2 = Sequential()\nmodel2.add(Dense(16, input_shape = (nf,), activation = \"relu\"))\nmodel2.add(Dense(8, activation = \"relu\"))\nmodel2.add(Dense(4, activation = \"relu\"))\nmodel2.add(Dense(5, activation = \"softmax\"))\nmodel2.compile(optimizer = Adam(0.01), loss = losses.sparse_categorical_crossentropy)\n\n# model3 설계\nmodel3 = Sequential()\nmodel3.add(Dense(32, input_shape = (nf,), activation = \"relu\"))\nmodel3.add(Dense(16, input_shape = (nf,), activation = \"relu\"))\nmodel3.add(Dense(8, activation = \"relu\"))\nmodel3.add(Dense(4, activation = \"relu\"))\nmodel3.add(Dense(5, activation = \"softmax\"))\nmodel3.compile(optimizer = Adam(0.01), loss = losses.sparse_categorical_crossentropy)\n\n\n\n2) 모델 학습\n\nfor i in range(1, 4) :\n       exec(f\"\"\"hisory{i}=model{i}.fit(x_train, y_train, epochs = 50,\n                                                            verbose = 0, validation_split = 0.2).history\"\"\")\n\n\n\n3) 모델별 train, val loss 시각화\n\nfig, axes = plt.subplots(1,3, figsize = (12 ,4))\n\nfor i in range(3) :\n      exec(f\"axes[{i}].plot(hisory{i+1}['loss'],label = 'train_loss')\")\n      exec(f\"axes[{i}].plot(hisory{i+1}['val_loss'], label = 'val_loss')\")\n      axes[i].legend()\n      axes[i].set_ylim(0.9,1.4)\nfig.tight_layout()\nfig.show()\n\n\n\n\n- val_loss를 살펴본 결과 model3이 가장 적합한 모델로 판정됨\n- 그런데…val_loss가 너무 들쑥날쑥하다….\n\n학습률을 낮추고 에포크를 늘려보자.\n\n\nmodel3 = Sequential()\nmodel3.add(Dense(32, input_shape = (nf,), activation = \"relu\"))\nmodel3.add(Dense(16, input_shape = (nf,), activation = \"relu\"))\nmodel3.add(Dense(8, activation = \"relu\"))\nmodel3.add(Dense(4, activation = \"relu\"))\nmodel3.add(Dense(5, activation = \"softmax\"))\nmodel3.compile(optimizer = Adam(0.001), loss = losses.sparse_categorical_crossentropy)\n\nhisory_final =model3.fit(x_train, y_train, epochs = 100,\n                                                            verbose = 0, validation_split = 0.2).history\n\n\nloss_v(hisory_final)\n\n\n\n\n- 전보다 안정된 loss양상을 보인다\n\n\n4) 선택된 모델의 예측 report 작성\n\npred = model3.predict(x_val).argmax(axis=1)\n\n46/46 [==============================] - 0s 2ms/step\n\n\n\nprint(confusion_matrix(y_val, pred))\nprint(classification_report(y_val, pred, zero_division = 0))\n\n[[  0  43  18   0   0]\n [  0 233 198   7   0]\n [  0 114 479  67   0]\n [  0   6 157  87   0]\n [  0   0  30  31   0]]\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        61\n           1       0.59      0.53      0.56       438\n           2       0.54      0.73      0.62       660\n           3       0.45      0.35      0.39       250\n           4       0.00      0.00      0.00        61\n\n    accuracy                           0.54      1470\n   macro avg       0.32      0.32      0.31      1470\nweighted avg       0.50      0.54      0.51      1470\n\n\n\n- 결과가 쓰레기다.\n- 이유 : 현재 0과 4는 아예 못맞춤…학습 데이터의 불균형이 문제인 것 같다.\n\ny_train.value_counts()\n\n2    1538\n1    1019\n3     630\n0     122\n4     119\nName: quality, dtype: int64\n\n\n\ny_val.value_counts()\n\n2    660\n1    438\n3    250\n4     61\n0     61\nName: quality, dtype: int64\n\n\n- 변수를 더 준비하면 모델 성능이 더 늘어날려나?\n\n그런 문제가 아닌 것 같다.."
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#데이터-이해-및-준비",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#데이터-이해-및-준비",
    "title": "02. 딥러닝 (3)",
    "section": "(1) 데이터 이해 및 준비",
    "text": "(1) 데이터 이해 및 준비\n\nfrom keras.datasets import mnist, fashion_mnist\n\n\n# 케라스 데이터셋으로 부터 mnist 불러오기\n(x_train, y_train), (x_val, y_val) = mnist.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11490434/11490434 [==============================] - 1s 0us/step\n\n\n- 이미지 한개당 28 x 28의 픽셀 차원을 가진다..\n\nx_train.shape\n\n(60000, 28, 28)\n\n\n- 숫자가 0인것만 시각화\n\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n\nfig, axes = plt.subplots(1,5, figsize = (12,4))\nfor i in range(5) :\n      im = axes[i].imshow(x_train[y_train ==0][i],cmap=plt.cm.binary)\n\n\n\n\n- 데이터 flatten \\(\\to\\) 28 x 28 = 784\n\nx_train = x_train.reshape(60000, -1)\nx_val = x_val.reshape(10000, -1)\n\n\nx_train.shape, x_val.shape\n\n((60000, 784), (10000, 784))\n\n\n- 이미지 데이터는 한 픽셀에서 0~255 의 숫자를 가진다.\n\n즉, 스케일링 시 굳이 뭐 스케일러를 쓸 필요없이 255로 나누면 끝난다.\n\n- 스케일링 수행\n\nx_train = x_train / 255.\nx_test = x_val / 255.\n\n\nnp.unique(y_train, return_counts=True)\n\n(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n array([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]))\n\n\n\nnp.unique(y_val, return_counts=True)\n\n(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009]))"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#모델링",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#모델링",
    "title": "02. 딥러닝 (3)",
    "section": "(2) 모델링",
    "text": "(2) 모델링\n\nnf = x_train.shape[1]\nnf\n\n784\n\n\n\nmodel = Sequential()\nmodel.add(Dense(10, input_shape = (nf,), activation = \"softmax\"))\nmodel.summary()\n\nModel: \"sequential_25\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_64 (Dense)            (None, 10)                7850      \n                                                                 \n=================================================================\nTotal params: 7850 (30.66 KB)\nTrainable params: 7850 (30.66 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nmodel.compile(optimizer = Adam(0.001), loss = losses.sparse_categorical_crossentropy)\nh = model.fit(x_train, y_train, epochs = 20, validation_split = 0.2, verbose = 0).history\n\n\nloss_v(h)"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#예측-3",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#예측-3",
    "title": "02. 딥러닝 (3)",
    "section": "(3) 예측",
    "text": "(3) 예측\n\npred = model.predict(x_test).argmax(axis = 1)\n\n313/313 [==============================] - 1s 2ms/step\n\n\n\nprint(confusion_matrix(y_val, pred))\nprint(classification_report(y_val, pred))\n\n[[ 954    0    2    2    0    7   12    2    1    0]\n [   0 1118    3    1    0    1    4    2    6    0]\n [   4   13  926   14    4    4   15    8   40    4]\n [   3    0   15  932    0   21    2   11   18    8]\n [   1    3   10    1  899    0   13    4    9   42]\n [   7    2    4   37    6  770   19    6   33    8]\n [   9    3    9    1    6    7  920    1    2    0]\n [   1    9   20    7    4    1    0  945    2   39]\n [   7   10    7   23    7   24   11    9  865   11]\n [  10    8    1   11   15    4    0   17    5  938]]\n              precision    recall  f1-score   support\n\n           0       0.96      0.97      0.97       980\n           1       0.96      0.99      0.97      1135\n           2       0.93      0.90      0.91      1032\n           3       0.91      0.92      0.91      1010\n           4       0.96      0.92      0.93       982\n           5       0.92      0.86      0.89       892\n           6       0.92      0.96      0.94       958\n           7       0.94      0.92      0.93      1028\n           8       0.88      0.89      0.88       974\n           9       0.89      0.93      0.91      1009\n\n    accuracy                           0.93     10000\n   macro avg       0.93      0.93      0.93     10000\nweighted avg       0.93      0.93      0.93     10000"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#layer-정의",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#layer-정의",
    "title": "02. 딥러닝 (3)",
    "section": "(1) Layer 정의",
    "text": "(1) Layer 정의\n\nd1   = [Dense(20, input_shape = (nf,), activation = \"relu\"), Dense(10, activation = \"softmax\")]\nd2   = [Dense(20, input_shape = (nf,), activation = \"relu\"),Dense(80, activation = \"relu\"), Dense(10, activation = \"softmax\")]"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#model-학습",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#model-학습",
    "title": "02. 딥러닝 (3)",
    "section": "(2) model 학습",
    "text": "(2) model 학습\n\nfor i in range(1,3) :\n    exec(f\"model{i} =  Sequential(d{i})\")\n    exec(f\"model{i}.compile(optimizer = Adam(0.001),loss = losses.sparse_categorical_crossentropy)\")\n    exec(f\"h{i}= model{i}.fit(x_train,y_train, validation_split = 0.2, verbose = 0, epochs = 20).history\")"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#model-확인",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#model-확인",
    "title": "02. 딥러닝 (3)",
    "section": "(3) model 확인",
    "text": "(3) model 확인\n\nmodel1.summary()\n\nModel: \"sequential_31\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_75 (Dense)            (None, 20)                15700     \n                                                                 \n dense_76 (Dense)            (None, 10)                210       \n                                                                 \n=================================================================\nTotal params: 15910 (62.15 KB)\nTrainable params: 15910 (62.15 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nmodel2.summary()\n\nModel: \"sequential_32\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_77 (Dense)            (None, 20)                15700     \n                                                                 \n dense_78 (Dense)            (None, 80)                1680      \n                                                                 \n dense_79 (Dense)            (None, 10)                810       \n                                                                 \n=================================================================\nTotal params: 18190 (71.05 KB)\nTrainable params: 18190 (71.05 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#train-val-loss-비교",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#train-val-loss-비교",
    "title": "02. 딥러닝 (3)",
    "section": "(4) train, val loss 비교",
    "text": "(4) train, val loss 비교\n\nfig, axes = plt.subplots(1,2, figsize=(12, 4))\n\nfor i in range(2) :\n        exec(f\"axes[{i}].plot(h{i+1}['loss'], '--.', label = 'loss', alpha = 0.5)\")\n        exec(f'axes[{i}].plot(h{i+1}[\"val_loss\"], \"--.\", label = \"val_loss\", alpha = 0.5)')\n        exec(f'axes[{i}].legend()')\n        exec(f'axes[{i}].set_ylim(0,0.5)')"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#예측성능-확인",
    "href": "posts/DX/05. 딥러닝/2023-10-05-02. 딥러닝 (3).html#예측성능-확인",
    "title": "02. 딥러닝 (3)",
    "section": "(5) 예측성능 확인",
    "text": "(5) 예측성능 확인\n\np1 = model1.predict(x_test).argmax(axis = 1)\np2 = model2.predict(x_test).argmax(axis = 1)\n\nprint(classification_report(y_val, p1))\nprint(classification_report(y_val, p2))\n\n313/313 [==============================] - 0s 1ms/step\n313/313 [==============================] - 1s 2ms/step\n              precision    recall  f1-score   support\n\n           0       0.97      0.98      0.98       980\n           1       0.98      0.99      0.98      1135\n           2       0.97      0.94      0.96      1032\n           3       0.94      0.96      0.95      1010\n           4       0.96      0.95      0.95       982\n           5       0.95      0.95      0.95       892\n           6       0.96      0.97      0.97       958\n           7       0.97      0.95      0.96      1028\n           8       0.94      0.95      0.95       974\n           9       0.95      0.94      0.94      1009\n\n    accuracy                           0.96     10000\n   macro avg       0.96      0.96      0.96     10000\nweighted avg       0.96      0.96      0.96     10000\n\n              precision    recall  f1-score   support\n\n           0       0.97      0.98      0.98       980\n           1       0.98      0.99      0.98      1135\n           2       0.97      0.96      0.96      1032\n           3       0.95      0.96      0.96      1010\n           4       0.98      0.95      0.97       982\n           5       0.97      0.95      0.96       892\n           6       0.97      0.98      0.98       958\n           7       0.96      0.97      0.97      1028\n           8       0.97      0.94      0.96       974\n           9       0.94      0.97      0.95      1009\n\n    accuracy                           0.97     10000\n   macro avg       0.97      0.97      0.97     10000\nweighted avg       0.97      0.97      0.97     10000"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-03-00. 딥러닝 (1).html",
    "href": "posts/DX/05. 딥러닝/2023-10-03-00. 딥러닝 (1).html",
    "title": "00. 딥러닝 (1)",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.backend import clear_session\n\nimport plotly.express as px"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-03-00. 딥러닝 (1).html#전처리",
    "href": "posts/DX/05. 딥러닝/2023-10-03-00. 딥러닝 (1).html#전처리",
    "title": "00. 딥러닝 (1)",
    "section": "(1) 전처리",
    "text": "(1) 전처리\n\nscaler = MinMaxScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.fit_transform(x_test)"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-03-00. 딥러닝 (1).html#import-1",
    "href": "posts/DX/05. 딥러닝/2023-10-03-00. 딥러닝 (1).html#import-1",
    "title": "00. 딥러닝 (1)",
    "section": "(2) import",
    "text": "(2) import\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.backend import clear_session"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-03-00. 딥러닝 (1).html#모델-선언",
    "href": "posts/DX/05. 딥러닝/2023-10-03-00. 딥러닝 (1).html#모델-선언",
    "title": "00. 딥러닝 (1)",
    "section": "(3) 모델 선언",
    "text": "(3) 모델 선언\n\nn_features = x_train.shape[1] ## 컬럼 개수\n\n\n# 메모리 정리\nclear_session()\n\n# Sequential 타입 모델 선언\nmodel = Sequential(Dense(1, input_shape = (n_features,)))\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 4 (16.00 Byte)\nTrainable params: 4 (16.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n- 초기 셋팅된 파라미터 값을 살펴보자.\n\n일단 아래와 같은 값으로 기본 셋팅이 되어있다…\n\n\nmodel.weights\n\n[&lt;tf.Variable 'dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n array([[ 0.21143973],\n        [-0.36714107],\n        [ 1.0211834 ]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;]\n\n\n- 일단 초기셋팅한값으로 test data를 적합시 얼만큼 잘 예측하는지 시각화 해보자..\n\nimport tensorflow.experimental.numpy as tnp\ntnp.experimental_enable_numpy_behavior() ## tnf 를 numpy 처럼 사용할 수 있도록 해줌\n\n- 일단 \\(X\\)를 우리가 잘 알고 있는 회귀 분석 형태로 바꿔주고 \\(y_{init}\\)를 예측\n\\[\\bf \\hat {y}_{init} = X \\bf{\\hat {\\beta}_{init}}\\]\n\nw_init = np.concatenate([tnp.array(model.weights[1]).reshape(1,1),\n                         tnp.array(model.weights[0])],axis = 0)\n\nx_test_matrix = np.concatenate([np.ones(x_test.shape[0]).reshape(-1,1),\n                                 x_test],axis = 1)\n\ny_pred_init = x_test_matrix @ w_init\n\n\ny_pred_init = x_test_matrix @ w_init\n\n- 초기값이라 그런지 잘 예측하는 것 같지는 않음\n\n\nCode\nplt.figure(figsize = (4,4))\nplt.plot(y_test,label = r\"$(y_{test})$\",alpha = 0.5)\nplt.plot(y_pred_init,label = r\"$(\\hat {y}_{init})$\",alpha = 0.6)\nplt.title(r\"($y_{test}, \\hat {y}_{init}$)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n- 이제 keras에서 제공하는 compile과 fit을 이용하여 모델 적합후 비교해보자\n\n## compile\nmodel.compile(optimizer = \"adam\", loss = \"mse\")\n\n## fit\nmodel.fit(x_train, y_train)\n\n5/5 [==============================] - 6s 5ms/step - loss: 219.7330\n\n\n&lt;keras.src.callbacks.History at 0x7ddedaf4cac0&gt;\n\n\n- 예측값 저장 (기본 epoch = 5 이므로 y_pred_5라고 저장)\n\ny_pred_5 = model.predict(x_test)\n\n2/2 [==============================] - 0s 9ms/step\n\n\n\n#model.predict?\n\n- 여전히 성능이 쓰레기임\n\n\nCode\nplt.figure(figsize = (4,4))\nplt.plot(y_test,label = r\"$(y_{test})$\",alpha = 0.5)\nplt.plot(y_pred_init,label = r\"$(\\hat {y}_{init})$\",alpha = 0.6)\nplt.plot(y_pred_5,label = r\"$(\\hat {y}_{5})$\",alpha = 0.6)\nplt.title(r\"($y_{test}, \\hat {y}_{init}, \\hat {y}_{5}$)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n- epochs를 늘려서 다시 예측해보자\n\nmodel.fit?\n\n\nmodel.compile?\n\n\n## compile\nmodel.compile(optimizer = \"adam\", loss = \"mse\")\n\n## fit\nmodel.fit(x_train, y_train, epochs = 1000)\n\ny_pred_final = model.predict(x_test,verbose = 0)\n\nEpoch 1/1000\n5/5 [==============================] - 1s 6ms/step - loss: 109.4172\nEpoch 2/1000\n5/5 [==============================] - 0s 5ms/step - loss: 109.1868\nEpoch 3/1000\n5/5 [==============================] - 0s 5ms/step - loss: 108.9626\nEpoch 4/1000\n5/5 [==============================] - 0s 5ms/step - loss: 108.7299\nEpoch 5/1000\n5/5 [==============================] - 0s 5ms/step - loss: 108.4981\nEpoch 6/1000\n5/5 [==============================] - 0s 5ms/step - loss: 108.2746\nEpoch 7/1000\n5/5 [==============================] - 0s 5ms/step - loss: 108.0486\nEpoch 8/1000\n5/5 [==============================] - 0s 7ms/step - loss: 107.8185\nEpoch 9/1000\n5/5 [==============================] - 0s 7ms/step - loss: 107.6013\nEpoch 10/1000\n5/5 [==============================] - 0s 6ms/step - loss: 107.3712\nEpoch 11/1000\n5/5 [==============================] - 0s 5ms/step - loss: 107.1475\nEpoch 12/1000\n5/5 [==============================] - 0s 5ms/step - loss: 106.9191\nEpoch 13/1000\n5/5 [==============================] - 0s 5ms/step - loss: 106.6953\nEpoch 14/1000\n5/5 [==============================] - 0s 4ms/step - loss: 106.4616\nEpoch 15/1000\n5/5 [==============================] - 0s 5ms/step - loss: 106.2446\nEpoch 16/1000\n5/5 [==============================] - 0s 7ms/step - loss: 106.0105\nEpoch 17/1000\n5/5 [==============================] - 0s 6ms/step - loss: 105.7884\nEpoch 18/1000\n5/5 [==============================] - 0s 5ms/step - loss: 105.5647\nEpoch 19/1000\n5/5 [==============================] - 0s 5ms/step - loss: 105.3465\nEpoch 20/1000\n5/5 [==============================] - 0s 5ms/step - loss: 105.1217\nEpoch 21/1000\n5/5 [==============================] - 0s 5ms/step - loss: 104.8943\nEpoch 22/1000\n5/5 [==============================] - 0s 6ms/step - loss: 104.6774\nEpoch 23/1000\n5/5 [==============================] - 0s 7ms/step - loss: 104.4532\nEpoch 24/1000\n5/5 [==============================] - 0s 5ms/step - loss: 104.2285\nEpoch 25/1000\n5/5 [==============================] - 0s 6ms/step - loss: 104.0009\nEpoch 26/1000\n5/5 [==============================] - 0s 8ms/step - loss: 103.7801\nEpoch 27/1000\n5/5 [==============================] - 0s 5ms/step - loss: 103.5532\nEpoch 28/1000\n5/5 [==============================] - 0s 5ms/step - loss: 103.3343\nEpoch 29/1000\n5/5 [==============================] - 0s 5ms/step - loss: 103.1174\nEpoch 30/1000\n5/5 [==============================] - 0s 5ms/step - loss: 102.8944\nEpoch 31/1000\n5/5 [==============================] - 0s 4ms/step - loss: 102.6777\nEpoch 32/1000\n5/5 [==============================] - 0s 4ms/step - loss: 102.4613\nEpoch 33/1000\n5/5 [==============================] - 0s 6ms/step - loss: 102.2449\nEpoch 34/1000\n5/5 [==============================] - 0s 7ms/step - loss: 102.0379\nEpoch 35/1000\n5/5 [==============================] - 0s 6ms/step - loss: 101.8212\nEpoch 36/1000\n5/5 [==============================] - 0s 8ms/step - loss: 101.6037\nEpoch 37/1000\n5/5 [==============================] - 0s 6ms/step - loss: 101.4005\nEpoch 38/1000\n5/5 [==============================] - 0s 6ms/step - loss: 101.1897\nEpoch 39/1000\n5/5 [==============================] - 0s 5ms/step - loss: 100.9803\nEpoch 40/1000\n5/5 [==============================] - 0s 7ms/step - loss: 100.7659\nEpoch 41/1000\n5/5 [==============================] - 0s 7ms/step - loss: 100.5528\nEpoch 42/1000\n5/5 [==============================] - 0s 7ms/step - loss: 100.3409\nEpoch 43/1000\n5/5 [==============================] - 0s 6ms/step - loss: 100.1284\nEpoch 44/1000\n5/5 [==============================] - 0s 7ms/step - loss: 99.9098\nEpoch 45/1000\n5/5 [==============================] - 0s 7ms/step - loss: 99.7002\nEpoch 46/1000\n5/5 [==============================] - 0s 7ms/step - loss: 99.4929\nEpoch 47/1000\n5/5 [==============================] - 0s 6ms/step - loss: 99.2754\nEpoch 48/1000\n5/5 [==============================] - 0s 6ms/step - loss: 99.0601\nEpoch 49/1000\n5/5 [==============================] - 0s 7ms/step - loss: 98.8559\nEpoch 50/1000\n5/5 [==============================] - 0s 7ms/step - loss: 98.6416\nEpoch 51/1000\n5/5 [==============================] - 0s 8ms/step - loss: 98.4370\nEpoch 52/1000\n5/5 [==============================] - 0s 6ms/step - loss: 98.2309\nEpoch 53/1000\n5/5 [==============================] - 0s 6ms/step - loss: 98.0138\nEpoch 54/1000\n5/5 [==============================] - 0s 6ms/step - loss: 97.8093\nEpoch 55/1000\n5/5 [==============================] - 0s 6ms/step - loss: 97.6044\nEpoch 56/1000\n5/5 [==============================] - 0s 6ms/step - loss: 97.3957\nEpoch 57/1000\n5/5 [==============================] - 0s 6ms/step - loss: 97.1901\nEpoch 58/1000\n5/5 [==============================] - 0s 4ms/step - loss: 96.9841\nEpoch 59/1000\n5/5 [==============================] - 0s 5ms/step - loss: 96.7849\nEpoch 60/1000\n5/5 [==============================] - 0s 6ms/step - loss: 96.5910\nEpoch 61/1000\n5/5 [==============================] - 0s 7ms/step - loss: 96.3754\nEpoch 62/1000\n5/5 [==============================] - 0s 5ms/step - loss: 96.1752\nEpoch 63/1000\n5/5 [==============================] - 0s 7ms/step - loss: 95.9621\nEpoch 64/1000\n5/5 [==============================] - 0s 6ms/step - loss: 95.7588\nEpoch 65/1000\n5/5 [==============================] - 0s 6ms/step - loss: 95.5547\nEpoch 66/1000\n5/5 [==============================] - 0s 7ms/step - loss: 95.3460\nEpoch 67/1000\n5/5 [==============================] - 0s 6ms/step - loss: 95.1421\nEpoch 68/1000\n5/5 [==============================] - 0s 6ms/step - loss: 94.9393\nEpoch 69/1000\n5/5 [==============================] - 0s 8ms/step - loss: 94.7284\nEpoch 70/1000\n5/5 [==============================] - 0s 6ms/step - loss: 94.5297\nEpoch 71/1000\n5/5 [==============================] - 0s 7ms/step - loss: 94.3307\nEpoch 72/1000\n5/5 [==============================] - 0s 8ms/step - loss: 94.1278\nEpoch 73/1000\n5/5 [==============================] - 0s 7ms/step - loss: 93.9289\nEpoch 74/1000\n5/5 [==============================] - 0s 6ms/step - loss: 93.7272\nEpoch 75/1000\n5/5 [==============================] - 0s 7ms/step - loss: 93.5285\nEpoch 76/1000\n5/5 [==============================] - 0s 6ms/step - loss: 93.3294\nEpoch 77/1000\n5/5 [==============================] - 0s 7ms/step - loss: 93.1322\nEpoch 78/1000\n5/5 [==============================] - 0s 7ms/step - loss: 92.9389\nEpoch 79/1000\n5/5 [==============================] - 0s 6ms/step - loss: 92.7353\nEpoch 80/1000\n5/5 [==============================] - 0s 8ms/step - loss: 92.5323\nEpoch 81/1000\n5/5 [==============================] - 0s 11ms/step - loss: 92.3321\nEpoch 82/1000\n5/5 [==============================] - 0s 7ms/step - loss: 92.1339\nEpoch 83/1000\n5/5 [==============================] - 0s 8ms/step - loss: 91.9349\nEpoch 84/1000\n5/5 [==============================] - 0s 6ms/step - loss: 91.7386\nEpoch 85/1000\n5/5 [==============================] - 0s 9ms/step - loss: 91.5411\nEpoch 86/1000\n5/5 [==============================] - 0s 6ms/step - loss: 91.3418\nEpoch 87/1000\n5/5 [==============================] - 0s 5ms/step - loss: 91.1438\nEpoch 88/1000\n5/5 [==============================] - 0s 5ms/step - loss: 90.9459\nEpoch 89/1000\n5/5 [==============================] - 0s 7ms/step - loss: 90.7521\nEpoch 90/1000\n5/5 [==============================] - 0s 9ms/step - loss: 90.5549\nEpoch 91/1000\n5/5 [==============================] - 0s 8ms/step - loss: 90.3532\nEpoch 92/1000\n5/5 [==============================] - 0s 6ms/step - loss: 90.1659\nEpoch 93/1000\n5/5 [==============================] - 0s 5ms/step - loss: 89.9669\nEpoch 94/1000\n5/5 [==============================] - 0s 4ms/step - loss: 89.7709\nEpoch 95/1000\n5/5 [==============================] - 0s 4ms/step - loss: 89.5775\nEpoch 96/1000\n5/5 [==============================] - 0s 4ms/step - loss: 89.3882\nEpoch 97/1000\n5/5 [==============================] - 0s 9ms/step - loss: 89.1998\nEpoch 98/1000\n5/5 [==============================] - 0s 4ms/step - loss: 89.0053\nEpoch 99/1000\n5/5 [==============================] - 0s 5ms/step - loss: 88.8156\nEpoch 100/1000\n5/5 [==============================] - 0s 6ms/step - loss: 88.6220\nEpoch 101/1000\n5/5 [==============================] - 0s 5ms/step - loss: 88.4300\nEpoch 102/1000\n5/5 [==============================] - 0s 7ms/step - loss: 88.2422\nEpoch 103/1000\n5/5 [==============================] - 0s 13ms/step - loss: 88.0466\nEpoch 104/1000\n5/5 [==============================] - 0s 5ms/step - loss: 87.8566\nEpoch 105/1000\n5/5 [==============================] - 0s 5ms/step - loss: 87.6699\nEpoch 106/1000\n5/5 [==============================] - 0s 5ms/step - loss: 87.4792\nEpoch 107/1000\n5/5 [==============================] - 0s 6ms/step - loss: 87.2927\nEpoch 108/1000\n5/5 [==============================] - 0s 4ms/step - loss: 87.1079\nEpoch 109/1000\n5/5 [==============================] - 0s 9ms/step - loss: 86.9185\nEpoch 110/1000\n5/5 [==============================] - 0s 5ms/step - loss: 86.7362\nEpoch 111/1000\n5/5 [==============================] - 0s 5ms/step - loss: 86.5498\nEpoch 112/1000\n5/5 [==============================] - 0s 5ms/step - loss: 86.3623\nEpoch 113/1000\n5/5 [==============================] - 0s 5ms/step - loss: 86.1762\nEpoch 114/1000\n5/5 [==============================] - 0s 8ms/step - loss: 85.9827\nEpoch 115/1000\n5/5 [==============================] - 0s 9ms/step - loss: 85.8061\nEpoch 116/1000\n5/5 [==============================] - 0s 6ms/step - loss: 85.6136\nEpoch 117/1000\n5/5 [==============================] - 0s 4ms/step - loss: 85.4308\nEpoch 118/1000\n5/5 [==============================] - 0s 5ms/step - loss: 85.2473\nEpoch 119/1000\n5/5 [==============================] - 0s 4ms/step - loss: 85.0604\nEpoch 120/1000\n5/5 [==============================] - 0s 5ms/step - loss: 84.8720\nEpoch 121/1000\n5/5 [==============================] - 0s 4ms/step - loss: 84.6853\nEpoch 122/1000\n5/5 [==============================] - 0s 5ms/step - loss: 84.4953\nEpoch 123/1000\n5/5 [==============================] - 0s 4ms/step - loss: 84.3089\nEpoch 124/1000\n5/5 [==============================] - 0s 5ms/step - loss: 84.1167\nEpoch 125/1000\n5/5 [==============================] - 0s 4ms/step - loss: 83.9305\nEpoch 126/1000\n5/5 [==============================] - 0s 5ms/step - loss: 83.7400\nEpoch 127/1000\n5/5 [==============================] - 0s 6ms/step - loss: 83.5551\nEpoch 128/1000\n5/5 [==============================] - 0s 4ms/step - loss: 83.3687\nEpoch 129/1000\n5/5 [==============================] - 0s 4ms/step - loss: 83.1797\nEpoch 130/1000\n5/5 [==============================] - 0s 5ms/step - loss: 82.9989\nEpoch 131/1000\n5/5 [==============================] - 0s 4ms/step - loss: 82.8167\nEpoch 132/1000\n5/5 [==============================] - 0s 4ms/step - loss: 82.6315\nEpoch 133/1000\n5/5 [==============================] - 0s 6ms/step - loss: 82.4512\nEpoch 134/1000\n5/5 [==============================] - 0s 4ms/step - loss: 82.2664\nEpoch 135/1000\n5/5 [==============================] - 0s 4ms/step - loss: 82.0849\nEpoch 136/1000\n5/5 [==============================] - 0s 5ms/step - loss: 81.9010\nEpoch 137/1000\n5/5 [==============================] - 0s 5ms/step - loss: 81.7232\nEpoch 138/1000\n5/5 [==============================] - 0s 5ms/step - loss: 81.5365\nEpoch 139/1000\n5/5 [==============================] - 0s 4ms/step - loss: 81.3612\nEpoch 140/1000\n5/5 [==============================] - 0s 5ms/step - loss: 81.1757\nEpoch 141/1000\n5/5 [==============================] - 0s 4ms/step - loss: 80.9972\nEpoch 142/1000\n5/5 [==============================] - 0s 4ms/step - loss: 80.8143\nEpoch 143/1000\n5/5 [==============================] - 0s 4ms/step - loss: 80.6310\nEpoch 144/1000\n5/5 [==============================] - 0s 5ms/step - loss: 80.4510\nEpoch 145/1000\n5/5 [==============================] - 0s 5ms/step - loss: 80.2664\nEpoch 146/1000\n5/5 [==============================] - 0s 4ms/step - loss: 80.0901\nEpoch 147/1000\n5/5 [==============================] - 0s 5ms/step - loss: 79.9080\nEpoch 148/1000\n5/5 [==============================] - 0s 4ms/step - loss: 79.7365\nEpoch 149/1000\n5/5 [==============================] - 0s 5ms/step - loss: 79.5604\nEpoch 150/1000\n5/5 [==============================] - 0s 7ms/step - loss: 79.3853\nEpoch 151/1000\n5/5 [==============================] - 0s 5ms/step - loss: 79.2035\nEpoch 152/1000\n5/5 [==============================] - 0s 5ms/step - loss: 79.0299\nEpoch 153/1000\n5/5 [==============================] - 0s 5ms/step - loss: 78.8572\nEpoch 154/1000\n5/5 [==============================] - 0s 8ms/step - loss: 78.6763\nEpoch 155/1000\n5/5 [==============================] - 0s 6ms/step - loss: 78.5045\nEpoch 156/1000\n5/5 [==============================] - 0s 5ms/step - loss: 78.3235\nEpoch 157/1000\n5/5 [==============================] - 0s 5ms/step - loss: 78.1519\nEpoch 158/1000\n5/5 [==============================] - 0s 5ms/step - loss: 77.9853\nEpoch 159/1000\n5/5 [==============================] - 0s 6ms/step - loss: 77.8087\nEpoch 160/1000\n5/5 [==============================] - 0s 5ms/step - loss: 77.6422\nEpoch 161/1000\n5/5 [==============================] - 0s 8ms/step - loss: 77.4701\nEpoch 162/1000\n5/5 [==============================] - 0s 6ms/step - loss: 77.3006\nEpoch 163/1000\n5/5 [==============================] - 0s 5ms/step - loss: 77.1299\nEpoch 164/1000\n5/5 [==============================] - 0s 5ms/step - loss: 76.9655\nEpoch 165/1000\n5/5 [==============================] - 0s 5ms/step - loss: 76.7956\nEpoch 166/1000\n5/5 [==============================] - 0s 4ms/step - loss: 76.6254\nEpoch 167/1000\n5/5 [==============================] - 0s 4ms/step - loss: 76.4598\nEpoch 168/1000\n5/5 [==============================] - 0s 6ms/step - loss: 76.2882\nEpoch 169/1000\n5/5 [==============================] - 0s 6ms/step - loss: 76.1207\nEpoch 170/1000\n5/5 [==============================] - 0s 6ms/step - loss: 75.9494\nEpoch 171/1000\n5/5 [==============================] - 0s 6ms/step - loss: 75.7812\nEpoch 172/1000\n5/5 [==============================] - 0s 6ms/step - loss: 75.6123\nEpoch 173/1000\n5/5 [==============================] - 0s 6ms/step - loss: 75.4423\nEpoch 174/1000\n5/5 [==============================] - 0s 5ms/step - loss: 75.2715\nEpoch 175/1000\n5/5 [==============================] - 0s 5ms/step - loss: 75.1068\nEpoch 176/1000\n5/5 [==============================] - 0s 8ms/step - loss: 74.9383\nEpoch 177/1000\n5/5 [==============================] - 0s 6ms/step - loss: 74.7736\nEpoch 178/1000\n5/5 [==============================] - 0s 5ms/step - loss: 74.6112\nEpoch 179/1000\n5/5 [==============================] - 0s 5ms/step - loss: 74.4437\nEpoch 180/1000\n5/5 [==============================] - 0s 5ms/step - loss: 74.2781\nEpoch 181/1000\n5/5 [==============================] - 0s 5ms/step - loss: 74.1164\nEpoch 182/1000\n5/5 [==============================] - 0s 5ms/step - loss: 73.9501\nEpoch 183/1000\n5/5 [==============================] - 0s 4ms/step - loss: 73.7829\nEpoch 184/1000\n5/5 [==============================] - 0s 6ms/step - loss: 73.6218\nEpoch 185/1000\n5/5 [==============================] - 0s 5ms/step - loss: 73.4603\nEpoch 186/1000\n5/5 [==============================] - 0s 5ms/step - loss: 73.2937\nEpoch 187/1000\n5/5 [==============================] - 0s 5ms/step - loss: 73.1317\nEpoch 188/1000\n5/5 [==============================] - 0s 5ms/step - loss: 72.9703\nEpoch 189/1000\n5/5 [==============================] - 0s 9ms/step - loss: 72.8072\nEpoch 190/1000\n5/5 [==============================] - 0s 7ms/step - loss: 72.6443\nEpoch 191/1000\n5/5 [==============================] - 0s 5ms/step - loss: 72.4829\nEpoch 192/1000\n5/5 [==============================] - 0s 5ms/step - loss: 72.3209\nEpoch 193/1000\n5/5 [==============================] - 0s 7ms/step - loss: 72.1542\nEpoch 194/1000\n5/5 [==============================] - 0s 6ms/step - loss: 71.9928\nEpoch 195/1000\n5/5 [==============================] - 0s 5ms/step - loss: 71.8223\nEpoch 196/1000\n5/5 [==============================] - 0s 5ms/step - loss: 71.6613\nEpoch 197/1000\n5/5 [==============================] - 0s 5ms/step - loss: 71.4974\nEpoch 198/1000\n5/5 [==============================] - 0s 4ms/step - loss: 71.3324\nEpoch 199/1000\n5/5 [==============================] - 0s 5ms/step - loss: 71.1688\nEpoch 200/1000\n5/5 [==============================] - 0s 5ms/step - loss: 71.0114\nEpoch 201/1000\n5/5 [==============================] - 0s 5ms/step - loss: 70.8453\nEpoch 202/1000\n5/5 [==============================] - 0s 5ms/step - loss: 70.6870\nEpoch 203/1000\n5/5 [==============================] - 0s 5ms/step - loss: 70.5309\nEpoch 204/1000\n5/5 [==============================] - 0s 5ms/step - loss: 70.3735\nEpoch 205/1000\n5/5 [==============================] - 0s 4ms/step - loss: 70.2070\nEpoch 206/1000\n5/5 [==============================] - 0s 3ms/step - loss: 70.0521\nEpoch 207/1000\n5/5 [==============================] - 0s 4ms/step - loss: 69.8928\nEpoch 208/1000\n5/5 [==============================] - 0s 3ms/step - loss: 69.7288\nEpoch 209/1000\n5/5 [==============================] - 0s 4ms/step - loss: 69.5711\nEpoch 210/1000\n5/5 [==============================] - 0s 4ms/step - loss: 69.4174\nEpoch 211/1000\n5/5 [==============================] - 0s 6ms/step - loss: 69.2545\nEpoch 212/1000\n5/5 [==============================] - 0s 3ms/step - loss: 69.0938\nEpoch 213/1000\n5/5 [==============================] - 0s 3ms/step - loss: 68.9379\nEpoch 214/1000\n5/5 [==============================] - 0s 4ms/step - loss: 68.7842\nEpoch 215/1000\n5/5 [==============================] - 0s 4ms/step - loss: 68.6311\nEpoch 216/1000\n5/5 [==============================] - 0s 4ms/step - loss: 68.4770\nEpoch 217/1000\n5/5 [==============================] - 0s 3ms/step - loss: 68.3204\nEpoch 218/1000\n5/5 [==============================] - 0s 4ms/step - loss: 68.1703\nEpoch 219/1000\n5/5 [==============================] - 0s 4ms/step - loss: 68.0143\nEpoch 220/1000\n5/5 [==============================] - 0s 3ms/step - loss: 67.8609\nEpoch 221/1000\n5/5 [==============================] - 0s 4ms/step - loss: 67.7008\nEpoch 222/1000\n5/5 [==============================] - 0s 4ms/step - loss: 67.5489\nEpoch 223/1000\n5/5 [==============================] - 0s 4ms/step - loss: 67.3931\nEpoch 224/1000\n5/5 [==============================] - 0s 4ms/step - loss: 67.2387\nEpoch 225/1000\n5/5 [==============================] - 0s 6ms/step - loss: 67.0818\nEpoch 226/1000\n5/5 [==============================] - 0s 4ms/step - loss: 66.9349\nEpoch 227/1000\n5/5 [==============================] - 0s 3ms/step - loss: 66.7776\nEpoch 228/1000\n5/5 [==============================] - 0s 4ms/step - loss: 66.6269\nEpoch 229/1000\n5/5 [==============================] - 0s 4ms/step - loss: 66.4755\nEpoch 230/1000\n5/5 [==============================] - 0s 4ms/step - loss: 66.3197\nEpoch 231/1000\n5/5 [==============================] - 0s 4ms/step - loss: 66.1673\nEpoch 232/1000\n5/5 [==============================] - 0s 4ms/step - loss: 66.0209\nEpoch 233/1000\n5/5 [==============================] - 0s 4ms/step - loss: 65.8644\nEpoch 234/1000\n5/5 [==============================] - 0s 4ms/step - loss: 65.7159\nEpoch 235/1000\n5/5 [==============================] - 0s 5ms/step - loss: 65.5645\nEpoch 236/1000\n5/5 [==============================] - 0s 4ms/step - loss: 65.4146\nEpoch 237/1000\n5/5 [==============================] - 0s 4ms/step - loss: 65.2609\nEpoch 238/1000\n5/5 [==============================] - 0s 4ms/step - loss: 65.1096\nEpoch 239/1000\n5/5 [==============================] - 0s 4ms/step - loss: 64.9635\nEpoch 240/1000\n5/5 [==============================] - 0s 4ms/step - loss: 64.8139\nEpoch 241/1000\n5/5 [==============================] - 0s 4ms/step - loss: 64.6613\nEpoch 242/1000\n5/5 [==============================] - 0s 5ms/step - loss: 64.5168\nEpoch 243/1000\n5/5 [==============================] - 0s 4ms/step - loss: 64.3741\nEpoch 244/1000\n5/5 [==============================] - 0s 4ms/step - loss: 64.2218\nEpoch 245/1000\n5/5 [==============================] - 0s 4ms/step - loss: 64.0745\nEpoch 246/1000\n5/5 [==============================] - 0s 4ms/step - loss: 63.9305\nEpoch 247/1000\n5/5 [==============================] - 0s 4ms/step - loss: 63.7846\nEpoch 248/1000\n5/5 [==============================] - 0s 4ms/step - loss: 63.6388\nEpoch 249/1000\n5/5 [==============================] - 0s 4ms/step - loss: 63.4942\nEpoch 250/1000\n5/5 [==============================] - 0s 5ms/step - loss: 63.3476\nEpoch 251/1000\n5/5 [==============================] - 0s 4ms/step - loss: 63.1938\nEpoch 252/1000\n5/5 [==============================] - 0s 4ms/step - loss: 63.0518\nEpoch 253/1000\n5/5 [==============================] - 0s 4ms/step - loss: 62.9031\nEpoch 254/1000\n5/5 [==============================] - 0s 4ms/step - loss: 62.7571\nEpoch 255/1000\n5/5 [==============================] - 0s 4ms/step - loss: 62.6089\nEpoch 256/1000\n5/5 [==============================] - 0s 4ms/step - loss: 62.4629\nEpoch 257/1000\n5/5 [==============================] - 0s 4ms/step - loss: 62.3235\nEpoch 258/1000\n5/5 [==============================] - 0s 6ms/step - loss: 62.1681\nEpoch 259/1000\n5/5 [==============================] - 0s 4ms/step - loss: 62.0225\nEpoch 260/1000\n5/5 [==============================] - 0s 4ms/step - loss: 61.8753\nEpoch 261/1000\n5/5 [==============================] - 0s 4ms/step - loss: 61.7347\nEpoch 262/1000\n5/5 [==============================] - 0s 4ms/step - loss: 61.5874\nEpoch 263/1000\n5/5 [==============================] - 0s 7ms/step - loss: 61.4472\nEpoch 264/1000\n5/5 [==============================] - 0s 6ms/step - loss: 61.3046\nEpoch 265/1000\n5/5 [==============================] - 0s 4ms/step - loss: 61.1633\nEpoch 266/1000\n5/5 [==============================] - 0s 4ms/step - loss: 61.0200\nEpoch 267/1000\n5/5 [==============================] - 0s 5ms/step - loss: 60.8783\nEpoch 268/1000\n5/5 [==============================] - 0s 4ms/step - loss: 60.7334\nEpoch 269/1000\n5/5 [==============================] - 0s 4ms/step - loss: 60.5928\nEpoch 270/1000\n5/5 [==============================] - 0s 4ms/step - loss: 60.4547\nEpoch 271/1000\n5/5 [==============================] - 0s 4ms/step - loss: 60.3142\nEpoch 272/1000\n5/5 [==============================] - 0s 4ms/step - loss: 60.1754\nEpoch 273/1000\n5/5 [==============================] - 0s 4ms/step - loss: 60.0314\nEpoch 274/1000\n5/5 [==============================] - 0s 4ms/step - loss: 59.8938\nEpoch 275/1000\n5/5 [==============================] - 0s 4ms/step - loss: 59.7521\nEpoch 276/1000\n5/5 [==============================] - 0s 4ms/step - loss: 59.6135\nEpoch 277/1000\n5/5 [==============================] - 0s 4ms/step - loss: 59.4678\nEpoch 278/1000\n5/5 [==============================] - 0s 4ms/step - loss: 59.3272\nEpoch 279/1000\n5/5 [==============================] - 0s 4ms/step - loss: 59.1913\nEpoch 280/1000\n5/5 [==============================] - 0s 4ms/step - loss: 59.0542\nEpoch 281/1000\n5/5 [==============================] - 0s 4ms/step - loss: 58.9133\nEpoch 282/1000\n5/5 [==============================] - 0s 4ms/step - loss: 58.7831\nEpoch 283/1000\n5/5 [==============================] - 0s 4ms/step - loss: 58.6413\nEpoch 284/1000\n5/5 [==============================] - 0s 4ms/step - loss: 58.5093\nEpoch 285/1000\n5/5 [==============================] - 0s 4ms/step - loss: 58.3760\nEpoch 286/1000\n5/5 [==============================] - 0s 4ms/step - loss: 58.2422\nEpoch 287/1000\n5/5 [==============================] - 0s 4ms/step - loss: 58.1078\nEpoch 288/1000\n5/5 [==============================] - 0s 4ms/step - loss: 57.9742\nEpoch 289/1000\n5/5 [==============================] - 0s 4ms/step - loss: 57.8433\nEpoch 290/1000\n5/5 [==============================] - 0s 4ms/step - loss: 57.7075\nEpoch 291/1000\n5/5 [==============================] - 0s 4ms/step - loss: 57.5726\nEpoch 292/1000\n5/5 [==============================] - 0s 4ms/step - loss: 57.4412\nEpoch 293/1000\n5/5 [==============================] - 0s 4ms/step - loss: 57.3087\nEpoch 294/1000\n5/5 [==============================] - 0s 3ms/step - loss: 57.1790\nEpoch 295/1000\n5/5 [==============================] - 0s 4ms/step - loss: 57.0443\nEpoch 296/1000\n5/5 [==============================] - 0s 4ms/step - loss: 56.9131\nEpoch 297/1000\n5/5 [==============================] - 0s 4ms/step - loss: 56.7780\nEpoch 298/1000\n5/5 [==============================] - 0s 4ms/step - loss: 56.6474\nEpoch 299/1000\n5/5 [==============================] - 0s 4ms/step - loss: 56.5108\nEpoch 300/1000\n5/5 [==============================] - 0s 5ms/step - loss: 56.3796\nEpoch 301/1000\n5/5 [==============================] - 0s 4ms/step - loss: 56.2542\nEpoch 302/1000\n5/5 [==============================] - 0s 4ms/step - loss: 56.1182\nEpoch 303/1000\n5/5 [==============================] - 0s 3ms/step - loss: 55.9865\nEpoch 304/1000\n5/5 [==============================] - 0s 4ms/step - loss: 55.8526\nEpoch 305/1000\n5/5 [==============================] - 0s 4ms/step - loss: 55.7226\nEpoch 306/1000\n5/5 [==============================] - 0s 5ms/step - loss: 55.5918\nEpoch 307/1000\n5/5 [==============================] - 0s 4ms/step - loss: 55.4582\nEpoch 308/1000\n5/5 [==============================] - 0s 4ms/step - loss: 55.3261\nEpoch 309/1000\n5/5 [==============================] - 0s 4ms/step - loss: 55.1961\nEpoch 310/1000\n5/5 [==============================] - 0s 4ms/step - loss: 55.0635\nEpoch 311/1000\n5/5 [==============================] - 0s 4ms/step - loss: 54.9340\nEpoch 312/1000\n5/5 [==============================] - 0s 4ms/step - loss: 54.8050\nEpoch 313/1000\n5/5 [==============================] - 0s 4ms/step - loss: 54.6801\nEpoch 314/1000\n5/5 [==============================] - 0s 5ms/step - loss: 54.5537\nEpoch 315/1000\n5/5 [==============================] - 0s 4ms/step - loss: 54.4235\nEpoch 316/1000\n5/5 [==============================] - 0s 4ms/step - loss: 54.2932\nEpoch 317/1000\n5/5 [==============================] - 0s 4ms/step - loss: 54.1632\nEpoch 318/1000\n5/5 [==============================] - 0s 4ms/step - loss: 54.0377\nEpoch 319/1000\n5/5 [==============================] - 0s 4ms/step - loss: 53.9120\nEpoch 320/1000\n5/5 [==============================] - 0s 4ms/step - loss: 53.7802\nEpoch 321/1000\n5/5 [==============================] - 0s 4ms/step - loss: 53.6514\nEpoch 322/1000\n5/5 [==============================] - 0s 4ms/step - loss: 53.5197\nEpoch 323/1000\n5/5 [==============================] - 0s 4ms/step - loss: 53.3958\nEpoch 324/1000\n5/5 [==============================] - 0s 4ms/step - loss: 53.2674\nEpoch 325/1000\n5/5 [==============================] - 0s 5ms/step - loss: 53.1462\nEpoch 326/1000\n5/5 [==============================] - 0s 4ms/step - loss: 53.0176\nEpoch 327/1000\n5/5 [==============================] - 0s 4ms/step - loss: 52.8951\nEpoch 328/1000\n5/5 [==============================] - 0s 5ms/step - loss: 52.7723\nEpoch 329/1000\n5/5 [==============================] - 0s 4ms/step - loss: 52.6477\nEpoch 330/1000\n5/5 [==============================] - 0s 4ms/step - loss: 52.5222\nEpoch 331/1000\n5/5 [==============================] - 0s 4ms/step - loss: 52.3993\nEpoch 332/1000\n5/5 [==============================] - 0s 4ms/step - loss: 52.2694\nEpoch 333/1000\n5/5 [==============================] - 0s 5ms/step - loss: 52.1483\nEpoch 334/1000\n5/5 [==============================] - 0s 4ms/step - loss: 52.0245\nEpoch 335/1000\n5/5 [==============================] - 0s 4ms/step - loss: 51.9006\nEpoch 336/1000\n5/5 [==============================] - 0s 4ms/step - loss: 51.7825\nEpoch 337/1000\n5/5 [==============================] - 0s 6ms/step - loss: 51.6610\nEpoch 338/1000\n5/5 [==============================] - 0s 4ms/step - loss: 51.5370\nEpoch 339/1000\n5/5 [==============================] - 0s 4ms/step - loss: 51.4139\nEpoch 340/1000\n5/5 [==============================] - 0s 5ms/step - loss: 51.2943\nEpoch 341/1000\n5/5 [==============================] - 0s 4ms/step - loss: 51.1699\nEpoch 342/1000\n5/5 [==============================] - 0s 4ms/step - loss: 51.0519\nEpoch 343/1000\n5/5 [==============================] - 0s 4ms/step - loss: 50.9301\nEpoch 344/1000\n5/5 [==============================] - 0s 3ms/step - loss: 50.8085\nEpoch 345/1000\n5/5 [==============================] - 0s 4ms/step - loss: 50.6923\nEpoch 346/1000\n5/5 [==============================] - 0s 4ms/step - loss: 50.5699\nEpoch 347/1000\n5/5 [==============================] - 0s 4ms/step - loss: 50.4486\nEpoch 348/1000\n5/5 [==============================] - 0s 4ms/step - loss: 50.3319\nEpoch 349/1000\n5/5 [==============================] - 0s 4ms/step - loss: 50.2106\nEpoch 350/1000\n5/5 [==============================] - 0s 4ms/step - loss: 50.0934\nEpoch 351/1000\n5/5 [==============================] - 0s 5ms/step - loss: 49.9753\nEpoch 352/1000\n5/5 [==============================] - 0s 6ms/step - loss: 49.8542\nEpoch 353/1000\n5/5 [==============================] - 0s 4ms/step - loss: 49.7329\nEpoch 354/1000\n5/5 [==============================] - 0s 5ms/step - loss: 49.6102\nEpoch 355/1000\n5/5 [==============================] - 0s 5ms/step - loss: 49.4957\nEpoch 356/1000\n5/5 [==============================] - 0s 4ms/step - loss: 49.3757\nEpoch 357/1000\n5/5 [==============================] - 0s 4ms/step - loss: 49.2565\nEpoch 358/1000\n5/5 [==============================] - 0s 4ms/step - loss: 49.1384\nEpoch 359/1000\n5/5 [==============================] - 0s 4ms/step - loss: 49.0170\nEpoch 360/1000\n5/5 [==============================] - 0s 4ms/step - loss: 48.8998\nEpoch 361/1000\n5/5 [==============================] - 0s 5ms/step - loss: 48.7802\nEpoch 362/1000\n5/5 [==============================] - 0s 4ms/step - loss: 48.6622\nEpoch 363/1000\n5/5 [==============================] - 0s 4ms/step - loss: 48.5403\nEpoch 364/1000\n5/5 [==============================] - 0s 4ms/step - loss: 48.4258\nEpoch 365/1000\n5/5 [==============================] - 0s 4ms/step - loss: 48.3085\nEpoch 366/1000\n5/5 [==============================] - 0s 4ms/step - loss: 48.1905\nEpoch 367/1000\n5/5 [==============================] - 0s 4ms/step - loss: 48.0724\nEpoch 368/1000\n5/5 [==============================] - 0s 4ms/step - loss: 47.9572\nEpoch 369/1000\n5/5 [==============================] - 0s 5ms/step - loss: 47.8384\nEpoch 370/1000\n5/5 [==============================] - 0s 4ms/step - loss: 47.7237\nEpoch 371/1000\n5/5 [==============================] - 0s 4ms/step - loss: 47.6069\nEpoch 372/1000\n5/5 [==============================] - 0s 4ms/step - loss: 47.4938\nEpoch 373/1000\n5/5 [==============================] - 0s 5ms/step - loss: 47.3777\nEpoch 374/1000\n5/5 [==============================] - 0s 4ms/step - loss: 47.2671\nEpoch 375/1000\n5/5 [==============================] - 0s 4ms/step - loss: 47.1494\nEpoch 376/1000\n5/5 [==============================] - 0s 4ms/step - loss: 47.0368\nEpoch 377/1000\n5/5 [==============================] - 0s 4ms/step - loss: 46.9225\nEpoch 378/1000\n5/5 [==============================] - 0s 4ms/step - loss: 46.8118\nEpoch 379/1000\n5/5 [==============================] - 0s 6ms/step - loss: 46.6949\nEpoch 380/1000\n5/5 [==============================] - 0s 4ms/step - loss: 46.5846\nEpoch 381/1000\n5/5 [==============================] - 0s 4ms/step - loss: 46.4744\nEpoch 382/1000\n5/5 [==============================] - 0s 4ms/step - loss: 46.3639\nEpoch 383/1000\n5/5 [==============================] - 0s 4ms/step - loss: 46.2491\nEpoch 384/1000\n5/5 [==============================] - 0s 4ms/step - loss: 46.1356\nEpoch 385/1000\n5/5 [==============================] - 0s 4ms/step - loss: 46.0280\nEpoch 386/1000\n5/5 [==============================] - 0s 4ms/step - loss: 45.9145\nEpoch 387/1000\n5/5 [==============================] - 0s 4ms/step - loss: 45.8056\nEpoch 388/1000\n5/5 [==============================] - 0s 4ms/step - loss: 45.6956\nEpoch 389/1000\n5/5 [==============================] - 0s 4ms/step - loss: 45.5848\nEpoch 390/1000\n5/5 [==============================] - 0s 4ms/step - loss: 45.4730\nEpoch 391/1000\n5/5 [==============================] - 0s 4ms/step - loss: 45.3627\nEpoch 392/1000\n5/5 [==============================] - 0s 4ms/step - loss: 45.2467\nEpoch 393/1000\n5/5 [==============================] - 0s 4ms/step - loss: 45.1336\nEpoch 394/1000\n5/5 [==============================] - 0s 4ms/step - loss: 45.0182\nEpoch 395/1000\n5/5 [==============================] - 0s 3ms/step - loss: 44.9069\nEpoch 396/1000\n5/5 [==============================] - 0s 4ms/step - loss: 44.7960\nEpoch 397/1000\n5/5 [==============================] - 0s 4ms/step - loss: 44.6828\nEpoch 398/1000\n5/5 [==============================] - 0s 4ms/step - loss: 44.5724\nEpoch 399/1000\n5/5 [==============================] - 0s 4ms/step - loss: 44.4627\nEpoch 400/1000\n5/5 [==============================] - 0s 4ms/step - loss: 44.3568\nEpoch 401/1000\n5/5 [==============================] - 0s 4ms/step - loss: 44.2480\nEpoch 402/1000\n5/5 [==============================] - 0s 5ms/step - loss: 44.1431\nEpoch 403/1000\n5/5 [==============================] - 0s 4ms/step - loss: 44.0344\nEpoch 404/1000\n5/5 [==============================] - 0s 4ms/step - loss: 43.9286\nEpoch 405/1000\n5/5 [==============================] - 0s 4ms/step - loss: 43.8260\nEpoch 406/1000\n5/5 [==============================] - 0s 4ms/step - loss: 43.7172\nEpoch 407/1000\n5/5 [==============================] - 0s 5ms/step - loss: 43.6140\nEpoch 408/1000\n5/5 [==============================] - 0s 4ms/step - loss: 43.5114\nEpoch 409/1000\n5/5 [==============================] - 0s 4ms/step - loss: 43.4022\nEpoch 410/1000\n5/5 [==============================] - 0s 4ms/step - loss: 43.2967\nEpoch 411/1000\n5/5 [==============================] - 0s 5ms/step - loss: 43.1923\nEpoch 412/1000\n5/5 [==============================] - 0s 4ms/step - loss: 43.0855\nEpoch 413/1000\n5/5 [==============================] - 0s 4ms/step - loss: 42.9789\nEpoch 414/1000\n5/5 [==============================] - 0s 4ms/step - loss: 42.8722\nEpoch 415/1000\n5/5 [==============================] - 0s 3ms/step - loss: 42.7677\nEpoch 416/1000\n5/5 [==============================] - 0s 4ms/step - loss: 42.6625\nEpoch 417/1000\n5/5 [==============================] - 0s 3ms/step - loss: 42.5556\nEpoch 418/1000\n5/5 [==============================] - 0s 3ms/step - loss: 42.4544\nEpoch 419/1000\n5/5 [==============================] - 0s 4ms/step - loss: 42.3536\nEpoch 420/1000\n5/5 [==============================] - 0s 4ms/step - loss: 42.2517\nEpoch 421/1000\n5/5 [==============================] - 0s 4ms/step - loss: 42.1512\nEpoch 422/1000\n5/5 [==============================] - 0s 4ms/step - loss: 42.0489\nEpoch 423/1000\n5/5 [==============================] - 0s 4ms/step - loss: 41.9487\nEpoch 424/1000\n5/5 [==============================] - 0s 4ms/step - loss: 41.8482\nEpoch 425/1000\n5/5 [==============================] - 0s 5ms/step - loss: 41.7451\nEpoch 426/1000\n5/5 [==============================] - 0s 4ms/step - loss: 41.6453\nEpoch 427/1000\n5/5 [==============================] - 0s 4ms/step - loss: 41.5382\nEpoch 428/1000\n5/5 [==============================] - 0s 4ms/step - loss: 41.4426\nEpoch 429/1000\n5/5 [==============================] - 0s 4ms/step - loss: 41.3368\nEpoch 430/1000\n5/5 [==============================] - 0s 4ms/step - loss: 41.2396\nEpoch 431/1000\n5/5 [==============================] - 0s 4ms/step - loss: 41.1349\nEpoch 432/1000\n5/5 [==============================] - 0s 4ms/step - loss: 41.0359\nEpoch 433/1000\n5/5 [==============================] - 0s 3ms/step - loss: 40.9330\nEpoch 434/1000\n5/5 [==============================] - 0s 4ms/step - loss: 40.8305\nEpoch 435/1000\n5/5 [==============================] - 0s 4ms/step - loss: 40.7265\nEpoch 436/1000\n5/5 [==============================] - 0s 6ms/step - loss: 40.6280\nEpoch 437/1000\n5/5 [==============================] - 0s 4ms/step - loss: 40.5216\nEpoch 438/1000\n5/5 [==============================] - 0s 4ms/step - loss: 40.4232\nEpoch 439/1000\n5/5 [==============================] - 0s 4ms/step - loss: 40.3209\nEpoch 440/1000\n5/5 [==============================] - 0s 5ms/step - loss: 40.2173\nEpoch 441/1000\n5/5 [==============================] - 0s 5ms/step - loss: 40.1160\nEpoch 442/1000\n5/5 [==============================] - 0s 5ms/step - loss: 40.0195\nEpoch 443/1000\n5/5 [==============================] - 0s 5ms/step - loss: 39.9192\nEpoch 444/1000\n5/5 [==============================] - 0s 5ms/step - loss: 39.8198\nEpoch 445/1000\n5/5 [==============================] - 0s 4ms/step - loss: 39.7249\nEpoch 446/1000\n5/5 [==============================] - 0s 4ms/step - loss: 39.6241\nEpoch 447/1000\n5/5 [==============================] - 0s 4ms/step - loss: 39.5308\nEpoch 448/1000\n5/5 [==============================] - 0s 4ms/step - loss: 39.4315\nEpoch 449/1000\n5/5 [==============================] - 0s 4ms/step - loss: 39.3370\nEpoch 450/1000\n5/5 [==============================] - 0s 4ms/step - loss: 39.2424\nEpoch 451/1000\n5/5 [==============================] - 0s 5ms/step - loss: 39.1430\nEpoch 452/1000\n5/5 [==============================] - 0s 5ms/step - loss: 39.0463\nEpoch 453/1000\n5/5 [==============================] - 0s 5ms/step - loss: 38.9474\nEpoch 454/1000\n5/5 [==============================] - 0s 5ms/step - loss: 38.8517\nEpoch 455/1000\n5/5 [==============================] - 0s 6ms/step - loss: 38.7598\nEpoch 456/1000\n5/5 [==============================] - 0s 5ms/step - loss: 38.6580\nEpoch 457/1000\n5/5 [==============================] - 0s 5ms/step - loss: 38.5671\nEpoch 458/1000\n5/5 [==============================] - 0s 6ms/step - loss: 38.4681\nEpoch 459/1000\n5/5 [==============================] - 0s 6ms/step - loss: 38.3767\nEpoch 460/1000\n5/5 [==============================] - 0s 5ms/step - loss: 38.2845\nEpoch 461/1000\n5/5 [==============================] - 0s 5ms/step - loss: 38.1909\nEpoch 462/1000\n5/5 [==============================] - 0s 5ms/step - loss: 38.0949\nEpoch 463/1000\n5/5 [==============================] - 0s 4ms/step - loss: 38.0021\nEpoch 464/1000\n5/5 [==============================] - 0s 4ms/step - loss: 37.9094\nEpoch 465/1000\n5/5 [==============================] - 0s 6ms/step - loss: 37.8159\nEpoch 466/1000\n5/5 [==============================] - 0s 6ms/step - loss: 37.7213\nEpoch 467/1000\n5/5 [==============================] - 0s 5ms/step - loss: 37.6292\nEpoch 468/1000\n5/5 [==============================] - 0s 5ms/step - loss: 37.5367\nEpoch 469/1000\n5/5 [==============================] - 0s 5ms/step - loss: 37.4457\nEpoch 470/1000\n5/5 [==============================] - 0s 4ms/step - loss: 37.3529\nEpoch 471/1000\n5/5 [==============================] - 0s 6ms/step - loss: 37.2637\nEpoch 472/1000\n5/5 [==============================] - 0s 6ms/step - loss: 37.1712\nEpoch 473/1000\n5/5 [==============================] - 0s 4ms/step - loss: 37.0785\nEpoch 474/1000\n5/5 [==============================] - 0s 4ms/step - loss: 36.9892\nEpoch 475/1000\n5/5 [==============================] - 0s 5ms/step - loss: 36.8996\nEpoch 476/1000\n5/5 [==============================] - 0s 6ms/step - loss: 36.8071\nEpoch 477/1000\n5/5 [==============================] - 0s 5ms/step - loss: 36.7154\nEpoch 478/1000\n5/5 [==============================] - 0s 5ms/step - loss: 36.6271\nEpoch 479/1000\n5/5 [==============================] - 0s 6ms/step - loss: 36.5370\nEpoch 480/1000\n5/5 [==============================] - 0s 5ms/step - loss: 36.4459\nEpoch 481/1000\n5/5 [==============================] - 0s 5ms/step - loss: 36.3619\nEpoch 482/1000\n5/5 [==============================] - 0s 5ms/step - loss: 36.2655\nEpoch 483/1000\n5/5 [==============================] - 0s 5ms/step - loss: 36.1804\nEpoch 484/1000\n5/5 [==============================] - 0s 5ms/step - loss: 36.0937\nEpoch 485/1000\n5/5 [==============================] - 0s 5ms/step - loss: 36.0053\nEpoch 486/1000\n5/5 [==============================] - 0s 5ms/step - loss: 35.9187\nEpoch 487/1000\n5/5 [==============================] - 0s 5ms/step - loss: 35.8312\nEpoch 488/1000\n5/5 [==============================] - 0s 7ms/step - loss: 35.7396\nEpoch 489/1000\n5/5 [==============================] - 0s 6ms/step - loss: 35.6515\nEpoch 490/1000\n5/5 [==============================] - 0s 6ms/step - loss: 35.5634\nEpoch 491/1000\n5/5 [==============================] - 0s 5ms/step - loss: 35.4748\nEpoch 492/1000\n5/5 [==============================] - 0s 5ms/step - loss: 35.3852\nEpoch 493/1000\n5/5 [==============================] - 0s 5ms/step - loss: 35.2970\nEpoch 494/1000\n5/5 [==============================] - 0s 4ms/step - loss: 35.2093\nEpoch 495/1000\n5/5 [==============================] - 0s 4ms/step - loss: 35.1211\nEpoch 496/1000\n5/5 [==============================] - 0s 4ms/step - loss: 35.0373\nEpoch 497/1000\n5/5 [==============================] - 0s 4ms/step - loss: 34.9543\nEpoch 498/1000\n5/5 [==============================] - 0s 4ms/step - loss: 34.8651\nEpoch 499/1000\n5/5 [==============================] - 0s 5ms/step - loss: 34.7807\nEpoch 500/1000\n5/5 [==============================] - 0s 4ms/step - loss: 34.6964\nEpoch 501/1000\n5/5 [==============================] - 0s 4ms/step - loss: 34.6105\nEpoch 502/1000\n5/5 [==============================] - 0s 4ms/step - loss: 34.5256\nEpoch 503/1000\n5/5 [==============================] - 0s 4ms/step - loss: 34.4368\nEpoch 504/1000\n5/5 [==============================] - 0s 4ms/step - loss: 34.3538\nEpoch 505/1000\n5/5 [==============================] - 0s 5ms/step - loss: 34.2686\nEpoch 506/1000\n5/5 [==============================] - 0s 4ms/step - loss: 34.1831\nEpoch 507/1000\n5/5 [==============================] - 0s 4ms/step - loss: 34.1008\nEpoch 508/1000\n5/5 [==============================] - 0s 4ms/step - loss: 34.0150\nEpoch 509/1000\n5/5 [==============================] - 0s 4ms/step - loss: 33.9333\nEpoch 510/1000\n5/5 [==============================] - 0s 4ms/step - loss: 33.8483\nEpoch 511/1000\n5/5 [==============================] - 0s 5ms/step - loss: 33.7659\nEpoch 512/1000\n5/5 [==============================] - 0s 4ms/step - loss: 33.6827\nEpoch 513/1000\n5/5 [==============================] - 0s 4ms/step - loss: 33.5989\nEpoch 514/1000\n5/5 [==============================] - 0s 4ms/step - loss: 33.5174\nEpoch 515/1000\n5/5 [==============================] - 0s 4ms/step - loss: 33.4336\nEpoch 516/1000\n5/5 [==============================] - 0s 7ms/step - loss: 33.3516\nEpoch 517/1000\n5/5 [==============================] - 0s 5ms/step - loss: 33.2701\nEpoch 518/1000\n5/5 [==============================] - 0s 6ms/step - loss: 33.1877\nEpoch 519/1000\n5/5 [==============================] - 0s 6ms/step - loss: 33.1066\nEpoch 520/1000\n5/5 [==============================] - 0s 5ms/step - loss: 33.0231\nEpoch 521/1000\n5/5 [==============================] - 0s 6ms/step - loss: 32.9393\nEpoch 522/1000\n5/5 [==============================] - 0s 6ms/step - loss: 32.8592\nEpoch 523/1000\n5/5 [==============================] - 0s 5ms/step - loss: 32.7752\nEpoch 524/1000\n5/5 [==============================] - 0s 5ms/step - loss: 32.6948\nEpoch 525/1000\n5/5 [==============================] - 0s 5ms/step - loss: 32.6120\nEpoch 526/1000\n5/5 [==============================] - 0s 5ms/step - loss: 32.5309\nEpoch 527/1000\n5/5 [==============================] - 0s 5ms/step - loss: 32.4440\nEpoch 528/1000\n5/5 [==============================] - 0s 5ms/step - loss: 32.3647\nEpoch 529/1000\n5/5 [==============================] - 0s 3ms/step - loss: 32.2823\nEpoch 530/1000\n5/5 [==============================] - 0s 5ms/step - loss: 32.2004\nEpoch 531/1000\n5/5 [==============================] - 0s 5ms/step - loss: 32.1229\nEpoch 532/1000\n5/5 [==============================] - 0s 5ms/step - loss: 32.0423\nEpoch 533/1000\n5/5 [==============================] - 0s 5ms/step - loss: 31.9624\nEpoch 534/1000\n5/5 [==============================] - 0s 6ms/step - loss: 31.8825\nEpoch 535/1000\n5/5 [==============================] - 0s 4ms/step - loss: 31.8034\nEpoch 536/1000\n5/5 [==============================] - 0s 5ms/step - loss: 31.7254\nEpoch 537/1000\n5/5 [==============================] - 0s 5ms/step - loss: 31.6465\nEpoch 538/1000\n5/5 [==============================] - 0s 5ms/step - loss: 31.5661\nEpoch 539/1000\n5/5 [==============================] - 0s 5ms/step - loss: 31.4873\nEpoch 540/1000\n5/5 [==============================] - 0s 4ms/step - loss: 31.4104\nEpoch 541/1000\n5/5 [==============================] - 0s 5ms/step - loss: 31.3303\nEpoch 542/1000\n5/5 [==============================] - 0s 5ms/step - loss: 31.2503\nEpoch 543/1000\n5/5 [==============================] - 0s 5ms/step - loss: 31.1750\nEpoch 544/1000\n5/5 [==============================] - 0s 5ms/step - loss: 31.0955\nEpoch 545/1000\n5/5 [==============================] - 0s 5ms/step - loss: 31.0163\nEpoch 546/1000\n5/5 [==============================] - 0s 5ms/step - loss: 30.9374\nEpoch 547/1000\n5/5 [==============================] - 0s 5ms/step - loss: 30.8584\nEpoch 548/1000\n5/5 [==============================] - 0s 7ms/step - loss: 30.7822\nEpoch 549/1000\n5/5 [==============================] - 0s 5ms/step - loss: 30.7001\nEpoch 550/1000\n5/5 [==============================] - 0s 5ms/step - loss: 30.6283\nEpoch 551/1000\n5/5 [==============================] - 0s 5ms/step - loss: 30.5488\nEpoch 552/1000\n5/5 [==============================] - 0s 5ms/step - loss: 30.4735\nEpoch 553/1000\n5/5 [==============================] - 0s 6ms/step - loss: 30.3945\nEpoch 554/1000\n5/5 [==============================] - 0s 4ms/step - loss: 30.3201\nEpoch 555/1000\n5/5 [==============================] - 0s 6ms/step - loss: 30.2444\nEpoch 556/1000\n5/5 [==============================] - 0s 4ms/step - loss: 30.1680\nEpoch 557/1000\n5/5 [==============================] - 0s 5ms/step - loss: 30.0925\nEpoch 558/1000\n5/5 [==============================] - 0s 5ms/step - loss: 30.0174\nEpoch 559/1000\n5/5 [==============================] - 0s 5ms/step - loss: 29.9458\nEpoch 560/1000\n5/5 [==============================] - 0s 6ms/step - loss: 29.8712\nEpoch 561/1000\n5/5 [==============================] - 0s 5ms/step - loss: 29.7962\nEpoch 562/1000\n5/5 [==============================] - 0s 4ms/step - loss: 29.7245\nEpoch 563/1000\n5/5 [==============================] - 0s 5ms/step - loss: 29.6522\nEpoch 564/1000\n5/5 [==============================] - 0s 4ms/step - loss: 29.5799\nEpoch 565/1000\n5/5 [==============================] - 0s 4ms/step - loss: 29.5083\nEpoch 566/1000\n5/5 [==============================] - 0s 5ms/step - loss: 29.4381\nEpoch 567/1000\n5/5 [==============================] - 0s 5ms/step - loss: 29.3650\nEpoch 568/1000\n5/5 [==============================] - 0s 5ms/step - loss: 29.2906\nEpoch 569/1000\n5/5 [==============================] - 0s 4ms/step - loss: 29.2215\nEpoch 570/1000\n5/5 [==============================] - 0s 4ms/step - loss: 29.1483\nEpoch 571/1000\n5/5 [==============================] - 0s 6ms/step - loss: 29.0739\nEpoch 572/1000\n5/5 [==============================] - 0s 5ms/step - loss: 29.0021\nEpoch 573/1000\n5/5 [==============================] - 0s 4ms/step - loss: 28.9290\nEpoch 574/1000\n5/5 [==============================] - 0s 4ms/step - loss: 28.8564\nEpoch 575/1000\n5/5 [==============================] - 0s 4ms/step - loss: 28.7855\nEpoch 576/1000\n5/5 [==============================] - 0s 5ms/step - loss: 28.7110\nEpoch 577/1000\n5/5 [==============================] - 0s 4ms/step - loss: 28.6394\nEpoch 578/1000\n5/5 [==============================] - 0s 5ms/step - loss: 28.5659\nEpoch 579/1000\n5/5 [==============================] - 0s 5ms/step - loss: 28.4939\nEpoch 580/1000\n5/5 [==============================] - 0s 5ms/step - loss: 28.4196\nEpoch 581/1000\n5/5 [==============================] - 0s 5ms/step - loss: 28.3451\nEpoch 582/1000\n5/5 [==============================] - 0s 6ms/step - loss: 28.2769\nEpoch 583/1000\n5/5 [==============================] - 0s 5ms/step - loss: 28.2060\nEpoch 584/1000\n5/5 [==============================] - 0s 5ms/step - loss: 28.1330\nEpoch 585/1000\n5/5 [==============================] - 0s 5ms/step - loss: 28.0656\nEpoch 586/1000\n5/5 [==============================] - 0s 5ms/step - loss: 27.9918\nEpoch 587/1000\n5/5 [==============================] - 0s 8ms/step - loss: 27.9268\nEpoch 588/1000\n5/5 [==============================] - 0s 6ms/step - loss: 27.8551\nEpoch 589/1000\n5/5 [==============================] - 0s 5ms/step - loss: 27.7834\nEpoch 590/1000\n5/5 [==============================] - 0s 5ms/step - loss: 27.7162\nEpoch 591/1000\n5/5 [==============================] - 0s 5ms/step - loss: 27.6466\nEpoch 592/1000\n5/5 [==============================] - 0s 5ms/step - loss: 27.5792\nEpoch 593/1000\n5/5 [==============================] - 0s 4ms/step - loss: 27.5096\nEpoch 594/1000\n5/5 [==============================] - 0s 4ms/step - loss: 27.4396\nEpoch 595/1000\n5/5 [==============================] - 0s 4ms/step - loss: 27.3770\nEpoch 596/1000\n5/5 [==============================] - 0s 5ms/step - loss: 27.3092\nEpoch 597/1000\n5/5 [==============================] - 0s 5ms/step - loss: 27.2382\nEpoch 598/1000\n5/5 [==============================] - 0s 4ms/step - loss: 27.1730\nEpoch 599/1000\n5/5 [==============================] - 0s 5ms/step - loss: 27.1056\nEpoch 600/1000\n5/5 [==============================] - 0s 4ms/step - loss: 27.0381\nEpoch 601/1000\n5/5 [==============================] - 0s 4ms/step - loss: 26.9691\nEpoch 602/1000\n5/5 [==============================] - 0s 5ms/step - loss: 26.9043\nEpoch 603/1000\n5/5 [==============================] - 0s 5ms/step - loss: 26.8376\nEpoch 604/1000\n5/5 [==============================] - 0s 4ms/step - loss: 26.7712\nEpoch 605/1000\n5/5 [==============================] - 0s 4ms/step - loss: 26.7014\nEpoch 606/1000\n5/5 [==============================] - 0s 5ms/step - loss: 26.6381\nEpoch 607/1000\n5/5 [==============================] - 0s 5ms/step - loss: 26.5720\nEpoch 608/1000\n5/5 [==============================] - 0s 6ms/step - loss: 26.5011\nEpoch 609/1000\n5/5 [==============================] - 0s 4ms/step - loss: 26.4388\nEpoch 610/1000\n5/5 [==============================] - 0s 4ms/step - loss: 26.3725\nEpoch 611/1000\n5/5 [==============================] - 0s 4ms/step - loss: 26.3077\nEpoch 612/1000\n5/5 [==============================] - 0s 4ms/step - loss: 26.2456\nEpoch 613/1000\n5/5 [==============================] - 0s 5ms/step - loss: 26.1798\nEpoch 614/1000\n5/5 [==============================] - 0s 4ms/step - loss: 26.1112\nEpoch 615/1000\n5/5 [==============================] - 0s 4ms/step - loss: 26.0461\nEpoch 616/1000\n5/5 [==============================] - 0s 5ms/step - loss: 25.9779\nEpoch 617/1000\n5/5 [==============================] - 0s 4ms/step - loss: 25.9133\nEpoch 618/1000\n5/5 [==============================] - 0s 4ms/step - loss: 25.8432\nEpoch 619/1000\n5/5 [==============================] - 0s 4ms/step - loss: 25.7800\nEpoch 620/1000\n5/5 [==============================] - 0s 4ms/step - loss: 25.7182\nEpoch 621/1000\n5/5 [==============================] - 0s 4ms/step - loss: 25.6539\nEpoch 622/1000\n5/5 [==============================] - 0s 4ms/step - loss: 25.5888\nEpoch 623/1000\n5/5 [==============================] - 0s 4ms/step - loss: 25.5322\nEpoch 624/1000\n5/5 [==============================] - 0s 5ms/step - loss: 25.4698\nEpoch 625/1000\n5/5 [==============================] - 0s 4ms/step - loss: 25.4075\nEpoch 626/1000\n5/5 [==============================] - 0s 4ms/step - loss: 25.3453\nEpoch 627/1000\n5/5 [==============================] - 0s 4ms/step - loss: 25.2828\nEpoch 628/1000\n5/5 [==============================] - 0s 4ms/step - loss: 25.2210\nEpoch 629/1000\n5/5 [==============================] - 0s 4ms/step - loss: 25.1583\nEpoch 630/1000\n5/5 [==============================] - 0s 6ms/step - loss: 25.0965\nEpoch 631/1000\n5/5 [==============================] - 0s 5ms/step - loss: 25.0364\nEpoch 632/1000\n5/5 [==============================] - 0s 4ms/step - loss: 24.9744\nEpoch 633/1000\n5/5 [==============================] - 0s 4ms/step - loss: 24.9118\nEpoch 634/1000\n5/5 [==============================] - 0s 4ms/step - loss: 24.8497\nEpoch 635/1000\n5/5 [==============================] - 0s 5ms/step - loss: 24.7894\nEpoch 636/1000\n5/5 [==============================] - 0s 4ms/step - loss: 24.7274\nEpoch 637/1000\n5/5 [==============================] - 0s 4ms/step - loss: 24.6661\nEpoch 638/1000\n5/5 [==============================] - 0s 4ms/step - loss: 24.6014\nEpoch 639/1000\n5/5 [==============================] - 0s 5ms/step - loss: 24.5450\nEpoch 640/1000\n5/5 [==============================] - 0s 4ms/step - loss: 24.4826\nEpoch 641/1000\n5/5 [==============================] - 0s 4ms/step - loss: 24.4204\nEpoch 642/1000\n5/5 [==============================] - 0s 4ms/step - loss: 24.3599\nEpoch 643/1000\n5/5 [==============================] - 0s 4ms/step - loss: 24.2974\nEpoch 644/1000\n5/5 [==============================] - 0s 4ms/step - loss: 24.2401\nEpoch 645/1000\n5/5 [==============================] - 0s 5ms/step - loss: 24.1785\nEpoch 646/1000\n5/5 [==============================] - 0s 4ms/step - loss: 24.1187\nEpoch 647/1000\n5/5 [==============================] - 0s 5ms/step - loss: 24.0647\nEpoch 648/1000\n5/5 [==============================] - 0s 4ms/step - loss: 23.9968\nEpoch 649/1000\n5/5 [==============================] - 0s 4ms/step - loss: 23.9405\nEpoch 650/1000\n5/5 [==============================] - 0s 4ms/step - loss: 23.8835\nEpoch 651/1000\n5/5 [==============================] - 0s 4ms/step - loss: 23.8229\nEpoch 652/1000\n5/5 [==============================] - 0s 4ms/step - loss: 23.7650\nEpoch 653/1000\n5/5 [==============================] - 0s 4ms/step - loss: 23.7102\nEpoch 654/1000\n5/5 [==============================] - 0s 4ms/step - loss: 23.6512\nEpoch 655/1000\n5/5 [==============================] - 0s 4ms/step - loss: 23.5948\nEpoch 656/1000\n5/5 [==============================] - 0s 4ms/step - loss: 23.5357\nEpoch 657/1000\n5/5 [==============================] - 0s 4ms/step - loss: 23.4748\nEpoch 658/1000\n5/5 [==============================] - 0s 4ms/step - loss: 23.4178\nEpoch 659/1000\n5/5 [==============================] - 0s 4ms/step - loss: 23.3566\nEpoch 660/1000\n5/5 [==============================] - 0s 4ms/step - loss: 23.2999\nEpoch 661/1000\n5/5 [==============================] - 0s 4ms/step - loss: 23.2418\nEpoch 662/1000\n5/5 [==============================] - 0s 4ms/step - loss: 23.1838\nEpoch 663/1000\n5/5 [==============================] - 0s 4ms/step - loss: 23.1242\nEpoch 664/1000\n5/5 [==============================] - 0s 4ms/step - loss: 23.0704\nEpoch 665/1000\n5/5 [==============================] - 0s 4ms/step - loss: 23.0129\nEpoch 666/1000\n5/5 [==============================] - 0s 4ms/step - loss: 22.9543\nEpoch 667/1000\n5/5 [==============================] - 0s 4ms/step - loss: 22.9018\nEpoch 668/1000\n5/5 [==============================] - 0s 4ms/step - loss: 22.8433\nEpoch 669/1000\n5/5 [==============================] - 0s 4ms/step - loss: 22.7890\nEpoch 670/1000\n5/5 [==============================] - 0s 4ms/step - loss: 22.7336\nEpoch 671/1000\n5/5 [==============================] - 0s 3ms/step - loss: 22.6772\nEpoch 672/1000\n5/5 [==============================] - 0s 5ms/step - loss: 22.6222\nEpoch 673/1000\n5/5 [==============================] - 0s 4ms/step - loss: 22.5660\nEpoch 674/1000\n5/5 [==============================] - 0s 5ms/step - loss: 22.5096\nEpoch 675/1000\n5/5 [==============================] - 0s 4ms/step - loss: 22.4520\nEpoch 676/1000\n5/5 [==============================] - 0s 4ms/step - loss: 22.3993\nEpoch 677/1000\n5/5 [==============================] - 0s 5ms/step - loss: 22.3445\nEpoch 678/1000\n5/5 [==============================] - 0s 4ms/step - loss: 22.2849\nEpoch 679/1000\n5/5 [==============================] - 0s 5ms/step - loss: 22.2329\nEpoch 680/1000\n5/5 [==============================] - 0s 4ms/step - loss: 22.1730\nEpoch 681/1000\n5/5 [==============================] - 0s 4ms/step - loss: 22.1222\nEpoch 682/1000\n5/5 [==============================] - 0s 4ms/step - loss: 22.0637\nEpoch 683/1000\n5/5 [==============================] - 0s 4ms/step - loss: 22.0129\nEpoch 684/1000\n5/5 [==============================] - 0s 4ms/step - loss: 21.9549\nEpoch 685/1000\n5/5 [==============================] - 0s 4ms/step - loss: 21.9017\nEpoch 686/1000\n5/5 [==============================] - 0s 4ms/step - loss: 21.8466\nEpoch 687/1000\n5/5 [==============================] - 0s 4ms/step - loss: 21.7943\nEpoch 688/1000\n5/5 [==============================] - 0s 4ms/step - loss: 21.7382\nEpoch 689/1000\n5/5 [==============================] - 0s 4ms/step - loss: 21.6870\nEpoch 690/1000\n5/5 [==============================] - 0s 4ms/step - loss: 21.6327\nEpoch 691/1000\n5/5 [==============================] - 0s 4ms/step - loss: 21.5814\nEpoch 692/1000\n5/5 [==============================] - 0s 4ms/step - loss: 21.5298\nEpoch 693/1000\n5/5 [==============================] - 0s 4ms/step - loss: 21.4793\nEpoch 694/1000\n5/5 [==============================] - 0s 4ms/step - loss: 21.4253\nEpoch 695/1000\n5/5 [==============================] - 0s 4ms/step - loss: 21.3740\nEpoch 696/1000\n5/5 [==============================] - 0s 4ms/step - loss: 21.3258\nEpoch 697/1000\n5/5 [==============================] - 0s 3ms/step - loss: 21.2724\nEpoch 698/1000\n5/5 [==============================] - 0s 4ms/step - loss: 21.2199\nEpoch 699/1000\n5/5 [==============================] - 0s 4ms/step - loss: 21.1677\nEpoch 700/1000\n5/5 [==============================] - 0s 4ms/step - loss: 21.1189\nEpoch 701/1000\n5/5 [==============================] - 0s 4ms/step - loss: 21.0677\nEpoch 702/1000\n5/5 [==============================] - 0s 4ms/step - loss: 21.0157\nEpoch 703/1000\n5/5 [==============================] - 0s 4ms/step - loss: 20.9669\nEpoch 704/1000\n5/5 [==============================] - 0s 4ms/step - loss: 20.9148\nEpoch 705/1000\n5/5 [==============================] - 0s 4ms/step - loss: 20.8642\nEpoch 706/1000\n5/5 [==============================] - 0s 4ms/step - loss: 20.8117\nEpoch 707/1000\n5/5 [==============================] - 0s 4ms/step - loss: 20.7637\nEpoch 708/1000\n5/5 [==============================] - 0s 4ms/step - loss: 20.7118\nEpoch 709/1000\n5/5 [==============================] - 0s 4ms/step - loss: 20.6631\nEpoch 710/1000\n5/5 [==============================] - 0s 4ms/step - loss: 20.6139\nEpoch 711/1000\n5/5 [==============================] - 0s 4ms/step - loss: 20.5660\nEpoch 712/1000\n5/5 [==============================] - 0s 4ms/step - loss: 20.5182\nEpoch 713/1000\n5/5 [==============================] - 0s 4ms/step - loss: 20.4726\nEpoch 714/1000\n5/5 [==============================] - 0s 4ms/step - loss: 20.4229\nEpoch 715/1000\n5/5 [==============================] - 0s 5ms/step - loss: 20.3765\nEpoch 716/1000\n5/5 [==============================] - 0s 4ms/step - loss: 20.3263\nEpoch 717/1000\n5/5 [==============================] - 0s 4ms/step - loss: 20.2788\nEpoch 718/1000\n5/5 [==============================] - 0s 4ms/step - loss: 20.2285\nEpoch 719/1000\n5/5 [==============================] - 0s 5ms/step - loss: 20.1772\nEpoch 720/1000\n5/5 [==============================] - 0s 4ms/step - loss: 20.1296\nEpoch 721/1000\n5/5 [==============================] - 0s 4ms/step - loss: 20.0787\nEpoch 722/1000\n5/5 [==============================] - 0s 5ms/step - loss: 20.0297\nEpoch 723/1000\n5/5 [==============================] - 0s 4ms/step - loss: 19.9842\nEpoch 724/1000\n5/5 [==============================] - 0s 4ms/step - loss: 19.9311\nEpoch 725/1000\n5/5 [==============================] - 0s 4ms/step - loss: 19.8837\nEpoch 726/1000\n5/5 [==============================] - 0s 5ms/step - loss: 19.8337\nEpoch 727/1000\n5/5 [==============================] - 0s 5ms/step - loss: 19.7892\nEpoch 728/1000\n5/5 [==============================] - 0s 4ms/step - loss: 19.7411\nEpoch 729/1000\n5/5 [==============================] - 0s 4ms/step - loss: 19.6975\nEpoch 730/1000\n5/5 [==============================] - 0s 4ms/step - loss: 19.6529\nEpoch 731/1000\n5/5 [==============================] - 0s 4ms/step - loss: 19.6085\nEpoch 732/1000\n5/5 [==============================] - 0s 4ms/step - loss: 19.5596\nEpoch 733/1000\n5/5 [==============================] - 0s 4ms/step - loss: 19.5153\nEpoch 734/1000\n5/5 [==============================] - 0s 4ms/step - loss: 19.4715\nEpoch 735/1000\n5/5 [==============================] - 0s 4ms/step - loss: 19.4245\nEpoch 736/1000\n5/5 [==============================] - 0s 4ms/step - loss: 19.3801\nEpoch 737/1000\n5/5 [==============================] - 0s 4ms/step - loss: 19.3360\nEpoch 738/1000\n5/5 [==============================] - 0s 4ms/step - loss: 19.2912\nEpoch 739/1000\n5/5 [==============================] - 0s 4ms/step - loss: 19.2477\nEpoch 740/1000\n5/5 [==============================] - 0s 4ms/step - loss: 19.2046\nEpoch 741/1000\n5/5 [==============================] - 0s 4ms/step - loss: 19.1574\nEpoch 742/1000\n5/5 [==============================] - 0s 5ms/step - loss: 19.1171\nEpoch 743/1000\n5/5 [==============================] - 0s 4ms/step - loss: 19.0719\nEpoch 744/1000\n5/5 [==============================] - 0s 4ms/step - loss: 19.0285\nEpoch 745/1000\n5/5 [==============================] - 0s 5ms/step - loss: 18.9849\nEpoch 746/1000\n5/5 [==============================] - 0s 4ms/step - loss: 18.9404\nEpoch 747/1000\n5/5 [==============================] - 0s 5ms/step - loss: 18.8968\nEpoch 748/1000\n5/5 [==============================] - 0s 4ms/step - loss: 18.8528\nEpoch 749/1000\n5/5 [==============================] - 0s 4ms/step - loss: 18.8114\nEpoch 750/1000\n5/5 [==============================] - 0s 4ms/step - loss: 18.7648\nEpoch 751/1000\n5/5 [==============================] - 0s 4ms/step - loss: 18.7215\nEpoch 752/1000\n5/5 [==============================] - 0s 4ms/step - loss: 18.6781\nEpoch 753/1000\n5/5 [==============================] - 0s 4ms/step - loss: 18.6329\nEpoch 754/1000\n5/5 [==============================] - 0s 4ms/step - loss: 18.5904\nEpoch 755/1000\n5/5 [==============================] - 0s 4ms/step - loss: 18.5466\nEpoch 756/1000\n5/5 [==============================] - 0s 4ms/step - loss: 18.5034\nEpoch 757/1000\n5/5 [==============================] - 0s 4ms/step - loss: 18.4576\nEpoch 758/1000\n5/5 [==============================] - 0s 4ms/step - loss: 18.4149\nEpoch 759/1000\n5/5 [==============================] - 0s 4ms/step - loss: 18.3709\nEpoch 760/1000\n5/5 [==============================] - 0s 5ms/step - loss: 18.3308\nEpoch 761/1000\n5/5 [==============================] - 0s 6ms/step - loss: 18.2876\nEpoch 762/1000\n5/5 [==============================] - 0s 5ms/step - loss: 18.2470\nEpoch 763/1000\n5/5 [==============================] - 0s 4ms/step - loss: 18.2037\nEpoch 764/1000\n5/5 [==============================] - 0s 4ms/step - loss: 18.1628\nEpoch 765/1000\n5/5 [==============================] - 0s 4ms/step - loss: 18.1210\nEpoch 766/1000\n5/5 [==============================] - 0s 4ms/step - loss: 18.0808\nEpoch 767/1000\n5/5 [==============================] - 0s 6ms/step - loss: 18.0376\nEpoch 768/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.9990\nEpoch 769/1000\n5/5 [==============================] - 0s 5ms/step - loss: 17.9573\nEpoch 770/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.9145\nEpoch 771/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.8727\nEpoch 772/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.8322\nEpoch 773/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.7922\nEpoch 774/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.7527\nEpoch 775/1000\n5/5 [==============================] - 0s 5ms/step - loss: 17.7128\nEpoch 776/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.6747\nEpoch 777/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.6327\nEpoch 778/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.5915\nEpoch 779/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.5535\nEpoch 780/1000\n5/5 [==============================] - 0s 5ms/step - loss: 17.5148\nEpoch 781/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.4732\nEpoch 782/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.4351\nEpoch 783/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.3966\nEpoch 784/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.3589\nEpoch 785/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.3195\nEpoch 786/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.2806\nEpoch 787/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.2435\nEpoch 788/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.2042\nEpoch 789/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.1644\nEpoch 790/1000\n5/5 [==============================] - 0s 7ms/step - loss: 17.1272\nEpoch 791/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.0902\nEpoch 792/1000\n5/5 [==============================] - 0s 4ms/step - loss: 17.0499\nEpoch 793/1000\n5/5 [==============================] - 0s 5ms/step - loss: 17.0135\nEpoch 794/1000\n5/5 [==============================] - 0s 5ms/step - loss: 16.9723\nEpoch 795/1000\n5/5 [==============================] - 0s 5ms/step - loss: 16.9364\nEpoch 796/1000\n5/5 [==============================] - 0s 4ms/step - loss: 16.8968\nEpoch 797/1000\n5/5 [==============================] - 0s 4ms/step - loss: 16.8590\nEpoch 798/1000\n5/5 [==============================] - 0s 5ms/step - loss: 16.8213\nEpoch 799/1000\n5/5 [==============================] - 0s 5ms/step - loss: 16.7827\nEpoch 800/1000\n5/5 [==============================] - 0s 6ms/step - loss: 16.7473\nEpoch 801/1000\n5/5 [==============================] - 0s 4ms/step - loss: 16.7087\nEpoch 802/1000\n5/5 [==============================] - 0s 5ms/step - loss: 16.6713\nEpoch 803/1000\n5/5 [==============================] - 0s 5ms/step - loss: 16.6355\nEpoch 804/1000\n5/5 [==============================] - 0s 5ms/step - loss: 16.5973\nEpoch 805/1000\n5/5 [==============================] - 0s 4ms/step - loss: 16.5621\nEpoch 806/1000\n5/5 [==============================] - 0s 4ms/step - loss: 16.5261\nEpoch 807/1000\n5/5 [==============================] - 0s 4ms/step - loss: 16.4891\nEpoch 808/1000\n5/5 [==============================] - 0s 4ms/step - loss: 16.4557\nEpoch 809/1000\n5/5 [==============================] - 0s 4ms/step - loss: 16.4184\nEpoch 810/1000\n5/5 [==============================] - 0s 4ms/step - loss: 16.3851\nEpoch 811/1000\n5/5 [==============================] - 0s 4ms/step - loss: 16.3494\nEpoch 812/1000\n5/5 [==============================] - 0s 4ms/step - loss: 16.3170\nEpoch 813/1000\n5/5 [==============================] - 0s 4ms/step - loss: 16.2836\nEpoch 814/1000\n5/5 [==============================] - 0s 4ms/step - loss: 16.2497\nEpoch 815/1000\n5/5 [==============================] - 0s 4ms/step - loss: 16.2143\nEpoch 816/1000\n5/5 [==============================] - 0s 4ms/step - loss: 16.1805\nEpoch 817/1000\n5/5 [==============================] - 0s 4ms/step - loss: 16.1487\nEpoch 818/1000\n5/5 [==============================] - 0s 5ms/step - loss: 16.1131\nEpoch 819/1000\n5/5 [==============================] - 0s 5ms/step - loss: 16.0810\nEpoch 820/1000\n5/5 [==============================] - 0s 5ms/step - loss: 16.0450\nEpoch 821/1000\n5/5 [==============================] - 0s 5ms/step - loss: 16.0117\nEpoch 822/1000\n5/5 [==============================] - 0s 4ms/step - loss: 15.9790\nEpoch 823/1000\n5/5 [==============================] - 0s 4ms/step - loss: 15.9432\nEpoch 824/1000\n5/5 [==============================] - 0s 5ms/step - loss: 15.9103\nEpoch 825/1000\n5/5 [==============================] - 0s 4ms/step - loss: 15.8774\nEpoch 826/1000\n5/5 [==============================] - 0s 5ms/step - loss: 15.8420\nEpoch 827/1000\n5/5 [==============================] - 0s 5ms/step - loss: 15.8084\nEpoch 828/1000\n5/5 [==============================] - 0s 5ms/step - loss: 15.7753\nEpoch 829/1000\n5/5 [==============================] - 0s 4ms/step - loss: 15.7433\nEpoch 830/1000\n5/5 [==============================] - 0s 4ms/step - loss: 15.7075\nEpoch 831/1000\n5/5 [==============================] - 0s 4ms/step - loss: 15.6762\nEpoch 832/1000\n5/5 [==============================] - 0s 4ms/step - loss: 15.6431\nEpoch 833/1000\n5/5 [==============================] - 0s 5ms/step - loss: 15.6102\nEpoch 834/1000\n5/5 [==============================] - 0s 4ms/step - loss: 15.5775\nEpoch 835/1000\n5/5 [==============================] - 0s 4ms/step - loss: 15.5461\nEpoch 836/1000\n5/5 [==============================] - 0s 4ms/step - loss: 15.5122\nEpoch 837/1000\n5/5 [==============================] - 0s 5ms/step - loss: 15.4812\nEpoch 838/1000\n5/5 [==============================] - 0s 4ms/step - loss: 15.4486\nEpoch 839/1000\n5/5 [==============================] - 0s 5ms/step - loss: 15.4148\nEpoch 840/1000\n5/5 [==============================] - 0s 5ms/step - loss: 15.3823\nEpoch 841/1000\n5/5 [==============================] - 0s 4ms/step - loss: 15.3497\nEpoch 842/1000\n5/5 [==============================] - 0s 4ms/step - loss: 15.3188\nEpoch 843/1000\n5/5 [==============================] - 0s 4ms/step - loss: 15.2887\nEpoch 844/1000\n5/5 [==============================] - 0s 5ms/step - loss: 15.2593\nEpoch 845/1000\n5/5 [==============================] - 0s 5ms/step - loss: 15.2279\nEpoch 846/1000\n5/5 [==============================] - 0s 4ms/step - loss: 15.1976\nEpoch 847/1000\n5/5 [==============================] - 0s 5ms/step - loss: 15.1683\nEpoch 848/1000\n5/5 [==============================] - 0s 4ms/step - loss: 15.1384\nEpoch 849/1000\n5/5 [==============================] - 0s 4ms/step - loss: 15.1088\nEpoch 850/1000\n5/5 [==============================] - 0s 4ms/step - loss: 15.0790\nEpoch 851/1000\n5/5 [==============================] - 0s 4ms/step - loss: 15.0501\nEpoch 852/1000\n5/5 [==============================] - 0s 4ms/step - loss: 15.0181\nEpoch 853/1000\n5/5 [==============================] - 0s 5ms/step - loss: 14.9893\nEpoch 854/1000\n5/5 [==============================] - 0s 5ms/step - loss: 14.9603\nEpoch 855/1000\n5/5 [==============================] - 0s 4ms/step - loss: 14.9315\nEpoch 856/1000\n5/5 [==============================] - 0s 5ms/step - loss: 14.9007\nEpoch 857/1000\n5/5 [==============================] - 0s 4ms/step - loss: 14.8717\nEpoch 858/1000\n5/5 [==============================] - 0s 5ms/step - loss: 14.8432\nEpoch 859/1000\n5/5 [==============================] - 0s 4ms/step - loss: 14.8141\nEpoch 860/1000\n5/5 [==============================] - 0s 5ms/step - loss: 14.7878\nEpoch 861/1000\n5/5 [==============================] - 0s 4ms/step - loss: 14.7549\nEpoch 862/1000\n5/5 [==============================] - 0s 4ms/step - loss: 14.7251\nEpoch 863/1000\n5/5 [==============================] - 0s 4ms/step - loss: 14.6958\nEpoch 864/1000\n5/5 [==============================] - 0s 4ms/step - loss: 14.6679\nEpoch 865/1000\n5/5 [==============================] - 0s 4ms/step - loss: 14.6372\nEpoch 866/1000\n5/5 [==============================] - 0s 4ms/step - loss: 14.6066\nEpoch 867/1000\n5/5 [==============================] - 0s 4ms/step - loss: 14.5777\nEpoch 868/1000\n5/5 [==============================] - 0s 5ms/step - loss: 14.5464\nEpoch 869/1000\n5/5 [==============================] - 0s 5ms/step - loss: 14.5178\nEpoch 870/1000\n5/5 [==============================] - 0s 5ms/step - loss: 14.4886\nEpoch 871/1000\n5/5 [==============================] - 0s 5ms/step - loss: 14.4583\nEpoch 872/1000\n5/5 [==============================] - 0s 4ms/step - loss: 14.4269\nEpoch 873/1000\n5/5 [==============================] - 0s 5ms/step - loss: 14.4006\nEpoch 874/1000\n5/5 [==============================] - 0s 5ms/step - loss: 14.3709\nEpoch 875/1000\n5/5 [==============================] - 0s 4ms/step - loss: 14.3406\nEpoch 876/1000\n5/5 [==============================] - 0s 5ms/step - loss: 14.3137\nEpoch 877/1000\n5/5 [==============================] - 0s 4ms/step - loss: 14.2828\nEpoch 878/1000\n5/5 [==============================] - 0s 5ms/step - loss: 14.2540\nEpoch 879/1000\n5/5 [==============================] - 0s 5ms/step - loss: 14.2294\nEpoch 880/1000\n5/5 [==============================] - 0s 5ms/step - loss: 14.2012\nEpoch 881/1000\n5/5 [==============================] - 0s 5ms/step - loss: 14.1744\nEpoch 882/1000\n5/5 [==============================] - 0s 4ms/step - loss: 14.1492\nEpoch 883/1000\n5/5 [==============================] - 0s 4ms/step - loss: 14.1223\nEpoch 884/1000\n5/5 [==============================] - 0s 4ms/step - loss: 14.0976\nEpoch 885/1000\n5/5 [==============================] - 0s 4ms/step - loss: 14.0702\nEpoch 886/1000\n5/5 [==============================] - 0s 4ms/step - loss: 14.0455\nEpoch 887/1000\n5/5 [==============================] - 0s 4ms/step - loss: 14.0173\nEpoch 888/1000\n5/5 [==============================] - 0s 5ms/step - loss: 13.9909\nEpoch 889/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.9662\nEpoch 890/1000\n5/5 [==============================] - 0s 5ms/step - loss: 13.9401\nEpoch 891/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.9123\nEpoch 892/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.8879\nEpoch 893/1000\n5/5 [==============================] - 0s 5ms/step - loss: 13.8615\nEpoch 894/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.8345\nEpoch 895/1000\n5/5 [==============================] - 0s 5ms/step - loss: 13.8085\nEpoch 896/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.7844\nEpoch 897/1000\n5/5 [==============================] - 0s 5ms/step - loss: 13.7580\nEpoch 898/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.7312\nEpoch 899/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.7062\nEpoch 900/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.6777\nEpoch 901/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.6538\nEpoch 902/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.6265\nEpoch 903/1000\n5/5 [==============================] - 0s 5ms/step - loss: 13.6034\nEpoch 904/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.5778\nEpoch 905/1000\n5/5 [==============================] - 0s 5ms/step - loss: 13.5521\nEpoch 906/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.5253\nEpoch 907/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.5041\nEpoch 908/1000\n5/5 [==============================] - 0s 5ms/step - loss: 13.4790\nEpoch 909/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.4532\nEpoch 910/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.4305\nEpoch 911/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.4080\nEpoch 912/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.3839\nEpoch 913/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.3604\nEpoch 914/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.3384\nEpoch 915/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.3132\nEpoch 916/1000\n5/5 [==============================] - 0s 5ms/step - loss: 13.2904\nEpoch 917/1000\n5/5 [==============================] - 0s 5ms/step - loss: 13.2656\nEpoch 918/1000\n5/5 [==============================] - 0s 5ms/step - loss: 13.2435\nEpoch 919/1000\n5/5 [==============================] - 0s 5ms/step - loss: 13.2204\nEpoch 920/1000\n5/5 [==============================] - 0s 5ms/step - loss: 13.1998\nEpoch 921/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.1765\nEpoch 922/1000\n5/5 [==============================] - 0s 5ms/step - loss: 13.1545\nEpoch 923/1000\n5/5 [==============================] - 0s 5ms/step - loss: 13.1336\nEpoch 924/1000\n5/5 [==============================] - 0s 6ms/step - loss: 13.1085\nEpoch 925/1000\n5/5 [==============================] - 0s 5ms/step - loss: 13.0871\nEpoch 926/1000\n5/5 [==============================] - 0s 5ms/step - loss: 13.0648\nEpoch 927/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.0433\nEpoch 928/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.0205\nEpoch 929/1000\n5/5 [==============================] - 0s 4ms/step - loss: 13.0012\nEpoch 930/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.9775\nEpoch 931/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.9545\nEpoch 932/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.9339\nEpoch 933/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.9135\nEpoch 934/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.8928\nEpoch 935/1000\n5/5 [==============================] - 0s 5ms/step - loss: 12.8698\nEpoch 936/1000\n5/5 [==============================] - 0s 5ms/step - loss: 12.8485\nEpoch 937/1000\n5/5 [==============================] - 0s 5ms/step - loss: 12.8285\nEpoch 938/1000\n5/5 [==============================] - 0s 5ms/step - loss: 12.8063\nEpoch 939/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.7837\nEpoch 940/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.7615\nEpoch 941/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.7387\nEpoch 942/1000\n5/5 [==============================] - 0s 5ms/step - loss: 12.7165\nEpoch 943/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.6954\nEpoch 944/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.6738\nEpoch 945/1000\n5/5 [==============================] - 0s 5ms/step - loss: 12.6491\nEpoch 946/1000\n5/5 [==============================] - 0s 5ms/step - loss: 12.6287\nEpoch 947/1000\n5/5 [==============================] - 0s 5ms/step - loss: 12.6080\nEpoch 948/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.5878\nEpoch 949/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.5661\nEpoch 950/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.5463\nEpoch 951/1000\n5/5 [==============================] - 0s 5ms/step - loss: 12.5268\nEpoch 952/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.5058\nEpoch 953/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.4866\nEpoch 954/1000\n5/5 [==============================] - 0s 6ms/step - loss: 12.4691\nEpoch 955/1000\n5/5 [==============================] - 0s 6ms/step - loss: 12.4456\nEpoch 956/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.4269\nEpoch 957/1000\n5/5 [==============================] - 0s 5ms/step - loss: 12.4066\nEpoch 958/1000\n5/5 [==============================] - 0s 5ms/step - loss: 12.3865\nEpoch 959/1000\n5/5 [==============================] - 0s 5ms/step - loss: 12.3668\nEpoch 960/1000\n5/5 [==============================] - 0s 5ms/step - loss: 12.3471\nEpoch 961/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.3278\nEpoch 962/1000\n5/5 [==============================] - 0s 5ms/step - loss: 12.3070\nEpoch 963/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.2914\nEpoch 964/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.2710\nEpoch 965/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.2539\nEpoch 966/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.2342\nEpoch 967/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.2169\nEpoch 968/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.1991\nEpoch 969/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.1802\nEpoch 970/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.1609\nEpoch 971/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.1426\nEpoch 972/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.1249\nEpoch 973/1000\n5/5 [==============================] - 0s 5ms/step - loss: 12.1083\nEpoch 974/1000\n5/5 [==============================] - 0s 4ms/step - loss: 12.0897\nEpoch 975/1000\n5/5 [==============================] - 0s 5ms/step - loss: 12.0725\nEpoch 976/1000\n5/5 [==============================] - 0s 5ms/step - loss: 12.0546\nEpoch 977/1000\n5/5 [==============================] - 0s 5ms/step - loss: 12.0373\nEpoch 978/1000\n5/5 [==============================] - 0s 5ms/step - loss: 12.0178\nEpoch 979/1000\n5/5 [==============================] - 0s 6ms/step - loss: 12.0016\nEpoch 980/1000\n5/5 [==============================] - 0s 6ms/step - loss: 11.9824\nEpoch 981/1000\n5/5 [==============================] - 0s 6ms/step - loss: 11.9669\nEpoch 982/1000\n5/5 [==============================] - 0s 5ms/step - loss: 11.9476\nEpoch 983/1000\n5/5 [==============================] - 0s 5ms/step - loss: 11.9313\nEpoch 984/1000\n5/5 [==============================] - 0s 4ms/step - loss: 11.9138\nEpoch 985/1000\n5/5 [==============================] - 0s 4ms/step - loss: 11.8972\nEpoch 986/1000\n5/5 [==============================] - 0s 6ms/step - loss: 11.8816\nEpoch 987/1000\n5/5 [==============================] - 0s 5ms/step - loss: 11.8637\nEpoch 988/1000\n5/5 [==============================] - 0s 6ms/step - loss: 11.8482\nEpoch 989/1000\n5/5 [==============================] - 0s 5ms/step - loss: 11.8293\nEpoch 990/1000\n5/5 [==============================] - 0s 5ms/step - loss: 11.8132\nEpoch 991/1000\n5/5 [==============================] - 0s 5ms/step - loss: 11.7973\nEpoch 992/1000\n5/5 [==============================] - 0s 6ms/step - loss: 11.7813\nEpoch 993/1000\n5/5 [==============================] - 0s 5ms/step - loss: 11.7659\nEpoch 994/1000\n5/5 [==============================] - 0s 6ms/step - loss: 11.7494\nEpoch 995/1000\n5/5 [==============================] - 0s 5ms/step - loss: 11.7349\nEpoch 996/1000\n5/5 [==============================] - 0s 5ms/step - loss: 11.7208\nEpoch 997/1000\n5/5 [==============================] - 0s 4ms/step - loss: 11.7033\nEpoch 998/1000\n5/5 [==============================] - 0s 5ms/step - loss: 11.6881\nEpoch 999/1000\n5/5 [==============================] - 0s 5ms/step - loss: 11.6716\nEpoch 1000/1000\n5/5 [==============================] - 0s 5ms/step - loss: 11.6556\n\n\nWARNING:tensorflow:6 out of the last 11 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7dde4c3111b0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\n- epoch을 1000번쯤 셋팅하니 잘 예측한다..\n\n\nCode\nplt.figure(figsize = (12,4))\nplt.plot(y_test,label = r\"$y_{test}$\",alpha = 0.5)\nplt.plot(y_pred_init,label = r\"$\\hat {y}_{init}$\",alpha = 0.6)\nplt.plot(y_pred_5,label = r\"$\\hat {y}_{5}$\",alpha = 0.6)\nplt.plot(y_pred_final,label = r\"$\\hat {y}_{final}$\",alpha = 0.6)\nplt.title(r\"($y_{test}, \\hat {y}_{init}, \\hat {y}_{5},\\hat {y}_{final}$)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n- 검증해보자\n\n검증결과 이런 간단한 회귀 문제는 시간을 들여서 딥러닝 모델을 사용할 필요가 없어보인다.\n비교해보니, 걸리는 시간은 딥러닝 모델이 훨씬 길고, 예측 성능도 떨어진다.\n\n\nprint(\"ML model\")\nprint(f'RMSE  : {mean_squared_error(y_test, lm.pred, squared=False)}')\nprint(f'MAE   : {mean_absolute_error(y_test, lm.pred)}')\nprint(f'MAPE  : {mean_absolute_percentage_error(y_test, lm.pred)}')\n\nML model\nRMSE  : 1.5547807224924604\nMAE   : 1.2573619368638342\nMAPE  : 0.11717490489730135\n\n\n\nprint(\"DL model\")\nprint(f'RMSE  : {mean_squared_error(y_test, y_pred_final, squared=False)}')\nprint(f'MAE   : {mean_absolute_error(y_test, y_pred_final)}')\nprint(f'MAPE  : {mean_absolute_percentage_error(y_test, y_pred_final)}')\n\nDL model\nRMSE  : 2.895100478652866\nMAE   : 2.1337850681940713\nMAPE  : 0.15886854745380438"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-03-00. 딥러닝 (1).html#exercise-1.-carseat-회귀",
    "href": "posts/DX/05. 딥러닝/2023-10-03-00. 딥러닝 (1).html#exercise-1.-carseat-회귀",
    "title": "00. 딥러닝 (1)",
    "section": "exercise 1. Carseat (회귀)",
    "text": "exercise 1. Carseat (회귀)\n\n(1) 데이터 준비 및 이해\n\npath = 'https://raw.githubusercontent.com/DA4BAM/dataset/master/Carseats.csv'\ncarseat = pd.read_csv(path)\ncarseat.head()\n\n\n  \n    \n\n\n\n\n\n\nSales\nCompPrice\nIncome\nAdvertising\nPopulation\nPrice\nShelveLoc\nAge\nEducation\nUrban\nUS\n\n\n\n\n0\n9.50\n138\n73\n11\n276\n120\nBad\n42\n17\nYes\nYes\n\n\n1\n11.22\n111\n48\n16\n260\n83\nGood\n65\n10\nYes\nYes\n\n\n2\n10.06\n113\n35\n10\n269\n80\nMedium\n59\n12\nYes\nYes\n\n\n3\n7.40\n117\n100\n4\n466\n97\nMedium\n55\n14\nYes\nYes\n\n\n4\n4.15\n141\n64\n3\n340\n128\nBad\n38\n13\nYes\nNo\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- x, y 분리\n\ntarget = \"Sales\"\n\nx = carseat.drop(target, axis = 1)\ny = carseat[target]\n\n- 가변수화\n\ncat_cols = ['ShelveLoc', 'Education', 'US', 'Urban']\nx = pd.get_dummies(x, columns = cat_cols, drop_first = True)\n\n- 데이터셋 분할\n\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=.2, random_state = 20)\n\n- scaling\n\nscaler = MinMaxScaler()\nx_train = scaler.fit_transform(x_train)\nx_val = scaler.transform(x_val)\n\n\n\n(2) 모델링\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.backend import clear_session\n\n\nn_f = x_train.shape[1]\n\n- 일단 비교를 위해 epochs = 5, 1000 으로 셋팅\n\ngpu name을 확인하고 돌리기\n\n\nimport tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\ndevice_name\n\n'/device:GPU:0'\n\n\n\nwith tf.device('/device:GPU:0'):\n  model1 = Sequential( Dense(1, input_shape = (n_f,)) )\n\n  model1.compile(optimizer = \"adam\", loss = \"mse\")\n  model1.fit(x_train, y_train, epochs = 5, verbose=0)\n\nwith tf.device('/device:GPU:0'):\n  model2 = Sequential( Dense(1, input_shape = (n_f,)) )\n\n  model2.compile(optimizer = \"adam\", loss = \"mse\")\n  model2.fit(x_train, y_train, epochs = 1000,verbose=0)\n\n- 결과 시각화\n\n\nCode\ny_pred_5 = model1.predict(x_val)\ny_pred_1000 = model2.predict(x_val)\n\nfig, axes = plt.subplots(1,2,figsize = (12, 4))\n\nax1, ax2 = axes\n\nax1.plot(y_test, label = r\"$y$\")\nax1.plot(y_pred_5, label = r\"$\\hat y_{5}$\")\nax1.legend()\n\nax2.plot(y_test, label = r\"$y$\")\nax2.plot(y_pred_1000, label = r\"$\\hat y_{1000}$\")\nax2.legend()\n\nfig.tight_layout()\nplt.show()\n\n\n3/3 [==============================] - 0s 13ms/step\n3/3 [==============================] - 0s 6ms/step\n\n\n\n\n\n- 뭐 epoch를 1000번으로 해도 딱히 성능이 좋아보이지 않음\n\n일단 두 model의 성능을 비교해보자\n\n\nprint(\"model1 : epochs = 5\")\nprint(f'RMSE  : {mean_squared_error(y_val, y_pred_5, squared=False)}')\nprint(f'MAE   : {mean_absolute_error(y_val, y_pred_5)}')\nprint(\"\\n-------------------------\\n\")\nprint(\"model1 : epochs = 1000\")\nprint(f'RMSE  : {mean_squared_error(y_val, y_pred_1000, squared=False)}')\nprint(f'MAE   : {mean_absolute_error(y_val, y_pred_1000)}')\n\nmodel1 : epochs = 5\nRMSE  : 7.343708509578071\nMAE   : 6.769834949548357\n\n-------------------------\n\nmodel1 : epochs = 1000\nRMSE  : 2.216046941057742\nMAE   : 1.7544673845767975"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-03-00. 딥러닝 (1).html#excercise-2.-mobile-분류",
    "href": "posts/DX/05. 딥러닝/2023-10-03-00. 딥러닝 (1).html#excercise-2.-mobile-분류",
    "title": "00. 딥러닝 (1)",
    "section": "excercise 2. mobile (분류)",
    "text": "excercise 2. mobile (분류)\n\n(1) 데이터 이해 및 준비\n\npath = \"https://raw.githubusercontent.com/DA4BAM/dataset/master/mobile_churn_simple.csv\"\ndata = pd.read_csv(path)\ndata['CHURN'] = data['CHURN'].map({'STAY':0, 'LEAVE':1})\ndata.head()\n\n\n  \n    \n\n\n\n\n\n\nINCOME\nOVERAGE\nLEFTOVER\nHOUSE\nHANDSET_PRICE\nOVER_15MINS_CALLS_PER_MONTH\nAVERAGE_CALL_DURATION\nCHURN\n\n\n\n\n0\n31953\n0\n6\n313378\n161\n0\n4\n0\n\n\n1\n36147\n0\n13\n800586\n244\n0\n6\n0\n\n\n2\n27273\n230\n0\n305049\n201\n16\n15\n0\n\n\n3\n120070\n38\n33\n788235\n780\n3\n2\n1\n\n\n4\n29215\n208\n85\n224784\n241\n21\n1\n0\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ntarget = 'CHURN'\nx = data.drop(target, axis=1)\ny = data[target]\n\n- 데이터 분할\n\nx_train, x_val, y_train, y_val = train_test_split(x,y, test_size = .2)\n\n- scaling\n\nscaler = MinMaxScaler()\nx_train = scaler.fit_transform(x_train)\nx_val = scaler.fit_transform(x_val)\n\n\n\n(2) 모델링\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.backend import clear_session\n\n\nnf = x_train.shape[1]\n\n\ny_train.unique()\n\narray([0, 1])\n\n\n- 여기서도 epochs에 따른 모델들 사이 성능 차이 비교\n\nimport tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\ndevice_name\n\n'/device:GPU:0'\n\n\n\ne = [5,100]\n\nfor i in range(2) :\n  with tf.device('/device:GPU:0'):\n    exec(f\"model{i} = Sequential([ Dense(1, input_shape = (nf,), activation = 'sigmoid') ])\")\n    exec(f\"model{i}.compile(optimizer='adam', loss='binary_crossentropy')\")\n    exec(f\"model{i}.fit(x_train,y_train,epochs = {e[i]},verbose=1)\")\n\nEpoch 1/5\n500/500 [==============================] - 3s 4ms/step - loss: 0.7537\nEpoch 2/5\n500/500 [==============================] - 1s 2ms/step - loss: 0.7043\nEpoch 3/5\n500/500 [==============================] - 1s 2ms/step - loss: 0.6776\nEpoch 4/5\n500/500 [==============================] - 1s 2ms/step - loss: 0.6612\nEpoch 5/5\n500/500 [==============================] - 1s 2ms/step - loss: 0.6513\nEpoch 1/100\n500/500 [==============================] - 2s 3ms/step - loss: 0.6921\nEpoch 2/100\n500/500 [==============================] - 2s 3ms/step - loss: 0.6716\nEpoch 3/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6589\nEpoch 4/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6509\nEpoch 5/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6457\nEpoch 6/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6423\nEpoch 7/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6399\nEpoch 8/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6383\nEpoch 9/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6372\nEpoch 10/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6363\nEpoch 11/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6358\nEpoch 12/100\n500/500 [==============================] - 1s 3ms/step - loss: 0.6354\nEpoch 13/100\n500/500 [==============================] - 1s 3ms/step - loss: 0.6351\nEpoch 14/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6348\nEpoch 15/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6346\nEpoch 16/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6344\nEpoch 17/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6343\nEpoch 18/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6341\nEpoch 19/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6341\nEpoch 20/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6340\nEpoch 21/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6339\nEpoch 22/100\n500/500 [==============================] - 1s 3ms/step - loss: 0.6339\nEpoch 23/100\n500/500 [==============================] - 2s 3ms/step - loss: 0.6338\nEpoch 24/100\n500/500 [==============================] - 1s 3ms/step - loss: 0.6338\nEpoch 25/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6337\nEpoch 26/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6337\nEpoch 27/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6337\nEpoch 28/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6336\nEpoch 29/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6336\nEpoch 30/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6336\nEpoch 31/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6336\nEpoch 32/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6336\nEpoch 33/100\n500/500 [==============================] - 1s 3ms/step - loss: 0.6335\nEpoch 34/100\n500/500 [==============================] - 2s 3ms/step - loss: 0.6335\nEpoch 35/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 36/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6336\nEpoch 37/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 38/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 39/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 40/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 41/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 42/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 43/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 44/100\n500/500 [==============================] - 2s 3ms/step - loss: 0.6335\nEpoch 45/100\n500/500 [==============================] - 1s 3ms/step - loss: 0.6335\nEpoch 46/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 47/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 48/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 49/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 50/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 51/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 52/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 53/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 54/100\n500/500 [==============================] - 1s 3ms/step - loss: 0.6335\nEpoch 55/100\n500/500 [==============================] - 2s 3ms/step - loss: 0.6335\nEpoch 56/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 57/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 58/100\n500/500 [==============================] - 1s 3ms/step - loss: 0.6335\nEpoch 59/100\n500/500 [==============================] - 2s 3ms/step - loss: 0.6335\nEpoch 60/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 61/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 62/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 63/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 64/100\n500/500 [==============================] - 1s 3ms/step - loss: 0.6335\nEpoch 65/100\n500/500 [==============================] - 2s 3ms/step - loss: 0.6335\nEpoch 66/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 67/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 68/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 69/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 70/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 71/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 72/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 73/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 74/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 75/100\n500/500 [==============================] - 1s 3ms/step - loss: 0.6335\nEpoch 76/100\n500/500 [==============================] - 1s 3ms/step - loss: 0.6334\nEpoch 77/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 78/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 79/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 80/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 81/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 82/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 83/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 84/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 85/100\n500/500 [==============================] - 1s 3ms/step - loss: 0.6335\nEpoch 86/100\n500/500 [==============================] - 2s 3ms/step - loss: 0.6334\nEpoch 87/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 88/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 89/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 90/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 91/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 92/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 93/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 94/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 95/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\nEpoch 96/100\n500/500 [==============================] - 2s 3ms/step - loss: 0.6334\nEpoch 97/100\n500/500 [==============================] - 1s 3ms/step - loss: 0.6334\nEpoch 98/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 99/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6334\nEpoch 100/100\n500/500 [==============================] - 1s 2ms/step - loss: 0.6335\n\n\n\ntrain_error_5 = model0.history.history[\"loss\"]\ntrain_error_100  = model1.history.history[\"loss\"]\n\n- epoch에 따른 train loss를 살펴본 결과 특정 epoch이후에는 loss값이 변화가 미비하다…\n\n대충보니 epoch = 10정도만 해도 될 것같음..\n\n\nplt.figure(figsize = (4,4))\nplt.plot(train_error_5,label = \"epochs = 5\")\nplt.plot(train_error_100,label = \"epochs = 100\")\nplt.legend()\nplt.show()\n\n\n\n\n- epoch = 10으로 셋팅 후 결과 리포트 작성\n\nmodel2 = Sequential([ Dense(1, input_shape = (nf,), activation = 'sigmoid') ])\nmodel2.compile(optimizer='adam', loss='binary_crossentropy')\nmodel2.fit(x_train,y_train,epochs = 10,verbose=1)\n\nEpoch 1/10\n500/500 [==============================] - 2s 2ms/step - loss: 0.7028\nEpoch 2/10\n500/500 [==============================] - 1s 3ms/step - loss: 0.6771\nEpoch 3/10\n500/500 [==============================] - 1s 2ms/step - loss: 0.6625\nEpoch 4/10\n500/500 [==============================] - 1s 2ms/step - loss: 0.6532\nEpoch 5/10\n500/500 [==============================] - 1s 2ms/step - loss: 0.6472\nEpoch 6/10\n500/500 [==============================] - 1s 3ms/step - loss: 0.6431\nEpoch 7/10\n500/500 [==============================] - 1s 3ms/step - loss: 0.6403\nEpoch 8/10\n500/500 [==============================] - 1s 2ms/step - loss: 0.6384\nEpoch 9/10\n500/500 [==============================] - 1s 2ms/step - loss: 0.6371\nEpoch 10/10\n500/500 [==============================] - 1s 2ms/step - loss: 0.6362\n\n\n&lt;keras.src.callbacks.History at 0x7dddd9705c00&gt;\n\n\n\nplt.figure(figsize = (4,4))\nplt.title(\"logistic model with DL (epochs = 10)\")\nplt.plot(model2.history.history[\"loss\"])\nplt.show()\n\n\n\n\n\ny_pred = np.where(model2.predict(x_val)&gt;= 0.5, 1, 0)\n\nprint(confusion_matrix(y_val, y_pred))\nprint('-'*50)\nprint(classification_report(y_val, y_pred))\n\n125/125 [==============================] - 0s 2ms/step\n[[1320  727]\n [ 729 1224]]\n--------------------------------------------------\n              precision    recall  f1-score   support\n\n           0       0.64      0.64      0.64      2047\n           1       0.63      0.63      0.63      1953\n\n    accuracy                           0.64      4000\n   macro avg       0.64      0.64      0.64      4000\nweighted avg       0.64      0.64      0.64      4000\n\n\n\n- 최종 결과 시각화\n\nfrom sklearn.metrics import *\npre = precision_score(y_val, y_pred)\nrecall = recall_score(y_val, y_pred)\nf1 = f1_score(y_val, y_pred)\nacc = accuracy_score(y_val, y_pred)\n\n\nmeasure = [\"precision\", \"recall\", \"F1-score\", \"accuracy\"]\nvalue = [pre,recall,f1,acc]\n\nresult = pd.DataFrame(value,columns = [\"value\"])\nresult[\"measure\"] = measure\nfig = result.plot(x = \"measure\", y = \"value\", kind = \"bar\",\n            backend = \"plotly\",color = \"measure\")\n\nfig.update_yaxes(range = [0.62,0.64])\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\nsummary\n1 epoch을 늘려도 성능이 그렇게 좋지 않음\n\nfrom keras.optimizers import Adam\nAdam?\n\n2 아마도 기존에 제공하는 학습률이 너무 작아서 로컬 미니멈에 빠지는 것 같음\n\n아래를 통해서 Adam에서 기본적으로 제공하는 학습률이 0.001임을 확인 가능\n\nfrom keras.optimizers import Adam\nAdam?\n3 또한, 모델 적합시 검증용 데이터를 이용하지 않아 학습시 잘못된 학습을 한 경우에도 그냥 반영하는 것 같음\n4 생각해볼 수 있는 접근법\n\n그렇다면, 학습 시 검증용 데이터를 이용\n그리고, epoch와 learning_rate를 적절히 이용한다면..모델 성능을 높일 수 있지 않을까??\n그리고 은닉층을 여러개 추가한다면???"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-03-00. 딥러닝 (1).html#excercise-3.-boston-회귀",
    "href": "posts/DX/05. 딥러닝/2023-10-03-00. 딥러닝 (1).html#excercise-3.-boston-회귀",
    "title": "00. 딥러닝 (1)",
    "section": "excercise 3. Boston (회귀)",
    "text": "excercise 3. Boston (회귀)\n\n(1) 데이터 이해 및 준비\n\npath = 'https://raw.githubusercontent.com/DA4BAM/dataset/master/boston.csv'\ndata = pd.read_csv(path)\ndata.head()\n\n\n  \n    \n\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n5.33\n36.2\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ntarget = \"medv\"\n\nx = data.drop(target, axis = 1)\ny= data[target]\n\n- 데이터 분할\n\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=.2, random_state = 20)\n\n- scaling\n\n# 스케일러 선언\nscaler = MinMaxScaler()\n\n# train 셋으로 fitting & 적용\nx_train = scaler.fit_transform(x_train)\n\n# validation 셋은 적용만!\nx_val = scaler.transform(x_val)\n\n\n\n(2) 모델링\n- 이번시간에 알아볼 것!\n\n학습률은 0.01, activation = \"relu\", optimizer = \"adam\", epochs = 50 고정!\nvalidation_split = 0.2로 고정\n은닉층과 은닉노드 개수에 따라 어떻게 모델 성능이 변화하는지 비교해보자.\n\n\n\n\nmodel\n은닉층\n각 층에서 은닉노드의 개수\nparams\n\n\n\n\nmodel1\n2\n(2,1)\n26 + 3 = 29\n\n\nmodel2\n2\n(8,1)\n104 + 9 = 113\n\n\nmodel3\n2\n(8,4,1)\n104 + 36 + 5 = 145\n\n\n\n\nimport\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.backend import clear_session\nfrom keras.optimizers import Adam\n\n\n\nmodel1 설계\n\nnf = x_train.shape[1]\nlr = 0.01\nat = \"relu\" ## 활성화 함수\ne = 50 ## epochs\n\n\n\n\nmodel\n은닉층\n각 층에서 은닉노드의 개수\nparams\n\n\n\n\nmodel1\n2\n(2,1)\n26 + 3 = 29\n\n\nmodel2\n2\n(8,1)\n104 + 9 = 113\n\n\nmodel3\n2\n(8,4,1)\n104 + 36 + 5 = 145\n\n\n\n\nclear_session()\nmodel1 = Sequential(Dense(2, input_shape =(nf,),activation=at)) ## 첫 번째 은닉층\nmodel1.add(Dense(1,activation=at))\n\nmodel1.summary() ## 각 층에서 param 개수와 전체 param 개수 확인\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 2)                 26        \n                                                                 \n dense_1 (Dense)             (None, 1)                 3         \n                                                                 \n=================================================================\nTotal params: 29 (116.00 Byte)\nTrainable params: 29 (116.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n- compile & fit\n\nmodel1.compile(optimizer = Adam(learning_rate = lr), loss = \"mse\")\n\n\nimport tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\ndevice_name\n\n'/device:GPU:0'\n\n\n\nwith tf.device('/device:GPU:0') :\n     model1.fit(x_train,y_train,epochs = e, validation_split = 0.2, verbose = 0)\n\n- train 및 val error 저장\n\ntotal_train = pd.DataFrame()\ntotal_val = pd.DataFrame()\nmodel1_train_loss = model1.history.history[\"loss\"]\nmodel1_val_loss = model1.history.history[\"val_loss\"]\n\ntotal_train.loc[:,0] = model1_train_loss\ntotal_val.loc[:,0] = model1_val_loss\n\n- predict\n\ntotal_pred = pd.DataFrame()\nmodel1_pred = model1.predict(x_val).reshape(-1)\ntotal_pred.loc[:,0] = model1_pred\n#total_pred\n\n4/4 [==============================] - 0s 2ms/step\n\n\n\n\nmodel2 설계\n\n\n\nmodel\n은닉층\n각 층에서 은닉노드의 개수\nparams\n\n\n\n\nmodel1\n2\n(2,1)\n26 + 3 = 29\n\n\nmodel2\n2\n(8,1)\n104 + 9 = 113\n\n\nmodel3\n2\n(8,4,1)\n104 + 36 + 5 = 145\n\n\n\n\nmodel2 = Sequential(Dense(8, input_shape = (nf,), activation = at))\n\nmodel2.add(Dense(1,activation = at))\n\nmodel2.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_2 (Dense)             (None, 8)                 104       \n                                                                 \n dense_3 (Dense)             (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 113 (452.00 Byte)\nTrainable params: 113 (452.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n- compile & fit\n\nmodel2.compile(optimizer = Adam(learning_rate = lr), loss = \"mse\")\n\n\nwith tf.device('/device:GPU:0') :\n     model2.fit(x_train,y_train,epochs = e, validation_split = 0.2, verbose = 0)\n\n\nmodel2_train_loss = model2.history.history[\"loss\"]\nmodel2_val_loss = model2.history.history[\"val_loss\"]\n\ntotal_train.loc[:,1] = model2_train_loss\ntotal_val.loc[:,1] = model2_val_loss\n\n\nmodel2_pred = model2.predict(x_val).reshape(-1)\ntotal_pred.loc[:,1] = model2_pred\n\n4/4 [==============================] - 0s 3ms/step\n\n\n\n\nmodel3 설계\n\n\n\nmodel\n은닉층\n각 층에서 은닉노드의 개수\nparams\n\n\n\n\nmodel1\n2\n(2,1)\n26 + 3 = 29\n\n\nmodel2\n2\n(8,1)\n104 + 9 = 113\n\n\nmodel3\n3\n(8,4,1)\n104 + 36 + 5 = 145\n\n\n\n\nmodel3 = Sequential(Dense(8, input_shape = (nf,), activation = at))\n\nmodel3.add(Dense(4, activation = at))\n\nmodel3.add(Dense(1, activation = at))\nmodel3.summary()\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_4 (Dense)             (None, 8)                 104       \n                                                                 \n dense_5 (Dense)             (None, 4)                 36        \n                                                                 \n dense_6 (Dense)             (None, 1)                 5         \n                                                                 \n=================================================================\nTotal params: 145 (580.00 Byte)\nTrainable params: 145 (580.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nmodel3.compile(optimizer=Adam(learning_rate=lr), loss = \"mse\")\n\n\nwith tf.device('/device:GPU:0') :\n     model3.fit(x_train,y_train,epochs = e, validation_split = 0.2, verbose = 0)\n\n\nmodel3_train_loss = model3.history.history[\"loss\"]\nmodel3_val_loss = model3.history.history[\"val_loss\"]\n\ntotal_train.loc[:,2] = model3_train_loss\ntotal_val.loc[:,2] = model3_val_loss\n\nmodel3_pred = model3.predict(x_val).reshape(-1)\ntotal_pred.loc[:,2] = model3_pred\n\n4/4 [==============================] - 0s 3ms/step\n\n\n-결과 시각화\n\n\n\nmodel\n은닉층\n각 층에서 은닉노드의 개수\nparams\n\n\n\n\nmodel1\n2\n(2,1)\n26 + 3 = 29\n\n\nmodel2\n2\n(8,1)\n104 + 9 = 113\n\n\nmodel3\n3\n(8,4,1)\n104 + 36 + 5 = 145\n\n\n\n\ntotal_train.columns = [\"model1\", \"model2\",\"model3\"]\ntotal_val.columns = [\"model1\", \"model2\",\"model3\"]\ntotal_pred.columns = [\"model1\", \"model2\",\"model3\"]\n\n- tidydata 생성\n\n\nCode\ntotal_train2 = total_train.melt(var_name = \"model\",\n                 value_name =\"loss\")\ntotal_train2[\"label\"] = \"train\"\ntotal_train2[\"epochs\"] = list(range(1,51))*3\n\n\ntotal_val2 = total_val.melt(var_name = \"model\",\n                 value_name =\"loss\")\ntotal_val2[\"label\"] = \"val\"\ntotal_val2[\"epochs\"] = list(range(1,51))*3\n\n\ntotal1 = pd.concat([total_train2, total_val2],axis = 0)\n\n\n- train, val loss 시각화\n\nfig = total1.plot(x= \"epochs\", y = \"loss\",\n            color = \"label\", facet_col = \"model\",kind = \"scatter\",\n            backend = \"plotly\", width = 1200, height = 400, opacity=0.5)\nfig.show()\n\n\n\n\n\n                                \n                                            \n\n\n\n\n- train, val loss만 보면 model2가 가장 잘 적합된 모델인 것 같다.\n- test data 예측결과 시각화\n\ntotal_pred[\"y_true\"] = y_val.reset_index(drop=True)\n\n\n\nCode\nfig, axes = plt.subplots(1,3,figsize = (12,4))\n\nax1, ax2, ax3 = axes\nax1.plot(total_pred[\"y_true\"], label =  r\"$y_{true}$\", alpha = 0.5)\nax1.plot(total_pred[\"model1\"],\"--g\", label =  r\"$y_{model1}$\")\nax1.legend()\n\nax2.plot(total_pred[\"y_true\"], label =  r\"$y_{true}$\", alpha = 0.5)\nax2.plot(total_pred[\"model2\"],\"--r\", label =  r\"$y_{model2}$\")\nax2.legend()\n\nax3.plot(total_pred[\"y_true\"], label =  r\"$y_{true}$\", alpha = 0.5)\nax3.plot(total_pred[\"model3\"],\"--b\", label =  r\"$y_{model3}$\")\nax3.legend()\n\nfig.tight_layout()\n\n\n\n\n\n- test data에 대한 예측결과 model3가 가장 잘 예측하고 있는것 같다.\n\n근데 이것을 수치적으로 확인하기 위해 MAE 수치만 비교해보자.\n\n\nmodel1_mae = mean_absolute_error(y_val, model1_pred)\nmodel2_mae = mean_absolute_error(y_val, model2_pred)\nmodel3_mae = mean_absolute_error(y_val, model3_pred)\n\nmae = [model1_mae, model2_mae, model3_mae]\nmodel = [\"model1\", \"model2\",\"model3\"]\n\nfig = pd.DataFrame({\"model\": model,\"mae\" : mae}).\\\n      plot(kind = \"bar\",backend = \"plotly\",\n           x = \"model\", y= \"mae\",color = \"model\",width = 400, height = 400,\n              title = \"Mean Absolute error by model\")\nfig.update_yaxes(range = (3,4.5))\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n\n(3) summary\n\n\n\nmodel\n은닉층\n각 층에서 은닉노드의 개수\nparams\n\n\n\n\nmodel1\n2\n(2,1)\n26 + 3 = 29\n\n\nmodel2\n2\n(8,1)\n104 + 9 = 113\n\n\nmodel3\n3\n(8,4,1)\n104 + 36 + 5 = 145\n\n\n\n\nepochs, activation, learning_rate, optimizer를 고정시켜놓고 은닉층의 수와 노드의 수에 따라 예측 성능을 비교해봤음\n비교결과 model2가 가장 학습 데이터에 적합했으나 실제로는 model3와 비교해보면 과적합 문제가 의심됨.\ninsight1 : 은닉층의 수와 노드의 수 즉, 파라미터 수를 어떻게 조절하냐에 따라서도 모델 성능이 달라질 수 있음!!"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-19-07. 머신러닝 (6).html",
    "href": "posts/DX/04. 머신러닝/2023-09-19-07. 머신러닝 (6).html",
    "title": "07. 머신러닝 (6)",
    "section": "",
    "text": "# 기본 라이브러리 가져오기\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import *\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n\ndata = pd.read_excel('bankrupt.xlsx')\ndata.head()\n\n\n\n\n\n\n\n\ntarget\ncol1\ncol2\ncol3\ncol4\ncol5\ncol6\ncol7\ncol8\ncol9\n...\ncol84\ncol85\ncol86\ncol87\ncol88\ncol89\ncol90\ncol91\ncol92\ncol93\n\n\n\n\n0\n1\n0.370594\n0.424389\n0.405750\n0.601457\n0.601457\n0.998969\n0.796887\n0.808809\n0.302646\n...\n0.118250\n0.716845\n0.009219\n0.622879\n0.601453\n0.827890\n0.290202\n0.026601\n0.564050\n0.016469\n\n\n1\n1\n0.464291\n0.538214\n0.516730\n0.610235\n0.610235\n0.998946\n0.797380\n0.809301\n0.303556\n...\n0.047775\n0.795297\n0.008323\n0.623652\n0.610237\n0.839969\n0.283846\n0.264577\n0.570175\n0.020794\n\n\n2\n1\n0.426071\n0.499019\n0.472295\n0.601450\n0.601364\n0.998857\n0.796403\n0.808388\n0.302035\n...\n0.025346\n0.774670\n0.040003\n0.623841\n0.601449\n0.836774\n0.290189\n0.026555\n0.563706\n0.016474\n\n\n3\n1\n0.399844\n0.451265\n0.457733\n0.583541\n0.583541\n0.998700\n0.796967\n0.808966\n0.303350\n...\n0.067250\n0.739555\n0.003252\n0.622929\n0.583538\n0.834697\n0.281721\n0.026697\n0.564663\n0.023982\n\n\n4\n1\n0.465022\n0.538432\n0.522298\n0.598783\n0.598783\n0.998973\n0.797366\n0.809304\n0.303475\n...\n0.047725\n0.795016\n0.003878\n0.623521\n0.598782\n0.839973\n0.278514\n0.024752\n0.575617\n0.035490\n\n\n\n\n5 rows × 94 columns\n\n\n\n\n#data.info()\n\n\ntarget = \"target\"\n\nx = data.drop(target, axis = 1)\ny = data[target]\n\n\nx_train, x_val, y_train, y_val = train_test_split(x,y, random_state = 1, test_size = .3)\n\n- 거리계산 기반 차원축소이므로 스케일링이 필요\n\nscaler = MinMaxScaler()\n\nx_train_s = scaler.fit_transform(x_train)\nx_val_s = scaler.fit_transform(x_val)\n\n# 옵션 데이터프레임 변환\nx_train_s = pd.DataFrame(x_train_s, columns = list(x))\nx_val_s = pd.DataFrame(x_val_s, columns = list(x))\n\n\n\n\n주성분 만들기\n\n전체 변수의 수로 주성분을 생성\n분산 누적그래프를 그리고 적절한 주성분 수 선택\n\n적용\n\n저차원 시각화\n지도학습 연계\n\n\n\nfrom sklearn.decomposition import PCA\n\nn = x_train_s.shape[1]\nfor i in range(1,n+1):\n    exec(f\"pca{i} = PCA(n_components = {i})\")\n    exec(f\"x_train_pca{i} = pca{i}.fit_transform(x_train_s)\")\n\n- 적절한 주성분의 개수는 7개가 좋아보임\n\nplt.figure(figsize = (12, 4))\nplt.plot(pca93.explained_variance_ratio_,\"--.\",alpha=0.3)\nplt.xticks(range(1,n+1,2))\nplt.show()\n\n\n\n\n- 주성분 개수를 12개로 설정!\n\nx_train_pc = pd.DataFrame(x_train_pca12)\nx_val_pc = pd.DataFrame(pca12.fit_transform(x_val_s))\nx_train_pc.columns = [\"PC\" + str(i) for i  in range(1,13)]\nx_val_pc.columns = [\"PC\" + str(i) for i  in range(1,13)]\n\n- 상위 2개의 주성분을 뽑아 시각화 해보기\n\nsns.scatterplot(x = 'PC1', y = 'PC2', data = x_train_pc, hue = y_train)\n\n&lt;Axes: xlabel='PC1', ylabel='PC2'&gt;\n\n\n\n\n\n- 주성분간 상관관계 확인\n\nplt.figure(figsize = (12, 4))\nsns.heatmap(x_train_pc.corr(),\n            annot = True,\n            fmt = \".2f\",\n            cmap = \"Blues\")\n\n&lt;Axes: &gt;\n\n\n\n\n\n- 지도학습으로 연계\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import *\n\nmodel = LogisticRegression()\nmodel.fit(x_train_s, y_train)\n\ny_pred = model.predict(x_val_s)\n\n\nprint(classification_report(y_val, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.97      1.00      0.98      1975\n           1       0.25      0.01      0.03        71\n\n    accuracy                           0.96      2046\n   macro avg       0.61      0.51      0.50      2046\nweighted avg       0.94      0.96      0.95      2046\n\n\n\n\nacc,pre,recall, f1 = accuracy_score(y_val,y_pred),precision_score(y_val,y_pred), recall_score(y_val, y_pred),f1_score(y_val,y_pred)\n\n\n\nCode\npd.DataFrame({\"score\" : [acc, pre, recall, f1],\n              \"measure\" : [\"accuracy\", \"precision\",\"recall\",\"f1_score\"]}).\\\n                sort_values(\"score\",ascending = False).\\\n                plot(x = \"score\", y = \"measure\", kind = \"barh\",\n                     backend = \"plotly\", color = \"measure\")\n\n\n\n                                                \n\n\n- 주성분분석으로 데이터 모델링\n\nKNN\n\n\nmodel2 = LogisticRegression()\nmodel2.fit(x_train_pc, y_train)\n\ny_pred = model2.predict(x_val_pc)\n\n\nconfusion_matrix(y_val, y_pred)\n\narray([[1970,    5],\n       [  71,    0]], dtype=int64)\n\n\n\nprint(classification_report(y_val, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.97      1.00      0.98      1975\n           1       0.00      0.00      0.00        71\n\n    accuracy                           0.96      2046\n   macro avg       0.48      0.50      0.49      2046\nweighted avg       0.93      0.96      0.95      2046"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-19-07. 머신러닝 (6).html#pca",
    "href": "posts/DX/04. 머신러닝/2023-09-19-07. 머신러닝 (6).html#pca",
    "title": "07. 머신러닝 (6)",
    "section": "",
    "text": "주성분 만들기\n\n전체 변수의 수로 주성분을 생성\n분산 누적그래프를 그리고 적절한 주성분 수 선택\n\n적용\n\n저차원 시각화\n지도학습 연계\n\n\n\nfrom sklearn.decomposition import PCA\n\nn = x_train_s.shape[1]\nfor i in range(1,n+1):\n    exec(f\"pca{i} = PCA(n_components = {i})\")\n    exec(f\"x_train_pca{i} = pca{i}.fit_transform(x_train_s)\")\n\n- 적절한 주성분의 개수는 7개가 좋아보임\n\nplt.figure(figsize = (12, 4))\nplt.plot(pca93.explained_variance_ratio_,\"--.\",alpha=0.3)\nplt.xticks(range(1,n+1,2))\nplt.show()\n\n\n\n\n- 주성분 개수를 12개로 설정!\n\nx_train_pc = pd.DataFrame(x_train_pca12)\nx_val_pc = pd.DataFrame(pca12.fit_transform(x_val_s))\nx_train_pc.columns = [\"PC\" + str(i) for i  in range(1,13)]\nx_val_pc.columns = [\"PC\" + str(i) for i  in range(1,13)]\n\n- 상위 2개의 주성분을 뽑아 시각화 해보기\n\nsns.scatterplot(x = 'PC1', y = 'PC2', data = x_train_pc, hue = y_train)\n\n&lt;Axes: xlabel='PC1', ylabel='PC2'&gt;\n\n\n\n\n\n- 주성분간 상관관계 확인\n\nplt.figure(figsize = (12, 4))\nsns.heatmap(x_train_pc.corr(),\n            annot = True,\n            fmt = \".2f\",\n            cmap = \"Blues\")\n\n&lt;Axes: &gt;\n\n\n\n\n\n- 지도학습으로 연계\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import *\n\nmodel = LogisticRegression()\nmodel.fit(x_train_s, y_train)\n\ny_pred = model.predict(x_val_s)\n\n\nprint(classification_report(y_val, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.97      1.00      0.98      1975\n           1       0.25      0.01      0.03        71\n\n    accuracy                           0.96      2046\n   macro avg       0.61      0.51      0.50      2046\nweighted avg       0.94      0.96      0.95      2046\n\n\n\n\nacc,pre,recall, f1 = accuracy_score(y_val,y_pred),precision_score(y_val,y_pred), recall_score(y_val, y_pred),f1_score(y_val,y_pred)\n\n\n\nCode\npd.DataFrame({\"score\" : [acc, pre, recall, f1],\n              \"measure\" : [\"accuracy\", \"precision\",\"recall\",\"f1_score\"]}).\\\n                sort_values(\"score\",ascending = False).\\\n                plot(x = \"score\", y = \"measure\", kind = \"barh\",\n                     backend = \"plotly\", color = \"measure\")\n\n\n\n                                                \n\n\n- 주성분분석으로 데이터 모델링\n\nKNN\n\n\nmodel2 = LogisticRegression()\nmodel2.fit(x_train_pc, y_train)\n\ny_pred = model2.predict(x_val_pc)\n\n\nconfusion_matrix(y_val, y_pred)\n\narray([[1970,    5],\n       [  71,    0]], dtype=int64)\n\n\n\nprint(classification_report(y_val, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.97      1.00      0.98      1975\n           1       0.00      0.00      0.00        71\n\n    accuracy                           0.96      2046\n   macro avg       0.48      0.50      0.49      2046\nweighted avg       0.93      0.96      0.95      2046"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-15-05. 종합실습.html",
    "href": "posts/DX/04. 머신러닝/2023-09-15-05. 종합실습.html",
    "title": "05. 종합실습",
    "section": "",
    "text": "0. import\n\n# 라이브러리 불러오기\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport plotly.io as pio\nimport plotly.express as px\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n\nwarnings.filterwarnings(action='ignore')\n%config InlineBackend.figure_format = 'retina'\n\n\n\n1. 데이터 이해\n\n# 데이터 불러오기\npath = 'https://raw.githubusercontent.com/jangrae/csv/master/airline_satisfaction_small.csv'\ndata = pd.read_csv(path)\n\n\n# 데이터 살펴보기\ndata.head()\n\n\n\n\n\n\n\n\nid\ngender\ncustomer_type\nage\ntype_of_travel\nclass\nflight_distance\ninflight_wifi_service\ndeparture/arrival_time_convenient\nease_of_online_booking\n...\ninflight_entertainment\non-board_service\nleg_room_service\nbaggage_handling\ncheckin_service\ninflight_service\ncleanliness\ndeparture_delay_in_minutes\narrival_delay_in_minutes\nsatisfaction\n\n\n\n\n0\n70172\nMale\nLoyal Customer\n13\nPersonal Travel\nEco Plus\n460\n3\n4\n3\n...\n5\n4\n3\n4\n4\n5\n5\n25\n18.0\n0\n\n\n1\n5047\nMale\ndisloyal Customer\n25\nBusiness travel\nBusiness\n235\n3\n2\n3\n...\n1\n1\n5\n3\n1\n4\n1\n1\n6.0\n0\n\n\n2\n110028\nFemale\nLoyal Customer\n26\nBusiness travel\nBusiness\n1142\n2\n2\n2\n...\n5\n4\n3\n4\n4\n4\n5\n0\n0.0\n1\n\n\n3\n24026\nFemale\nLoyal Customer\n25\nBusiness travel\nBusiness\n562\n2\n5\n5\n...\n2\n2\n5\n3\n1\n4\n2\n11\n9.0\n0\n\n\n4\n119299\nMale\nLoyal Customer\n61\nBusiness travel\nBusiness\n214\n3\n3\n3\n...\n3\n3\n4\n4\n3\n3\n3\n0\n0.0\n1\n\n\n\n\n5 rows × 24 columns\n\n\n\n데이터 설명\n\nid : 탑승자 고유 아이디\ngender: 성별 (Female, Male)\ncustomer_type: 고객 유형 (Loyal customer, disloyal customer)\nage: 탑승자 나이\ntype_of_travel: 비행 목적(Personal Travel, Business Travel)\nclass: 등급 (Business, Eco, Eco Plus)\nflight_distance: 비행 거리\ninflight_wifi_service: 와이파이 서비스 만족도 (0:N/A; 1-5)\ndeparture/arrival_time_convenient: 출발, 도착 시간 만족도 (0:N/A; 1-5)\nease_of_online_booking: 온라인 부킹 만족도 (0:N/A; 1-5)\ngate_location: 게이트 위치 만족도 (0:N/A; 1-5)\nfood_and_drink: 식사와 음료 만족도 (0:N/A; 1-5)\nonline_boarding: 온라인 보딩 만족도 (0:N/A; 1-5)\nseat_comfort: 좌석 편안함 만족도 (0:N/A; 1-5)\ninflight_entertainment: 기내 엔터테인먼트 만족도 (0:N/A; 1-5)\non-board_service: 온 보드 서비스 만족도 (0:N/A; 1-5)\nleg_room_service: 다리 공간 만족도 (0:N/A; 1-5)\nbaggage_handling: 수하물 처리 만족도 (0:N/A; 1-5)\ncheck-in_service: 체크인 서비스 만족도 (0:N/A; 1-5)\ninflight_service: 기내 서비스 만족도 (0:N/A; 1-5)\ncleanliness: 청결 만족도 (0:N/A; 1-5)\ndeparture_delay_in_minutes: 출발 지연 시간(분)\narrival_delay_in_minutes: 도착 지연 시간(분)\nsatisfaction: 항공 만족도(1: Satisfaction, 0: Neutral or Dissatisfaction) - Target\n\n\n\n2. 데이터 준비\n\n# 변수 제거\nd_cols = [\"id\", \"departure/arrival_time_convenient\", \"gate_location\", \"departure_delay_in_minutes\"]\n\n\ndata.drop(d_cols,axis=1, inplace = True)\ndata.head()\n\n\n\n\n\n\n\n\ngender\ncustomer_type\nage\ntype_of_travel\nclass\nflight_distance\ninflight_wifi_service\nease_of_online_booking\nfood_and_drink\nonline_boarding\nseat_comfort\ninflight_entertainment\non-board_service\nleg_room_service\nbaggage_handling\ncheckin_service\ninflight_service\ncleanliness\narrival_delay_in_minutes\nsatisfaction\n\n\n\n\n0\nMale\nLoyal Customer\n13\nPersonal Travel\nEco Plus\n460\n3\n3\n5\n3\n5\n5\n4\n3\n4\n4\n5\n5\n18.0\n0\n\n\n1\nMale\ndisloyal Customer\n25\nBusiness travel\nBusiness\n235\n3\n3\n1\n3\n1\n1\n1\n5\n3\n1\n4\n1\n6.0\n0\n\n\n2\nFemale\nLoyal Customer\n26\nBusiness travel\nBusiness\n1142\n2\n2\n5\n5\n5\n5\n4\n3\n4\n4\n4\n5\n0.0\n1\n\n\n3\nFemale\nLoyal Customer\n25\nBusiness travel\nBusiness\n562\n2\n5\n2\n2\n2\n2\n2\n5\n3\n1\n4\n2\n9.0\n0\n\n\n4\nMale\nLoyal Customer\n61\nBusiness travel\nBusiness\n214\n3\n3\n4\n5\n5\n3\n3\n4\n4\n3\n3\n3\n0.0\n1\n\n\n\n\n\n\n\n\n# 결측치 제거\nprint(data.isna().sum())\n\nplt.hist(data[\"arrival_delay_in_minutes\"],bins = 50)\nplt.show()\n\ngender                      0\ncustomer_type               0\nage                         0\ntype_of_travel              0\nclass                       0\nflight_distance             0\ninflight_wifi_service       0\nease_of_online_booking      0\nfood_and_drink              0\nonline_boarding             0\nseat_comfort                0\ninflight_entertainment      0\non-board_service            0\nleg_room_service            0\nbaggage_handling            0\ncheckin_service             0\ninflight_service            0\ncleanliness                 0\narrival_delay_in_minutes    6\nsatisfaction                0\ndtype: int64\n\n\n\n\n\n- 그래프를 그려본 결과 대부분의 값들이 0값으로 몰려있고, 왼쪽으로 치우쳐진 형태이다.\n\n결측값을 0으로 대체\n\n\ndata[\"arrival_delay_in_minutes\"].fillna(0,inplace = True)\ndata.isna().sum()\n\ngender                      0\ncustomer_type               0\nage                         0\ntype_of_travel              0\nclass                       0\nflight_distance             0\ninflight_wifi_service       0\nease_of_online_booking      0\nfood_and_drink              0\nonline_boarding             0\nseat_comfort                0\ninflight_entertainment      0\non-board_service            0\nleg_room_service            0\nbaggage_handling            0\ncheckin_service             0\ninflight_service            0\ncleanliness                 0\narrival_delay_in_minutes    0\nsatisfaction                0\ndtype: int64\n\n\n- x, y 분리\n\ntarget = \"satisfaction\"\n\nx = data.drop(target, axis = 1)\ny = data[target]\n\n- 가변수화\n\nd = [\"gender\", \"customer_type\", \"type_of_travel\", \"class\"]\n\nx = pd.get_dummies(x, columns = d, drop_first = True, dtype = float)\nx.head()\n\n\n\n\n\n\n\n\nage\nflight_distance\ninflight_wifi_service\nease_of_online_booking\nfood_and_drink\nonline_boarding\nseat_comfort\ninflight_entertainment\non-board_service\nleg_room_service\nbaggage_handling\ncheckin_service\ninflight_service\ncleanliness\narrival_delay_in_minutes\ngender_Male\ncustomer_type_disloyal Customer\ntype_of_travel_Personal Travel\nclass_Eco\nclass_Eco Plus\n\n\n\n\n0\n13\n460\n3\n3\n5\n3\n5\n5\n4\n3\n4\n4\n5\n5\n18.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n1\n25\n235\n3\n3\n1\n3\n1\n1\n1\n5\n3\n1\n4\n1\n6.0\n1.0\n1.0\n0.0\n0.0\n0.0\n\n\n2\n26\n1142\n2\n2\n5\n5\n5\n5\n4\n3\n4\n4\n4\n5\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n25\n562\n2\n5\n2\n2\n2\n2\n2\n5\n3\n1\n4\n2\n9.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n61\n214\n3\n3\n4\n5\n5\n3\n3\n4\n4\n3\n3\n3\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n- 학습용, 평가용 데이터 분리\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 1, test_size = 0.3)\n\n- 정규화\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# 정규화\nscaler = MinMaxScaler()\nscaler.fit(x_train)\nx_train_s = scaler.transform(x_train)\nx_test_s = scaler.transform(x_test)\n\n\n\n3. 모델 성능 예측 (k-fold cv)\n\n# 1. knn\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nknn = KNeighborsClassifier(n_neighbors=5)\n\nknn.cv = cross_val_score(knn, x_train_s, y_train, cv = 5)\n\nknn.cv_m = knn.cv.mean()\n\n# 2. tree\nfrom sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(max_depth = 5, random_state = 1)\n\ntree.cv = cross_val_score(tree, x_train, y_train, cv = 5)\n\ntree.cv_m = tree.cv.mean()\n\n# 3. logistic\nfrom sklearn.linear_model import LogisticRegression\nlogit = LogisticRegression()\nlogit.cv = cross_val_score(logit, x_train, y_train, cv = 5)\n\nlogit.cv_m = logit.cv.mean()\n\n# 4. RF\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(max_depth = 5,  random_state = 1)\n\nrf.cv = cross_val_score(rf, x_train,y_train)\n\nrf.cv_m = rf.cv.mean()\n\n# 5. XGBoost\nfrom xgboost import XGBClassifier\nxgb =  XGBClassifier(max_depth = 5, random_state = 1)\n\nxgb.cv = cross_val_score(xgb, x_train, y_train, cv = 5)\n\nxgb.cv_m  = xgb.cv.mean()\n\nfrom lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(max_depth = 5, random_state = 1,verbose = -100) \n\nlgbm.cv = cross_val_score(lgbm, x_train, y_train, cv = 5)\n\nlgbm.cv_m = lgbm.cv.mean()\n\n\nresult = [knn.cv_m, tree.cv_m, logit.cv_m, rf.cv_m,  xgb.cv_m, lgbm.cv_m]\nmodel = [\"knn\",\"tree\",\"logit\", \"rf\", \"xgb\", \"lgbm\"]\n\nfig = pd.DataFrame({\"model\" : model, \"result\" : result}).\\\n            sort_values(\"result\", ascending = False).plot(x = \"result\", y = \"model\", kind = \"bar\", backend = \"plotly\",color = \"model\")\n\nfig.update_xaxes(range = [0.7, 1.0])\n\n\n                                                \n\n\n\n\n4. 모델 튜닝(여기부터 다시…)\n- XGB가 가장 성능이 잘 된 모델이라고 가정\n\nfrom sklearn.model_selection import GridSearchCV\nmodel = XGBClassifier(max_depth= 5, random_state = 1)\n\nparams=  {\"max_depth\" : range(1,21)}\n\nmodel = GridSearchCV(model,\n                     params,\n                     cv=5,\n                     scoring='r2')\n\n\nmodel.fit(x_train, y_train)\n\nGridSearchCV(cv=5,\n             estimator=XGBClassifier(base_score=None, booster=None,\n                                     callbacks=None, colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None, device=None,\n                                     early_stopping_rounds=None,\n                                     enable_categorical=False, eval_metric=None,\n                                     feature_types=None, gamma=None,\n                                     grow_policy=None, importance_type=None,\n                                     interaction_constraints=None,\n                                     learning_rate=None, max_bin=None,\n                                     max_cat_threshold=None,\n                                     max_cat_to_onehot=None,\n                                     max_delta_step=None, max_depth=5,\n                                     max_leaves=None, min_child_weight=None,\n                                     missing=nan, monotone_constraints=None,\n                                     multi_strategy=None, n_estimators=None,\n                                     n_jobs=None, num_parallel_tree=None,\n                                     random_state=1, ...),\n             param_grid={'max_depth': range(1, 21)}, scoring='r2')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=XGBClassifier(base_score=None, booster=None,\n                                     callbacks=None, colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None, device=None,\n                                     early_stopping_rounds=None,\n                                     enable_categorical=False, eval_metric=None,\n                                     feature_types=None, gamma=None,\n                                     grow_policy=None, importance_type=None,\n                                     interaction_constraints=None,\n                                     learning_rate=None, max_bin=None,\n                                     max_cat_threshold=None,\n                                     max_cat_to_onehot=None,\n                                     max_delta_step=None, max_depth=5,\n                                     max_leaves=None, min_child_weight=None,\n                                     missing=nan, monotone_constraints=None,\n                                     multi_strategy=None, n_estimators=None,\n                                     n_jobs=None, num_parallel_tree=None,\n                                     random_state=1, ...),\n             param_grid={'max_depth': range(1, 21)}, scoring='r2')estimator: XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=1, ...)XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=1, ...)\n\n\n\ny_pred = model.predict(x_test)\n\n\n# 예측 결과 확인\nprint(model.best_params_)\nprint(model.best_score_)\n\n{'max_depth': 3}\n0.7376528379505405\n\n\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nmeasure = [\"accuracy\", \"precision\", \"reacll\", \"f1-score\"]\n\nacc = accuracy_score(y_test, y_pred)\npre = precision_score(y_test, y_pred)\nre = recall_score(y_test, y_pred)\nf1_score = f1_score(y_test, y_pred)\n\n\npd.DataFrame({\"measure\" : measure, \"score\" : [acc, pre, re, f1_score]})\n\n\n\n\n\n\n\n\nmeasure\nscore\n\n\n\n\n0\naccuracy\n0.931525\n\n\n1\nprecision\n0.949102\n\n\n2\nreacll\n0.898017\n\n\n3\nf1-score\n0.922853"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-14-03. 머신러닝 (3).html",
    "href": "posts/DX/04. 머신러닝/2023-09-14-03. 머신러닝 (3).html",
    "title": "03. 머신러닝 (3)",
    "section": "",
    "text": "= 지금까지 모델을 선언하고 학습한 후 바로 평가를 진행함\n\n일반화 성능, 즉 이후 새로운 데이터에 대한 모델의 성능을 예측하지 못한 상태에서 최종 평가를 수행\n검증용 데이터가 모델의 일반화된 성능을 예측할 수 있게 도와 줌\n지만 이것 역시 단 하나의 데이터 셋에 대한 추정일 뿐\n하나의 데이터 셋에서 얻은 성능으로 정확도에 확신을 가질 수 없음\n국 더욱 정교한 평가 절차가 필요함\n\n\n\n\n모든 데이터가 평가에 한 번, 학습에 k-1번 사용\nK개의 분할(Fold)에 대한 성능을 예측 → 평균과 표준편차 계산 → 일반화 성능\n단, k는 2 이상이 되어야 함(최소한 한 개씩의 학습용, 검증용 데이터가 필요)\n\n\n\n\n\n장점\n\n\n모든 데이터를 학습과 평가에 사용할 수 있음\n반복 학습과 평가를 통해 정확도를 향상시킬 수 있음\n데이터가 부족해서 발생하는 과소적합 문제를 방지할 수 있음\n평가에 사용되는 데이터의 편향을 막을 수 있음\n좀 더 일반화된 모델을 만들 수 있음\n\n\n단점\n\n\n반복 횟수가 많아서 모델 학습과 평가에 많은 시간이 소요됨\n\n\n\n\n\n# 라이브러리 불러오기\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\nwarnings.filterwarnings(action='ignore')\n%config InlineBackend.figure_format='retina'\n\n\n\n데이터설명\n\nPregnancies: 임신 횟수\nGlucose: 포도당 부하 검사 수치\nBloodPressure: 혈압(mm Hg)\nSkinThickness: 팔 삼두근 뒤쪽의 피하지방 측정값(mm)\nInsulin: 혈청 인슐린(mu U/ml)\nBMI: 체질량지수(체중(kg)/키(m))^2\nDiabetesPedigreeFunction: 당뇨 내력 가중치 값\nAge: 나이\nOutcome: 클래스 결정 값(0 또는 1)\n\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/diabetes.csv'\ndata = pd.read_csv(path)\n\n\n# 데이터 살펴보기\ndata.head()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\n\n# 기술통계 확인\ndata.describe()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\ncount\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n\n\nmean\n3.845052\n120.894531\n69.105469\n20.536458\n79.799479\n31.992578\n0.471876\n33.240885\n0.348958\n\n\nstd\n3.369578\n31.972618\n19.355807\n15.952218\n115.244002\n7.884160\n0.331329\n11.760232\n0.476951\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.078000\n21.000000\n0.000000\n\n\n25%\n1.000000\n99.000000\n62.000000\n0.000000\n0.000000\n27.300000\n0.243750\n24.000000\n0.000000\n\n\n50%\n3.000000\n117.000000\n72.000000\n23.000000\n30.500000\n32.000000\n0.372500\n29.000000\n0.000000\n\n\n75%\n6.000000\n140.250000\n80.000000\n32.000000\n127.250000\n36.600000\n0.626250\n41.000000\n1.000000\n\n\nmax\n17.000000\n199.000000\n122.000000\n99.000000\n846.000000\n67.100000\n2.420000\n81.000000\n1.000000\n\n\n\n\n\n\n\n\n\n\n\n# Target 확인\ntarget = 'Outcome'\n\n# 데이터 분리\nx = data.drop(target, axis=1)\ny = data.loc[:, target]\n\n\n# 라이브러리 불러오기\nfrom sklearn.model_selection import train_test_split\n\n# 학습용, 평가용 데이터 7:3으로 분리\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n\n- 정규화\n\nKNN 알고리즘을 사용하기 위해 정규화를 진행\n\n\n# 모듈 불러오기\nfrom sklearn.preprocessing import MinMaxScaler\n\n# 정규화\nscaler = MinMaxScaler()\nscaler.fit(x_train)\nx_train_s = scaler.transform(x_train)\nx_test_s = scaler.transform(x_test)\n\n\n\n\n- 의사결정나무\n\n# 불러오기\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# 선언하기\nmodel = DecisionTreeClassifier(max_depth=5,  random_state=1)\n\n# 검증하기\ncv_score = cross_val_score(model, x_train, y_train, cv=10)\n\n# 확인\nprint(cv_score)\nprint('평균:', cv_score.mean())\nprint('표준편차:', cv_score.std())\n\n# 기록\nresult = {}\nresult[\"Decision Tree\"] = cv_score.mean()\n\n[0.66666667 0.75925926 0.74074074 0.64814815 0.7037037  0.74074074\n 0.75925926 0.81132075 0.79245283 0.67924528]\n평균: 0.7301537386443047\n표준편차: 0.05141448587329709\n\n\n- KNN\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel2 = KNeighborsClassifier(n_neighbors=5)\n\ncv_score = cross_val_score(model2, x_train_s, y_train,cv=10)\n\n# 확인\nprint(cv_score)\nprint('평균:', cv_score.mean())\nprint('표준편차:', cv_score.std())\nresult[\"KNN\"] = cv_score.mean()\n\n[0.64814815 0.68518519 0.72222222 0.64814815 0.72222222 0.74074074\n 0.68518519 0.66037736 0.77358491 0.60377358]\n평균: 0.6889587700908455\n표준편차: 0.04846522080635871\n\n\n- 로지스틱\n\nfrom sklearn.linear_model  import LogisticRegression\n\nmodel3 = LogisticRegression()\n\ncv_score = cross_val_score(model3, x_train, y_train, cv = 10)\n\nresult[\"Logistic\"] = cv_score.mean()\n\n- 결과\n\nk-fold 교차검증결과 : Logisitc 모형이 성능이 가장 좋아보인다.\n\n\nresult\n\n{'Decision Tree': 0.7301537386443047,\n 'KNN': 0.6889587700908455,\n 'Logistic': 0.7690426275331936}\n\n\n\n\n\n\nplt.figure(figsize = (4,4))\nplt.barh(y=list(result), width = result.values(), height = 0.5)\n\n&lt;BarContainer object of 3 artists&gt;\n\n\n\n\n\n\n\n\n- fitting을 다시 하는 이유\n\ntrain data set의 전체 데이터로 학습한 적은 없음……\n\n\ndef score(range = [0.5, 0.71]) : \n            fig = pd.DataFrame({\"score\" : [acc,pre,recall,f1_score],\n                            \"measure\" : [\"acc\",\"precision\",\"recall\",\"f1_score\"]}).\\\n                                    plot(x = \"measure\", y = \"score\",  color = \"measure\",\n                                            backend = \"plotly\", kind =  \"bar\",height = 500, width = 600)\n            fig.update_yaxes(range = range)\n            return fig\n\n\nmodel3.fit(x_train, y_train)\n\ny_pred = model3.predict(x_test)\n\nfrom sklearn.metrics import * \n\nacc = accuracy_score(y_test,y_pred)\npre = precision_score(y_test,y_pred)\nrecall = recall_score(y_test,y_pred)\nf1_score = f1_score(y_test,y_pred)\n\n\nscore(range = [0.5,0.8])\n\n\n                                                \n\n\n\n\n\n\n\n\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/boston.csv'\ndata = pd.read_csv(path)\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 506 entries, 0 to 505\nData columns (total 14 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   crim     506 non-null    float64\n 1   zn       506 non-null    float64\n 2   indus    506 non-null    float64\n 3   chas     506 non-null    int64  \n 4   nox      506 non-null    float64\n 5   rm       506 non-null    float64\n 6   age      506 non-null    float64\n 7   dis      506 non-null    float64\n 8   rad      506 non-null    int64  \n 9   tax      506 non-null    int64  \n 10  ptratio  506 non-null    float64\n 11  black    506 non-null    float64\n 12  lstat    506 non-null    float64\n 13  medv     506 non-null    float64\ndtypes: float64(11), int64(3)\nmemory usage: 55.5 KB\n\n\n\ndata.head()\n\n\n\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nblack\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n\n\n\n\ndata.isna().sum()\n\ncrim       0\nzn         0\nindus      0\nchas       0\nnox        0\nrm         0\nage        0\ndis        0\nrad        0\ntax        0\nptratio    0\nblack      0\nlstat      0\nmedv       0\ndtype: int64\n\n\n\n\n\n\ntarget = \"medv\"\n\nx = data.drop(target, axis = 1)\ny = data[[target]]\n\nfrom sklearn.preprocessing import MinMaxScaler\n# 정규화\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 1, test_size = 0.3)\n\nscaler = MinMaxScaler()\nscaler.fit(x_train)\nx_train_s = scaler.transform(x_train)\nx_test_s = scaler.transform(x_test)\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\nmodel1 = DecisionTreeRegressor(max_depth = 5)\nmodel2 = KNeighborsRegressor(n_neighbors=5)\nmodel3 = LinearRegression()\n\ncv_score1 = cross_val_score(model1, x_train, y_train, cv = 10, scoring = \"r2\")\ncv_score2 = cross_val_score(model2, x_train_s, y_train, cv = 10, scoring = \"r2\")\ncv_score3 = cross_val_score(model3, x_train, y_train, cv = 10, scoring = \"r2\")\n\nresult = pd.DataFrame({\"model\" : [\"tree\", \"knn\", \"linear reg\"],\n                            \"score\" : [cv_score1.mean(),cv_score2.mean(), cv_score3.mean()]})\n\n\nresult.sort_values(\"score\", ascending = False).\\\n                    plot(y = \"model\", x= \"score\", kind =\"barh\",\n                            backend = \"plotly\", color = \"model\")\n\n\n                                                \n\n\n\n\n\n\nmodel = DecisionTreeRegressor()\n\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\n\nmeasure = [\"MAE\", \"R2\"]\nscore = [mean_absolute_error(y_test, y_pred) ,r2_score(y_test, y_pred)]\n\n\npd.DataFrame({ \"measure\" : measure, \n                             \"score\" : score}).sort_values(\"score\", ascending = False).\\\n                                        plot( y= \"measure\", x=\"score\", kind = \"barh\", backend =\"plotly\", color = \"measure\")\n\n\n                                                \n\n\n\n\n\n\n\n\n데이터 설명\n\nCOLLEGE: 대학 졸업여부\nINCOME: 연수입\nOVERAGE: 월평균 초과사용 시간(분)\nLEFTOVER: 월평균 잔여시간비율(%)\nHOUSE: 집값\nHANDSET_PRICE: 스마트폰 가격\nOVER_15MINS_CALLS_PER_MONTH: 월평균 장기통화(15분이상) 횟수\nAVERAGE_CALL_DURATION: 평균 통화 시간\nREPORTED_SATISFACTION: 만족도 설문조사 결과\nREPORTED_USAGE_LEVEL: 사용도 자가진단 결과\nCONSIDERING_CHANGE_OF_PLAN: 향후 변경계획 설문조사 결과\nCHURN: 이탈(번호이동) 여부 (Target 변수)\n\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/mobile_cust_churn.csv'\ndata = pd.read_csv(path)\n\n\n# 데이터 살펴보기\ndata.head()\n\n\n\n\n\n\n\n\nid\nCOLLEGE\nINCOME\nOVERAGE\nLEFTOVER\nHOUSE\nHANDSET_PRICE\nOVER_15MINS_CALLS_PER_MONTH\nAVERAGE_CALL_DURATION\nREPORTED_SATISFACTION\nREPORTED_USAGE_LEVEL\nCONSIDERING_CHANGE_OF_PLAN\nCHURN\n\n\n\n\n0\n1\n0\n31953\n0\n6\n313378\n161\n0\n4\nunsat\nlittle\nno\nSTAY\n\n\n1\n2\n1\n36147\n0\n13\n800586\n244\n0\n6\nunsat\nlittle\nconsidering\nSTAY\n\n\n2\n3\n1\n27273\n230\n0\n305049\n201\n16\n15\nunsat\nvery_little\nperhaps\nSTAY\n\n\n3\n4\n0\n120070\n38\n33\n788235\n780\n3\n2\nunsat\nvery_high\nconsidering\nLEAVE\n\n\n4\n5\n1\n29215\n208\n85\n224784\n241\n21\n1\nvery_unsat\nlittle\nnever_thought\nSTAY\n\n\n\n\n\n\n\n\n\n\n\ndata.drop(\"id\", axis = 1, inplace =True)\ndata.head()\n\n\n\n\n\n\n\n\nCOLLEGE\nINCOME\nOVERAGE\nLEFTOVER\nHOUSE\nHANDSET_PRICE\nOVER_15MINS_CALLS_PER_MONTH\nAVERAGE_CALL_DURATION\nREPORTED_SATISFACTION\nREPORTED_USAGE_LEVEL\nCONSIDERING_CHANGE_OF_PLAN\nCHURN\n\n\n\n\n0\n0\n31953\n0\n6\n313378\n161\n0\n4\nunsat\nlittle\nno\nSTAY\n\n\n1\n1\n36147\n0\n13\n800586\n244\n0\n6\nunsat\nlittle\nconsidering\nSTAY\n\n\n2\n1\n27273\n230\n0\n305049\n201\n16\n15\nunsat\nvery_little\nperhaps\nSTAY\n\n\n3\n0\n120070\n38\n33\n788235\n780\n3\n2\nunsat\nvery_high\nconsidering\nLEAVE\n\n\n4\n1\n29215\n208\n85\n224784\n241\n21\n1\nvery_unsat\nlittle\nnever_thought\nSTAY\n\n\n\n\n\n\n\n- x, y 분리\n\ntarget = \"CHURN\"\n\nx = data.drop(target, axis = 1)\ny = [1 if i == \"LEAVE\" else 0 for i in data[target]]\n\n- 가변수화\n\nd_cols = [\"REPORTED_SATISFACTION\", \"REPORTED_USAGE_LEVEL\", \"CONSIDERING_CHANGE_OF_PLAN\"]\n\nx = pd.get_dummies(x, columns= d_cols, drop_first = True, dtype = float)\nx.head()\n\n\n\n\n\n\n\n\nCOLLEGE\nINCOME\nOVERAGE\nLEFTOVER\nHOUSE\nHANDSET_PRICE\nOVER_15MINS_CALLS_PER_MONTH\nAVERAGE_CALL_DURATION\nREPORTED_SATISFACTION_sat\nREPORTED_SATISFACTION_unsat\nREPORTED_SATISFACTION_very_sat\nREPORTED_SATISFACTION_very_unsat\nREPORTED_USAGE_LEVEL_high\nREPORTED_USAGE_LEVEL_little\nREPORTED_USAGE_LEVEL_very_high\nREPORTED_USAGE_LEVEL_very_little\nCONSIDERING_CHANGE_OF_PLAN_considering\nCONSIDERING_CHANGE_OF_PLAN_never_thought\nCONSIDERING_CHANGE_OF_PLAN_no\nCONSIDERING_CHANGE_OF_PLAN_perhaps\n\n\n\n\n0\n0\n31953\n0\n6\n313378\n161\n0\n4\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n1\n1\n36147\n0\n13\n800586\n244\n0\n6\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n2\n1\n27273\n230\n0\n305049\n201\n16\n15\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0\n120070\n38\n33\n788235\n780\n3\n2\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n4\n1\n29215\n208\n85\n224784\n241\n21\n1\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n\n- 학습용 평가 데이터 분리\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 1, test_size = 0.3)\n\n- 정규화\n\nfrom sklearn.preprocessing import MinMaxScaler\n# 정규화\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 1, test_size = 0.3)\n\nscaler = MinMaxScaler()\nscaler.fit(x_train)\nx_train_s = scaler.transform(x_train)\nx_test_s = scaler.transform(x_test)\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nmodel1 = DecisionTreeClassifier(max_depth= 5, random_state = 1)\nmodel2 = KNeighborsClassifier(n_neighbors=5)\nmodel3 = LogisticRegression(random_state = 1)\n\ncvs1 = cross_val_score(model1, x_train, y_train, cv = 10).mean()\ncvs2 = cross_val_score(model2, x_train_s, y_train, cv = 10).mean()\ncvs3 = cross_val_score(model3, x_train, y_train, cv = 10).mean()\n\nresult = pd.DataFrame({\"model\" : [\"tree\", \"knn\", \"logistic\"],\n                                           \"score\" : [cvs1, cvs2, cvs3]})\n\n\nresult.sort_values(\"score\", ascending = False).\\\n                        plot(x = \"score\", y = \"model\", kind = \"barh\",\n                                backend = \"plotly\",color = \"model\")\n\n\n                                                \n\n\n\n\n\n\nmodel = DecisionTreeClassifier()\n\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\n\nfrom sklearn import metrics\nacc, pre, recall, f1_score = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), metrics.f1_score(y_test, y_pred)\n\n\ndef score(range = [0.5, 0.71]) : \n            fig = pd.DataFrame({\"score\" : [acc,pre,recall,f1_score],\n                            \"measure\" : [\"acc\",\"precision\",\"recall\",\"f1_score\"]}).\\\n                                    plot(x = \"measure\", y = \"score\",  color = \"measure\",\n                                            backend = \"plotly\", kind =  \"bar\",height = 500, width = 600)\n            fig.update_yaxes(range = range)\n            return fig\n\n\nscore([0.5,0.65])"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-14-03. 머신러닝 (3).html#개념",
    "href": "posts/DX/04. 머신러닝/2023-09-14-03. 머신러닝 (3).html#개념",
    "title": "03. 머신러닝 (3)",
    "section": "",
    "text": "모든 데이터가 평가에 한 번, 학습에 k-1번 사용\nK개의 분할(Fold)에 대한 성능을 예측 → 평균과 표준편차 계산 → 일반화 성능\n단, k는 2 이상이 되어야 함(최소한 한 개씩의 학습용, 검증용 데이터가 필요)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-14-03. 머신러닝 (3).html#장단점",
    "href": "posts/DX/04. 머신러닝/2023-09-14-03. 머신러닝 (3).html#장단점",
    "title": "03. 머신러닝 (3)",
    "section": "",
    "text": "장점\n\n\n모든 데이터를 학습과 평가에 사용할 수 있음\n반복 학습과 평가를 통해 정확도를 향상시킬 수 있음\n데이터가 부족해서 발생하는 과소적합 문제를 방지할 수 있음\n평가에 사용되는 데이터의 편향을 막을 수 있음\n좀 더 일반화된 모델을 만들 수 있음\n\n\n단점\n\n\n반복 횟수가 많아서 모델 학습과 평가에 많은 시간이 소요됨"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-14-03. 머신러닝 (3).html#실습",
    "href": "posts/DX/04. 머신러닝/2023-09-14-03. 머신러닝 (3).html#실습",
    "title": "03. 머신러닝 (3)",
    "section": "",
    "text": "# 라이브러리 불러오기\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\nwarnings.filterwarnings(action='ignore')\n%config InlineBackend.figure_format='retina'\n\n\n\n데이터설명\n\nPregnancies: 임신 횟수\nGlucose: 포도당 부하 검사 수치\nBloodPressure: 혈압(mm Hg)\nSkinThickness: 팔 삼두근 뒤쪽의 피하지방 측정값(mm)\nInsulin: 혈청 인슐린(mu U/ml)\nBMI: 체질량지수(체중(kg)/키(m))^2\nDiabetesPedigreeFunction: 당뇨 내력 가중치 값\nAge: 나이\nOutcome: 클래스 결정 값(0 또는 1)\n\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/diabetes.csv'\ndata = pd.read_csv(path)\n\n\n# 데이터 살펴보기\ndata.head()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\n\n# 기술통계 확인\ndata.describe()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\ncount\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n\n\nmean\n3.845052\n120.894531\n69.105469\n20.536458\n79.799479\n31.992578\n0.471876\n33.240885\n0.348958\n\n\nstd\n3.369578\n31.972618\n19.355807\n15.952218\n115.244002\n7.884160\n0.331329\n11.760232\n0.476951\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.078000\n21.000000\n0.000000\n\n\n25%\n1.000000\n99.000000\n62.000000\n0.000000\n0.000000\n27.300000\n0.243750\n24.000000\n0.000000\n\n\n50%\n3.000000\n117.000000\n72.000000\n23.000000\n30.500000\n32.000000\n0.372500\n29.000000\n0.000000\n\n\n75%\n6.000000\n140.250000\n80.000000\n32.000000\n127.250000\n36.600000\n0.626250\n41.000000\n1.000000\n\n\nmax\n17.000000\n199.000000\n122.000000\n99.000000\n846.000000\n67.100000\n2.420000\n81.000000\n1.000000\n\n\n\n\n\n\n\n\n\n\n\n# Target 확인\ntarget = 'Outcome'\n\n# 데이터 분리\nx = data.drop(target, axis=1)\ny = data.loc[:, target]\n\n\n# 라이브러리 불러오기\nfrom sklearn.model_selection import train_test_split\n\n# 학습용, 평가용 데이터 7:3으로 분리\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n\n- 정규화\n\nKNN 알고리즘을 사용하기 위해 정규화를 진행\n\n\n# 모듈 불러오기\nfrom sklearn.preprocessing import MinMaxScaler\n\n# 정규화\nscaler = MinMaxScaler()\nscaler.fit(x_train)\nx_train_s = scaler.transform(x_train)\nx_test_s = scaler.transform(x_test)\n\n\n\n\n- 의사결정나무\n\n# 불러오기\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# 선언하기\nmodel = DecisionTreeClassifier(max_depth=5,  random_state=1)\n\n# 검증하기\ncv_score = cross_val_score(model, x_train, y_train, cv=10)\n\n# 확인\nprint(cv_score)\nprint('평균:', cv_score.mean())\nprint('표준편차:', cv_score.std())\n\n# 기록\nresult = {}\nresult[\"Decision Tree\"] = cv_score.mean()\n\n[0.66666667 0.75925926 0.74074074 0.64814815 0.7037037  0.74074074\n 0.75925926 0.81132075 0.79245283 0.67924528]\n평균: 0.7301537386443047\n표준편차: 0.05141448587329709\n\n\n- KNN\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel2 = KNeighborsClassifier(n_neighbors=5)\n\ncv_score = cross_val_score(model2, x_train_s, y_train,cv=10)\n\n# 확인\nprint(cv_score)\nprint('평균:', cv_score.mean())\nprint('표준편차:', cv_score.std())\nresult[\"KNN\"] = cv_score.mean()\n\n[0.64814815 0.68518519 0.72222222 0.64814815 0.72222222 0.74074074\n 0.68518519 0.66037736 0.77358491 0.60377358]\n평균: 0.6889587700908455\n표준편차: 0.04846522080635871\n\n\n- 로지스틱\n\nfrom sklearn.linear_model  import LogisticRegression\n\nmodel3 = LogisticRegression()\n\ncv_score = cross_val_score(model3, x_train, y_train, cv = 10)\n\nresult[\"Logistic\"] = cv_score.mean()\n\n- 결과\n\nk-fold 교차검증결과 : Logisitc 모형이 성능이 가장 좋아보인다.\n\n\nresult\n\n{'Decision Tree': 0.7301537386443047,\n 'KNN': 0.6889587700908455,\n 'Logistic': 0.7690426275331936}\n\n\n\n\n\n\nplt.figure(figsize = (4,4))\nplt.barh(y=list(result), width = result.values(), height = 0.5)\n\n&lt;BarContainer object of 3 artists&gt;\n\n\n\n\n\n\n\n\n- fitting을 다시 하는 이유\n\ntrain data set의 전체 데이터로 학습한 적은 없음……\n\n\ndef score(range = [0.5, 0.71]) : \n            fig = pd.DataFrame({\"score\" : [acc,pre,recall,f1_score],\n                            \"measure\" : [\"acc\",\"precision\",\"recall\",\"f1_score\"]}).\\\n                                    plot(x = \"measure\", y = \"score\",  color = \"measure\",\n                                            backend = \"plotly\", kind =  \"bar\",height = 500, width = 600)\n            fig.update_yaxes(range = range)\n            return fig\n\n\nmodel3.fit(x_train, y_train)\n\ny_pred = model3.predict(x_test)\n\nfrom sklearn.metrics import * \n\nacc = accuracy_score(y_test,y_pred)\npre = precision_score(y_test,y_pred)\nrecall = recall_score(y_test,y_pred)\nf1_score = f1_score(y_test,y_pred)\n\n\nscore(range = [0.5,0.8])"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-14-03. 머신러닝 (3).html#exercise.-1",
    "href": "posts/DX/04. 머신러닝/2023-09-14-03. 머신러닝 (3).html#exercise.-1",
    "title": "03. 머신러닝 (3)",
    "section": "",
    "text": "# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/boston.csv'\ndata = pd.read_csv(path)\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 506 entries, 0 to 505\nData columns (total 14 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   crim     506 non-null    float64\n 1   zn       506 non-null    float64\n 2   indus    506 non-null    float64\n 3   chas     506 non-null    int64  \n 4   nox      506 non-null    float64\n 5   rm       506 non-null    float64\n 6   age      506 non-null    float64\n 7   dis      506 non-null    float64\n 8   rad      506 non-null    int64  \n 9   tax      506 non-null    int64  \n 10  ptratio  506 non-null    float64\n 11  black    506 non-null    float64\n 12  lstat    506 non-null    float64\n 13  medv     506 non-null    float64\ndtypes: float64(11), int64(3)\nmemory usage: 55.5 KB\n\n\n\ndata.head()\n\n\n\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nblack\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n\n\n\n\ndata.isna().sum()\n\ncrim       0\nzn         0\nindus      0\nchas       0\nnox        0\nrm         0\nage        0\ndis        0\nrad        0\ntax        0\nptratio    0\nblack      0\nlstat      0\nmedv       0\ndtype: int64\n\n\n\n\n\n\ntarget = \"medv\"\n\nx = data.drop(target, axis = 1)\ny = data[[target]]\n\nfrom sklearn.preprocessing import MinMaxScaler\n# 정규화\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 1, test_size = 0.3)\n\nscaler = MinMaxScaler()\nscaler.fit(x_train)\nx_train_s = scaler.transform(x_train)\nx_test_s = scaler.transform(x_test)\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\nmodel1 = DecisionTreeRegressor(max_depth = 5)\nmodel2 = KNeighborsRegressor(n_neighbors=5)\nmodel3 = LinearRegression()\n\ncv_score1 = cross_val_score(model1, x_train, y_train, cv = 10, scoring = \"r2\")\ncv_score2 = cross_val_score(model2, x_train_s, y_train, cv = 10, scoring = \"r2\")\ncv_score3 = cross_val_score(model3, x_train, y_train, cv = 10, scoring = \"r2\")\n\nresult = pd.DataFrame({\"model\" : [\"tree\", \"knn\", \"linear reg\"],\n                            \"score\" : [cv_score1.mean(),cv_score2.mean(), cv_score3.mean()]})\n\n\nresult.sort_values(\"score\", ascending = False).\\\n                    plot(y = \"model\", x= \"score\", kind =\"barh\",\n                            backend = \"plotly\", color = \"model\")\n\n\n                                                \n\n\n\n\n\n\nmodel = DecisionTreeRegressor()\n\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\n\nmeasure = [\"MAE\", \"R2\"]\nscore = [mean_absolute_error(y_test, y_pred) ,r2_score(y_test, y_pred)]\n\n\npd.DataFrame({ \"measure\" : measure, \n                             \"score\" : score}).sort_values(\"score\", ascending = False).\\\n                                        plot( y= \"measure\", x=\"score\", kind = \"barh\", backend =\"plotly\", color = \"measure\")"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-14-03. 머신러닝 (3).html#exercise.-2",
    "href": "posts/DX/04. 머신러닝/2023-09-14-03. 머신러닝 (3).html#exercise.-2",
    "title": "03. 머신러닝 (3)",
    "section": "",
    "text": "데이터 설명\n\nCOLLEGE: 대학 졸업여부\nINCOME: 연수입\nOVERAGE: 월평균 초과사용 시간(분)\nLEFTOVER: 월평균 잔여시간비율(%)\nHOUSE: 집값\nHANDSET_PRICE: 스마트폰 가격\nOVER_15MINS_CALLS_PER_MONTH: 월평균 장기통화(15분이상) 횟수\nAVERAGE_CALL_DURATION: 평균 통화 시간\nREPORTED_SATISFACTION: 만족도 설문조사 결과\nREPORTED_USAGE_LEVEL: 사용도 자가진단 결과\nCONSIDERING_CHANGE_OF_PLAN: 향후 변경계획 설문조사 결과\nCHURN: 이탈(번호이동) 여부 (Target 변수)\n\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/mobile_cust_churn.csv'\ndata = pd.read_csv(path)\n\n\n# 데이터 살펴보기\ndata.head()\n\n\n\n\n\n\n\n\nid\nCOLLEGE\nINCOME\nOVERAGE\nLEFTOVER\nHOUSE\nHANDSET_PRICE\nOVER_15MINS_CALLS_PER_MONTH\nAVERAGE_CALL_DURATION\nREPORTED_SATISFACTION\nREPORTED_USAGE_LEVEL\nCONSIDERING_CHANGE_OF_PLAN\nCHURN\n\n\n\n\n0\n1\n0\n31953\n0\n6\n313378\n161\n0\n4\nunsat\nlittle\nno\nSTAY\n\n\n1\n2\n1\n36147\n0\n13\n800586\n244\n0\n6\nunsat\nlittle\nconsidering\nSTAY\n\n\n2\n3\n1\n27273\n230\n0\n305049\n201\n16\n15\nunsat\nvery_little\nperhaps\nSTAY\n\n\n3\n4\n0\n120070\n38\n33\n788235\n780\n3\n2\nunsat\nvery_high\nconsidering\nLEAVE\n\n\n4\n5\n1\n29215\n208\n85\n224784\n241\n21\n1\nvery_unsat\nlittle\nnever_thought\nSTAY\n\n\n\n\n\n\n\n\n\n\n\ndata.drop(\"id\", axis = 1, inplace =True)\ndata.head()\n\n\n\n\n\n\n\n\nCOLLEGE\nINCOME\nOVERAGE\nLEFTOVER\nHOUSE\nHANDSET_PRICE\nOVER_15MINS_CALLS_PER_MONTH\nAVERAGE_CALL_DURATION\nREPORTED_SATISFACTION\nREPORTED_USAGE_LEVEL\nCONSIDERING_CHANGE_OF_PLAN\nCHURN\n\n\n\n\n0\n0\n31953\n0\n6\n313378\n161\n0\n4\nunsat\nlittle\nno\nSTAY\n\n\n1\n1\n36147\n0\n13\n800586\n244\n0\n6\nunsat\nlittle\nconsidering\nSTAY\n\n\n2\n1\n27273\n230\n0\n305049\n201\n16\n15\nunsat\nvery_little\nperhaps\nSTAY\n\n\n3\n0\n120070\n38\n33\n788235\n780\n3\n2\nunsat\nvery_high\nconsidering\nLEAVE\n\n\n4\n1\n29215\n208\n85\n224784\n241\n21\n1\nvery_unsat\nlittle\nnever_thought\nSTAY\n\n\n\n\n\n\n\n- x, y 분리\n\ntarget = \"CHURN\"\n\nx = data.drop(target, axis = 1)\ny = [1 if i == \"LEAVE\" else 0 for i in data[target]]\n\n- 가변수화\n\nd_cols = [\"REPORTED_SATISFACTION\", \"REPORTED_USAGE_LEVEL\", \"CONSIDERING_CHANGE_OF_PLAN\"]\n\nx = pd.get_dummies(x, columns= d_cols, drop_first = True, dtype = float)\nx.head()\n\n\n\n\n\n\n\n\nCOLLEGE\nINCOME\nOVERAGE\nLEFTOVER\nHOUSE\nHANDSET_PRICE\nOVER_15MINS_CALLS_PER_MONTH\nAVERAGE_CALL_DURATION\nREPORTED_SATISFACTION_sat\nREPORTED_SATISFACTION_unsat\nREPORTED_SATISFACTION_very_sat\nREPORTED_SATISFACTION_very_unsat\nREPORTED_USAGE_LEVEL_high\nREPORTED_USAGE_LEVEL_little\nREPORTED_USAGE_LEVEL_very_high\nREPORTED_USAGE_LEVEL_very_little\nCONSIDERING_CHANGE_OF_PLAN_considering\nCONSIDERING_CHANGE_OF_PLAN_never_thought\nCONSIDERING_CHANGE_OF_PLAN_no\nCONSIDERING_CHANGE_OF_PLAN_perhaps\n\n\n\n\n0\n0\n31953\n0\n6\n313378\n161\n0\n4\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n1\n1\n36147\n0\n13\n800586\n244\n0\n6\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n2\n1\n27273\n230\n0\n305049\n201\n16\n15\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0\n120070\n38\n33\n788235\n780\n3\n2\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n4\n1\n29215\n208\n85\n224784\n241\n21\n1\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n\n- 학습용 평가 데이터 분리\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 1, test_size = 0.3)\n\n- 정규화\n\nfrom sklearn.preprocessing import MinMaxScaler\n# 정규화\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 1, test_size = 0.3)\n\nscaler = MinMaxScaler()\nscaler.fit(x_train)\nx_train_s = scaler.transform(x_train)\nx_test_s = scaler.transform(x_test)\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nmodel1 = DecisionTreeClassifier(max_depth= 5, random_state = 1)\nmodel2 = KNeighborsClassifier(n_neighbors=5)\nmodel3 = LogisticRegression(random_state = 1)\n\ncvs1 = cross_val_score(model1, x_train, y_train, cv = 10).mean()\ncvs2 = cross_val_score(model2, x_train_s, y_train, cv = 10).mean()\ncvs3 = cross_val_score(model3, x_train, y_train, cv = 10).mean()\n\nresult = pd.DataFrame({\"model\" : [\"tree\", \"knn\", \"logistic\"],\n                                           \"score\" : [cvs1, cvs2, cvs3]})\n\n\nresult.sort_values(\"score\", ascending = False).\\\n                        plot(x = \"score\", y = \"model\", kind = \"barh\",\n                                backend = \"plotly\",color = \"model\")\n\n\n                                                \n\n\n\n\n\n\nmodel = DecisionTreeClassifier()\n\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\n\nfrom sklearn import metrics\nacc, pre, recall, f1_score = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), metrics.f1_score(y_test, y_pred)\n\n\ndef score(range = [0.5, 0.71]) : \n            fig = pd.DataFrame({\"score\" : [acc,pre,recall,f1_score],\n                            \"measure\" : [\"acc\",\"precision\",\"recall\",\"f1_score\"]}).\\\n                                    plot(x = \"measure\", y = \"score\",  color = \"measure\",\n                                            backend = \"plotly\", kind =  \"bar\",height = 500, width = 600)\n            fig.update_yaxes(range = range)\n            return fig\n\n\nscore([0.5,0.65])"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html",
    "title": "01. 머신러닝 (1)",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nfrom sklearn.model_selection import train_test_split\n\nwarnings.filterwarnings(action='ignore')\n%config InlineBackend.figure_format = 'retina'"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#데이터로드",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#데이터로드",
    "title": "01. 머신러닝 (1)",
    "section": "데이터로드",
    "text": "데이터로드\n\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/airquality_simple.csv'\ndata = pd.read_csv(path)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#데이터-준비",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#데이터-준비",
    "title": "01. 머신러닝 (1)",
    "section": "데이터 준비",
    "text": "데이터 준비\n- 결측치 체우기\n\n# 전날 값으로 결측치 채우기\ndata.fillna(method='ffill', inplace=True)\n\n# 확인\ndata.isnull().sum()\n\nOzone      0\nSolar.R    0\nWind       0\nTemp       0\nMonth      0\nDay        0\ndtype: int64\n\n\n- 변수제거\n\n# 변수 제거\ndrop_cols = ['Month', 'Day']\ndata.drop(drop_cols, axis=1, inplace=True)\n\n# 확인\ndata.head()\n\n\n\n\n\n\n\n\nOzone\nSolar.R\nWind\nTemp\n\n\n\n\n0\n41\n190.0\n7.4\n67\n\n\n1\n36\n118.0\n8.0\n72\n\n\n2\n12\n149.0\n12.6\n74\n\n\n3\n18\n313.0\n11.5\n62\n\n\n4\n19\n313.0\n14.3\n56\n\n\n\n\n\n\n\n- \\((x,y)\\) 분리\n\n# target 확인\ntarget = 'Ozone'\n\n# 데이터 분리\nx = data.drop(target, axis=1)\ny = data.loc[:, target]\n\n- 학습용, 평가용 데이터 분리\n\n# 모듈 불러오기\nfrom sklearn.model_selection import train_test_split\n\n# 7:3으로 분리\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#모델링",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#모델링",
    "title": "01. 머신러닝 (1)",
    "section": "모델링",
    "text": "모델링\n\n# 1단계: 불러오기\nfrom sklearn.linear_model import LinearRegression\n\n# 2단계 : 모델선언\nmodel = LinearRegression()\n\n# 3단계 : 학습하기\n\nmodel.fit(x_train, y_train)\n\n# 4단계 : 예측하기\ny_pred = model.predict(x_test)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#모형-평가",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#모형-평가",
    "title": "01. 머신러닝 (1)",
    "section": "모형 평가",
    "text": "모형 평가\n\n\nCode\nfrom sklearn.metrics import *\n\n\nprint(\"MAE :\", mean_absolute_error(y_test,y_pred))\nprint(\"MSE :\", mean_squared_error(y_test,y_pred))\nprint(\"RMSE :\", mean_squared_error(y_test,y_pred)**(1/2))\nprint(\"MAPE :\", mean_absolute_percentage_error(y_test,y_pred))\nprint(\"R2\", r2_score(y_test,y_pred))\n\n\nMAE : 13.976843190385711\nMSE : 341.678874066819\nRMSE : 18.484557718993955\nMAPE : 0.47185976988482603\nR2 0.5744131358040061\n\n\n\nprint(\"MAE : \", mean_absolute_error(y_test,y_pred))\n\nMAE :  13.976843190385711"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#step0.-import",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#step0.-import",
    "title": "01. 머신러닝 (1)",
    "section": "step0. import",
    "text": "step0. import\n\nimport statsmodels.api as sm\nfrom ISLP.models import (ModelSpec as MS,\nsummarize,poly)\nfrom sklearn.metrics import * ## 모델 평가지표\nfrom sklearn.model_selection import train_test_split"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#step-1.-xy-분리-및-훈련데이터와-평가-데이터-분리",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#step-1.-xy-분리-및-훈련데이터와-평가-데이터-분리",
    "title": "01. 머신러닝 (1)",
    "section": "step 1. \\((X,y)\\) 분리 및 훈련데이터와 평가 데이터 분리",
    "text": "step 1. \\((X,y)\\) 분리 및 훈련데이터와 평가 데이터 분리\n\ntarget = \"Ozone\"\n\nx = data.drop(target, axis=1)\nX = MS(x).fit_transform(data)\ny = data[target]\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#step-2.-최소제곱법을-이용하여-모델을-적합하겠다고-선언",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#step-2.-최소제곱법을-이용하여-모델을-적합하겠다고-선언",
    "title": "01. 머신러닝 (1)",
    "section": "step 2. 최소제곱법을 이용하여 모델을 적합하겠다고 선언",
    "text": "step 2. 최소제곱법을 이용하여 모델을 적합하겠다고 선언\n\nmodel = sm.OLS(y_train,x_train)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#step-3.-모형에-대한-유의성-확인",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#step-3.-모형에-대한-유의성-확인",
    "title": "01. 머신러닝 (1)",
    "section": "step 3. 모형에 대한 유의성 확인",
    "text": "step 3. 모형에 대한 유의성 확인\n\nmodel.fit().summary().tables[0]\n\n\nOLS Regression Results\n\n\nDep. Variable:\nOzone\nR-squared:\n0.576\n\n\nModel:\nOLS\nAdj. R-squared:\n0.564\n\n\nMethod:\nLeast Squares\nF-statistic:\n46.73\n\n\nDate:\nTue, 12 Sep 2023\nProb (F-statistic):\n3.80e-19\n\n\nTime:\n16:51:34\nLog-Likelihood:\n-469.88\n\n\nNo. Observations:\n107\nAIC:\n947.8\n\n\nDf Residuals:\n103\nBIC:\n958.5\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#step-4.-개별-회귀-계수에-대한-유의성-확인",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#step-4.-개별-회귀-계수에-대한-유의성-확인",
    "title": "01. 머신러닝 (1)",
    "section": "step 4. 개별 회귀 계수에 대한 유의성 확인",
    "text": "step 4. 개별 회귀 계수에 대한 유의성 확인\n\nmodel.fit().summary().tables[1]\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nintercept\n-59.5062\n20.882\n-2.850\n0.005\n-100.920\n-18.092\n\n\nSolar.R\n0.0560\n0.022\n2.555\n0.012\n0.013\n0.099\n\n\nWind\n-3.3585\n0.604\n-5.564\n0.000\n-4.556\n-2.161\n\n\nTemp\n1.5799\n0.234\n6.760\n0.000\n1.116\n2.043"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#step-5.-평가-데이터에-대한-모형-평가",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#step-5.-평가-데이터에-대한-모형-평가",
    "title": "01. 머신러닝 (1)",
    "section": "step 5. 평가 데이터에 대한 모형 평가",
    "text": "step 5. 평가 데이터에 대한 모형 평가\n\nl_model = model.fit()\n\ny_pred = l_model.predict(x_test)\n\n\n\nCode\nprint(\"MAE :\", mean_absolute_error(y_test,y_pred))\nprint(\"MSE :\", mean_squared_error(y_test,y_pred))\nprint(\"RMSE :\", mean_squared_error(y_test,y_pred)**(1/2))\nprint(\"MAPE :\", mean_absolute_percentage_error(y_test,y_pred))\nprint(\"R2\", r2_score(y_test,y_pred))\n\n\nMAE : 13.976843190385711\nMSE : 341.6788740668187\nRMSE : 18.484557718993948\nMAPE : 0.4718597698848258\nR2 0.5744131358040064"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#import-1",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#import-1",
    "title": "01. 머신러닝 (1)",
    "section": "0. import",
    "text": "0. import\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\nimport statsmodels.api as sm\nfrom ISLP.models import (ModelSpec as MS,\nsummarize,poly)\nfrom sklearn.metrics import * ## 모델 평가지표\nfrom sklearn.model_selection import train_test_split"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#데이터-로드",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#데이터-로드",
    "title": "01. 머신러닝 (1)",
    "section": "1. 데이터 로드",
    "text": "1. 데이터 로드\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/income_happy.csv'\ndata = pd.read_csv(path)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#데이터-이해",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#데이터-이해",
    "title": "01. 머신러닝 (1)",
    "section": "2. 데이터 이해",
    "text": "2. 데이터 이해\n- income : 수입 (단위 : 10,000$)\n- happiness : 행복정도 (1 ~ 10)\n\ndata.shape\n\n(498, 2)\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 498 entries, 0 to 497\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   income     498 non-null    float64\n 1   happiness  498 non-null    float64\ndtypes: float64(2)\nmemory usage: 7.9 KB\n\n\n- 기술통계 확인\n\ndata.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nincome\n498.0\n4.466902\n1.737527\n1.506275\n3.006256\n4.423710\n5.991913\n7.481521\n\n\nhappiness\n498.0\n3.392859\n1.432813\n0.266044\n2.265864\n3.472536\n4.502621\n6.863388\n\n\n\n\n\n\n\n\ndata.corr()\n\n\n\n\n\n\n\n\nincome\nhappiness\n\n\n\n\nincome\n1.000000\n0.865634\n\n\nhappiness\n0.865634\n1.000000"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#모델링1-강의",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#모델링1-강의",
    "title": "01. 머신러닝 (1)",
    "section": "모델링1 : 강의",
    "text": "모델링1 : 강의\n\nstep 1. 데이터셋 분리\n\ntarget = \"income\"\nx = data.drop(target, axis = 1)\ny = data[target]\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1, test_size = 0.3)\n\n\n\nstep 2. 모델 선언\n\nmodel1 = LinearRegression()\n\n\n\nstep 3. 모델 fit\n\nmodel1.fit(x_train,y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\nstep 4. predict\n\ny_pred = model1.predict(x_test)\n\n\n\nstep 5. 성능 평가\n\n\nCode\nprint(\"MAE :\", mean_absolute_error(y_test,y_pred))\nprint(\"MSE :\", mean_squared_error(y_test,y_pred))\nprint(\"RMSE :\", mean_squared_error(y_test,y_pred)**(1/2))\nprint(\"MAPE :\", mean_absolute_percentage_error(y_test,y_pred))\nprint(\"R2\", r2_score(y_test,y_pred))\n\n\nMAE : 0.6968383731427477\nMSE : 0.7812638184683306\nRMSE : 0.8838912933547488\nMAPE : 0.20292339169383194\nR2 0.7540887980989224\n\n\n\n\nstep 5. 시각화\n\nplt.figure(figsize = (4,4))\nplt.plot(x_test,y_test,\".r\",alpha = 0.3,label =r\"$(x,y)$\")\nplt.plot(x_test,y_pred,\".b\",alpha = 0.3,label =r\"$(x,\\hat {y})$\" )\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x20636f2d350&gt;"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#모델링-2-islp",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#모델링-2-islp",
    "title": "01. 머신러닝 (1)",
    "section": "모델링 2 : ISLP",
    "text": "모델링 2 : ISLP\n\n\nCode\nimport statsmodels.api as sm\nfrom ISLP.models import (ModelSpec as MS,\nsummarize,poly)\nfrom sklearn.metrics import * ## 모델 평가지표\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/income_happy.csv'\ndata = pd.read_csv(path)\n\n\nstep 1. 데이터셋 분리\n\ntarget = \"income\"\n\nx = data.drop(target, axis=1)\nX = MS(x).fit_transform(data)\ny = data[target]\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\n\n\nstep 2. 모델 선언\n\nmodel2 = sm.OLS(y_train, x_train).fit()\n\n\n\nstep 3. 모델 report 확인\n\nresults = model2.summary()\n\n\nresults.tables[0]\n\n\nOLS Regression Results\n\n\nDep. Variable:\nincome\nR-squared:\n0.747\n\n\nModel:\nOLS\nAdj. R-squared:\n0.746\n\n\nMethod:\nLeast Squares\nF-statistic:\n1020.\n\n\nDate:\nTue, 12 Sep 2023\nProb (F-statistic):\n3.14e-105\n\n\nTime:\n16:51:35\nLog-Likelihood:\n-442.54\n\n\nNo. Observations:\n348\nAIC:\n889.1\n\n\nDf Residuals:\n346\nBIC:\n896.8\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\nresults.tables[1]\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nintercept\n0.9640\n0.119\n8.121\n0.000\n0.731\n1.197\n\n\nhappiness\n1.0337\n0.032\n31.945\n0.000\n0.970\n1.097\n\n\n\n\n\n\n\nstep 4. 평가 데이터에 대한 모형 평가\n\ny_pred = model2.predict(x_test)\n\n\nfrom sklearn.metrics import *\n\n\nprint(\"MAE :\", mean_absolute_error(y_test,y_pred))\nprint(\"MSE :\", mean_squared_error(y_test,y_pred))\nprint(\"RMSE :\", mean_squared_error(y_test,y_pred)**(1/2))\nprint(\"MAPE :\", mean_absolute_percentage_error(y_test,y_pred))\nprint(\"R2\", r2_score(y_test,y_pred))\n\nMAE : 0.6968383731427479\nMSE : 0.7812638184683306\nRMSE : 0.8838912933547488\nMAPE : 0.2029233916938319\nR2 0.7540887980989224\n\n\n\n\nstep 5. 결과 시각화\n\nplt.figure(figsize = (4,4))\nplt.plot(x_test[\"happiness\"],y_test,\".r\",alpha = 0.3,label =r\"$(x,y)$\")\nplt.plot(x_test[\"happiness\"],y_pred,\".b\",alpha = 0.3,label =r\"$(x,\\hat {y})$\" )\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x2063232d350&gt;"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#summary-1",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#summary-1",
    "title": "01. 머신러닝 (1)",
    "section": "summary 1",
    "text": "summary 1\n- 강의에서 다룬 sklearn.linear_model에서는 model fitting 시 다음과 같은 구조로 입력해야 한다.\n\n예측변수가 첫 번째인자, 반응변수가 두 번째 인자\n\nfrom sklearn.linear_model import LinearRegression\n\nmodel1.fit(x_train,y_train)\n- ISLP에서는 위와 반대!\n\nmodel1.fit?\n\n\nSignature: model1.fit(X, y, sample_weight=None)\nDocstring:\nFit linear model.\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    Training data.\ny : array-like of shape (n_samples,) or (n_samples, n_targets)\n    Target values. Will be cast to X's dtype if necessary.\nsample_weight : array-like of shape (n_samples,), default=None\n    Individual weights for each sample.\n    .. versionadded:: 0.17\n       parameter *sample_weight* support to LinearRegression.\nReturns\n-------\nself : object\n    Fitted Estimator.\nFile:      c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages\\sklearn\\linear_model\\_base.py\nType:      method\n\n\n\nimport statsmodels.api as sm\nfrom ISLP.models import (ModelSpec as MS, summarize, poly)\nmodel2 = sm.OLS(y_train, x_train).fit()\n\nsm.OLS?\n\n\nInit signature: sm.OLS(endog, exog=None, missing='none', hasconst=None, **kwargs)\nDocstring:     \nOrdinary Least Squares\nParameters\n----------\nendog : array_like\n    A 1-d endogenous response variable. The dependent variable.\nexog : array_like\n    A nobs x k array where `nobs` is the number of observations and `k`\n    is the number of regressors. An intercept is not included by default\n    and should be added by the user. See\n    :func:`statsmodels.tools.add_constant`.\nmissing : str\n    Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n    checking is done. If 'drop', any observations with nans are dropped.\n    If 'raise', an error is raised. Default is 'none'.\nhasconst : None or bool\n    Indicates whether the RHS includes a user-supplied constant. If True,\n    a constant is not checked for and k_constant is set to 1 and all\n    result statistics are calculated as if a constant is present. If\n    False, a constant is not checked for and k_constant is set to 0.\n**kwargs\n    Extra arguments that are used to set model properties when using the\n    formula interface.\nAttributes\n----------\nweights : scalar\n    Has an attribute weights = array(1.0) due to inheritance from WLS.\nSee Also\n--------\nWLS : Fit a linear model using Weighted Least Squares.\nGLS : Fit a linear model using Generalized Least Squares.\nNotes\n-----\nNo constant is added by the model unless you are using formulas.\nExamples\n--------\n&gt;&gt;&gt; import statsmodels.api as sm\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; duncan_prestige = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\n&gt;&gt;&gt; Y = duncan_prestige.data['income']\n&gt;&gt;&gt; X = duncan_prestige.data['education']\n&gt;&gt;&gt; X = sm.add_constant(X)\n&gt;&gt;&gt; model = sm.OLS(Y,X)\n&gt;&gt;&gt; results = model.fit()\n&gt;&gt;&gt; results.params\nconst        10.603498\neducation     0.594859\ndtype: float64\n&gt;&gt;&gt; results.tvalues\nconst        2.039813\neducation    6.892802\ndtype: float64\n&gt;&gt;&gt; print(results.t_test([1, 0]))\n                             Test for Constraints\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nc0            10.6035      5.198      2.040      0.048       0.120      21.087\n==============================================================================\n&gt;&gt;&gt; print(results.f_test(np.identity(2)))\n&lt;F test: F=array([[159.63031026]]), p=1.2607168903696672e-20,\n df_denom=43, df_num=2&gt;\nFile:           c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages\\statsmodels\\regression\\linear_model.py\nType:           type\nSubclasses:     \n\n\n\n- 인자값을 명시해주고 전달하면 상관이 없으나, 귀찮으니 그냥 바꿔가면서 쓰자 핳"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#데이터-로드-및-확인",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#데이터-로드-및-확인",
    "title": "01. 머신러닝 (1)",
    "section": "1. 데이터 로드 및 확인",
    "text": "1. 데이터 로드 및 확인\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/admission_simple.csv'\ndata = pd.read_csv(path)\n\n\n# 상/하위 몇 개 행 확인\ndata.head()\n\n\n\n\n\n\n\n\nGRE\nTOEFL\nRANK\nSOP\nLOR\nGPA\nRESEARCH\nADMIT\n\n\n\n\n0\n337\n118\n4\n4.5\n4.5\n9.65\n1\n1\n\n\n1\n324\n107\n4\n4.0\n4.5\n8.87\n1\n1\n\n\n2\n316\n104\n3\n3.0\n3.5\n8.00\n1\n0\n\n\n3\n322\n110\n3\n3.5\n2.5\n8.67\n1\n1\n\n\n4\n314\n103\n2\n2.0\n3.0\n8.21\n0\n0\n\n\n\n\n\n\n\n\n# 변수 확인\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 500 entries, 0 to 499\nData columns (total 8 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   GRE       500 non-null    int64  \n 1   TOEFL     500 non-null    int64  \n 2   RANK      500 non-null    int64  \n 3   SOP       500 non-null    float64\n 4   LOR       500 non-null    float64\n 5   GPA       500 non-null    float64\n 6   RESEARCH  500 non-null    int64  \n 7   ADMIT     500 non-null    int64  \ndtypes: float64(3), int64(5)\nmemory usage: 31.4 KB\n\n\n\n# 기술통계 확인\ndata.describe()\n\n\n\n\n\n\n\n\nGRE\nTOEFL\nRANK\nSOP\nLOR\nGPA\nRESEARCH\nADMIT\n\n\n\n\ncount\n500.000000\n500.000000\n500.000000\n500.000000\n500.00000\n500.000000\n500.000000\n500.000000\n\n\nmean\n316.472000\n107.192000\n3.114000\n3.374000\n3.48400\n8.576440\n0.560000\n0.436000\n\n\nstd\n11.295148\n6.081868\n1.143512\n0.991004\n0.92545\n0.604813\n0.496884\n0.496384\n\n\nmin\n290.000000\n92.000000\n1.000000\n1.000000\n1.00000\n6.800000\n0.000000\n0.000000\n\n\n25%\n308.000000\n103.000000\n2.000000\n2.500000\n3.00000\n8.127500\n0.000000\n0.000000\n\n\n50%\n317.000000\n107.000000\n3.000000\n3.500000\n3.50000\n8.560000\n1.000000\n0.000000\n\n\n75%\n325.000000\n112.000000\n4.000000\n4.000000\n4.00000\n9.040000\n1.000000\n1.000000\n\n\nmax\n340.000000\n120.000000\n5.000000\n5.000000\n5.00000\n9.920000\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#데이터-셋-분리",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#데이터-셋-분리",
    "title": "01. 머신러닝 (1)",
    "section": "2. 데이터 셋 분리",
    "text": "2. 데이터 셋 분리\n\ntarget = 'ADMIT'\n\n# 데이터 분리\nx = data.drop(target, axis=1)\ny = data.loc[:, target]\n\n# 모듈 불러오기\nfrom sklearn.model_selection import train_test_split\n\n# 7:3으로 분리\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#모델링-1",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#모델링-1",
    "title": "01. 머신러닝 (1)",
    "section": "3. 모델링",
    "text": "3. 모델링\n\n# 1단계: 불러오기\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# 2단계: 선언하기\nmodel = KNeighborsClassifier()\n\n# 3단계: 학습하기\nmodel.fit(x_train, y_train)\n\n# 4단계: 예측하기\ny_pred = model.predict(x_test)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#예측결과-시각화",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#예측결과-시각화",
    "title": "01. 머신러닝 (1)",
    "section": "4. 예측결과 시각화",
    "text": "4. 예측결과 시각화\n\nfrom sklearn.metrics import confusion_matrix\n\nprint(confusion_matrix(y_test,y_pred))\n\n[[76  8]\n [16 50]]\n\n\n\nplt.figure(figsize = (4,4))\nsns.heatmap(confusion_matrix(y_test,y_pred),\n            annot = True,\n            cmap = \"Blues\",\n            cbar = False)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n5. 정확도, 정밀도, 재현율 계산\n\nprint(f\"accuarcy : {accuracy_score(y_test,y_pred):.2f}\")\npre = precision_score(y_test,y_pred,average=\"binary\")\npre2 = precision_score(y_test,y_pred,average=None)\nprint(f\"precision : {pre :.2f}\")\nprint(f\"precision of 0 : {pre2[0] :.2f}, precision of 0 : {pre2[1] : .2f}\")\nprint(f\"recall of 0: {recall_score(y_test,y_pred,average=None)[0]:.2f}, recall of 1: {recall_score(y_test,y_pred,average=None)[1]:.2f}\")\nprint(f\"F1-score of 0: {f1_score(y_test,y_pred,average=None)[0]:.2f}, F1-score of 1: {f1_score(y_test,y_pred,average=None)[1]:.2f}\")\n\naccuarcy : 0.84\nprecision : 0.86\nprecision of 0 : 0.83, precision of 0 :  0.86\nrecall of 0: 0.90, recall of 1: 0.76\nF1-score of 0: 0.86, F1-score of 1: 0.81"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#import-2",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#import-2",
    "title": "01. 머신러닝 (1)",
    "section": "0. import",
    "text": "0. import\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\nimport statsmodels.api as sm\nfrom ISLP.models import (ModelSpec as MS,\nsummarize,poly)\nfrom sklearn.metrics import * ## 모델 평가지표\nfrom sklearn.model_selection import train_test_split"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#데이터-로드-및-eda",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#데이터-로드-및-eda",
    "title": "01. 머신러닝 (1)",
    "section": "1. 데이터 로드 및 EDA",
    "text": "1. 데이터 로드 및 EDA\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/titanic.csv'\ndata = pd.read_csv(path)\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\n\ndata.isna().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#데이터-전처리",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#데이터-전처리",
    "title": "01. 머신러닝 (1)",
    "section": "2. 데이터 전처리",
    "text": "2. 데이터 전처리\n- 필요없는 변수 제거\n\n# Title  열 추가 (이름에서 Mr, Mrs)만 뽑기\n\ndata[\"Title\"] = data[\"Name\"].str.extract(\"([a-zA-z]+)\\.\")\n\n- Master까지만 자르고 나머지를 others로 묶어버리자\n\ndata.Title.value_counts()\n\nTitle\nMr          517\nMiss        182\nMrs         125\nMaster       40\nDr            7\nRev           6\nMlle          2\nMajor         2\nCol           2\nCountess      1\nCapt          1\nMs            1\nSir           1\nLady          1\nMme           1\nDon           1\nJonkheer      1\nName: count, dtype: int64\n\n\n\nmain_title = [\"Mr\",\"Miss\",\"Mrs\",\"Master\"]\n\n\ndata.loc[data['Title'].isin(main_title)==False, 'Title'] = 'Others'\n\n\ndata.Title.value_counts()\n\nTitle\nMr        517\nMiss      182\nMrs       125\nMaster     40\nOthers     27\nName: count, dtype: int64\n\n\n- 삭제할 컬럼 지정\n\nd_cols = [\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"]\n\ndata.drop(d_cols, axis=1,inplace=True)\ndata.head()\n\n\n\n\n\n\n\n\nSurvived\nPclass\nSex\nAge\nSibSp\nParch\nFare\nEmbarked\nTitle\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nMr\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nMrs\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nMiss\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nMrs\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nMr\n\n\n\n\n\n\n\n- 결측치 채우기\n\n## 탑승자 Title별, 나이의 중앙값.\n## 4개의 값을 transform을 이용하여 전체의 행의 개수로 맞추고 결측치를 채운다.\ntitle_age_median = data.groupby(\"Title\")[\"Age\"].transform(\"median\")\n\n\ntitle_age_median.unique()\n\narray([30. , 35. , 21. ,  3.5, 44.5])\n\n\n\n# Age 결측치를 각 title의 중앙값으로 바꾸기\n\ndata.Age.fillna(title_age_median, inplace=True)\n\n# Embarked 최빈값 'S'로 채우기\n\ndata.Embarked.fillna(value = \"S\", inplace=True)\n\n\ndata.isna().sum()\n\nSurvived    0\nPclass      0\nSex         0\nAge         0\nSibSp       0\nParch       0\nFare        0\nEmbarked    0\nTitle       0\ndtype: int64"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#데이터-분리-및-가변수화",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#데이터-분리-및-가변수화",
    "title": "01. 머신러닝 (1)",
    "section": "3. 데이터 분리 및 가변수화",
    "text": "3. 데이터 분리 및 가변수화\n\ntarget = \"Survived\"\n\nx = data.drop(target, axis=1)\ny = data[target]\n\nd_cols = [\"Pclass\", \"Sex\", \"Embarked\", \"Title\"]\n\nx = pd.get_dummies(x, columns = d_cols, drop_first = True, dtype=float)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#훈련-및-평가",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#훈련-및-평가",
    "title": "01. 머신러닝 (1)",
    "section": "4. 훈련 및 평가",
    "text": "4. 훈련 및 평가\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1, test_size=0.3)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#모델링-knn",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#모델링-knn",
    "title": "01. 머신러닝 (1)",
    "section": "5. 모델링 KNN",
    "text": "5. 모델링 KNN\n\nx_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 623 entries, 114 to 37\nData columns (total 13 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   Age           623 non-null    float64\n 1   SibSp         623 non-null    int64  \n 2   Parch         623 non-null    int64  \n 3   Fare          623 non-null    float64\n 4   Pclass_2      623 non-null    float64\n 5   Pclass_3      623 non-null    float64\n 6   Sex_male      623 non-null    float64\n 7   Embarked_Q    623 non-null    float64\n 8   Embarked_S    623 non-null    float64\n 9   Title_Miss    623 non-null    float64\n 10  Title_Mr      623 non-null    float64\n 11  Title_Mrs     623 non-null    float64\n 12  Title_Others  623 non-null    float64\ndtypes: float64(11), int64(2)\nmemory usage: 68.1 KB\n\n\n\n# 1단계: 불러오기\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# 2단계: 선언하기\nmodel = KNeighborsClassifier()\n\n# 3단계: 학습하기\nmodel.fit(x_train, y_train)\n\n# 4단계: 예측하기\ny_pred = model.predict(x_test.values)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#분류-성능-평가",
    "href": "posts/DX/04. 머신러닝/2023-09-12-01. 머신러닝 (1).html#분류-성능-평가",
    "title": "01. 머신러닝 (1)",
    "section": "6. 분류 성능 평가",
    "text": "6. 분류 성능 평가\n\nfrom sklearn.metrics import *\n\n\nacc = accuracy_score(y_pred, y_test)\npre = precision_score(y_pred,y_test,average = None)\nre = recall_score(y_pred,y_test,average = None)\nf = f1_score(y_pred,y_test,average = None)\n\n\ndied = pd.DataFrame({\"measure\" : [\"acc\",\"precision\",\"recall\",\"f1-score\"],\n              \"value\" : [acc, pre[0], re[0], f[0]],\n              \"label\" : [\"died\"]*4})\n\nsurvived = pd.DataFrame({\"measure\" : [\"acc\",\"precision\",\"recall\",\"f1-score\"],\n              \"value\" : [acc, pre[1], re[1], f[1]],\n              \"label\" : [\"Survived\"]*4})\n\n\npd.concat([died, survived],axis=0)\n\n\n\n\n\n\n\n\nmeasure\nvalue\nlabel\n\n\n\n\n0\nacc\n0.686567\ndied\n\n\n1\nprecision\n0.823529\ndied\n\n\n2\nrecall\n0.688525\ndied\n\n\n3\nf1-score\n0.750000\ndied\n\n\n0\nacc\n0.686567\nSurvived\n\n\n1\nprecision\n0.504348\nSurvived\n\n\n2\nrecall\n0.682353\nSurvived\n\n\n3\nf1-score\n0.580000\nSurvived\n\n\n\n\n\n\n\n\nfig = pd.concat([died, survived],axis=0).\\\n            plot(x= \"label\", y = \"value\", color= \"label\", kind= \"bar\",\n                facet_col = \"measure\",facet_col_wrap = 2, backend= \"plotly\",width=500,height=500)\n\nfig.update_yaxes(range = [0.45, 0.85])"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-05-01. 데이터 수집 (2).html",
    "href": "posts/DX/03. 데이터 수집/2023-09-05-01. 데이터 수집 (2).html",
    "title": "01. 데이터 수집 (2)",
    "section": "",
    "text": "- 크롤링 단계\n\n크롬 개발자 도구를 이용하여 URL을 찾기\nrequest(url) &lt;-&gt; response( data(json,html) )\ndata(json, html) \\(\\to\\) list, dict \\(\\to\\) 데이터프레임으로 변환!"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-05-01. 데이터 수집 (2).html#step-1.-request-token-얻기",
    "href": "posts/DX/03. 데이터 수집/2023-09-05-01. 데이터 수집 (2).html#step-1.-request-token-얻기",
    "title": "01. 데이터 수집 (2)",
    "section": "step 1. request token 얻기",
    "text": "step 1. request token 얻기\n1. 링크로 가서 로그인\n2. application 등록\n\n3 발급받은 Id와 key값 가져오기\n\nclient_id = \"mLybjqT4lxorLlFR0q21\"\n\nclient_secret = \"AhgPjWhfH_\"\n\n- 잠시요약\n\n어제와 다른점 1 : API를 등록하고 key값을 받아서 request를 해야함\n어제와 다른점 2 : 문서를 확인해서 url을 받아옴"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-05-01. 데이터 수집 (2).html#step2.-파파고-번역-api",
    "href": "posts/DX/03. 데이터 수집/2023-09-05-01. 데이터 수집 (2).html#step2.-파파고-번역-api",
    "title": "01. 데이터 수집 (2)",
    "section": "step2. 파파고 번역 api",
    "text": "step2. 파파고 번역 api\n\n파파고 번역 api\n\n\nhttps://developers.naver.com/docs/papago/\n사용법\n\nhttps://developers.naver.com/docs/papago/papago-nmt-api-reference.md\n\n\n\n(1) url, header data, 임의 data를 입력\n\n# url\nurl = \"https://openapi.naver.com/v1/papago/n2mt\"\n\n# header data\nheaders = {\n    \"Content-Type\" : \"application/json\",\n    \"X-Naver-Client-Id\" : client_id,\n    \"X-Naver-Client-Secret\" : client_secret\n}\n\n# data\nko_text = \"웹크롤링은 재미있고 돈이 됩니다.\"\nparams = {\"source\" : \"ko\", \"target\" : \"en\", \"text\" : ko_text}\nparams\n\n{'source': 'ko', 'target': 'en', 'text': '웹크롤링은 재미있고 돈이 됩니다.'}\n\n\n\n\n(2) Request(url,headers,data), Response\n- 일단 에러(400)이 뜬다. 당황하지말자\n\nresponse = requests.post(url,data=params, headers=headers)\nresponse\n\n&lt;Response [400]&gt;\n\n\n\nresponse.text\n\n'{\"errorCode\":\"-10001\",\"errorMessage\":\"INVALID_REQUEST\"}'\n\n\n- 에러를 확인헤보니 내가 요청할 때 뭔가 잘못요청했다….\n\n이유 : 웹 환경에서는 한글을 쓸 수가 없음 (영문, 숫자, 특수문자만 사용이 가능하다.)\n'웹크롤링은 재미있고 돈이 됩니다.' 이것 때문에 그럼\n인코딩 후 인코딩한 데이터를 전달하면 끝!\n\n\njson.dumps(params)\n\n'{\"source\": \"ko\", \"target\": \"en\", \"text\": \"\\\\uc6f9\\\\ud06c\\\\ub864\\\\ub9c1\\\\uc740 \\\\uc7ac\\\\ubbf8\\\\uc788\\\\uace0 \\\\ub3c8\\\\uc774 \\\\ub429\\\\ub2c8\\\\ub2e4.\"}'\n\n\n- 인코딩한 데이터를 전달하니 정상적으로 작동한다!\n\nresponse = requests.post(url,data=json.dumps(params), headers=headers)\nresponse\n\n&lt;Response [200]&gt;\n\n\n\nresponse.json()\n\n{'message': {'result': {'srcLangType': 'ko',\n   'tarLangType': 'en',\n   'translatedText': 'Web crawling is fun and money.',\n   'engineType': 'N2MT'},\n  '@type': 'response',\n  '@service': 'naverservice.nmt.proxy',\n  '@version': '1.0.0'}}\n\n\n- 위에 딕셔너리의 구조에 맞게 접근하여 번역된 글씨체를 읽어옴\n\nen_text = response.json()[\"message\"][\"result\"][\"translatedText\"]\nen_text\n\n'Web crawling is fun and money.'\n\n\n\n\n(3) 위 과정을 함수로 작성\n\ndef translate (ko_text) :\n    # 1. 어플리케이션 key가 필요\n        client_id = \"mLybjqT4lxorLlFR0q21\"\n\n        client_secret = \"AhgPjWhfH_\"\n    # 2. url, header, 요청할 data가 필요\n        url = \"https://openapi.naver.com/v1/papago/n2mt\"\n        headers = {\n            \"Content-Type\" : \"application/json\",\n            \"X-Naver-Client-Id\" : client_id,\n            \"X-Naver-Client-Secret\" : client_secret\n        }\n        params = {\"source\" : \"ko\", \"target\" : \"en\", \"text\" : ko_text}\n    # 3. request(url,header,json.dumper(한국어 데이터)) -&gt; response\n        response = requests.post(url,data=json.dumps(params), headers=headers)\n    # 4. response.json()[\"message\"][\"result\"][\"translatedText\"]\n        en_text = response.json()[\"message\"][\"result\"][\"translatedText\"]\n        \n        return en_text\n\n\ntranslate(\"난 짱\")\n\n\"I'm the best\"\n\n\n\n\n(4) 한글 엑셀파일을 영문으로…\n\n%ls covid.xlsx\n\n D 드라이브의 볼륨: 새 볼륨\n 볼륨 일련 번호: EC13-2D50\n\n D:\\projects\\mysite2\\posts\\DX\\03. 데이터 수집 디렉터리\n\n2023-09-05  오전 11:16             8,911 covid.xlsx\n               1개 파일               8,911 바이트\n               0개 디렉터리  1,985,536,040,960 바이트 남음\n\n\n\ndf =  pd.read_excel(\"covid.xlsx\")\ndf.head()\n\n\n\n\n\n\n\n\ncategory\ntitle\n\n\n\n\n0\n101\nSK바이오사이언스, 코로나19 백신 임상 3상 시험계획 제출\n\n\n1\n102\n고양시 노래연습장 코로나19 누적확진 41명\n\n\n2\n103\n코로나19 신규 감염, 28일 오후 9시까지 542명\n\n\n3\n103\n프로야구 수도권 구단서 코로나19 확진자 발생\n\n\n4\n104\n\"코로나 확진자 '0명'인 날은 절대 오지 않는다\" 美전문가\n\n\n\n\n\n\n\n\ndf[\"en_title\"] = df[\"title\"].apply(translate)\ndf.head()\n\n\n\n\n\n\n\n\ncategory\ntitle\nen_title\n\n\n\n\n0\n101\nSK바이오사이언스, 코로나19 백신 임상 3상 시험계획 제출\nSK Bioscience Submits Phase 3 Trial Plan for C...\n\n\n1\n102\n고양시 노래연습장 코로나19 누적확진 41명\n41 cumulative confirmed cases of COVID-19 at t...\n\n\n2\n103\n코로나19 신규 감염, 28일 오후 9시까지 542명\nNew COVID-19 infections, 542 people by 9 p.m. ...\n\n\n3\n103\n프로야구 수도권 구단서 코로나19 확진자 발생\nA confirmed case of COVID-19 occurred at a clu...\n\n\n4\n104\n\"코로나 확진자 '0명'인 날은 절대 오지 않는다\" 美전문가\n\"The day when there are '0' confirmed cases of..."
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-05-01. 데이터 수집 (2).html#일반-인증키decoding을-복사",
    "href": "posts/DX/03. 데이터 수집/2023-09-05-01. 데이터 수집 (2).html#일반-인증키decoding을-복사",
    "title": "01. 데이터 수집 (2)",
    "section": "1. 일반 인증키(Decoding)을 복사",
    "text": "1. 일반 인증키(Decoding)을 복사\n\nkey = \"2CjDf/qtlvs6cVGGPWlOxuA17ErE10FWrFyaKbxNGn6A0xcnckRQLFzLIMF2X1/3K/bfREKbKEv/PkfB7zVSuw==\""
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-05-01. 데이터 수집 (2).html#url-key-zonename",
    "href": "posts/DX/03. 데이터 수집/2023-09-05-01. 데이터 수집 (2).html#url-key-zonename",
    "title": "01. 데이터 수집 (2)",
    "section": "2. url + key + zonename",
    "text": "2. url + key + zonename\n\nzonename = \"서울역\"\nurl = 'http://apis.data.go.kr/1613000/CarSharingInfoService/getCarZoneListByName'\nurl += f\"?serviceKey={key}&zoneName={zonename}&_type=json\"\nurl\n\n'http://apis.data.go.kr/1613000/CarSharingInfoService/getCarZoneListByName?serviceKey=2CjDf/qtlvs6cVGGPWlOxuA17ErE10FWrFyaKbxNGn6A0xcnckRQLFzLIMF2X1/3K/bfREKbKEv/PkfB7zVSuw==&zoneName=서울역&_type=json'"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-05-01. 데이터 수집 (2).html#response",
    "href": "posts/DX/03. 데이터 수집/2023-09-05-01. 데이터 수집 (2).html#response",
    "title": "01. 데이터 수집 (2)",
    "section": "3. response",
    "text": "3. response\n\nresponse = requests.get(url)\n#response.json()\n\n\ndata = response.json()[\"response\"][\"body\"][\"items\"][\"item\"]\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\naddress\nlatitude\nlongitude\ntype\nzoneId\nzoneName\n\n\n\n\n0\n서울 중구 남대문로5가 581\n37.554298\n126.973892\n1\nS0000808\n서울역 10번출구(서울시티타워)(운영종료)\n\n\n1\n서울 용산구 동자동 43-205 서울역(철도역)\n37.555862\n126.970505\n2\n10519\n서울역사\n\n\n2\n서울 용산구 동자동 45 센트레빌아스테리움서울\n37.551563\n126.973236\n2\n10981\n서울역 12번출구(KDB생명타워)\n\n\n3\n서울 중구 남대문로5가 827 T타워\n37.553902\n126.975677\n2\n12196\n서울역 10번출구 옆(T타워)\n\n\n4\n서울 용산구 청파로 369(서계동)\n37.552456\n126.968330\n2\n1754\n롯데렌탈 서울역지점 2층(B)\n\n\n5\n서울 용산구 동자동 56 트윈시티 남산\n37.551189\n126.972939\n2\n7426\n서울역 12번출구(갈월동)\n\n\n6\n서울 용산구 서계동 47-2 대한통운서울지사\n37.552486\n126.968964\n2\n9442\n서울역 15번출구(국립극단옆EV)"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-05-01. 데이터 수집 (2).html#html-intro",
    "href": "posts/DX/03. 데이터 수집/2023-09-05-01. 데이터 수집 (2).html#html-intro",
    "title": "01. 데이터 수집 (2)",
    "section": "1. html intro",
    "text": "1. html intro\n- 웹페이지의 레이아웃 및 텍스트 데이터를 출력하는 언어\n- 아래와 같은 계층적 구조를 가짐 (각각의 계층은 같은 계층끼리도 서로 하위에 포함시킬 수 있다.)\n\nelement : 시작 tag와 끝 tag로 구성 (&lt;div&gt; python /&lt;div&gt;)\n\ntag :\n\ntext\n\n\n\n- 시작태그(&lt;div&gt;) 안에는 속성값이 들어있다. ((id, class, attribute 등등))\n- id, class : 엘리먼트를 선택하기위한 용도로 사용되는 속성값 (id는 고유값, 클래스는 중복된 값을 가질 수 있으나.. 띄어쓰기로 구분한다.)\n- attribute : 정보를 저장하기 위한 속성값\n\nimg src = \"url......\""
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-05-01. 데이터 수집 (2).html#tag-종류",
    "href": "posts/DX/03. 데이터 수집/2023-09-05-01. 데이터 수집 (2).html#tag-종류",
    "title": "01. 데이터 수집 (2)",
    "section": "2. tag 종류",
    "text": "2. tag 종류\n- div : 레이아웃을 나타내는 tag\n- p, span : 문자열을 나타내주는 tag\n- ul, li : 목록을 나타내주는 tag\n- a : link를 나타내주는 tag -&gt; 속성값으로 href를 가짐\n- img : 이미지, source 약자인 src를 속성값으로 가지는 tag"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-05-01. 데이터 수집 (2).html#css-selector",
    "href": "posts/DX/03. 데이터 수집/2023-09-05-01. 데이터 수집 (2).html#css-selector",
    "title": "01. 데이터 수집 (2)",
    "section": "3. css selector",
    "text": "3. css selector\n- 엘리먼트를 선택하기 위한 문법\n- 아래와 같은 두 개의 element가 있다고하자.\n&lt;div&gt; id=\"d1\" class = \"data no1\" data1 value = \"1\"&lt;/div&gt;\n&lt;p&gt; id=\"d2\" class = \"data no2\" data2&lt;/p&gt;\n- element 선택 1. tag를 이용\n\ntag -&gt; div -&gt; data1\ntag -&gt; p -&gt; data2\n\n- element 선택 2. id를 이용\n\n#d1 : 엘리먼트 1 선택\n\n- element 선택 3. class를 이용\n\nclass : .data1 \\(\\to\\) data1,data2 엘리먼트 선택\nclass : .no1, .no2 \\(\\to\\) data1,data2 엘리먼트 선택\n\n- 속성값을 이용하여 선택\n\nattribute : [value = “1”] : data1 엘리먼트 선택\n\n- n번째 엘리먼트 선택법\n\np:nth-child(2) : data2 엘리먼트 선택 (각 레이아웃에서 두 번째 엘리먼트 중에 tag가 p이면 선택하라는 의미)\n\n&lt;div&gt;\n    &lt;p class=\"d\"&gt;data1&lt;/p&gt;\n    &lt;p class=\" \"&gt;data2&lt;/p&gt;\n    &lt;span class=\"d\" data3&gt; \n&lt;\\div&gt;\n\n* .d:nth-child(2) : `두 번째` 엘리먼트 중에 `class`가 `d`이면 선택하라는 의미 $\\to$ 아무것도 선택안됌\n* .d:nth-child(3) : `data3` 엘리먼트 선택\n\n***\n\n`-` 계층적 선택\n\n```python\n&lt;div&gt;\n    &lt;p class=\"d\"&gt;data1&lt;/p&gt;\n    &lt;p class=\" \"&gt;data2&lt;/p&gt;\n    &lt;span class=\"d\" data3&gt; \n    &lt;div class= \"\"&gt; &lt;p class=\"d\"&gt;data4&lt;/p&gt; &lt;/div&gt;\n&lt;/div&gt;\n\n* div &gt; p : data1, data2 선택 (한 단계 아래의 엘리먼트 중에서!)\n\n* div p : data1, data2, data3 엘리먼트 선택 (모든 하위 엘리먼트 중에서!)\n\n## 실습 1 : 네이버 연관 검색어 수집\n\n`-` 정적 데이터 수집(`html`)\n\n`-` 삼성전자 연관검색어 수집\n\n### 0. import\n\n::: {.cell tags='[]' execution_count=84}\n``` {.python .cell-code}\nimport pandas as pd \nimport requests \nfrom bs4 import BeautifulSoup\n:::\n\n1. URL 찾기\n\nquery = '삼성전자' \nurl = f'https://search.naver.com/search.naver?query={query}' \nurl\n\n'https://search.naver.com/search.naver?query=삼성전자'\n\n\n\n\n2. request(url), response(data)\n\nresponse = requests.get(url)\nresponse\n\n&lt;Response [200]&gt;\n\n\n\n\n3. html(str), bs object\n- bs 오브젝트 생성\n\ndom = BeautifulSoup(response.text,\"html.parser\")\n\ntype(dom)\n\nbs4.BeautifulSoup\n\n\n\n\n4. 크롤링\n- 브라우저에서 해당 연관검색어의 selector을 카피\n\nselector = \"#nx_right_related_keywords &gt; div &gt; div.related_srch &gt; ul &gt; li\"\n\n- .select(css-selector) \\(\\to\\) text\n\nelements = dom.select(selector)\n\n\nlen(elements)\n\n10\n\n\n- .select_one(css-selector) \\(\\to\\) text\n\nelements[0].select_one(\".tit\").text\n\n'삼성전자주가'\n\n\n- 리스트 컴프리헨션으로 연관검색어 정리\n\nkeywords = [i.select_one(\".tit\").text for i in elements]\n#pd.DataFrame({\"keywords\" : keywords})\n\n\n\n5. 함수로 작성\n\ndef c(query = '삼성전자') : \n    # 1. url 입력\n    url = f'https://search.naver.com/search.naver?query={query}' \n    # 2. requset -&gt; response\n    response = requests.get(url)\n    # 3. bs객체 생성\n    dom = BeautifulSoup(response.text,\"html.parser\")\n    # 4. 연관검색어 개체의 selector 카피\n    selector = \"#nx_right_related_keywords &gt; div &gt; div.related_srch &gt; ul &gt; li\"\n    # 5. 키워드로 저장\n    elements = dom.select(selector)\n    keywords = [i.select_one(\".tit\").text for i in elements]\n    \n    return pd.DataFrame({\"keywords\" : keywords})\n\n\n#c()"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-05-01. 데이터 수집 (2).html#실습-2-지마켓-크롤링",
    "href": "posts/DX/03. 데이터 수집/2023-09-05-01. 데이터 수집 (2).html#실습-2-지마켓-크롤링",
    "title": "01. 데이터 수집 (2)",
    "section": "실습 2 : 지마켓 크롤링",
    "text": "실습 2 : 지마켓 크롤링\n- 지마켓 베스트 상품 크롤링\n\n1. url 등록\n\nurl = \"https://www.gmarket.co.kr/n/best\"\n\n\n\n2 response\n\nresponse = requests.get(url)\n\n\n\n3 bs객체 생성\n\ndom = BeautifulSoup(response.text,\"html.parser\")\n\n\n\n4 selector 카피\n\nselector = \"#gBestWrap &gt; div.best-list &gt; ul &gt; li\"\n\n\n\n5 elements 생성 후 타이틀, 이미지url, 할인가 불러오기\n\nelements = dom.select(selector)\n\n\nelement = elements[0]\n\n\ndata = {\n    \"title\": element.select_one(\".itemname\").text,\n    \"img\" : \"http:\" + element.select_one(\"img\").get(\"src\"),\n    \"sprice\": element.select_one(\".s-price\").text\n}\n\n\ndata\n\n{'title': '자연산 손질 통 오징어 10미(1.3kg내외)',\n 'img': 'http://gdimg.gmarket.co.kr/3106295509/still/300?ver=20230905',\n 'sprice': '할인가24,500원 30%'}\n\n\n\n\n6 loop로 모든 데이터를 읽은 후 데이터 프레임을 변환\n\nitems = []\nfor element in elements :\n    items.append({\n    \"title\": element.select_one(\".itemname\").text,\n    \"img\" : \"http:\" + element.select_one(\"img\").get(\"src\"),\n    \"sprice\": element.select_one(\".s-price\").text\n    })\n\n\ndf = pd.DataFrame(items)\n\n\n\n7 상품 이미지 데이터 저장\n(1) 디렉토리 생성\n\nimport os\npath = \"imgs\"\nos.makedirs(path)\n\n\n%ls\n\n D 드라이브의 볼륨: 새 볼륨\n 볼륨 일련 번호: EC13-2D50\n\n D:\\projects\\mysite2\\posts\\DX\\03. 데이터 수집 디렉터리\n\n2023-09-05  오후 03:56    &lt;DIR&gt;          .\n2023-09-04  오후 05:12    &lt;DIR&gt;          ..\n2023-09-05  오전 09:31    &lt;DIR&gt;          .ipynb_checkpoints\n2023-09-04  오후 05:11           235,320 2023-09-04-00. 데이터 수집 (1).ipynb\n2023-09-05  오후 03:54            44,753 2023-09-05-01. 데이터 수집 (2).ipynb\n2023-09-05  오전 10:18            60,688 API.png\n2023-09-05  오전 11:16             8,911 covid.xlsx\n2023-09-05  오후 03:56    &lt;DIR&gt;          imgs\n               4개 파일             349,672 바이트\n               4개 디렉터리  1,985,535,983,616 바이트 남음\n\n\n(2) img-url\n\nlink = df.loc[0,\"img\"]\nlink\n\n'http://gdimg.gmarket.co.kr/3106295509/still/300?ver=20230905'\n\n\n(3) request\n\nresponse = requests.get(link)\nresponse\n\n&lt;Response [200]&gt;\n\n\n(4) 이미지 저장\n\nwith open(f\"{path}/test.jpg\",\"wb\") as file :\n    file.write(response.content)\n\n(5) 이미지 열어보기\n\nfrom PIL import Image as pil\npil.open(f\"{path}/test.jpg\")\n\n\n\n\n(6) loop를 이용하여 작성\n\nfor idx, data in df[:5].iterrows() : \n    link = data[\"img\"]\n    response = requests.get(link)\n    with open(f\"{path}/{idx}.jpg\",\"wb\") as file :\n        file.write(response.content)\n\n오 신기 신기\n\npil.open(f\"{path}/3.jpg\")"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-09-01-02. 데이터 분석 (2).html",
    "href": "posts/DX/02. 데이터 분석/2023-09-01-02. 데이터 분석 (2).html",
    "title": "02. 데이터 분석 (3)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n# import random as rd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.mosaicplot import mosaic      #mosaic plot!\n\nimport scipy.stats as spst"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-09-01-02. 데이터 분석 (2).html#시각화-1.-survived---age",
    "href": "posts/DX/02. 데이터 분석/2023-09-01-02. 데이터 분석 (2).html#시각화-1.-survived---age",
    "title": "02. 데이터 분석 (3)",
    "section": "시각화 1. (Survived <-> Age)",
    "text": "시각화 1. (Survived &lt;-&gt; Age)\n\ncommon_norm = True (기본값) : 칵 클래스의 비율을 합쳐서 표시 (두 그래프의 아래 면적의 합이 1)\ncommon_norm = False: 각각의 클래스로 니누어서 비율을 표시 (각 클래스의 아래 면적은 1)\nmultiple = ‘fill’: 각 클래스에 따른 비율을 비교 (각 클래스의 아래 면적은 1)\n\n\ntitanic.Survived.value_counts(normalize=True)\n\nSurvived\n0    0.616162\n1    0.383838\nName: proportion, dtype: float64\n\n\n\n\nCode\nfig, axes = plt.subplots(2,3,figsize=(12,8))\n\n(ax1,ax2,ax3), (ax4,ax5,ax6) = axes\n\nsns.histplot(x='Age', data = titanic, hue = 'Survived',ax=ax1,stat=\"density\")\nsns.histplot(x='Age', data = titanic, hue = 'Survived', common_norm = False,ax=ax2,stat=\"density\")\nsns.histplot(x='Age', data = titanic, hue = 'Survived', multiple=\"fill\",ax=ax3)\n\nax3.axhline(titanic['Survived'].mean(), color = 'r')\nsns.kdeplot(x='Age', data = titanic, hue ='Survived',ax=ax4)\nsns.kdeplot(x='Age', data = titanic, hue ='Survived',\n            common_norm = False,ax=ax5)\nsns.kdeplot(x='Age', data = titanic, hue ='Survived'\n            , multiple = 'fill',ax=ax6)\nax6.axhline(titanic['Survived'].mean(), color = 'r')\nfig.tight_layout()"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-09-01-02. 데이터 분석 (2).html#시각화-2.-survived---fare",
    "href": "posts/DX/02. 데이터 분석/2023-09-01-02. 데이터 분석 (2).html#시각화-2.-survived---fare",
    "title": "02. 데이터 분석 (3)",
    "section": "시각화 2. (Survived <-> Fare)",
    "text": "시각화 2. (Survived &lt;-&gt; Fare)\n\n\nCode\nfig, axes = plt.subplots(2,3,figsize=(12,8))\n\n(ax1,ax2,ax3), (ax4,ax5,ax6) = axes\n\nsns.histplot(x='Fare', data = titanic, hue = 'Survived',bins=40,ax=ax1,stat=\"density\")\nsns.histplot(x='Fare', data = titanic, hue = 'Survived',bins=40, common_norm = False,ax=ax2,stat=\"density\")\nsns.histplot(x='Fare', data = titanic, hue = 'Survived', multiple=\"fill\",ax=ax3,bins=40)\n\nax3.axhline(titanic['Survived'].mean(), color = 'r')\nsns.kdeplot(x='Fare', data = titanic, hue ='Survived',ax=ax4)\nsns.kdeplot(x='Fare', data = titanic, hue ='Survived',\n            common_norm = False,ax=ax5)\nsns.kdeplot(x='Fare', data = titanic, hue ='Survived'\n            , multiple = 'fill',ax=ax6)\nax6.axhline(titanic['Survived'].mean(), color = 'r')\nfig.tight_layout()\n\n\n\n\n\n\nx = np.linspace(0,1,100)\ny = [1]*100\n\nplt.plot(x,y)"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html",
    "href": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html",
    "title": "00. 데이터 분석 (1)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams['font.family'] = 'Malgun Gothic'\nplt.rcParams['axes.unicode_minus'] = False"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#기본-정보-조회",
    "href": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#기본-정보-조회",
    "title": "00. 데이터 분석 (1)",
    "section": "기본 정보 조회",
    "text": "기본 정보 조회\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 8 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   Fare         891 non-null    float64\n 7   Embarked     889 non-null    object \ndtypes: float64(2), int64(3), object(3)\nmemory usage: 55.8+ KB"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#조건-조회",
    "href": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#조건-조회",
    "title": "00. 데이터 분석 (1)",
    "section": "조건 조회",
    "text": "조건 조회\n- 객실 등급(Pclass) 1등급, 나이(Age) 10살 이하 탑승객 조회\n\ndf.loc[ map(lambda x,y  : (x==1) & (y &lt;=10), df.Pclass, df.Age), : ]\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nFare\nEmbarked\n\n\n\n\n297\n298\n0\n1\nAllison, Miss. Helen Loraine\nfemale\n2.00\n151.5500\nSouthampton\n\n\n305\n306\n1\n1\nAllison, Master. Hudson Trevor\nmale\n0.92\n151.5500\nSouthampton\n\n\n445\n446\n1\n1\nDodge, Master. Washington\nmale\n4.00\n81.8583\nSouthampton\n\n\n\n\n\n\n\n- 객실 등급(Pclass)별 탑승객 수\n\ndf.Pclass.value_counts().reset_index()\n\n\n\n\n\n\n\n\nPclass\ncount\n\n\n\n\n0\n3\n491\n\n\n1\n1\n216\n\n\n2\n2\n184\n\n\n\n\n\n\n\n- 성별(Sex)이 남자인 탑승객과 여자인 탑승객의 나이를 각각 저장하시오.\n\n_m = df.loc[df.Sex==\"male\",[\"Sex\",\"Age\"]]\n_f = df.loc[df.Sex==\"female\",[\"Age\",\"Sex\"]]\n\n- 나이(Age)에 NaN이 아닌 탑승객을 조회하시오.\n\ndf.loc[df.Age ==True, :]\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nFare\nEmbarked\n\n\n\n\n164\n165\n0\n3\nPanula, Master. Eino Viljami\nmale\n1.0\n39.6875\nSouthampton\n\n\n172\n173\n1\n3\nJohnson, Miss. Eleanor Ileen\nfemale\n1.0\n11.1333\nSouthampton\n\n\n183\n184\n1\n2\nBecker, Master. Richard F\nmale\n1.0\n39.0000\nSouthampton\n\n\n381\n382\n1\n3\nNakid, Miss. Maria (\"Mary\")\nfemale\n1.0\n15.7417\nCherbourg\n\n\n386\n387\n0\n3\nGoodwin, Master. Sidney Leonard\nmale\n1.0\n46.9000\nSouthampton\n\n\n788\n789\n1\n3\nDean, Master. Bertram Vere\nmale\n1.0\n20.5750\nSouthampton\n\n\n827\n828\n1\n2\nMallet, Master. Andre\nmale\n1.0\n37.0042\nCherbourg\n\n\n\n\n\n\n\n- 아래의 데이터에서 날짜(Date)가, 1973-05-01, 1973-06-01, 1973-07-01.1973-08-01을 조회\n\npath = 'https://raw.githubusercontent.com/DA4BAM/dataset/master/air2.csv'\nair = pd.read_csv(path)\n\n\nd_l = [\"1973-05-01\", \"1973-06-01\", \"1973-07-01\",\"1973-08-01\"]\n\n\nair.loc[map(lambda x : x in d_l, air.Date),:]\n\n\n\n\n\n\n\n\nOzone\nSolar.R\nWind\nTemp\nDate\n\n\n\n\n0\n41\n190.0\n7.4\n67\n1973-05-01\n\n\n31\n34\n286.0\n8.6\n78\n1973-06-01\n\n\n61\n135\n269.0\n4.1\n84\n1973-07-01\n\n\n92\n39\n83.0\n6.9\n81\n1973-08-01\n\n\n\n\n\n\n\n- 오존 농도 10~20 사이의 데이터를 조회\n\nair.loc[air.Ozone.between(10,20),:].head()\n\n\n\n\n\n\n\n\nOzone\nSolar.R\nWind\nTemp\nDate\n\n\n\n\n2\n12\n149.0\n12.6\n74\n1973-05-03\n\n\n3\n18\n313.0\n11.5\n62\n1973-05-04\n\n\n4\n19\nNaN\n14.3\n56\n1973-05-05\n\n\n7\n19\n99.0\n13.8\n59\n1973-05-08\n\n\n9\n20\n194.0\n8.6\n69\n1973-05-10"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#값-변경",
    "href": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#값-변경",
    "title": "00. 데이터 분석 (1)",
    "section": "값 변경",
    "text": "값 변경\n[titanic] 승선지역(Embarked)을 변경 * Southamton –&gt; S * Cherbourg –&gt; C * Queenstown –&gt; Q\n\ndf.Embarked\n\n0      Southampton\n1        Cherbourg\n2      Southampton\n3      Southampton\n4      Southampton\n          ...     \n886    Southampton\n887    Southampton\n888    Southampton\n889      Cherbourg\n890     Queenstown\nName: Embarked, Length: 891, dtype: object\n\n\n\ndf.Embarked=df[\"Embarked\"].map({\"Southampton\":\"S\",\n                   \"Cherbourg\" : \"C\",\n                   \"Queenstown\":\"Q\"})\n\n\ndf.Embarked.unique()\n\narray(['S', 'C', 'Q', nan], dtype=object)\n\n\n- [titanic] 운임(Fare)을 다음과 같이 변경\n\n&lt;= 30 ==&gt; ‘L’\n&lt;= 100 ==&gt; ‘M’\n100 &lt; ==&gt; ‘H’\n\n\nbins = [-np.Inf, 30, 100, np.Inf]\n\n\nlabels = list(\"LMH\")\n\n\ndf.Fare = pd.cut(df.Fare,bins = bins, labels= labels)\n\n\ndf.Fare.unique()\n\n['L', 'M', 'H']\nCategories (3, object): ['L' &lt; 'M' &lt; 'H']\n\n\n\nnp.where(\\(\\star\\star\\star\\))\n- [titanic] 성별(Sex)을 다음과 같이 변경\n\nfemale ==&gt; 0\nmale ==&gt; 1\n\n\ndf.Sex = np.where(df.Sex==\"male\",1,0).tolist()\n\n\ndf.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nFare\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\n1\n22.0\nL\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n0\n38.0\nM\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\n0\n26.0\nL\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n0\n35.0\nM\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\n1\n35.0\nL\nS"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#데이터-로드",
    "href": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#데이터-로드",
    "title": "00. 데이터 분석 (1)",
    "section": "데이터 로드",
    "text": "데이터 로드\n\npath = 'https://raw.githubusercontent.com/DA4BAM/dataset/master/airquality_simple2.csv'\ndata = pd.read_csv(path)\ndata['Date'] = pd.to_datetime(data['Date'])\ndata.dropna(axis = 0, inplace = True)\ndata.head()\n\n\n\n\n\n\n\n\nOzone\nSolar.R\nWind\nTemp\nDate\n\n\n\n\n0\n34.0\n286.0\n8.6\n78.0\n1973-06-01\n\n\n1\n29.0\n287.0\n9.7\n74.0\n1973-06-02\n\n\n2\n18.0\n242.0\n16.1\n67.0\n1973-06-03\n\n\n3\n48.0\n186.0\n9.2\n84.0\n1973-06-04\n\n\n4\n49.0\n220.0\n8.6\n85.0\n1973-06-05"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#기본-차트-그리기-matplotlib",
    "href": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#기본-차트-그리기-matplotlib",
    "title": "00. 데이터 분석 (1)",
    "section": "기본 차트 그리기 matplotlib",
    "text": "기본 차트 그리기 matplotlib\n\n기본\n\ndata.plot(x=\"Date\",y=\"Temp\",figsize=(10,2))\n\n&lt;Axes: xlabel='Date'&gt;\n\n\n\n\n\n\ndata.plot(x=\"Date\", y= \"Ozone\",figsize=(10,3),\n            xlabel =\"Date\",ylabel=\"Ozone\",title = \"Daily Airquality\")\n\n&lt;Axes: title={'center': 'Daily Airquality'}, xlabel='Date', ylabel='Ozone'&gt;\n\n\n\n\n\n\ndata.plot(x=\"Date\", y= \"Ozone\",figsize=(10,3),\n            xlabel =\"Date\",ylabel=\"Ozone\",title = \"Daily Airquality\",\n             color = \"green\",marker=\"o\",alpha=0.3)\n\n&lt;Axes: title={'center': 'Daily Airquality'}, xlabel='Date', ylabel='Ozone'&gt;\n\n\n\n\n\n\n\n그래프 겹처그리기\n\nplt.plot(data.Date, data.Temp,\"--r\")\nplt.plot(data.Date, data.Ozone,\"--g\")\nplt.legend([\"Temp\",\"Ozone\"],loc=\"upper right\")\nplt.title(\"Temp, Ozone\")\nplt.grid()\nplt.xlabel(\"Date\")\nplt.ylabel(\"value\")\nplt.xticks(rotation=45)\n\n(array([1247., 1251., 1255., 1259., 1263., 1267., 1271., 1275., 1277.]),\n [Text(1247.0, 0, '1973-06-01'),\n  Text(1251.0, 0, '1973-06-05'),\n  Text(1255.0, 0, '1973-06-09'),\n  Text(1259.0, 0, '1973-06-13'),\n  Text(1263.0, 0, '1973-06-17'),\n  Text(1267.0, 0, '1973-06-21'),\n  Text(1271.0, 0, '1973-06-25'),\n  Text(1275.0, 0, '1973-06-29'),\n  Text(1277.0, 0, '1973-07-01')])\n\n\n\n\n\n\n\n그래프 범위\n\ndata.plot(x=\"Date\",y=\"Ozone\",\n          ylim=(min(data.Ozone)-1,max(data.Ozone)+1),\n          grid=True,ylabel=\"Ozone\",figsize=(4,4))\n\n&lt;Axes: xlabel='Date', ylabel='Ozone'&gt;\n\n\n\n\n\n\n\n다중 그래프 그리기\n- 방법 1 : 내가 쓰던 방식\n\nfig,axes =plt.subplots(1,4,figsize=(12,4))\nfig.autofmt_xdate(rotation=45)\nl1 = data.columns\ncolor = [\"red\",\"green\",\"blue\",\"orange\"]\nfor i in range(4) :\n    axes[i].plot(data[\"Date\"],data[l1[i]],color=color[i],linestyle=\"dashed\",alpha=0.4)\n    axes[i].set_title(l1[i])\n\nfig.tight_layout()\n\n\n\n\n\nplt.figure(figsize = (12,4))\nplt.subplot(1,3,1)\nplt.plot('Date', 'Temp', data = data)\nplt.title('Temp')\nplt.xticks(rotation = 40)\nplt.grid()\n\nplt.subplot(1,3,2)\nplt.plot('Date', 'Wind', data = data)\nplt.title('Wind')\nplt.xticks(rotation = 40)\nplt.grid()\n\nplt.subplot(1,3,3)\nplt.plot('Date', 'Ozone', data = data)\nplt.title('Ozone')\nplt.xticks(rotation = 40)\nplt.grid()\n\nplt.tight_layout() # 그래프간 간격을 적절히 맞추기\nplt.show()"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#데이터-로드-1",
    "href": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#데이터-로드-1",
    "title": "00. 데이터 분석 (1)",
    "section": "데이터 로드",
    "text": "데이터 로드\n\npath1 = 'https://raw.githubusercontent.com/DA4BAM/dataset/master/titanic_simple.csv'\ntitanic = pd.read_csv(path1)\npath2 = 'https://raw.githubusercontent.com/DA4BAM/dataset/master/air2.csv'\nair = pd.read_csv(path2)"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#분포-시각화-hist",
    "href": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#분포-시각화-hist",
    "title": "00. 데이터 분석 (1)",
    "section": "분포 시각화 (hist)",
    "text": "분포 시각화 (hist)\n\nbasic\n\nplt.hist(titanic.Fare, bins = 5, edgecolor = 'gray')\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n구간개수 조절\n\nplt.hist(titanic.Fare, bins = 30, edgecolor = 'gray')\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\nfig, axes = plt.subplots(1,2, figsize= (10,4))\n\n(ax1,ax2)= axes\nax1.hist(titanic.Fare,bins = 5, edgecolor = 'gray')\nax1.set_title(\"hist of Fare (bins =5)\")\nax2.hist(titanic.Fare,bins = 30, edgecolor = 'gray')\nax2.set_title(\"hist of Fare (bins =30)\")\n\nText(0.5, 1.0, 'hist of Fare (bins =30)')\n\n\n\n\n\n\n\nseaborn\n\nfig, axes = plt.subplots(1,2, figsize= (10,4))\n\n(ax1,ax2)= axes\nsns.histplot(x=titanic.Fare,ax=ax1, bins = 5)\nsns.histplot(x=titanic.Fare,ax=ax2, bins = 30)\nax1.set_title(\"hist of Fare (bins =5)\")\nax2.set_title(\"hist of Fare (bins =30)\")\n\nText(0.5, 1.0, 'hist of Fare (bins =30)')\n\n\n\n\n\n\n\n\n밀도함수 그래프 (kde plot)\n- 히스토그램의 구간(bin)에 따라 그래프의 모양이 달라진다.\n\n밀도함수 그래프는 막대의 너비를 가정하지 않고 모든 점에서 데이터의 밀도(확률)을 추정\n이는 커널 밀도 방식을 사용하기 때문에 모양이 달라져 발생할 수 있는 혼동을 방지할 수 있다.\n\n\n#sns.histplot(titanic[\"Fare\"])\nsns.kdeplot(titanic[\"Fare\"])\n\n&lt;Axes: xlabel='Fare', ylabel='Density'&gt;\n\n\n\n\n\n\ntitanic.Fare.describe()\n\ncount    891.000000\nmean      32.204208\nstd       49.693429\nmin        0.000000\n25%        7.910400\n50%       14.454200\n75%       31.000000\nmax      512.329200\nName: Fare, dtype: float64\n\n\n- 위 그래프는 0보다 작은 값도 표현하고 있다.\n\n원 데이터의 0보다 작은값은 없지만 컴퓨터에서 확률을 추정하려고 하다보니 위 같이 표현된 것 뿐이다.\n\n- 아래와 같이 히스토그램과 같이 그릴 수도 있다.\n\nsns.histplot(x=titanic.Age,kde=True)\n\n&lt;Axes: xlabel='Age', ylabel='Count'&gt;\n\n\n\n\n\n\n\nboxplot\n- 주의사항 : Nan값이 있으면 그려지지 않는다!\n1 matplot\n\ntitanic.plot(kind=\"box\", y=\"Age\")\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ntitanic.plot(kind=\"box\", y=\"Age\",vert=False)\n\n&lt;Axes: &gt;\n\n\n\n\n\n2 seaborn\n\nfig, axes = plt.subplots(1,2,figsize=(8,4))\nax1,ax2=axes\nsns.boxplot(x = titanic['Age'],ax=ax1)\nsns.boxplot(y = titanic['Age'],ax=ax2)\n\n&lt;Axes: ylabel='Age'&gt;"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#import-1",
    "href": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#import-1",
    "title": "00. 데이터 분석 (1)",
    "section": "import",
    "text": "import\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom ISLP import load_data\nimport seaborn as sns\n\n\ndata = load_data(\"Boston\")\n\n\ndata.head()\n\n\n\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nlstat\nmedv\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n5.33\n36.2\n\n\n\n\n\n\n\n변수설명\n\n\nmedv : 1978 보스턴 주택 가격, 506개 타운의 주택 가격 중앙값 (단위 1,000 달러) &lt;== Target\n\n\n\ncrim 범죄율\nzn 25,000 평방피트를 초과 거주지역 비율\nindus 비소매상업지역 면적 비율\nchas 찰스강변 위치(범주 : 강변1, 아니면 0)\nnox 일산화질소 농도\nrm 주택당 방 수\nage 1940년 이전에 건축된 주택의 비율\ndis 직업센터의 거리\nrad 방사형 고속도로까지의 거리\ntax 재산세율\nptratio 학생/교사 비율\nlstat 인구 중 하위 계층 비율"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#주어진-변수에-대한-hist-boxplot을-그리는-함수-작성",
    "href": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#주어진-변수에-대한-hist-boxplot을-그리는-함수-작성",
    "title": "00. 데이터 분석 (1)",
    "section": "주어진 변수에 대한 hist, boxplot을 그리는 함수 작성",
    "text": "주어진 변수에 대한 hist, boxplot을 그리는 함수 작성\n\ndef f(data,var,bins=30) : \n    fig,axes = plt.subplots(1,2, figsize=(8,3))\n    ax1,ax2 = axes\n    \n    sns.histplot(x=data[var],kde = True,ax=ax1,bins=bins)\n    ax1.set_title(f\"hist of {var}\")\n    sns.boxplot(x=data[var],ax=ax2)\n    ax2.set_title(f\"boxplot of {var}\")\n\n\nmedv(집값)\n\nf(data,\"medv\")\n\n\n\n\n\n정규분포와 유사한 형태이나 medv=50에서 이상치로 보이는 수치들이 보인다.\n대부분에 데이터들이 중앙에 모여있다.\n\n\n\ncrim(범죄율)\n\nf(data,\"crim\",bins=10)\n\n\n\n\n\n범죄율의 분포를 살펴보니 최솟값이 0.6% 이며 최댓값은 89.97%이다.\n대부분은 범죄율이 굉장히 낮다.\n박스플랏의 꼬리가길게 늘어짐\n로그변환 등을 고려해봐도 괜찮을것 같다.\n\n\n\nlstat(하위계층 비율)\n\nf(data,\"lstat\",bins=50)\n\n\n\n\n\n하위계층의 정의가 뭔지.. 궁금해진다.\n하위계층의 비율이 대부분 10 ~ 20% 구간에 몰려있다.\n\n\n\nptratio(교사1명당 학생수)\n\nf(data,\"ptratio\")\n\n\n\n\n\n교육환경이 좋은 동네는 교사 1명당 학생수가 높나?? \\(\\to\\) 이런걸 보고싶을 때가 있으니 역시 다차원 그래프 표현이 필요한 것 같다.\n\n\n\ntax(재산세)\n\nf(data,\"tax\")\n\n\n\n\n\n제산세가 굉장히 높은 구간이 있는데 부자동네인가? 라는 생각을 가지게 된다.\n또한, 제산세가 높은 구간의 비중이 가장 높은데 재산세에 대한 기준이 궁금해진다.\n분포를 살펴본 결과 2개의 분포가 보인다. \\(\\to\\) 두 개의 그룹으로 나누어서 살펴보아야 한다."
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#데이터-전처리",
    "href": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#데이터-전처리",
    "title": "00. 데이터 분석 (1)",
    "section": "데이터 전처리",
    "text": "데이터 전처리\n\ne_table = titanic.Embarked.\\\n            value_counts(normalize=True).reset_index()\ne_table\n\n\n\n\n\n\n\n\nEmbarked\nproportion\n\n\n\n\n0\nSouthampton\n0.724409\n\n\n1\nCherbourg\n0.188976\n\n\n2\nQueenstown\n0.086614\n\n\n\n\n\n\n\n- 생존률이 매우 낮다.\n\n고민 1 : 그럼 구명보트에 가까운사람, 1등석에 탄 사람들만 살았나?? (이런 상황에 대한 도메인 지식이 필요할 때가 있다….)\n\n\ntitanic.Survived.\\\n            value_counts(normalize=True).\\\n                    reset_index()\n\n\n\n\n\n\n\n\nSurvived\nproportion\n\n\n\n\n0\n0\n0.616162\n\n\n1\n1\n0.383838"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#시각화-1.-bar-chart",
    "href": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#시각화-1.-bar-chart",
    "title": "00. 데이터 분석 (1)",
    "section": "시각화 1. bar chart",
    "text": "시각화 1. bar chart\n\nfig,axe = plt.subplots(1,2,figsize=(8,3))\nax1,ax2 =axe\nsns.countplot(y= titanic.Pclass,ax=ax1)\nax1.set_title(\"count of Pclass\")\nsns.countplot(y= titanic.Embarked,ax=ax2)\nax2.set_title(\"count of Embarked\")\nfig.tight_layout()"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#시각화-2.-pie-chart",
    "href": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#시각화-2.-pie-chart",
    "title": "00. 데이터 분석 (1)",
    "section": "시각화 2. Pie chart",
    "text": "시각화 2. Pie chart\n- 범주형 비율을 비교할 때 사용\n\n사실 난 그렇게 좋아하지 않음… \\(\\to\\) 비율로 먼가 어떤 현상을 설명할 때 직관적으로 와닿지가 않는다…\n\n\ntitanic.Pclass.\\\n        value_counts()\n\nPclass\n3    491\n1    216\n2    184\nName: count, dtype: int64\n\n\n- 기본\n\ntitanic.Pclass.\\\n        value_counts().\\\n                plot(kind=\"pie\",y=\"count\",\n                     autopct= \"%.2f%%\")\n\n&lt;Axes: ylabel='count'&gt;\n\n\n\n\n\n- 시작점과 방향 지정\n\ntitanic.Pclass.\\\n        value_counts().\\\n                plot(kind=\"pie\",y=\"count\",\n                     autopct= \"%.2f%%\", startangle=90,\n                    counterclock = False)\n\n&lt;Axes: ylabel='count'&gt;"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#소감",
    "href": "posts/DX/02. 데이터 분석/2023-08-28-00. 데이터 분석 (1).html#소감",
    "title": "00. 데이터 분석 (1)",
    "section": "소감",
    "text": "소감\n\n확실히 나는 데이터를 보고 인사이트를 도출하는 능력이 떨어진다.\n비즈니즈적 의사결정을 위해선 많은 도메인에서의 지식이 필요한 것 같다….\n다른 에이블러님들이 인사이트를 도출하시는 것들을 보고 많이 반성했다….\n데이터를 잘 다루는 것은 누구나 할 수 있는거라고 생각하나 인사이트를 도출하기 위해선 평소에 다양한 분야에 관심을 가져야 할 필요성을 느꼈다.\n부족한점을 알고 있었지만, 오늘 더 뼈저리게 느낀것 같다…"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html",
    "title": "03. numpy & pandas (4)",
    "section": "",
    "text": "import plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#import-및-data-load",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#import-및-data-load",
    "title": "03. numpy & pandas (4)",
    "section": "import 및 data load",
    "text": "import 및 data load\n\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/tips(2).csv'\n\n\ndf = pd.read_csv(path)\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 244 entries, 0 to 243\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   total_bill_amount  244 non-null    float64\n 1   tip                244 non-null    float64\n 2   male_female        244 non-null    object \n 3   smoke_yes_no       244 non-null    object \n 4   week_name          244 non-null    object \n 5   dinner_lunch       244 non-null    object \n 6   size               244 non-null    int64  \ndtypes: float64(2), int64(1), object(4)\nmemory usage: 13.5+ KB"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#컬럼-이름-변경-1.-rename",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#컬럼-이름-변경-1.-rename",
    "title": "03. numpy & pandas (4)",
    "section": "컬럼 이름 변경 1. rename",
    "text": "컬럼 이름 변경 1. rename\n\ndf.rename(columns = {\"total_bill_amount\" : \"total_bill\",\n            \"male_female\" : \"sex\",\n            \"smoke_yes_no\" : \"smoker\",\n            \"week_name\": \"day\",\n             \"dinner_lunch\" : \"time\"},inplace=True) \n\n\ndf\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n239\n29.03\n5.92\nMale\nNo\nSat\nDinner\n3\n\n\n240\n27.18\n2.00\nFemale\nYes\nSat\nDinner\n2\n\n\n241\n22.67\n2.00\nMale\nYes\nSat\nDinner\n2\n\n\n242\n17.82\n1.75\nMale\nNo\nSat\nDinner\n2\n\n\n243\n18.78\n3.00\nFemale\nNo\nThur\nDinner\n2\n\n\n\n\n244 rows × 7 columns"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#컬럼-이름-변경-2.-list",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#컬럼-이름-변경-2.-list",
    "title": "03. numpy & pandas (4)",
    "section": "컬럼 이름 변경 2. list",
    "text": "컬럼 이름 변경 2. list\n\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/tips(2).csv'\ndf = pd.read_csv(path)\n\n\ndf.columns = ['total_bill', 'tip', 'sex', 'smoker', 'day', 'time', 'size']\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#컬럼-추가-1.-df.eval",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#컬럼-추가-1.-df.eval",
    "title": "03. numpy & pandas (4)",
    "section": "컬럼 추가 1. df.eval",
    "text": "컬럼 추가 1. df.eval\n\ndf = df.eval(\"f_amt = total_bill + tip\")\ndf.head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\nf_amt\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n18.00\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n12.00\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n24.51\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n26.99\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n28.20"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#컬럼추가-2.-dff_amt",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#컬럼추가-2.-dff_amt",
    "title": "03. numpy & pandas (4)",
    "section": "컬럼추가 2. df[\"f_amt\"]",
    "text": "컬럼추가 2. df[\"f_amt\"]\n\ndf[\"f_amt\"] = df.total_bill + df.tip\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\nf_amt\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n18.00\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n12.00\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n24.51\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n26.99\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n28.20"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#컬럼추가-3.-insert",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#컬럼추가-3.-insert",
    "title": "03. numpy & pandas (4)",
    "section": "컬럼추가 3. insert()",
    "text": "컬럼추가 3. insert()\n\ndf.insert(1,\"div_tb\",df[\"total_bill\"]/df[\"size\"] )\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntotal_bill\ndiv_tb\ntip\nsex\nsmoker\nday\ntime\nsize\nf_amt\n\n\n\n\n0\n16.99\n8.495000\n1.01\nFemale\nNo\nSun\nDinner\n2\n18.00\n\n\n1\n10.34\n3.446667\n1.66\nMale\nNo\nSun\nDinner\n3\n12.00\n\n\n2\n21.01\n7.003333\n3.50\nMale\nNo\nSun\nDinner\n3\n24.51\n\n\n3\n23.68\n11.840000\n3.31\nMale\nNo\nSun\nDinner\n2\n26.99\n\n\n4\n24.59\n6.147500\n3.61\nFemale\nNo\nSun\nDinner\n4\n28.20\n\n\n\n\n\n\n\n- day변수를 이용하여 휴일 변수 holiday열을 만들기\n\ndf.day.value_counts()\n\nday\nSat     87\nSun     76\nThur    62\nFri     19\nName: count, dtype: int64\n\n\n\ndf[\"holiday\"]=[1 if \"S\" in i  else 0 for i in df.day]\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntotal_bill\ndiv_tb\ntip\nsex\nsmoker\nday\ntime\nsize\nf_amt\nholiday\n\n\n\n\n0\n16.99\n8.495000\n1.01\nFemale\nNo\nSun\nDinner\n2\n18.00\n1\n\n\n1\n10.34\n3.446667\n1.66\nMale\nNo\nSun\nDinner\n3\n12.00\n1\n\n\n2\n21.01\n7.003333\n3.50\nMale\nNo\nSun\nDinner\n3\n24.51\n1\n\n\n3\n23.68\n11.840000\n3.31\nMale\nNo\nSun\nDinner\n2\n26.99\n1\n\n\n4\n24.59\n6.147500\n3.61\nFemale\nNo\nSun\nDinner\n4\n28.20\n1"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#컬럼-삭제",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#컬럼-삭제",
    "title": "03. numpy & pandas (4)",
    "section": "컬럼 삭제",
    "text": "컬럼 삭제\n- 단일 열 삭제\n\ndf.drop(\"tip\",axis=1).head()\n\n\n\n\n\n\n\n\ntotal_bill\ndiv_tb\nsex\nsmoker\nday\ntime\nsize\nf_amt\nholiday\n\n\n\n\n0\n16.99\n8.495000\nFemale\nNo\nSun\nDinner\n2\n18.00\n1\n\n\n1\n10.34\n3.446667\nMale\nNo\nSun\nDinner\n3\n12.00\n1\n\n\n2\n21.01\n7.003333\nMale\nNo\nSun\nDinner\n3\n24.51\n1\n\n\n3\n23.68\n11.840000\nMale\nNo\nSun\nDinner\n2\n26.99\n1\n\n\n4\n24.59\n6.147500\nFemale\nNo\nSun\nDinner\n4\n28.20\n1\n\n\n\n\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntotal_bill\ndiv_tb\ntip\nsex\nsmoker\nday\ntime\nsize\nf_amt\nholiday\n\n\n\n\n0\n16.99\n8.495000\n1.01\nFemale\nNo\nSun\nDinner\n2\n18.00\n1\n\n\n1\n10.34\n3.446667\n1.66\nMale\nNo\nSun\nDinner\n3\n12.00\n1\n\n\n2\n21.01\n7.003333\n3.50\nMale\nNo\nSun\nDinner\n3\n24.51\n1\n\n\n3\n23.68\n11.840000\n3.31\nMale\nNo\nSun\nDinner\n2\n26.99\n1\n\n\n4\n24.59\n6.147500\n3.61\nFemale\nNo\nSun\nDinner\n4\n28.20\n1\n\n\n\n\n\n\n\n- 다중 열 삭제\n\ndf.drop([\"tip\",\"sex\"],axis=1).head()\n\n\n\n\n\n\n\n\ntotal_bill\ndiv_tb\nsmoker\nday\ntime\nsize\nf_amt\nholiday\n\n\n\n\n0\n16.99\n8.495000\nNo\nSun\nDinner\n2\n18.00\n1\n\n\n1\n10.34\n3.446667\nNo\nSun\nDinner\n3\n12.00\n1\n\n\n2\n21.01\n7.003333\nNo\nSun\nDinner\n3\n24.51\n1\n\n\n3\n23.68\n11.840000\nNo\nSun\nDinner\n2\n26.99\n1\n\n\n4\n24.59\n6.147500\nNo\nSun\nDinner\n4\n28.20\n1"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#범주값-변경",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#범주값-변경",
    "title": "03. numpy & pandas (4)",
    "section": "범주값 변경",
    "text": "범주값 변경\n\n1. map\n- 남자는 1, 여자는 0으로 인코딩\n\ndf[\"sex\"] = df[\"sex\"].map({\"Male\":0,\"Female\":1})\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntotal_bill\ndiv_tb\ntip\nsex\nsmoker\nday\ntime\nsize\nf_amt\nholiday\n\n\n\n\n0\n16.99\n8.495000\n1.01\n1\nNo\nSun\nDinner\n2\n18.00\n1\n\n\n1\n10.34\n3.446667\n1.66\n0\nNo\nSun\nDinner\n3\n12.00\n1\n\n\n2\n21.01\n7.003333\n3.50\n0\nNo\nSun\nDinner\n3\n24.51\n1\n\n\n3\n23.68\n11.840000\n3.31\n0\nNo\nSun\nDinner\n2\n26.99\n1\n\n\n4\n24.59\n6.147500\n3.61\n1\nNo\nSun\nDinner\n4\n28.20\n1\n\n\n\n\n\n\n\n\n\n2. replace\n\ndf.sex = df.sex.replace([0,1],[\"Male\",\"Female\"])\n\n\n_df = df\n\n\n_df.sex = df.sex.replace({\"Male\":0,\"Female\":1})\n\n\n_df.head()\n\n\n\n\n\n\n\n\ntotal_bill\ndiv_tb\ntip\nsex\nsmoker\nday\ntime\nsize\nf_amt\nholiday\n\n\n\n\n0\n16.99\n8.495000\n1.01\n1\nNo\nSun\nDinner\n2\n18.00\n1\n\n\n1\n10.34\n3.446667\n1.66\n0\nNo\nSun\nDinner\n3\n12.00\n1\n\n\n2\n21.01\n7.003333\n3.50\n0\nNo\nSun\nDinner\n3\n24.51\n1\n\n\n3\n23.68\n11.840000\n3.31\n0\nNo\nSun\nDinner\n2\n26.99\n1\n\n\n4\n24.59\n6.147500\n3.61\n1\nNo\nSun\nDinner\n4\n28.20\n1\n\n\n\n\n\n\n\n\ndf.time = df.time.replace([\"Dinner\",\"Lunch\"],[0,1])\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntotal_bill\ndiv_tb\ntip\nsex\nsmoker\nday\ntime\nsize\nf_amt\nholiday\n\n\n\n\n0\n16.99\n8.495000\n1.01\n1\nNo\nSun\n0\n2\n18.00\n1\n\n\n1\n10.34\n3.446667\n1.66\n0\nNo\nSun\n0\n3\n12.00\n1\n\n\n2\n21.01\n7.003333\n3.50\n0\nNo\nSun\n0\n3\n24.51\n1\n\n\n3\n23.68\n11.840000\n3.31\n0\nNo\nSun\n0\n2\n26.99\n1\n\n\n4\n24.59\n6.147500\n3.61\n1\nNo\nSun\n0\n4\n28.20\n1\n\n\n\n\n\n\n\n\ndf.smoker = df.smoker.replace([\"No\",\"Yes\"],[0,1])\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntotal_bill\ndiv_tb\ntip\nsex\nsmoker\nday\ntime\nsize\nf_amt\nholiday\n\n\n\n\n0\n16.99\n8.495000\n1.01\n1\n0\nSun\n0\n2\n18.00\n1\n\n\n1\n10.34\n3.446667\n1.66\n0\n0\nSun\n0\n3\n12.00\n1\n\n\n2\n21.01\n7.003333\n3.50\n0\n0\nSun\n0\n3\n24.51\n1\n\n\n3\n23.68\n11.840000\n3.31\n0\n0\nSun\n0\n2\n26.99\n1\n\n\n4\n24.59\n6.147500\n3.61\n1\n0\nSun\n0\n4\n28.20\n1"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#범주값-만들기",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#범주값-만들기",
    "title": "03. numpy & pandas (4)",
    "section": "범주값 만들기",
    "text": "범주값 만들기\n\n1. cut\n- 크기(그룹의 수) 를 기준으로 구간을 나눈다.\n\nlabel = list(\"ABCD\")\n\n\n_df[\"g1\"] = pd.cut(df[\"tip\"],4,labels = label)\n\n\n_df[\"g1\"].value_counts()\n\ng1\nA    163\nB     69\nC     10\nD      2\nName: count, dtype: int64\n\n\n- 결과가 좀 그렇다…..\n\n_df.boxplot(backend=\"plotly\",y=\"tip\",x=\"g1\",color=\"g1\")\n\n\n                                                \n\n\n\n\n2. qcut\n- 간격(bins)외 lables을 건네줘서 구체적으로 나눈다. \\(\\to\\) 항상, 간격 = labels + 1\n\n예비학습 (qcut x)\n\n\n\n그룹\n구간\n\n\n\n\nA\n\\(x \\leq\\) 3\n\n\nB\n3 &lt; \\(x \\leq\\) 6\n\n\nC\n6 &lt; \\(x\\)\n\n\n\n\ntest = pd.DataFrame(np.arange(0,10),columns=[\"num\"])\n\n\nlable = list(\"ABC\")\n\n\nbin = [-np.Inf,3,6,np.Inf]\n\n\npd.DataFrame(np.arange(0,10)).shape\n\n(10, 1)\n\n\n\ntest[\"g\"]= pd.cut(np.arange(0,10),bins = bin,labels = lable)\n\n\ntest\n\n\n\n\n\n\n\n\nnum\ng\n\n\n\n\n0\n0\nA\n\n\n1\n1\nA\n\n\n2\n2\nA\n\n\n3\n3\nA\n\n\n4\n4\nB\n\n\n5\n5\nB\n\n\n6\n6\nB\n\n\n7\n7\nC\n\n\n8\n8\nC\n\n\n9\n9\nC\n\n\n\n\n\n\n\n\n\n실습 qcut (o)\n\n_df[\"tip\"].describe()\n\ncount    244.000000\nmean       2.998279\nstd        1.383638\nmin        1.000000\n25%        2.000000\n50%        2.900000\n75%        3.562500\nmax       10.000000\nName: tip, dtype: float64\n\n\n\nA = [-np.Inf, 2.0, 2.9, 3.5625, np.Inf]\n\n\nlabels = list(\"ABCD\")\n\n\n_df[\"g2\"]= pd.cut(_df[\"tip\"], bins=A,labels=labels)\n\n\n_df[\"g3\"] = pd.qcut(_df[\"tip\"],4)\n\n- 4분위수로 값을 나누어도 개수가 다른 이유는 값이 같은 것들이 존재하기 때문이다..\n\n_df[\"g3\"].value_counts()\n\ng3\n(0.999, 2.0]     78\n(2.9, 3.562]     61\n(3.562, 10.0]    61\n(2.0, 2.9]       44\nName: count, dtype: int64\n\n\n\n_df.g2.value_counts()\n\ng2\nA    78\nC    61\nD    61\nB    44\nName: count, dtype: int64\n\n\n\n_df.boxplot(backend=\"plotly\",y=\"tip\",x=\"g2\",color=\"g2\")\n\n\n                                                \n\n\n\ntotal_bill 변수의 4분위수를 저장\n\n\nq1 = df.total_bill.describe()[\"25%\"]\nq2 = df.total_bill.describe()[\"50%\"]\nq3 = df.total_bill.describe()[\"75%\"]\n\n\nq1,q2,q3\n\n(13.3475, 17.795, 24.127499999999998)\n\n\n\ndf[\"t_g1\"] = pd.cut(df[\"total_bill\"], bins = [-np.Inf,q1,q2,q3,np.Inf],labels = list(\"ABCD\"))\n\n\ndf.t_g1.value_counts()\n\nt_g1\nA    61\nB    61\nC    61\nD    61\nName: count, dtype: int64\n\n\n\n_df.t_g1.value_counts()\n\nt_g1\nA    61\nB    61\nC    61\nD    61\nName: count, dtype: int64\n\n\n\n_df.boxplot(backend=\"plotly\",y=\"total_bill\",x=\"t_g1\",color=\"t_g1\")"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#결측치-처리",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#결측치-처리",
    "title": "03. numpy & pandas (4)",
    "section": "결측치 처리",
    "text": "결측치 처리\n\ndata load\n[airquality 데이터 셋 정보]\n\nOzone: 오존 농도\n\nSolar.R: 태양복사량\nWind: 풍속\nTemp: 기온\nMonth: 월\nDay: 일\n\n\n\nCode\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/airquality.csv'\ndf = pd.read_csv(path)\n\n# 확인\ndf.head()\n\n\n\n\n\n\n\n\n\nOzone\nSolar.R\nWind\nTemp\nMonth\nDay\n\n\n\n\n0\n41.0\n190.0\n7.4\n67\n5\n1\n\n\n1\n36.0\n118.0\n8.0\n72\n5\n2\n\n\n2\n12.0\n149.0\n12.6\n74\n5\n3\n\n\n3\n18.0\n313.0\n11.5\n62\n5\n4\n\n\n4\nNaN\nNaN\n14.3\n56\n5\n5\n\n\n\n\n\n\n\n\n\n결측치 확인법(\\(\\star\\star\\))\n- 확인 1. df.into() : Ozone, Solar.R에서 결측치가 있는 것으로 보인다.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 153 entries, 0 to 152\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    116 non-null    float64\n 1   Solar.R  146 non-null    float64\n 2   Wind     153 non-null    float64\n 3   Temp     153 non-null    int64  \n 4   Month    153 non-null    int64  \n 5   Day      153 non-null    int64  \ndtypes: float64(3), int64(3)\nmemory usage: 7.3 KB\n\n\n- 확인 2. df.isna()==df.isnull()\n\n#df.isna()\ndf.isnull().head()\n\n\n\n\n\n\n\n\nOzone\nSolar.R\nWind\nTemp\nMonth\nDay\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\n\ndf.notnull().head()\n\n\n\n\n\n\n\n\nOzone\nSolar.R\nWind\nTemp\nMonth\nDay\n\n\n\n\n0\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n1\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n2\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n3\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n4\nFalse\nFalse\nTrue\nTrue\nTrue\nTrue\n\n\n\n\n\n\n\n- 방법 3 : df.isna().sum()\n\ndf.isna().sum()\n\nOzone      37\nSolar.R     7\nWind        0\nTemp        0\nMonth       0\nDay         0\ndtype: int64\n\n\n- 결측치 비율 구하기\n\ndf.isna().sum() / len(df)\n\nOzone      0.241830\nSolar.R    0.045752\nWind       0.000000\nTemp       0.000000\nMonth      0.000000\nDay        0.000000\ndtype: float64\n\n\n\n\n결측치 제거\n\naxis=1로 하면 열이 통째로 날아가지 지양하자\n\n\n_df=df.dropna(axis=0).reset_index(drop=True)\n\n\n_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 111 entries, 0 to 110\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    111 non-null    float64\n 1   Solar.R  111 non-null    float64\n 2   Wind     111 non-null    float64\n 3   Temp     111 non-null    int64  \n 4   Month    111 non-null    int64  \n 5   Day      111 non-null    int64  \ndtypes: float64(3), int64(3)\nmemory usage: 5.3 KB\n\n\n- 제거된 행 확인\n\nlen(df)-len(_df)\n\n42\n\n\n\n\n특정 열에 결측치가 있는 행 제거\n\n_df = df.copy()\n\n\n_df.dropna(subset = [\"Ozone\"],axis=0,inplace=True)\n\n\nlen(df) - len(_df)\n\n37\n\n\n\n_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 116 entries, 0 to 152\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    116 non-null    float64\n 1   Solar.R  111 non-null    float64\n 2   Wind     116 non-null    float64\n 3   Temp     116 non-null    int64  \n 4   Month    116 non-null    int64  \n 5   Day      116 non-null    int64  \ndtypes: float64(3), int64(3)\nmemory usage: 6.3 KB\n\n\n\n_df.isna().sum()\n\nOzone      0\nSolar.R    5\nWind       0\nTemp       0\nMonth      0\nDay        0\ndtype: int64\n\n\n\n\n결측치 채우기\n- 평균값으로 채우기\n\n_df = df.copy()\n\n\nmo = _df.Ozone.mean()\n\n\n_df[\"Ozone\"].fillna(mo,inplace=True)\n\n\n_df.isna().sum()\n\nOzone      0\nSolar.R    7\nWind       0\nTemp       0\nMonth      0\nDay        0\ndtype: int64\n\n\n- 특정값으로 채우기 : Solar.R==na $\\to$ 0\n\n_df[\"Solar.R\"].fillna(0,inplace=True)\n\n\n_df.isna().sum()\n\nOzone      0\nSolar.R    0\nWind       0\nTemp       0\nMonth      0\nDay        0\ndtype: int64\n\n\n- 직전 행 또는 바로 다음 행의 값으로 채우기\n\n해당 데이터는 시계열 데이터 이므로 직전이나, 다음 행의 값으로 결측치를 대체해도 될 것 같다.\n\n\n_df = df.copy()\n\n- 직전값으로 채우기 \\(\\to\\) fillna(,method=“ffill”)\n\n_df.Ozone.fillna(method=\"ffill\",inplace=True)\n\n\n_df.isna().sum()\n\nOzone      0\nSolar.R    7\nWind       0\nTemp       0\nMonth      0\nDay        0\ndtype: int64\n\n\n- 다음값으로 채우기 \\(\\to\\) fillna(, method = “bfill”)\n\n_df[\"Solar.R\"].fillna(method = \"bfill\",inplace=True)\n\n\n_df.isna().sum()\n\nOzone      0\nSolar.R    0\nWind       0\nTemp       0\nMonth      0\nDay        0\ndtype: int64\n\n\n- 선형보간법으로 채우기 \\(\\to\\) interpolate()\n- 다음과 같이 결측치를 선형방식으로 채운다. (빨간색점!)\n\n\nCode\nx = [1,2,3,4]\ny = [1,2,3,4]\nn = [\"T\",\"T\",\"F\",\"T\"]\na = pd.DataFrame([x,y,n]).T\na.columns=list(\"xyn\")\n\na.plot(kind= \"scatter\",x=\"x\",y=\"y\",color=\"n\",backend=\"plotly\")\n\n\n\n                                                \n\n\n\n_df = df.copy()\n_df[\"Ozone\"].interpolate(method=\"linear\",inplace=True)\n_df[\"Solar.R\"].interpolate(method=\"linear\",inplace=True)\n_df.isna().sum()\n\nOzone      0\nSolar.R    0\nWind       0\nTemp       0\nMonth      0\nDay        0\ndtype: int64\n\n\n\n\n\n참고 : 이상치\n1 데이터 로드\n\ntip = px.data.tips()\ntip.head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n2 열 데이터 분포 확인 (hist)\n- tip데이터를 살펴본결과 몇몇 이상치들이 보인다.\n\n\nCode\ntip.plot(backend = \"plotly\",kind = \"hist\", x=\"tip\")\n\n\n\n                                                \n\n\n3 boxplot\n\n\nCode\ntip.plot(backend=\"plotly\", kind=\"box\",x=\"tip\",width=500,height=400)\n\n\n\n                                                \n\n\n4 Q3에서 Q1에서 뺀다\n\nq1 = tip.tip.describe()[\"25%\"]\nq3 = tip.tip.describe()[\"75%\"]\n\n\niqr = q3-q1\n\n5 $ $\n\n_m = q1-iqr*1.5\n_M = q3+iqr*1.5\n\n\n_m,_M\n\n(-0.34375, 5.90625)\n\n\n해당 범위를 벗어나는 값을 이상치로 판단\n\\[\\text{outlier} \\leq \\text{Q1 - IQR}\\times 1.5\\quad \\& \\quad \\text{outlier} \\geq \\text{Q3 + IQR}\\times 1.5\\]\n6 이상치 판별 컬럼 추가\n\ntip[\"t_out\"] = [\"outlier\" if (i &lt;= q1-iqr*1.5) or ((i &gt;= q3+iqr*1.5)) else \"normal\" for i in tip.tip]\n\n\ntip.t_out.value_counts()\n\nt_out\nnormal     235\noutlier      9\nName: count, dtype: int64\n\n\n\n\nCode\ntip.plot(backend=\"plotly\", kind=\"box\",y=\"tip\",color = \"t_out\",points=\"all\",width=500,height=400)\n\n\n\n                                                \n\n\n\n\nCode\ntip.plot(backend=\"plotly\", kind=\"scatter\",x=\"tip\",y=\"tip\",color = \"t_out\",width=500,height=400)"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#가변수-만들기",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-22-03. numpy & pandas (4).html#가변수-만들기",
    "title": "03. numpy & pandas (4)",
    "section": "가변수 만들기",
    "text": "가변수 만들기\n\n\n\nsurvived\n\\(Y_1\\)\n\\(Y_2\\)\n\n\n\n\nYes\n1\n0\n\n\nNo\n0\n1\n\n\n\n\n1. 데이터 로드\n\ntip  = px.data.tips()\n\n\ntip.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 244 entries, 0 to 243\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   total_bill  244 non-null    float64\n 1   tip         244 non-null    float64\n 2   sex         244 non-null    object \n 3   smoker      244 non-null    object \n 4   day         244 non-null    object \n 5   time        244 non-null    object \n 6   size        244 non-null    int64  \ndtypes: float64(2), int64(1), object(4)\nmemory usage: 13.5+ KB\n\n\n\n\n2. 개별 변수 처리\n(1) 가변수 처리할 컬럼을 정의\n\nd_cols = [\"sex\"]\n\n\ndefault \\(\\to\\) drop_first = False\ndrop_first = True \\(\\to\\) 첫 번째로 생성된 가변수를 삭제\nsex열은 사라지고 가변수 처리된 변수들만 생성됨\n\n(2) 가변수 처리\n\npd.get_dummies(tip,columns = d_cols).head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsmoker\nday\ntime\nsize\nsex_Female\nsex_Male\n\n\n\n\n0\n16.99\n1.01\nNo\nSun\nDinner\n2\nTrue\nFalse\n\n\n1\n10.34\n1.66\nNo\nSun\nDinner\n3\nFalse\nTrue\n\n\n2\n21.01\n3.50\nNo\nSun\nDinner\n3\nFalse\nTrue\n\n\n3\n23.68\n3.31\nNo\nSun\nDinner\n2\nFalse\nTrue\n\n\n4\n24.59\n3.61\nNo\nSun\nDinner\n4\nTrue\nFalse\n\n\n\n\n\n\n\n\n_df = pd.get_dummies(tip,columns = d_cols,drop_first= True)\n\n\n_df.plot(kind = \"box\", x= \"sex_Male\", y=\"tip\" , backend =\"plotly\",color = \"sex_Male\", width=400,height =400)\n\n\n                                                \n\n\n\n\n3. 여러 변수를 가변수 처리\n\nd_cols = [\"smoker\",\"day\",\"time\",\"sex\"]\n\n_df = pd.get_dummies(tip,columns = d_cols,drop_first=True)\n\n\n_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 244 entries, 0 to 243\nData columns (total 9 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   total_bill  244 non-null    float64\n 1   tip         244 non-null    float64\n 2   size        244 non-null    int64  \n 3   smoker_Yes  244 non-null    bool   \n 4   day_Sat     244 non-null    bool   \n 5   day_Sun     244 non-null    bool   \n 6   day_Thur    244 non-null    bool   \n 7   time_Lunch  244 non-null    bool   \n 8   sex_Male    244 non-null    bool   \ndtypes: bool(6), float64(2), int64(1)\nmemory usage: 7.3 KB\n\n\n\n_df.plot(kind = \"box\", x= \"smoker_Yes\", y=\"tip\" , backend =\"plotly\",color = \"sex_Male\", width=800,height =400,points=\"all\")"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html",
    "title": "01. numpy & pandas (2)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#list-to-df",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#list-to-df",
    "title": "01. numpy & pandas (2)",
    "section": "list \\(\\to\\) df",
    "text": "list \\(\\to\\) df\n\na1 = list(np.random.randint(0,100,10))\na2 = list(np.random.randint(0,100,10))\na3 = list(np.random.randint(0,100,10))\na4= list(np.random.randint(0,100,10))\n\ndf = pd.DataFrame([a1,a2,a3,a4]).T\n\n\ndf.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n81\n45\n16\n73\n\n\n1\n74\n74\n11\n50\n\n\n2\n0\n85\n13\n77\n\n\n3\n97\n53\n97\n52\n\n\n4\n31\n21\n84\n14\n\n\n\n\n\n\n\n\n날짜 데이터 \\(\\to\\) df.index\n\ndate = pd.date_range(\"2023-08-18\",\"2023-08-27\",freq =\"D\")\n\n\ndf.index = date\n\n\n\n컬럼 이름 생성\n\nname = [\"a\" + str(i) for i in range(len(df.columns))]\n\n\ndf.columns= name\n\n\ndf.head()\n\n\n\n\n\n\n\n\na0\na1\na2\na3\n\n\n\n\n2023-08-18\n81\n45\n16\n73\n\n\n2023-08-19\n74\n74\n11\n50\n\n\n2023-08-20\n0\n85\n13\n77\n\n\n2023-08-21\n97\n53\n97\n52\n\n\n2023-08-22\n31\n21\n84\n14"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#dict-to-df-starstarstar",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#dict-to-df-starstarstar",
    "title": "01. numpy & pandas (2)",
    "section": "dict \\(\\to\\) df (\\(\\star\\star\\star\\))",
    "text": "dict \\(\\to\\) df (\\(\\star\\star\\star\\))\n\nimport random\n\n\ncode = [\"a\" + str(i) for i in range(10)]\nheight = np.linspace(160,180,10)\nweight = np.linspace(60,80,10)\nsmoke = random.choices([True, False],k=10) ## 복원 추출\ndate = pd.date_range(\"2023-08-18\",\"2023-08-27\",freq =\"D\")\n\ndic = {\"code\" : code,\n      \"height\" : height,\n      \"weight\" : weight,\n      \"smoke\" : smoke}\n\ndf = pd.DataFrame(dic,index= date)\n\n\ndf.head()\n\n\n\n\n\n\n\n\ncode\nheight\nweight\nsmoke\n\n\n\n\n2023-08-18\na0\n160.000000\n60.000000\nFalse\n\n\n2023-08-19\na1\n162.222222\n62.222222\nTrue\n\n\n2023-08-20\na2\n164.444444\n64.444444\nTrue\n\n\n2023-08-21\na3\n166.666667\n66.666667\nFalse\n\n\n2023-08-22\na4\n168.888889\n68.888889\nTrue"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#인덱스-정의",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#인덱스-정의",
    "title": "01. numpy & pandas (2)",
    "section": "인덱스 정의",
    "text": "인덱스 정의\n\n_lst = random.choices(\"ABC\",k= len(df)) ## 중복허용\n_n = list(np.random.choice(range(1, len(df)+1), len(df), replace=False)) ## 중복 허용 안함\n\n\nindex = [i + str(j) for i,j in zip(_lst,_n)]\n\n\ndf.index = index"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#인덱스-중복-확인",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#인덱스-중복-확인",
    "title": "01. numpy & pandas (2)",
    "section": "인덱스 중복 확인",
    "text": "인덱스 중복 확인\n\nlen(df) == len(df.index.unique())\n\nTrue"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#인덱스-생성확인",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#인덱스-생성확인",
    "title": "01. numpy & pandas (2)",
    "section": "인덱스 생성확인",
    "text": "인덱스 생성확인\n\ndf.head()\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\nA115\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\nB64\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\nA172\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\nA215\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\nB86\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 234 entries, A115 to C75\nData columns (total 11 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   manufacturer  234 non-null    object \n 1   model         234 non-null    object \n 2   displ         234 non-null    float64\n 3   year          234 non-null    int64  \n 4   cyl           234 non-null    int64  \n 5   trans         234 non-null    object \n 6   drv           234 non-null    object \n 7   cty           234 non-null    int64  \n 8   hwy           234 non-null    int64  \n 9   fl            234 non-null    object \n 10  class         234 non-null    object \ndtypes: float64(1), int64(4), object(6)\nmemory usage: 21.9+ KB"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#df-load-index-1",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#df-load-index-1",
    "title": "01. numpy & pandas (2)",
    "section": "df load + index (1)",
    "text": "df load + index (1)\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/main/posts/mpg.csv\",\n                 index_col=\"manufacturer\")\n\n\ndf.head()\n\n\n\n\n\n\n\n\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\nmanufacturer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#df-load-index-2",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#df-load-index-2",
    "title": "01. numpy & pandas (2)",
    "section": "df load + index (2)",
    "text": "df load + index (2)\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/main/posts/mpg.csv\")\ndf = df.set_index(\"manufacturer\") # == df.set_index(\"manufacturer\",inplace= True)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\nmanufacturer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\n\n\n\n\n\n- 인덱스 이름 삭제\n\ndf.index.name = None\ndf.head()\n\n\n\n\n\n\n\n\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#인덱스-삭제index-버리기-x",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#인덱스-삭제index-버리기-x",
    "title": "01. numpy & pandas (2)",
    "section": "인덱스 삭제(index 버리기 x)",
    "text": "인덱스 삭제(index 버리기 x)\n\n_df = df.reset_index()\n_df.head()\n\n\n\n\n\n\n\n\nindex\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\n4\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#인덱스-삭제index-버리기-o",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#인덱스-삭제index-버리기-o",
    "title": "01. numpy & pandas (2)",
    "section": "인덱스 삭제(index 버리기 o)",
    "text": "인덱스 삭제(index 버리기 o)\n\n_df = df.reset_index(drop=True)\n_df.head()\n\n\n\n\n\n\n\n\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n0\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\n1\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\n2\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\n3\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\n4\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#import-1",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#import-1",
    "title": "01. numpy & pandas (2)",
    "section": "import",
    "text": "import\n\nimport random\nimport pandas as pd\nimport plotly.express as px\n\n- tip 데이터는 plotly 모듈에서 제공한다.\n\ntip = px.data.tips()\n\n\ntip.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 244 entries, 0 to 243\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   total_bill  244 non-null    float64\n 1   tip         244 non-null    float64\n 2   sex         244 non-null    object \n 3   smoker      244 non-null    object \n 4   day         244 non-null    object \n 5   time        244 non-null    object \n 6   size        244 non-null    int64  \ndtypes: float64(2), int64(1), object(4)\nmemory usage: 13.5+ KB\n\n\n\ntip.shape\n\n(244, 7)\n\n\n\ntip.index\n\nRangeIndex(start=0, stop=244, step=1)\n\n\n- 값 확인\n\ntip.values\n\narray([[16.99, 1.01, 'Female', ..., 'Sun', 'Dinner', 2],\n       [10.34, 1.66, 'Male', ..., 'Sun', 'Dinner', 3],\n       [21.01, 3.5, 'Male', ..., 'Sun', 'Dinner', 3],\n       ...,\n       [22.67, 2.0, 'Male', ..., 'Sat', 'Dinner', 2],\n       [17.82, 1.75, 'Male', ..., 'Sat', 'Dinner', 2],\n       [18.78, 3.0, 'Female', ..., 'Thur', 'Dinner', 2]], dtype=object)\n\n\n- 기초통계정보 확인\n\ntip.describe()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsize\n\n\n\n\ncount\n244.000000\n244.000000\n244.000000\n\n\nmean\n19.785943\n2.998279\n2.569672\n\n\nstd\n8.902412\n1.383638\n0.951100\n\n\nmin\n3.070000\n1.000000\n1.000000\n\n\n25%\n13.347500\n2.000000\n2.000000\n\n\n50%\n17.795000\n2.900000\n2.000000\n\n\n75%\n24.127500\n3.562500\n3.000000\n\n\nmax\n50.810000\n10.000000\n6.000000\n\n\n\n\n\n\n\n\ntip.head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#sort",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#sort",
    "title": "01. numpy & pandas (2)",
    "section": "sort",
    "text": "sort\n- total_bill 기준 오름차순 정렬\n\ntip.sort_values(by = \"total_bill\").head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n67\n3.07\n1.00\nFemale\nYes\nSat\nDinner\n1\n\n\n92\n5.75\n1.00\nFemale\nYes\nFri\nDinner\n2\n\n\n111\n7.25\n1.00\nFemale\nNo\nSat\nDinner\n1\n\n\n172\n7.25\n5.15\nMale\nYes\nSun\nDinner\n2\n\n\n149\n7.51\n2.00\nMale\nNo\nThur\nLunch\n2\n\n\n\n\n\n\n\n- 내림차순 정렬\n\ntip.sort_values(by = \"total_bill\",ascending=False).head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n170\n50.81\n10.00\nMale\nYes\nSat\nDinner\n3\n\n\n212\n48.33\n9.00\nMale\nNo\nSat\nDinner\n4\n\n\n59\n48.27\n6.73\nMale\nNo\nSat\nDinner\n4\n\n\n156\n48.17\n5.00\nMale\nNo\nSun\nDinner\n6\n\n\n182\n45.35\n3.50\nMale\nYes\nSun\nDinner\n3\n\n\n\n\n\n\n\n- 복합열 정렬 : total_bill은 오름차순, tip은 내림차순 기준으로 정렬\n\ntip.sort_values(by = [\"total_bill\",\"tip\"],ascending=[False,True])\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n170\n50.81\n10.00\nMale\nYes\nSat\nDinner\n3\n\n\n212\n48.33\n9.00\nMale\nNo\nSat\nDinner\n4\n\n\n59\n48.27\n6.73\nMale\nNo\nSat\nDinner\n4\n\n\n156\n48.17\n5.00\nMale\nNo\nSun\nDinner\n6\n\n\n182\n45.35\n3.50\nMale\nYes\nSun\nDinner\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n149\n7.51\n2.00\nMale\nNo\nThur\nLunch\n2\n\n\n111\n7.25\n1.00\nFemale\nNo\nSat\nDinner\n1\n\n\n172\n7.25\n5.15\nMale\nYes\nSun\nDinner\n2\n\n\n92\n5.75\n1.00\nFemale\nYes\nFri\nDinner\n2\n\n\n67\n3.07\n1.00\nFemale\nYes\nSat\nDinner\n1\n\n\n\n\n244 rows × 7 columns"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#unique값-확인-범주형-데이터",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#unique값-확인-범주형-데이터",
    "title": "01. numpy & pandas (2)",
    "section": "unique값 확인 (범주형 데이터)",
    "text": "unique값 확인 (범주형 데이터)\n- day열의 unique값 확인\n\n#tip.day\n\n\ntip.day.unique()\n\narray(['Sun', 'Sat', 'Thur', 'Fri'], dtype=object)\n\n\n- unique값 개수 확인\n\ntip.day.value_counts()\n\nSat     87\nSun     76\nThur    62\nFri     19\nName: day, dtype: int64\n\n\n- 시각화\n\ntip.day.value_counts().plot(kind=\"barh\",title=\"count of day\",figsize = (4,4))\n\n&lt;Axes: title={'center': 'count of day'}&gt;\n\n\n\n\n\n- smoker 몇 명이나 있는지 확인\n\ntip.smoker.value_counts()\n\nNo     151\nYes     93\nName: smoker, dtype: int64\n\n\n- 비율 확인\n\ntip.smoker.value_counts() / len(tip) # = tip.smoker.value_counts(normalize = True)\n\nNo     0.618852\nYes    0.381148\nName: smoker, dtype: float64\n\n\n- 비율 시각화\n\n(tip.smoker.value_counts() / len(tip)).plot(kind=\"bar\",title=\"count of smoker\",figsize = (4,4))\n\n&lt;Axes: title={'center': 'count of smoker'}&gt;\n\n\n\n\n\n- total_bill의 lineplot 확인\n\ntip.total_bill.plot(figsize=(4,4))\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#최빈값",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#최빈값",
    "title": "01. numpy & pandas (2)",
    "section": "최빈값",
    "text": "최빈값\n\ntip.day.value_counts()\n\nSat     87\nSun     76\nThur    62\nFri     19\nName: day, dtype: int64\n\n\n\ntip.day.mode()\n\n0    Sat\nName: day, dtype: object\n\n\n\ntip.smoker.mode()\n\n0    No\nName: smoker, dtype: object"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#df.sum",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-01. numpy & pandas (2).html#df.sum",
    "title": "01. numpy & pandas (2)",
    "section": "df.sum",
    "text": "df.sum\n\naxis=0 (개별 컬럼의 합을 계산)\n\ntip.head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\ntip.sum(axis=0)\n\ntotal_bill                                              4827.77\ntip                                                      731.58\nsex           FemaleMaleMaleMaleFemaleMaleMaleMaleMaleMaleMa...\nsmoker        NoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNo...\nday           SunSunSunSunSunSunSunSunSunSunSunSunSunSunSunS...\ntime          DinnerDinnerDinnerDinnerDinnerDinnerDinnerDinn...\nsize                                                        627\ndtype: object\n\n\n\n\n평균과 중앙값\n\ntip[[\"total_bill\",\"tip\"]].mean()\n\ntotal_bill    19.785943\ntip            2.998279\ndtype: float64\n\n\n\ntip[[\"total_bill\",\"tip\"]].median()\n\ntotal_bill    17.795\ntip            2.900\ndtype: float64"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-15-Extra 04. 클래스 탐구 (3).html",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-15-Extra 04. 클래스 탐구 (3).html",
    "title": "Extra 04. 클래스 탐구 (3)",
    "section": "",
    "text": "- 클래스를 조금 수정하고 싶을때, 아래와 같은 문법을 이용하면 편리하다.(지난 챕터 복기!!)\nclass 새로운클래스 (수정할 클래스) :\n    def 수정 및 추가 함수 : \n        return ...\n\n\n\nclass a1 :\n    def __init__(self,v) :\n        ## 여기는 a1클래스\n        print(\"init 클래스에서 정의된 __init__을 실행합니다.\")\n        self.v = v\n\n\nclass a2 (a1):\n    def show(self) :\n        ## 여기는 a2클래스\n        print(\"a2 클래스에서 정의한 show를 실행합니다.\")\n        print(f\"value = {self.v}\")\n\n\na = a2(5)\n\ninit 클래스에서 정의된 __init__을 실행합니다.\n\n\n\na.show()\n\na2 클래스에서 정의한 show를 실행합니다.\nvalue = 5\n\n\n\na.show??\n\n\nSignature: a.show()\nDocstring: &lt;no docstring&gt;\nSource:   \n    def show(self) :\n        ## 여기는 a2클래스\n        print(\"a2 클래스에서 정의한 show를 실행합니다.\")\n        print(f\"value = {self.v}\")\nFile:      c:\\users\\user\\appdata\\local\\temp\\ipykernel_15308\\1064789111.py\nType:      method\n\n\n\n\na.__init__??\n\n\nSignature: a.__init__(v)\nDocstring: Initialize self.  See help(type(self)) for accurate signature.\nSource:   \n    def __init__(self,v) :\n        ## 여기는 a1클래스\n        print(\"init 클래스에서 정의된 __init__을 실행합니다.\")\n        self.v = v\nFile:      c:\\users\\user\\appdata\\local\\temp\\ipykernel_15308\\3688838073.py\nType:      method"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-15-Extra 04. 클래스 탐구 (3).html#ex1",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-15-Extra 04. 클래스 탐구 (3).html#ex1",
    "title": "Extra 04. 클래스 탐구 (3)",
    "section": "",
    "text": "class a1 :\n    def __init__(self,v) :\n        ## 여기는 a1클래스\n        print(\"init 클래스에서 정의된 __init__을 실행합니다.\")\n        self.v = v\n\n\nclass a2 (a1):\n    def show(self) :\n        ## 여기는 a2클래스\n        print(\"a2 클래스에서 정의한 show를 실행합니다.\")\n        print(f\"value = {self.v}\")\n\n\na = a2(5)\n\ninit 클래스에서 정의된 __init__을 실행합니다.\n\n\n\na.show()\n\na2 클래스에서 정의한 show를 실행합니다.\nvalue = 5\n\n\n\na.show??\n\n\nSignature: a.show()\nDocstring: &lt;no docstring&gt;\nSource:   \n    def show(self) :\n        ## 여기는 a2클래스\n        print(\"a2 클래스에서 정의한 show를 실행합니다.\")\n        print(f\"value = {self.v}\")\nFile:      c:\\users\\user\\appdata\\local\\temp\\ipykernel_15308\\1064789111.py\nType:      method\n\n\n\n\na.__init__??\n\n\nSignature: a.__init__(v)\nDocstring: Initialize self.  See help(type(self)) for accurate signature.\nSource:   \n    def __init__(self,v) :\n        ## 여기는 a1클래스\n        print(\"init 클래스에서 정의된 __init__을 실행합니다.\")\n        self.v = v\nFile:      c:\\users\\user\\appdata\\local\\temp\\ipykernel_15308\\3688838073.py\nType:      method"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-15-Extra 04. 클래스 탐구 (3).html#수퍼클래스",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-15-Extra 04. 클래스 탐구 (3).html#수퍼클래스",
    "title": "Extra 04. 클래스 탐구 (3)",
    "section": "수퍼클래스",
    "text": "수퍼클래스\n\n방법 1 : 직접 수퍼클래스 명시\n\nclass a3(a2) :\n    def __init__(self,v) :\n        ## a3클래스 명시\n        print(\"짠~!\")\n        a2.__init__(self,v)\n        print(\"짠짠~!!\")\n\n\ndeco = a3(5)\n\n짠~!\ninit 클래스에서 정의된 __init__을 실행합니다.\n짠짠~!!\n\n\n\ndeco.show??\n\n\nSignature: deco.show()\nDocstring: &lt;no docstring&gt;\nSource:   \n    def show(self) :\n        ## 여기는 a2클래스\n        print(\"a2 클래스에서 정의한 show를 실행합니다.\")\n        print(f\"value = {self.v}\")\nFile:      c:\\users\\user\\appdata\\local\\temp\\ipykernel_15308\\1064789111.py\nType:      method\n\n\n\n\ndeco.__init__??\n\n\nSignature: deco.__init__(v)\nDocstring: Initialize self.  See help(type(self)) for accurate signature.\nSource:   \n    def __init__(self,v) :\n        ## a3클래스 명시\n        print(\"짠~!\")\n        a2.__init__(self,v)\n        print(\"짠짠~!!\")\nFile:      c:\\users\\user\\appdata\\local\\temp\\ipykernel_15308\\1708323518.py\nType:      method\n\n\n\n\n\n방법 2 : super 이용, 생략 x)\n\nclass a3(a2) :\n    def __init__(self,v) :\n        ## a3클래스 명시\n        print(\"짠~!\")\n        super(a3,self).__init__(v)\n        print(\"짠짠~!!\")\n\n\na = a3(5)\n\n짠~!\ninit 클래스에서 정의된 __init__을 실행합니다.\n짠짠~!!\n\n\n\na.show??\n\n\nSignature: a.show()\nDocstring: &lt;no docstring&gt;\nSource:   \n    def show(self) :\n        ## 여기는 a2클래스\n        print(\"a2 클래스에서 정의한 show를 실행합니다.\")\n        print(f\"value = {self.v}\")\nFile:      c:\\users\\user\\appdata\\local\\temp\\ipykernel_15308\\1064789111.py\nType:      method\n\n\n\n\na.__init__??\n\n\nSignature: a.__init__(v)\nDocstring: Initialize self.  See help(type(self)) for accurate signature.\nSource:   \n    def __init__(self,v) :\n        ## a3클래스 명시\n        print(\"짠~!\")\n        super(a3,self).__init__(v)\n        print(\"짠짠~!!\")\nFile:      c:\\users\\user\\appdata\\local\\temp\\ipykernel_15308\\2997847713.py\nType:      method\n\n\n\n\n\n방법 3 : super 이용, 생략 o) (\\(\\star\\star\\star\\))\n\nclass a3(a2) :\n    def __init__(self,v) :\n        ## a3클래스 명시\n        print(\"짠~!\")\n        super().__init__(v)  ## 생략전 -&gt; super(a3,self).__init__(v)\n        print(\"짠짠~!!\")\n\n\na = a3(5)\n\n짠~!\ninit 클래스에서 정의된 __init__을 실행합니다.\n짠짠~!!\n\n\n\na3.show??\n\n\nSignature: a3.show(self)\nDocstring: &lt;no docstring&gt;\nSource:   \n    def show(self) :\n        ## 여기는 a2클래스\n        print(\"a2 클래스에서 정의한 show를 실행합니다.\")\n        print(f\"value = {self.v}\")\nFile:      c:\\users\\user\\appdata\\local\\temp\\ipykernel_15308\\1064789111.py\nType:      function\n\n\n\n\na3.__init__??\n\n\nSignature: a3.__init__(self, v)\nDocstring: Initialize self.  See help(type(self)) for accurate signature.\nSource:   \n    def __init__(self,v) :\n        ## a3클래스 명시\n        print(\"짠~!\")\n        super().__init__(v)  ## 생략전 -&gt; super(a3,self).__init__(v)\n        print(\"짠짠~!!\")\nFile:      c:\\users\\user\\appdata\\local\\temp\\ipykernel_15308\\2474030909.py\nType:      function\n\n\n\n\n\n방법 4 : super()이용, 방법 3을 이해하기위한 코드\n\nclass a4(a2) :\n    def __init__(self,v) :\n        ## a4클래스 \n        print(\"짠~!\")\n        super(__class__,self).__init__(v)  ## 생략전 -&gt; super(a3,self).__init__(v)\n        print(\"짠짠~!!\")\n\n\na = a4(5)\n\n짠~!\ninit 클래스에서 정의된 __init__을 실행합니다.\n짠짠~!!\n\n\n\na.show()\n\na2 클래스에서 정의한 show를 실행합니다.\nvalue = 5\n\n\n\na.show??\n\n\nSignature: a.show()\nDocstring: &lt;no docstring&gt;\nSource:   \n    def show(self) :\n        ## 여기는 a2클래스\n        print(\"a2 클래스에서 정의한 show를 실행합니다.\")\n        print(f\"value = {self.v}\")\nFile:      c:\\users\\user\\appdata\\local\\temp\\ipykernel_15308\\1064789111.py\nType:      method\n\n\n\n\na.__init__??\n\n\nSignature: a.__init__(v)\nDocstring: Initialize self.  See help(type(self)) for accurate signature.\nSource:   \n    def __init__(self,v) :\n        ## a4클래스 \n        print(\"짠~!\")\n        super(__class__,self).__init__(v)  ## 생략전 -&gt; super(a3,self).__init__(v)\n        print(\"짠짠~!!\")\nFile:      c:\\users\\user\\appdata\\local\\temp\\ipykernel_15308\\1093371131.py\nType:      method"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-15-Extra 04. 클래스 탐구 (3).html#다중상속-super-x",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-15-Extra 04. 클래스 탐구 (3).html#다중상속-super-x",
    "title": "Extra 04. 클래스 탐구 (3)",
    "section": "다중상속 (super X)",
    "text": "다중상속 (super X)\n\n일반적인 다중 상속\n- Add 클래스 선언\n\nclass Add:\n    def __init__(self,value):\n        self.value = value \n    def __add__(self,value2):\n        return self.value + value2\n\n- 초기화\n\na = Add(2)\n\n\na.value\n\n2\n\n\n\na + 5 \n\n7\n\n\n- Mul 클래스 선언\n\nclass Mul:\n    def __init__(self,value):\n        self.value = value \n    def __mul__(self,value2):\n        return self.value * value2\n\n\na = Mul(5)\n\n\na.value\n\n5\n\n\n- 더하기는 위에서 정의한적 없음\n\na+2\n\nTypeError: unsupported operand type(s) for +: 'Mul' and 'int'\n\n\n- 곱하기 수행\n\na*2\n\n10\n\n\n- 위 2개의 클래스를 상속\n\nclass am(Add,Mul) :\n      pass\n\n\na = am(5)\na.value\n\n5\n\n\n\na + 2\n\n7\n\n\n\na*5\n\n25\n\n\n\n\n다중상속(__init__이 겹친다…)\n\nclass Add:\n    def __init__(self,value):\n        print(\"Add클래스에서 정의된 __init__ 메소드가 실행됩니다\")\n        self.value = value \n    def __add__(self,value2):\n        return self.value + value2\n              \nclass Mul:\n    def __init__(self,value):\n        print(\"Mul클래스에서 정의된 __init__ 메소드가 실행됩니다\")        \n        self.value = value \n    def __mul__(self,value2):\n        return self.value * value2        \n    \nclass am(Add,Mul):\n    pass     \n\n- 현재 Add클래스가 우선 순위인 것 같다.\n\na = am(5)\n\nAdd클래스에서 정의된 __init__ 메소드가 실행됩니다\n\n\n\n\n믹스인 클래스(\\(\\star\\star\\star\\))\n\nclass Init:\n    def __init__(self,value):\n        ## 여기는 Init 클래스야 \n        print(\"Init클래스에서 정의된 __init__메소드를 실행합니다\")        \n        self.value = value\n\nclass Add(Init):\n    def __add__(self,value2):\n        return self.value + value2\n              \nclass Mul(Init):\n    def __mul__(self,value2):\n        return self.value * value2        \n    \nclass am(Add,Mul):\n    pass     \n\n\na = am(5)\n\nInit클래스에서 정의된 __init__메소드를 실행합니다\n\n\n\na + 2\n\n7\n\n\n\na*5\n\n25"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-15-Extra 04. 클래스 탐구 (3).html#다중상속-super-o",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-15-Extra 04. 클래스 탐구 (3).html#다중상속-super-o",
    "title": "Extra 04. 클래스 탐구 (3)",
    "section": "다중상속 (super O)",
    "text": "다중상속 (super O)\n\nsuper를 쓰지 않는 좋지 못한 예시\n\nclass Init:\n    def __init__(self,value):\n        ## 여기는 Init 클래스야 \n        print(\"Init클래스에서 정의된 __init__메소드를 실행합니다\")        \n        self.value = value\n\n- 위 intit을 상속 받아서\n\n초기값 = 초기값 x 2\n초기값 = 초기값 + 5\n\n를 객채 생성과 동시 에 수행하는 클래스를 각각 만듬\n\nclass a1(Init):\n    def __init__(self,value):\n        Init.__init__(self,value)\n        self.value = self.value * 2\n\n\na = a1(5)\na.value\n\nInit클래스에서 정의된 __init__메소드를 실행합니다\n\n\n10\n\n\n\nclass a2(Init):\n    def __init__(self,value):\n        Init.__init__(self,value)\n        self.value = self.value + 5\n\n\na = a2(4)\na.value\n\nInit클래스에서 정의된 __init__메소드를 실행합니다\n\n\n9\n\n\n- 근데 초기값 = 초기갑 x 2 + 5를 객체 생성과 동시에 수행해주는 클래스를 만들고 싶음.\n\nclass a3(a1,a2) :\n    print(\"초기값 = 초기값 x2 + 5를 수행ㅎ는 클래스입니다.\")\n    def __init__(self,value) :\n        a1.__init__(self,value)\n        a2.__init__(self,self.value)\n\n초기값 = 초기값 x2 + 5를 수행ㅎ는 클래스입니다.\n\n\n\na = a3(5)\n\nInit클래스에서 정의된 __init__메소드를 실행합니다\nInit클래스에서 정의된 __init__메소드를 실행합니다\n\n\n\na.value\n\n15\n\n\n\n싫은 이유\n\n코드가 지저분함\n먼가 상속이란 단어의 진정성이 없음\n\n\n\n\n\nsuper()을 활용한 좋은 예시\n\nclass Init(object):\n    def __init__(self,value):\n        ## 여기는 Init 클래스야 \n        print(\"Init클래스에서 정의된 __init__메소드를 실행합니다\")        \n        self.value = value\n        \nclass a1(Init):\n    def __init__(self,value):\n        super().__init__(value)\n        self.value = self.value * 2\n        \nclass a2(Init):\n    def __init__(self,value):\n        super().__init__(value)\n        self.value = self.value + 5\n        \nclass a3(a2,a1):\n    def __init__(self,value):\n        super().__init__(value)\n\n\na = a3(5)\n\nInit클래스에서 정의된 __init__메소드를 실행합니다\n\n\n\na.value\n\n15\n\n\n\n\nsuper의 사용방법\n- (“초기값 x 2 + 5) x 2” 를 수행해주는 클래스를 만들고 싶음.\n\nclass Init(object):\n    def __init__(self,value):\n        ## 여기는 Init 클래스야 \n        print(\"Init클래스에서 정의된 __init__메소드를 실행합니다\")        \n        self.value = value\n        \nclass a1(Init):\n    def __init__(self,value):\n        super().__init__(value)\n        self.value = self.value * 2\n        \nclass a2(Init):\n    def __init__(self,value):\n        super().__init__(value)\n        self.value = self.value + 5\n        \nclass a3(a2,a1): ## 파라미터 순서에 따라 mro 상속 순서가 달라짐\n    def __init__(self,value):\n        super().__init__(value)\n        super(a2,self).__init__(self.value)\n\n- super().__init__(value) : 상위 클래스인 init,a1,a2의 __init__를 순서대로 실행하되 중복실행하지 않는다.\n- super(a2,self).__init__(self.value) : a2보다 상위 클래스인 init, a1의 __init__을 순서대로 실행하되 중복실행은 하지 않음.\n\na = a3(5)\na.value\n\nInit클래스에서 정의된 __init__메소드를 실행합니다\nInit클래스에서 정의된 __init__메소드를 실행합니다\n\n\n30\n\n\n\na3.mro()\n\n[__main__.a3, __main__.a2, __main__.a1, __main__.Init, object]\n\n\n- 그냥 이게 더 낫지 않나?\n\nclass Init(object):\n    def __init__(self,value):\n        ## 여기는 Init 클래스야 \n        print(\"Init클래스에서 정의된 __init__메소드를 실행합니다\")        \n        self.value = value\n        \nclass a1(Init):\n    def aa1(self):\n        self.value = self.value * 2\n        \nclass a2(Init):\n    def aa2(self):\n        self.value = self.value + 5\n        \nclass a3(a2,a1):\n    def aa3(self):\n        self.aa1()\n        self.aa2()\n        self.aa1()\n\n\na = a3(5)\n\nInit클래스에서 정의된 __init__메소드를 실행합니다\n\n\n\na.value\n\n5\n\n\n\na.aa3()\n\n\na.value\n\n30\n\n\n- 그래도 이전 방법들을 잘 알아야한다.\n\n어떠한 클래스를 상속 받을때는 “내가 만든 클래스”가 아닐 경우가 대부분이다. 따라서 “애초부터 메소드가 겹치지 않게 클래스들을 깔끔하게 디자인을 하는것”은 불가능한 경우가 많다."
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-15-Extra 04. 클래스 탐구 (3).html#리스트의-상속",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-15-Extra 04. 클래스 탐구 (3).html#리스트의-상속",
    "title": "Extra 04. 클래스 탐구 (3)",
    "section": "리스트의 상속",
    "text": "리스트의 상속\n- list와 비슷하면서… 멤버들의 빈도가 계산되는 메소드를 갖는 나만의 list 클래스를 만들고 싶다.\n\nlst = list('asdfasssdfa')\nlst \n\n['a', 's', 'd', 'f', 'a', 's', 's', 's', 'd', 'f', 'a']\n\n\n- 각 원소들의 빈도\n\n{i : lst.count(i) for i in set(lst)}\n\n{'d': 2, 's': 4, 'f': 2, 'a': 3}\n\n\n\nlst.freq()를 수행하면 위의 결과가 나왔으면 좋겠음\n\n\nclass List(list) :\n    def freq(self) :\n        return {i : self.count(i) for i in set(self)}\n\n\nlst2 = List('asdfasssdfa')\n\n\nlst2\n\n['a', 's', 'd', 'f', 'a', 's', 's', 's', 'd', 'f', 'a']\n\n\n\nlst2.freq()\n\n{'d': 2, 's': 4, 'f': 2, 'a': 3}"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-10-Extra 02. 클래스 탐구 (1).html",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-10-Extra 02. 클래스 탐구 (1).html",
    "title": "Extra 02. 클래스 탐구 (1)",
    "section": "",
    "text": "from IPython.core.display import HTML\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n\nclass jkm:\n    def __init__(self) :\n        self.title = \"중요한건 꺽이지 않는 마음\"\n\n        self.url = \"https://github.com/guebin/PP2023/blob/main/posts/03_Class/JungGGuckMa.jpg?raw=true\"\n\n        self.Q = \"Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\"\n\n        self.A = \"A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\"\n\n        self.h1 = \"마음\"\n\n        self.html_str =  '''\n                    &lt;style&gt;\n                        .title {{\n                            font-family: \"Times New Roman\", serif;\n                            font-size: 30px;\n                            font-weight: 900;\n                        }}\n                        .text {{\n                            font-family: \"Arial\", sans-serif;\n                            font-size: 20px;\n                            font-style: italic;\n                        }}\n                        .highlight {{\n                            font-family: \"Montserrat\", monospace;\n                            font-size: 35px;\n                            font-weight: 900;\n                            text-decoration: underline; ## 밑줄\n                            font-style: normal;\n                            color: darkblue;\n                            background-color: #FFFF00;\n                        }}\n                    &lt;/style&gt;\n\n                    &lt;p class=\"title\"&gt;{tt1}&lt;/p&gt;\n                    &lt;img src={url} width=\"600\"&gt;\n                    &lt;p&gt; \\n &lt;/p&gt;\n                    &lt;p class=\"text\"&gt; {Q}&lt;/p&gt;\n                    &lt;p class=\"text\"&gt; {A}: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n                    &lt;p class=\"title\"&gt;중요한 것은 꺾이지 않는 &lt;span class=\"highlight\"&gt; {h1} &lt;/span&gt;&lt;/p&gt;\n                    '''\n        \n    def show(self):\n        _str = self.html_str.format(\n            tt1 = self.title,\n            url = self.url,\n            Q = self.Q,\n            A = self.A,\n            h1 = self.h1 )\n        display(HTML(_str))\n\n\ntest = jkm()\n\n\ntest.show()\n\n\n                    \n\n                    중요한건 꺽이지 않는 마음\n                    \n                     \n \n                     Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\n                     A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n                    중요한 것은 꺾이지 않는  마음 \n                    \n\n\n\n\n\n- 아래처럼 우리가 생성한 test의 타입을 확인하니 type 이 jkm 으로 나온다.\n\ntest?\n\n\nType:        jkm\nString form: &lt;__main__.jkm object at 0x000001E17B1E7850&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n- 아래의 리스트, 튜플, 리스트(튜플)의 타입을 확인해보자.\n\ntype([1,2,3])\n\nlist\n\n\n\ntype((1,2,3))\n\ntuple\n\n\n\ntype(list((1,2,3)))\n\nlist\n\n\n- 깨달음1. 우리가 어떤 인스턴스 객체를 생성할 떄 그 자료형은 파이썬 내부, 혹은 우리가 작성한 클래스의 이름이다.\n\n\n\n\na = \"123\"\nlist(a)\n\n['1', '2', '3']\n\n\n\na = list()\n\na.__init__(\"123\")\na\n\n['1', '2', '3']\n\n\n\na = list()\na.__init__('123') \na.__init__() # 리스트 최기화\na\n\n[]\n\n\n- 깨달음 2. 우리가 list(\"123\") 과 같은 메소드를 입력할 때 사실 자료형을 변환하는 것이 아니라, list 라는 클래스의 __init__()으로 인스턴스를 생성하는 것이었다.\n\n\n\n- 아래의 클래스를 관찰하자\n\nclass UpJump:\n    def __init__(self):\n        self.reset()\n    def up(self):\n        self.a = self.a + 1  \n        print(\"a의 값이 1 증가합니다.\")\n    def jump(self,jump_size):\n        self.a = self.a + jump_size      \n        print(\"a의 값이 {} 증가합니다.\".format(jump_size))\n    def show(self):\n        print('a={}'.format(self.a))\n    def reset(self):\n        self.a = 0\n        print(\"a의 값이 0으로 초기화 되었습니다.\")\n\n\na = UpJump()\n\na의 값이 0으로 초기화 되었습니다.\n\n\n\na?\n\n\nType:        UpJump\nString form: &lt;__main__.UpJump object at 0x000001E17A4FE650&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n\na.up()\n\na의 값이 1 증가합니다.\n\n\n\na.jump(-2)\n\na의 값이 -2 증가합니다.\n\n\n\na.reset()\n\na의 값이 0으로 초기화 되었습니다.\n\n\n\na.show()\n\na=0\n\n\n- show 함수를 살펴보자.\n\nshow 함수는 print와 비슷하다 \\(\\to\\) 그렇다면….?\nprint(a)를 하면 a.show() 와 동일한 효과를 내도록 만들 수 있을까?\n\n\nprint(a)\n\n&lt;__main__.UpJump object at 0x000001E17A4FE650&gt;\n\n\n\nprint는 파이썬의 내장기능이다. 내장기능을 우리가 마음대로 변환해서 사용하면 많은 문제들이 생긴다.\n\n\\(\\divideontimes\\) 그런데 a의 자료형에 해당하는 인스턴스들에 한정하여 print를 수정하는 방법이 있다면?\n\n즉, 다른 클래스 오브젝트들은 영향을 받지 않고, UpJump로 생성된 오브젝트들만 가능하게끔 하는 것이다.\n\n\n\n- 아래둘은 같은 역할을 한다. \\(\\to\\) 즉, print(“a”)는 print(\"a\".__str__())의 축약 버전이다.\n\n print(\"a\")\n\na\n\n\n\nprint(\"a\".__str__())\n\na\n\n\n- 우리가 정의한 show함수는 다음과 같다.\ndef show(self):\n        print('a={}'.format(self.a))\n- 즉, 작성한 show함수는 단지 print문을 호출하는 함수이므로, a.__str__()의 기능을 재정의하면? print(a)의 결과도 바뀌지 않을까?\n\n\n\n\ndef f():\n    print(\"강철\")\n    \nf()\n\n강철\n\n\n\ndef f() :\n    print(\"DX 강철\")\nf()\n\nDX 강철\n\n\n- 함수를 덮어씌울 수 있다는 것을 확인하였다.\n\n\n\n\ndef show() \\(\\to\\) def __str__(), print \\(\\to\\) return\n\n\nclass UpJump:\n    def __init__(self):\n        self.reset()\n    def up(self):\n        self.a = self.a + 1  \n        print(\"a의 값이 1 증가합니다.\")\n    def jump(self,jump_size):\n        self.a = self.a + jump_size      \n        print(\"a의 값이 {} 증가합니다.\".format(jump_size))\n    def __str__(self):\n        return 'a={}'.format(self.a)\n    def reset(self):\n        self.a = 0\n        print(\"a의 값이 0으로 초기화 되었습니다.\")\n\n\na = UpJump()\n\na의 값이 0으로 초기화 되었습니다.\n\n\n\na.__str__()\n\n'a=0'\n\n\n\nprint(a) ## 성공했다!!\n\na=0\n\n\n\n\n\n\n- 우리가 어떤 변수를 할당하고 실행할때 사용되는 내장 함수는 __repr__() \\(\\to\\) representation의 약자이다.\n- 그러면 __repr__도 우리가 정의할 수 있지 않을까?\n- __str()__ 과 비교해보자.\n\na = np.arange(4).reshape(2,2)\n\n\na.__str__()\n\n'[[0 1]\\n [2 3]]'\n\n\n\na.__repr__()\n\n'array([[0, 1],\\n       [2, 3]])'\n\n\n- print 문을 사용한 비교.\n\nprint(a.__str__())\n\n[[0 1]\n [2 3]]\n\n\n\nprint(a.__repr__())\n\narray([[0, 1],\n       [2, 3]])\n\n\n\n\ndef __repr__(self):\n    return 'a={}'.format(self.a)\n\nclass UpJump:\n    def __init__(self):\n        self.reset()\n    def up(self):\n        self.a = self.a + 1  \n        print(\"a의 값이 1 증가합니다.\")\n    def jump(self,jump_size):\n        self.a = self.a + jump_size      \n        print(\"a의 값이 {} 증가합니다.\".format(jump_size))\n    def __str__(self):\n        return 'a의 값은 {}입니다.'.format(self.a)\n    def reset(self):\n        self.a = 0\n        print(\"a의 값이 0으로 초기화 되었습니다.\")\n    def __repr__(self):\n        return 'a={}'.format(self.a)  \n\n\na = UpJump()\n\na의 값이 0으로 초기화 되었습니다.\n\n\n\nprint(a)\n\na의 값은 0입니다.\n\n\n\na\n\na=0\n\n\n\n\n\n- 만약 __repr__()만 정의되어 있고 __str__()이 정의되있지 않았다면 __repr__()의 내용이 __str__()의 내용을 대신한다. (단, 역은 성립하지 않음)\n\nclass UpJump:\n    def __init__(self):\n        self.reset()\n    def up(self):\n        self.a = self.a + 1  \n        print(\"a의 값이 1 증가합니다.\")\n    def jump(self,jump_size):\n        self.a = self.a + jump_size      \n        print(\"a의 값이 {} 증가합니다.\".format(jump_size))\n   # def __str__(self):\n    #    return 'a의 값은 {}입니다.'.format(self.a)\n    def reset(self):\n        self.a = 0\n        print(\"a의 값이 0으로 초기화 되었습니다.\")\n    def __repr__(self):\n        return 'a={}'.format(self.a)  \n\n\na = UpJump()\n\na의 값이 0으로 초기화 되었습니다.\n\n\n\na\n\na=0\n\n\n\nclass UpJump:\n    def __init__(self):\n        self.reset()\n    def up(self):\n        self.a = self.a + 1  \n        print(\"a의 값이 1 증가합니다.\")\n    def jump(self,jump_size):\n        self.a = self.a + jump_size      \n        print(\"a의 값이 {} 증가합니다.\".format(jump_size))\n    def __str__(self):\n        return 'a의 값은 {}입니다.'.format(self.a)\n    def reset(self):\n        self.a = 0\n        print(\"a의 값이 0으로 초기화 되었습니다.\")\n   # def __repr__(self):\n    #    return 'a={}'.format(self.a)  \n\n\na = UpJump()\n\na의 값이 0으로 초기화 되었습니다.\n\n\n\na\n\n&lt;__main__.UpJump at 0x1e17b574090&gt;\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\nprint(df.__repr__())\n\n   a  b\n0  1  2\n1  2  3\n2  3  4\n\n\n\nprint(df.__str__())\n\n   a  b\n0  1  2\n1  2  3\n2  3  4\n\n\n- 뭔가 이상하다. 앞서 배운대로라면 코드를 실행할 때 나오는 표처럼 예쁘게 나와야 하는데 그렇지 않다…\n- 아래를 살펴보자.\n\ndf._repr_html_()\n\n'&lt;div&gt;\\n&lt;style scoped&gt;\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n&lt;/style&gt;\\n&lt;table border=\"1\" class=\"dataframe\"&gt;\\n  &lt;thead&gt;\\n    &lt;tr style=\"text-align: right;\"&gt;\\n      &lt;th&gt;&lt;/th&gt;\\n      &lt;th&gt;a&lt;/th&gt;\\n      &lt;th&gt;b&lt;/th&gt;\\n    &lt;/tr&gt;\\n  &lt;/thead&gt;\\n  &lt;tbody&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;0&lt;/th&gt;\\n      &lt;td&gt;1&lt;/td&gt;\\n      &lt;td&gt;2&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;1&lt;/th&gt;\\n      &lt;td&gt;2&lt;/td&gt;\\n      &lt;td&gt;3&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;2&lt;/th&gt;\\n      &lt;td&gt;3&lt;/td&gt;\\n      &lt;td&gt;4&lt;/td&gt;\\n    &lt;/tr&gt;\\n  &lt;/tbody&gt;\\n&lt;/table&gt;\\n&lt;/div&gt;'\n\n\n\nHTML(df._repr_html_())\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n- 깨달음5. 데이터 프레임은 html로 작성되어 있으며 _repr_html_()은 html 구조를 확인할 수 있는 명령어이다!!\n- 그러면 df.__repr__()의 역할은?\n\n아 우리가 대화형 콘솔(anaconda prompt)에서 작성하면 나오는 출력형식을 지원한다!\n\n\n\n\n\n- 초기\ndef show(self):\n        _str = self.html_str.format(\n            tt1 = self.title,\n            url = self.url,\n            Q = self.Q,\n            A = self.A,\n            h1 = self.h1 )\n        display(HTML(_str))\n- 수정후\ndef _repr_html_(self):\n        _str = self.html_str.format(\n            tt1 = self.title,\n            url = self.url,\n            Q = self.Q,\n            A = self.A,\n            h1 = self.h1 )\n        return _str\n\nclass jkm:\n    def __init__(self) :\n        self.title = \"중요한건 꺽이지 않는 마음\"\n\n        self.url = \"https://github.com/guebin/PP2023/blob/main/posts/03_Class/JungGGuckMa.jpg?raw=true\"\n\n        self.Q = \"Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\"\n\n        self.A = \"A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\"\n\n        self.h1 = \"마음\"\n\n        self.html_str =  '''\n                    &lt;style&gt;\n                        .title {{\n                            font-family: \"Times New Roman\", serif;\n                            font-size: 30px;\n                            font-weight: 900;\n                        }}\n                        .text {{\n                            font-family: \"Arial\", sans-serif;\n                            font-size: 20px;\n                            font-style: italic;\n                        }}\n                        .highlight {{\n                            font-family: \"Montserrat\", monospace;\n                            font-size: 35px;\n                            font-weight: 900;\n                            text-decoration: underline; ## 밑줄\n                            font-style: normal;\n                            color: darkblue;\n                            background-color: #FFFF00;\n                        }}\n                    &lt;/style&gt;\n\n                    &lt;p class=\"title\"&gt;{tt1}&lt;/p&gt;\n                    &lt;img src={url} width=\"600\"&gt;\n                    &lt;p&gt; \\n &lt;/p&gt;\n                    &lt;p class=\"text\"&gt; {Q}&lt;/p&gt;\n                    &lt;p class=\"text\"&gt; {A}: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n                    &lt;p class=\"title\"&gt;중요한 것은 꺾이지 않는 &lt;span class=\"highlight\"&gt; {h1} &lt;/span&gt;&lt;/p&gt;\n                    '''\n        \n    def _repr_html_(self):\n                _str = self.html_str.format(\n                    tt1 = self.title,\n                    url = self.url,\n                    Q = self.Q,\n                    A = self.A,\n                    h1 = self.h1)\n                return _str\n\n\na = jkm()\na\n\n\n                    \n\n                    중요한건 꺽이지 않는 마음\n                    \n                     \n \n                     Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\n                     A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n                    중요한 것은 꺾이지 않는  마음"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-10-Extra 02. 클래스 탐구 (1).html#작성한-class-등록",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-10-Extra 02. 클래스 탐구 (1).html#작성한-class-등록",
    "title": "Extra 02. 클래스 탐구 (1)",
    "section": "",
    "text": "class jkm:\n    def __init__(self) :\n        self.title = \"중요한건 꺽이지 않는 마음\"\n\n        self.url = \"https://github.com/guebin/PP2023/blob/main/posts/03_Class/JungGGuckMa.jpg?raw=true\"\n\n        self.Q = \"Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\"\n\n        self.A = \"A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\"\n\n        self.h1 = \"마음\"\n\n        self.html_str =  '''\n                    &lt;style&gt;\n                        .title {{\n                            font-family: \"Times New Roman\", serif;\n                            font-size: 30px;\n                            font-weight: 900;\n                        }}\n                        .text {{\n                            font-family: \"Arial\", sans-serif;\n                            font-size: 20px;\n                            font-style: italic;\n                        }}\n                        .highlight {{\n                            font-family: \"Montserrat\", monospace;\n                            font-size: 35px;\n                            font-weight: 900;\n                            text-decoration: underline; ## 밑줄\n                            font-style: normal;\n                            color: darkblue;\n                            background-color: #FFFF00;\n                        }}\n                    &lt;/style&gt;\n\n                    &lt;p class=\"title\"&gt;{tt1}&lt;/p&gt;\n                    &lt;img src={url} width=\"600\"&gt;\n                    &lt;p&gt; \\n &lt;/p&gt;\n                    &lt;p class=\"text\"&gt; {Q}&lt;/p&gt;\n                    &lt;p class=\"text\"&gt; {A}: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n                    &lt;p class=\"title\"&gt;중요한 것은 꺾이지 않는 &lt;span class=\"highlight\"&gt; {h1} &lt;/span&gt;&lt;/p&gt;\n                    '''\n        \n    def show(self):\n        _str = self.html_str.format(\n            tt1 = self.title,\n            url = self.url,\n            Q = self.Q,\n            A = self.A,\n            h1 = self.h1 )\n        display(HTML(_str))\n\n\ntest = jkm()\n\n\ntest.show()\n\n\n                    \n\n                    중요한건 꺽이지 않는 마음\n                    \n                     \n \n                     Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\n                     A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n                    중요한 것은 꺾이지 않는  마음"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-10-Extra 02. 클래스 탐구 (1).html#깨달음-1.-type-class",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-10-Extra 02. 클래스 탐구 (1).html#깨달음-1.-type-class",
    "title": "Extra 02. 클래스 탐구 (1)",
    "section": "",
    "text": "- 아래처럼 우리가 생성한 test의 타입을 확인하니 type 이 jkm 으로 나온다.\n\ntest?\n\n\nType:        jkm\nString form: &lt;__main__.jkm object at 0x000001E17B1E7850&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n- 아래의 리스트, 튜플, 리스트(튜플)의 타입을 확인해보자.\n\ntype([1,2,3])\n\nlist\n\n\n\ntype((1,2,3))\n\ntuple\n\n\n\ntype(list((1,2,3)))\n\nlist\n\n\n- 깨달음1. 우리가 어떤 인스턴스 객체를 생성할 떄 그 자료형은 파이썬 내부, 혹은 우리가 작성한 클래스의 이름이다."
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-10-Extra 02. 클래스 탐구 (1).html#깨달음-2.-__init__",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-10-Extra 02. 클래스 탐구 (1).html#깨달음-2.-__init__",
    "title": "Extra 02. 클래스 탐구 (1)",
    "section": "",
    "text": "a = \"123\"\nlist(a)\n\n['1', '2', '3']\n\n\n\na = list()\n\na.__init__(\"123\")\na\n\n['1', '2', '3']\n\n\n\na = list()\na.__init__('123') \na.__init__() # 리스트 최기화\na\n\n[]\n\n\n- 깨달음 2. 우리가 list(\"123\") 과 같은 메소드를 입력할 때 사실 자료형을 변환하는 것이 아니라, list 라는 클래스의 __init__()으로 인스턴스를 생성하는 것이었다."
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-10-Extra 02. 클래스 탐구 (1).html#깨달음-3.-__str__",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-10-Extra 02. 클래스 탐구 (1).html#깨달음-3.-__str__",
    "title": "Extra 02. 클래스 탐구 (1)",
    "section": "",
    "text": "- 아래의 클래스를 관찰하자\n\nclass UpJump:\n    def __init__(self):\n        self.reset()\n    def up(self):\n        self.a = self.a + 1  \n        print(\"a의 값이 1 증가합니다.\")\n    def jump(self,jump_size):\n        self.a = self.a + jump_size      \n        print(\"a의 값이 {} 증가합니다.\".format(jump_size))\n    def show(self):\n        print('a={}'.format(self.a))\n    def reset(self):\n        self.a = 0\n        print(\"a의 값이 0으로 초기화 되었습니다.\")\n\n\na = UpJump()\n\na의 값이 0으로 초기화 되었습니다.\n\n\n\na?\n\n\nType:        UpJump\nString form: &lt;__main__.UpJump object at 0x000001E17A4FE650&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n\na.up()\n\na의 값이 1 증가합니다.\n\n\n\na.jump(-2)\n\na의 값이 -2 증가합니다.\n\n\n\na.reset()\n\na의 값이 0으로 초기화 되었습니다.\n\n\n\na.show()\n\na=0\n\n\n- show 함수를 살펴보자.\n\nshow 함수는 print와 비슷하다 \\(\\to\\) 그렇다면….?\nprint(a)를 하면 a.show() 와 동일한 효과를 내도록 만들 수 있을까?\n\n\nprint(a)\n\n&lt;__main__.UpJump object at 0x000001E17A4FE650&gt;\n\n\n\nprint는 파이썬의 내장기능이다. 내장기능을 우리가 마음대로 변환해서 사용하면 많은 문제들이 생긴다.\n\n\\(\\divideontimes\\) 그런데 a의 자료형에 해당하는 인스턴스들에 한정하여 print를 수정하는 방법이 있다면?\n\n즉, 다른 클래스 오브젝트들은 영향을 받지 않고, UpJump로 생성된 오브젝트들만 가능하게끔 하는 것이다.\n\n\n\n- 아래둘은 같은 역할을 한다. \\(\\to\\) 즉, print(“a”)는 print(\"a\".__str__())의 축약 버전이다.\n\n print(\"a\")\n\na\n\n\n\nprint(\"a\".__str__())\n\na\n\n\n- 우리가 정의한 show함수는 다음과 같다.\ndef show(self):\n        print('a={}'.format(self.a))\n- 즉, 작성한 show함수는 단지 print문을 호출하는 함수이므로, a.__str__()의 기능을 재정의하면? print(a)의 결과도 바뀌지 않을까?\n\n\n\n\ndef f():\n    print(\"강철\")\n    \nf()\n\n강철\n\n\n\ndef f() :\n    print(\"DX 강철\")\nf()\n\nDX 강철\n\n\n- 함수를 덮어씌울 수 있다는 것을 확인하였다.\n\n\n\n\ndef show() \\(\\to\\) def __str__(), print \\(\\to\\) return\n\n\nclass UpJump:\n    def __init__(self):\n        self.reset()\n    def up(self):\n        self.a = self.a + 1  \n        print(\"a의 값이 1 증가합니다.\")\n    def jump(self,jump_size):\n        self.a = self.a + jump_size      \n        print(\"a의 값이 {} 증가합니다.\".format(jump_size))\n    def __str__(self):\n        return 'a={}'.format(self.a)\n    def reset(self):\n        self.a = 0\n        print(\"a의 값이 0으로 초기화 되었습니다.\")\n\n\na = UpJump()\n\na의 값이 0으로 초기화 되었습니다.\n\n\n\na.__str__()\n\n'a=0'\n\n\n\nprint(a) ## 성공했다!!\n\na=0"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-10-Extra 02. 클래스 탐구 (1).html#깨달음-4.-__repr__",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-10-Extra 02. 클래스 탐구 (1).html#깨달음-4.-__repr__",
    "title": "Extra 02. 클래스 탐구 (1)",
    "section": "",
    "text": "- 우리가 어떤 변수를 할당하고 실행할때 사용되는 내장 함수는 __repr__() \\(\\to\\) representation의 약자이다.\n- 그러면 __repr__도 우리가 정의할 수 있지 않을까?\n- __str()__ 과 비교해보자.\n\na = np.arange(4).reshape(2,2)\n\n\na.__str__()\n\n'[[0 1]\\n [2 3]]'\n\n\n\na.__repr__()\n\n'array([[0, 1],\\n       [2, 3]])'\n\n\n- print 문을 사용한 비교.\n\nprint(a.__str__())\n\n[[0 1]\n [2 3]]\n\n\n\nprint(a.__repr__())\n\narray([[0, 1],\n       [2, 3]])\n\n\n\n\ndef __repr__(self):\n    return 'a={}'.format(self.a)\n\nclass UpJump:\n    def __init__(self):\n        self.reset()\n    def up(self):\n        self.a = self.a + 1  \n        print(\"a의 값이 1 증가합니다.\")\n    def jump(self,jump_size):\n        self.a = self.a + jump_size      \n        print(\"a의 값이 {} 증가합니다.\".format(jump_size))\n    def __str__(self):\n        return 'a의 값은 {}입니다.'.format(self.a)\n    def reset(self):\n        self.a = 0\n        print(\"a의 값이 0으로 초기화 되었습니다.\")\n    def __repr__(self):\n        return 'a={}'.format(self.a)  \n\n\na = UpJump()\n\na의 값이 0으로 초기화 되었습니다.\n\n\n\nprint(a)\n\na의 값은 0입니다.\n\n\n\na\n\na=0\n\n\n\n\n\n- 만약 __repr__()만 정의되어 있고 __str__()이 정의되있지 않았다면 __repr__()의 내용이 __str__()의 내용을 대신한다. (단, 역은 성립하지 않음)\n\nclass UpJump:\n    def __init__(self):\n        self.reset()\n    def up(self):\n        self.a = self.a + 1  \n        print(\"a의 값이 1 증가합니다.\")\n    def jump(self,jump_size):\n        self.a = self.a + jump_size      \n        print(\"a의 값이 {} 증가합니다.\".format(jump_size))\n   # def __str__(self):\n    #    return 'a의 값은 {}입니다.'.format(self.a)\n    def reset(self):\n        self.a = 0\n        print(\"a의 값이 0으로 초기화 되었습니다.\")\n    def __repr__(self):\n        return 'a={}'.format(self.a)  \n\n\na = UpJump()\n\na의 값이 0으로 초기화 되었습니다.\n\n\n\na\n\na=0\n\n\n\nclass UpJump:\n    def __init__(self):\n        self.reset()\n    def up(self):\n        self.a = self.a + 1  \n        print(\"a의 값이 1 증가합니다.\")\n    def jump(self,jump_size):\n        self.a = self.a + jump_size      \n        print(\"a의 값이 {} 증가합니다.\".format(jump_size))\n    def __str__(self):\n        return 'a의 값은 {}입니다.'.format(self.a)\n    def reset(self):\n        self.a = 0\n        print(\"a의 값이 0으로 초기화 되었습니다.\")\n   # def __repr__(self):\n    #    return 'a={}'.format(self.a)  \n\n\na = UpJump()\n\na의 값이 0으로 초기화 되었습니다.\n\n\n\na\n\n&lt;__main__.UpJump at 0x1e17b574090&gt;"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-10-Extra 02. 클래스 탐구 (1).html#깨달음-5.-_repr_html_",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-10-Extra 02. 클래스 탐구 (1).html#깨달음-5.-_repr_html_",
    "title": "Extra 02. 클래스 탐구 (1)",
    "section": "",
    "text": "df = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\nprint(df.__repr__())\n\n   a  b\n0  1  2\n1  2  3\n2  3  4\n\n\n\nprint(df.__str__())\n\n   a  b\n0  1  2\n1  2  3\n2  3  4\n\n\n- 뭔가 이상하다. 앞서 배운대로라면 코드를 실행할 때 나오는 표처럼 예쁘게 나와야 하는데 그렇지 않다…\n- 아래를 살펴보자.\n\ndf._repr_html_()\n\n'&lt;div&gt;\\n&lt;style scoped&gt;\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n&lt;/style&gt;\\n&lt;table border=\"1\" class=\"dataframe\"&gt;\\n  &lt;thead&gt;\\n    &lt;tr style=\"text-align: right;\"&gt;\\n      &lt;th&gt;&lt;/th&gt;\\n      &lt;th&gt;a&lt;/th&gt;\\n      &lt;th&gt;b&lt;/th&gt;\\n    &lt;/tr&gt;\\n  &lt;/thead&gt;\\n  &lt;tbody&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;0&lt;/th&gt;\\n      &lt;td&gt;1&lt;/td&gt;\\n      &lt;td&gt;2&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;1&lt;/th&gt;\\n      &lt;td&gt;2&lt;/td&gt;\\n      &lt;td&gt;3&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;2&lt;/th&gt;\\n      &lt;td&gt;3&lt;/td&gt;\\n      &lt;td&gt;4&lt;/td&gt;\\n    &lt;/tr&gt;\\n  &lt;/tbody&gt;\\n&lt;/table&gt;\\n&lt;/div&gt;'\n\n\n\nHTML(df._repr_html_())\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n- 깨달음5. 데이터 프레임은 html로 작성되어 있으며 _repr_html_()은 html 구조를 확인할 수 있는 명령어이다!!\n- 그러면 df.__repr__()의 역할은?\n\n아 우리가 대화형 콘솔(anaconda prompt)에서 작성하면 나오는 출력형식을 지원한다!\n\n\n\n\n\n- 초기\ndef show(self):\n        _str = self.html_str.format(\n            tt1 = self.title,\n            url = self.url,\n            Q = self.Q,\n            A = self.A,\n            h1 = self.h1 )\n        display(HTML(_str))\n- 수정후\ndef _repr_html_(self):\n        _str = self.html_str.format(\n            tt1 = self.title,\n            url = self.url,\n            Q = self.Q,\n            A = self.A,\n            h1 = self.h1 )\n        return _str\n\nclass jkm:\n    def __init__(self) :\n        self.title = \"중요한건 꺽이지 않는 마음\"\n\n        self.url = \"https://github.com/guebin/PP2023/blob/main/posts/03_Class/JungGGuckMa.jpg?raw=true\"\n\n        self.Q = \"Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\"\n\n        self.A = \"A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\"\n\n        self.h1 = \"마음\"\n\n        self.html_str =  '''\n                    &lt;style&gt;\n                        .title {{\n                            font-family: \"Times New Roman\", serif;\n                            font-size: 30px;\n                            font-weight: 900;\n                        }}\n                        .text {{\n                            font-family: \"Arial\", sans-serif;\n                            font-size: 20px;\n                            font-style: italic;\n                        }}\n                        .highlight {{\n                            font-family: \"Montserrat\", monospace;\n                            font-size: 35px;\n                            font-weight: 900;\n                            text-decoration: underline; ## 밑줄\n                            font-style: normal;\n                            color: darkblue;\n                            background-color: #FFFF00;\n                        }}\n                    &lt;/style&gt;\n\n                    &lt;p class=\"title\"&gt;{tt1}&lt;/p&gt;\n                    &lt;img src={url} width=\"600\"&gt;\n                    &lt;p&gt; \\n &lt;/p&gt;\n                    &lt;p class=\"text\"&gt; {Q}&lt;/p&gt;\n                    &lt;p class=\"text\"&gt; {A}: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n                    &lt;p class=\"title\"&gt;중요한 것은 꺾이지 않는 &lt;span class=\"highlight\"&gt; {h1} &lt;/span&gt;&lt;/p&gt;\n                    '''\n        \n    def _repr_html_(self):\n                _str = self.html_str.format(\n                    tt1 = self.title,\n                    url = self.url,\n                    Q = self.Q,\n                    A = self.A,\n                    h1 = self.h1)\n                return _str\n\n\na = jkm()\na\n\n\n                    \n\n                    중요한건 꺽이지 않는 마음\n                    \n                     \n \n                     Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\n                     A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n                    중요한 것은 꺾이지 않는  마음"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-08-Extra 00. 밈.html",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-08-Extra 00. 밈.html",
    "title": "Extra 00. 밈",
    "section": "",
    "text": "- 점프투파이썬 : 클래스는 과자틀과 비슷하다. 클래스란 똑같은 무엇인가를 계속 만들어 낼 수도 잇는 설계도면이고 객체란 클래스로 만든 피조물을 뜻한다.\n- 위키피디아 : 객체지향 프로그래밍에서 클래스는 상태(멤버 변수) 및 동작 구현(멤버 함수 또는 메서드)에 대한 초기값을 제공하는 객체 생성을 위한 확장 가능한 프로그램 코드 템플릿이다.\n- TCP 스쿨 : 클래스란 객체를 정의하는 틀 또는 설계도와 같은 의미로 사용\n- 교수님 생각 : 클래스는 복제, 변형, 재생산을 용이하게 하기 위해 만들어진 확장가능한 프로그램이 코드 단위(extensible program-code-template)이다.\n\n\nfrom IPython.core.display import HTML"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-08-Extra 00. 밈.html#sum_i15-ai-quad-a0.5",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-08-Extra 00. 밈.html#sum_i15-ai-quad-a0.5",
    "title": "Extra 00. 밈",
    "section": "\\(\\sum_{i=1}^{5} a^{i} \\quad a=0.5\\)",
    "text": "\\(\\sum_{i=1}^{5} a^{i} \\quad a=0.5\\)\n- 방법1\n\na = 0.5\n\n\na + a**2 + a**3 + a**4 +a**5\n\n0.96875\n\n\n- 방법2\n\na + \\\na**2 \\\n+ a**3 \\\n+ a**4\\\n+a**5\n\n0.96875\n\n\n\n```으로 선언하는 문자열\n- 에시 1\n\nstring = '\\n1. asdf\\n2. sdfa\\n3. dfas\\n4. fasd\\n'\nprint(string)\n\n\n1. asdf\n2. sdfa\n3. dfas\n4. fasd\n\n\n\n\nstring = \\\n'''\n1. asdf\n2. sdfa\n3. dfas\n4. fasd\n'''\nprint(string)\n\n\n1. asdf\n2. sdfa\n3. dfas\n4. fasd\n\n\n\n- 예시 2\n\nstring = \\\n'''\n1. asdf\n2. sdfa\n3. dfas\n4. fasd\n5. {}\n6. {}\n'''\n\n\nprint(string.format(\"aaa\",\"bbb\"))\n\n\n1. asdf\n2. sdfa\n3. dfas\n4. fasd\n5. aaa\n6. bbb\n\n\n\n\n\nHTML\n- 예제1. 텍스트 출력\n\nhtml_str = '''\n&lt;p&gt; 파이썬 프로그래밍 &lt;/p&gt;\n'''\n\n\nHTML(html_str)\n\n\n 파이썬 프로그래밍 \n\n\n- 예제 2 : 스타일 변경\n\nhtml_str = '''\n&lt;style&gt;\n    .title {\n         font-family : \"Times New Roman\", sefif;\n         font-size : 30px;\n         font-weight : 900;\n        }\n&lt;/style&gt;\n&lt;p class=\"title\"&gt; 파이썬 프로그래밍 &lt;/p&gt;\n'''\n\n\nHTML(html_str)\n\n\n\n 파이썬 프로그래밍 \n\n\n- 예제 3 : 이미지 삽입\n\nhtml_str = '''\n\n&lt;img src = \"hani.jpeg\" width = 300&gt;\n'''\n\n\nHTML(html_str)\n\n\n\n\n\n\n\ndisplay(HTML(html_str))"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-08-Extra 00. 밈.html#단계-밈의-구상",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-08-Extra 00. 밈.html#단계-밈의-구상",
    "title": "Extra 00. 밈",
    "section": "1단계 : 밈의 구상",
    "text": "1단계 : 밈의 구상\n\ntitle : 타이틀에 해당하는 텍스트\nimg : 데프트의 인터뷰 이미지\nQ : 인터뷰 질문에 해당하는 텍스트 (.text)\nA : 인터뷰 답변에 해당하는 텍스트 (.text)\nhighlight : “마음”\n\n\nhtml_str = '''\n&lt;style&gt;\n    .title {\n        font-family: \"Times New Roman\", serif;\n        font-size: 30px;\n        font-weight: 900;\n    }\n    .text {\n        font-family: \"Arial\", sans-serif;\n        font-size: 20px;\n        font-style: italic;\n    }\n    .highlight {\n        font-family: \"Montserrat\", monospace;\n        font-size: 35px;\n        font-weight: 900;\n        text-decoration: underline; ## 밑줄\n        font-style: normal;\n        color: darkblue;\n        background-color: #FFFF00;\n    }\n&lt;/style&gt;\n\n&lt;p class=\"title\"&gt;RGE전 패배는 괜찮다.&lt;/p&gt;\n&lt;img src=https://github.com/guebin/PP2023/blob/main/posts/03_Class/JungGGuckMa.jpg?raw=true width=\"600\"&gt;\n&lt;p&gt; \\n &lt;/p&gt;\n&lt;p class=\"text\"&gt; Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까?&lt;/p&gt;\n&lt;p class=\"text\"&gt; A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n&lt;p class=\"title\"&gt;중요한 것은 꺾이지 않는 &lt;span class=\"highlight\"&gt;마음&lt;/span&gt;&lt;/p&gt;\n'''\n\n\ndisplay(HTML(html_str))\n\n\n\n\nRGE전 패배는 괜찮다.\n\n \n \n Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까?\n A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n중요한 것은 꺾이지 않는 마음"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-08-Extra 00. 밈.html#단계-양식틀의-완성함수-이용",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-08-Extra 00. 밈.html#단계-양식틀의-완성함수-이용",
    "title": "Extra 00. 밈",
    "section": "2단계 : 양식틀의 완성(함수 이용)",
    "text": "2단계 : 양식틀의 완성(함수 이용)\n\nhtml_str = '''\n&lt;style&gt;\n    .title {{\n        font-family: \"Times New Roman\", serif;\n        font-size: 30px;\n        font-weight: 900;\n    }}\n    .text {{\n        font-family: \"Arial\", sans-serif;\n        font-size: 20px;\n        font-style: italic;\n    }}\n    .highlight {{\n        font-family: \"Montserrat\", monospace;\n        font-size: 35px;\n        font-weight: 900;\n        text-decoration: underline; ## 밑줄\n        font-style: normal;\n        color: darkblue;\n        background-color: #FFFF00;\n    }}\n&lt;/style&gt;\n\n&lt;p class=\"title\"&gt;{tt1}&lt;/p&gt;\n&lt;img src={url} width=\"600\"&gt;\n&lt;p&gt; \\n &lt;/p&gt;\n&lt;p class=\"text\"&gt; {Q}&lt;/p&gt;\n&lt;p class=\"text\"&gt; {A}: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n&lt;p class=\"title\"&gt;중요한 것은 꺾이지 않는 &lt;span class=\"highlight\"&gt; {h1} &lt;/span&gt;&lt;/p&gt;\n'''\n\n\ntitle = \"중요한건 꺽이지 않는 마음\"\n\nurl = \"https://github.com/guebin/PP2023/blob/main/posts/03_Class/JungGGuckMa.jpg?raw=true\"\n\nQ = \"Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\"\n\nA = \"A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\"\n\nh1 = \"마음\"\n\n\n_str = html_str.format(\n    tt1 = title,\n    url = url,\n    Q = Q,\n    A = A,\n    h1 = h1\n)\ndisplay(HTML(_str))\n\n\n\n\n중요한건 꺽이지 않는 마음\n\n \n \n Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\n A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n중요한 것은 꺾이지 않는  마음"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-08-Extra 00. 밈.html#단계-밈놀이함수",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-08-Extra 00. 밈.html#단계-밈놀이함수",
    "title": "Extra 00. 밈",
    "section": "3단계 : 밈놀이(함수)",
    "text": "3단계 : 밈놀이(함수)\n\ndef JM (html_str,title,url,Q,A,h1) : \n    _str = html_str.format(\n        tt1 = title,\n        url = url,\n        Q = Q,\n        A = A,\n        h1 = h1\n    )\n    display(HTML(_str))\n\n\nJM(html_str,title,url,Q,A,h1)\n\n\n\n\n중요한건 꺽이지 않는 마음\n\n \n \n Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\n A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n중요한 것은 꺾이지 않는  마음 \n\n\n\n이러한 코드들의 비판\n- 변수들이 정리가 되어있지 않고 산만함"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-08-Extra 00. 밈.html#단계-밈놀이-클래스",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-08-Extra 00. 밈.html#단계-밈놀이-클래스",
    "title": "Extra 00. 밈",
    "section": "4단계 밈놀이 (클래스)",
    "text": "4단계 밈놀이 (클래스)\n- 클래스 선언 : 도화지 만들기\n\nclass jkm :\n     pass\n\n\ntest = jkm()\ntest\n\n&lt;__main__.jkm at 0x1c07fdfa2d0&gt;\n\n\n\n뼈대 생성\n\ntest.title = \"중요한건 꺽이지 않는 마음\"\n\ntest.url = \"https://github.com/guebin/PP2023/blob/main/posts/03_Class/JungGGuckMa.jpg?raw=true\"\n\ntest.Q = \"Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\"\n\ntest.A = \"A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\"\n\ntest.h1 = \"마음\"\n\ntest.html_str =  '''\n&lt;style&gt;\n    .title {{\n        font-family: \"Times New Roman\", serif;\n        font-size: 30px;\n        font-weight: 900;\n    }}\n    .text {{\n        font-family: \"Arial\", sans-serif;\n        font-size: 20px;\n        font-style: italic;\n    }}\n    .highlight {{\n        font-family: \"Montserrat\", monospace;\n        font-size: 35px;\n        font-weight: 900;\n        text-decoration: underline; ## 밑줄\n        font-style: normal;\n        color: darkblue;\n        background-color: #FFFF00;\n    }}\n&lt;/style&gt;\n\n&lt;p class=\"title\"&gt;{tt1}&lt;/p&gt;\n&lt;img src={url} width=\"600\"&gt;\n&lt;p&gt; \\n &lt;/p&gt;\n&lt;p class=\"text\"&gt; {Q}&lt;/p&gt;\n&lt;p class=\"text\"&gt; {A}: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n&lt;p class=\"title\"&gt;중요한 것은 꺾이지 않는 &lt;span class=\"highlight\"&gt; {h1} &lt;/span&gt;&lt;/p&gt;\n'''\n\n\n\nshow 함수선언\n\ndef show(test):\n    _str = test.html_str.format(\n        tt1 = test.title,\n        url = test.url,\n        Q = test.Q,\n        A = test.A,\n        h1 = test.h1\n    )\n    display(HTML(_str))\n\n\nshow(test)\n\n\n\n\n중요한건 꺽이지 않는 마음\n\n \n \n Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\n A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n중요한 것은 꺾이지 않는  마음"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-16-04. Python Basic (5).html",
    "href": "posts/DX/00. 데이터 다루기/2023-08-16-04. Python Basic (5).html",
    "title": "04. Python Basic (5)",
    "section": "",
    "text": "*은 여러개의 인수를 튜플로 입력받아 처리해주는 메소드이다.\n\n\ndef a(*alpha) :\n    for a in alpha : \n        print(f\"입력된 문자는 {a}입니다.\")\n\n\na(\"A\",\"B\")\n\n입력된 문자는 A입니다.\n입력된 문자는 B입니다.\n\n\n- 또한, 임의의 매개변수를 여러개 전달받고, 고정변수를 따로 전달받아 아래와 같이 작성할 수 있다.\n\ndef a(*alpha,v) :\n    for a in alpha : \n        print(f\"입력된 문자는 {a}이며 종류는 {v}입니다.\")\n\n\na(\"A\",\"B\",v = \"알파벳\")\n\n입력된 문자는 A이며 종류는 알파벳입니다.\n입력된 문자는 B이며 종류는 알파벳입니다.\n\n\n\n단 고정값은 맨뒤에 전달해주어야 한다.\n\n\na(v = \"알파벳\",\"A\",\"B\")\n\nSyntaxError: positional argument follows keyword argument (1027399463.py, line 1)\n\n\n\n\n\n\\[\\text{result} = \\sum_{i=1}^{n} x^2\\]\n\ndef a(*a) :\n    result = sum([i**2 for i in a])\n    return result\n\n\na(1,2,3,4,5)\n\n55\n\n\n\nlst= [2,10,4]\n\n\ndef solution(sides):\n    sides.sort()\n    if sides[-1] &lt; sum(sides[:-1]) :\n        return 1\n    else :\n        return 2\n\n\nsolution(lst)\n\n2\n\n\n\n\n\n- 분수 덧셈후 결과의 기약분수의 분모와 분자를 출력\n\nimport math\n\n\ndef sol(n1,d1,n2,d2) :\n    d = d1*d2 ## 분모 \n    n = n1*d2 + n2*d1 ##분자\n\n    gcd = math.gcd(d,n) ## 최대공약수\n\n    return n//gcd,d//gcd\n\n\nn,d = sol(1,2,3,4)\n\n\nn\n\n5\n\n\n\nd\n\n4\n\n\n\n\n\n\n\n\na = 1\nb = -a\n\na == abs(b)\n\nTrue\n\n\n\n\n\n\na = list(\"abcd\")\n\nall(a)\n\nTrue\n\n\n- 0은 False와 같음\n\n0 == False\n\nTrue\n\n\n\na.append(0)\na\n\n['a', 'b', 'c', 'd', 0]\n\n\n\nall(a)\n\nFalse\n\n\n\n\n\n\na\n\n['a', 'b', 'c', 'd', 0]\n\n\n\nany(a)\n\nTrue\n\n\n\n\n\n\na\n\n['a', 'b', 'c', 'd', 0]\n\n\n\ndir(a)\n\n['__add__',\n '__class__',\n '__class_getitem__',\n '__contains__',\n '__delattr__',\n '__delitem__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__iadd__',\n '__imul__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__reversed__',\n '__rmul__',\n '__setattr__',\n '__setitem__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'append',\n 'clear',\n 'copy',\n 'count',\n 'extend',\n 'index',\n 'insert',\n 'pop',\n 'remove',\n 'reverse',\n 'sort']\n\n\n- 오리지날 메소드만 출력\n\nb = [i for i in dir(a) if i[0] != \"_\"]\n\n\nb\n\n['append',\n 'clear',\n 'copy',\n 'count',\n 'extend',\n 'index',\n 'insert',\n 'pop',\n 'remove',\n 'reverse',\n 'sort']\n\n\n\n\n\n- x를 y로 나눈 몫과 나머지를 튜플로 반환\n\ndivmod(5,2)\n\n(2, 1)\n\n\n\n\n\n- 코드의 실행을 입력받아 결과를 받음.\n\neval('10+20')\n\n30\n\n\n\n\n\n- 함수에 매개변수를 전달해 참인 경우만 결과를 반환 \\(\\to\\) filter(f,a)\n- ex1\n\n\n\ndef f(x) :\n    if x % 2 == 0 :\n        return x\n\n\na = [1,2,3,4,5]\n\n\nlist(filter(f,a))\n\n[2, 4]\n\n\n\n\n\n\nlist(filter(lambda x : x % 2==0,a))\n\n[2, 4]\n\n\n\n\n\n\n- 문자의 아스키 코드반환\n\nord(\"A\")\n\n65\n\n\n\n\n\n- 짝수 선택 예제\n\nimport numpy as np\nimport pandas as pd\n\n\nnp.random.seed(1)\ndf2= pd.DataFrame(np.random.normal(size=(10,4)),columns=list('ABCD'))\ndf2.head()\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n1.624345\n-0.611756\n-0.528172\n-1.072969\n\n\n1\n0.865408\n-2.301539\n1.744812\n-0.761207\n\n\n2\n0.319039\n-0.249370\n1.462108\n-2.060141\n\n\n3\n-0.322417\n-0.384054\n1.133769\n-1.099891\n\n\n4\n-0.172428\n-0.877858\n0.042214\n0.582815\n\n\n\n\n\n\n\n- A열에서 0보다 큰값들만 출력\n\ndf2.iloc[map(lambda x : x&gt;0, df2.A),:]\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n1.624345\n-0.611756\n-0.528172\n-1.072969\n\n\n1\n0.865408\n-2.301539\n1.744812\n-0.761207\n\n\n2\n0.319039\n-0.249370\n1.462108\n-2.060141\n\n\n6\n0.900856\n-0.683728\n-0.122890\n-0.935769\n\n\n\n\n\n\n\n- A,C열에서 둘다 0보다 큰값들만 출력\n\ndf2.loc[ map(lambda x,y : (x&gt;0) & (y&gt;0) , df2.A,df2.C),:]\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n1\n0.865408\n-2.301539\n1.744812\n-0.761207\n\n\n2\n0.319039\n-0.249370\n1.462108\n-2.060141\n\n\n\n\n\n\n\n- 짝수에는 곱하기2, 홀수에는 곱하기 3\n\na = list(range(1,11))\na\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n\nlist(map(lambda x : x*2 if x%2 ==0 else x*3,a))\n\n[3, 4, 9, 8, 15, 12, 21, 16, 27, 20]\n\n\n\n\n\n- 객체가 해당 자료형의 해당하는 객체인지 확인\n\na = list(\"abcd\")\nb = 1\n\nprint(isinstance(a,list))\nprint(isinstance(b,int))\n\nTrue\nTrue\n\n\n\n\n\n- 두 객체의 요소들을 쌍으로 묶어서 반환해줌\n\na = [1,2,3,4]\nb = [2,4,6,8]\nlist(zip(a,b))\n\n[(1, 2), (2, 4), (3, 6), (4, 8)]\n\n\n- 아래와 같이 인덱스 범위를 넘어가면 그냥 무시한다.\n\na = [1,2,3,4]\nb = [2,4,6,8,9]\nlist(zip(a,b))\n\n[(1, 2), (2, 4), (3, 6), (4, 8)]"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-16-04. Python Basic (5).html#ex1.-여러개-인수를-입력받아-처리하는-함수",
    "href": "posts/DX/00. 데이터 다루기/2023-08-16-04. Python Basic (5).html#ex1.-여러개-인수를-입력받아-처리하는-함수",
    "title": "04. Python Basic (5)",
    "section": "",
    "text": "*은 여러개의 인수를 튜플로 입력받아 처리해주는 메소드이다.\n\n\ndef a(*alpha) :\n    for a in alpha : \n        print(f\"입력된 문자는 {a}입니다.\")\n\n\na(\"A\",\"B\")\n\n입력된 문자는 A입니다.\n입력된 문자는 B입니다.\n\n\n- 또한, 임의의 매개변수를 여러개 전달받고, 고정변수를 따로 전달받아 아래와 같이 작성할 수 있다.\n\ndef a(*alpha,v) :\n    for a in alpha : \n        print(f\"입력된 문자는 {a}이며 종류는 {v}입니다.\")\n\n\na(\"A\",\"B\",v = \"알파벳\")\n\n입력된 문자는 A이며 종류는 알파벳입니다.\n입력된 문자는 B이며 종류는 알파벳입니다.\n\n\n\n단 고정값은 맨뒤에 전달해주어야 한다.\n\n\na(v = \"알파벳\",\"A\",\"B\")\n\nSyntaxError: positional argument follows keyword argument (1027399463.py, line 1)"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-16-04. Python Basic (5).html#ex2.-아래를-만족하는-함수를-작성하라.",
    "href": "posts/DX/00. 데이터 다루기/2023-08-16-04. Python Basic (5).html#ex2.-아래를-만족하는-함수를-작성하라.",
    "title": "04. Python Basic (5)",
    "section": "",
    "text": "\\[\\text{result} = \\sum_{i=1}^{n} x^2\\]\n\ndef a(*a) :\n    result = sum([i**2 for i in a])\n    return result\n\n\na(1,2,3,4,5)\n\n55\n\n\n\nlst= [2,10,4]\n\n\ndef solution(sides):\n    sides.sort()\n    if sides[-1] &lt; sum(sides[:-1]) :\n        return 1\n    else :\n        return 2\n\n\nsolution(lst)\n\n2"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-16-04. Python Basic (5).html#ex3.-여러-개의-값-return",
    "href": "posts/DX/00. 데이터 다루기/2023-08-16-04. Python Basic (5).html#ex3.-여러-개의-값-return",
    "title": "04. Python Basic (5)",
    "section": "",
    "text": "- 분수 덧셈후 결과의 기약분수의 분모와 분자를 출력\n\nimport math\n\n\ndef sol(n1,d1,n2,d2) :\n    d = d1*d2 ## 분모 \n    n = n1*d2 + n2*d1 ##분자\n\n    gcd = math.gcd(d,n) ## 최대공약수\n\n    return n//gcd,d//gcd\n\n\nn,d = sol(1,2,3,4)\n\n\nn\n\n5\n\n\n\nd\n\n4"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-16-04. Python Basic (5).html#내장함수",
    "href": "posts/DX/00. 데이터 다루기/2023-08-16-04. Python Basic (5).html#내장함수",
    "title": "04. Python Basic (5)",
    "section": "",
    "text": "a = 1\nb = -a\n\na == abs(b)\n\nTrue\n\n\n\n\n\n\na = list(\"abcd\")\n\nall(a)\n\nTrue\n\n\n- 0은 False와 같음\n\n0 == False\n\nTrue\n\n\n\na.append(0)\na\n\n['a', 'b', 'c', 'd', 0]\n\n\n\nall(a)\n\nFalse\n\n\n\n\n\n\na\n\n['a', 'b', 'c', 'd', 0]\n\n\n\nany(a)\n\nTrue\n\n\n\n\n\n\na\n\n['a', 'b', 'c', 'd', 0]\n\n\n\ndir(a)\n\n['__add__',\n '__class__',\n '__class_getitem__',\n '__contains__',\n '__delattr__',\n '__delitem__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__iadd__',\n '__imul__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__reversed__',\n '__rmul__',\n '__setattr__',\n '__setitem__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'append',\n 'clear',\n 'copy',\n 'count',\n 'extend',\n 'index',\n 'insert',\n 'pop',\n 'remove',\n 'reverse',\n 'sort']\n\n\n- 오리지날 메소드만 출력\n\nb = [i for i in dir(a) if i[0] != \"_\"]\n\n\nb\n\n['append',\n 'clear',\n 'copy',\n 'count',\n 'extend',\n 'index',\n 'insert',\n 'pop',\n 'remove',\n 'reverse',\n 'sort']\n\n\n\n\n\n- x를 y로 나눈 몫과 나머지를 튜플로 반환\n\ndivmod(5,2)\n\n(2, 1)\n\n\n\n\n\n- 코드의 실행을 입력받아 결과를 받음.\n\neval('10+20')\n\n30\n\n\n\n\n\n- 함수에 매개변수를 전달해 참인 경우만 결과를 반환 \\(\\to\\) filter(f,a)\n- ex1\n\n\n\ndef f(x) :\n    if x % 2 == 0 :\n        return x\n\n\na = [1,2,3,4,5]\n\n\nlist(filter(f,a))\n\n[2, 4]\n\n\n\n\n\n\nlist(filter(lambda x : x % 2==0,a))\n\n[2, 4]\n\n\n\n\n\n\n- 문자의 아스키 코드반환\n\nord(\"A\")\n\n65\n\n\n\n\n\n- 짝수 선택 예제\n\nimport numpy as np\nimport pandas as pd\n\n\nnp.random.seed(1)\ndf2= pd.DataFrame(np.random.normal(size=(10,4)),columns=list('ABCD'))\ndf2.head()\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n1.624345\n-0.611756\n-0.528172\n-1.072969\n\n\n1\n0.865408\n-2.301539\n1.744812\n-0.761207\n\n\n2\n0.319039\n-0.249370\n1.462108\n-2.060141\n\n\n3\n-0.322417\n-0.384054\n1.133769\n-1.099891\n\n\n4\n-0.172428\n-0.877858\n0.042214\n0.582815\n\n\n\n\n\n\n\n- A열에서 0보다 큰값들만 출력\n\ndf2.iloc[map(lambda x : x&gt;0, df2.A),:]\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n1.624345\n-0.611756\n-0.528172\n-1.072969\n\n\n1\n0.865408\n-2.301539\n1.744812\n-0.761207\n\n\n2\n0.319039\n-0.249370\n1.462108\n-2.060141\n\n\n6\n0.900856\n-0.683728\n-0.122890\n-0.935769\n\n\n\n\n\n\n\n- A,C열에서 둘다 0보다 큰값들만 출력\n\ndf2.loc[ map(lambda x,y : (x&gt;0) & (y&gt;0) , df2.A,df2.C),:]\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n1\n0.865408\n-2.301539\n1.744812\n-0.761207\n\n\n2\n0.319039\n-0.249370\n1.462108\n-2.060141\n\n\n\n\n\n\n\n- 짝수에는 곱하기2, 홀수에는 곱하기 3\n\na = list(range(1,11))\na\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n\nlist(map(lambda x : x*2 if x%2 ==0 else x*3,a))\n\n[3, 4, 9, 8, 15, 12, 21, 16, 27, 20]\n\n\n\n\n\n- 객체가 해당 자료형의 해당하는 객체인지 확인\n\na = list(\"abcd\")\nb = 1\n\nprint(isinstance(a,list))\nprint(isinstance(b,int))\n\nTrue\nTrue\n\n\n\n\n\n- 두 객체의 요소들을 쌍으로 묶어서 반환해줌\n\na = [1,2,3,4]\nb = [2,4,6,8]\nlist(zip(a,b))\n\n[(1, 2), (2, 4), (3, 6), (4, 8)]\n\n\n- 아래와 같이 인덱스 범위를 넘어가면 그냥 무시한다.\n\na = [1,2,3,4]\nb = [2,4,6,8,9]\nlist(zip(a,b))\n\n[(1, 2), (2, 4), (3, 6), (4, 8)]"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-16-04. Python Basic (5).html#sub",
    "href": "posts/DX/00. 데이터 다루기/2023-08-16-04. Python Basic (5).html#sub",
    "title": "04. Python Basic (5)",
    "section": "sub",
    "text": "sub\n\np = \"강철 : 961230-1111111 호연 : 961231-1111111\" \n\n\nimport re\n\n- 해당 패턴을 아래와 같이 기억\n\nmask = re.compile(\"(\\d{6})[-]\\d{7}\")\n\n- g&lt;?&gt; : 그룹을 의미하며 위 컴파일에서 ()로 구분한다. \\(\\to\\) g&lt;0&gt;은 전체 패턴을 의미\n- () : g&lt;1&gt;\n\nmask.sub(\"\\g&lt;1&gt;-XXXXXXX\",p)\n\n'강철 : 961230-XXXXXXX 호연 : 961231-XXXXXXX'\n\n\n- ()[-] : g&lt;0&gt;\n\nmask.sub(\"\\g&lt;0&gt;-XXXXXXX\",p)\n\n'강철 : 961230-1111111-XXXXXXX 호연 : 961231-1111111-XXXXXXX'\n\n\n- 아래와 같이 1명만 바꿀 수 있음\n\nmask.sub(\"\\g&lt;1&gt;-XXXXXXX\",p,count=1)\n\n'강철 : 961230-XXXXXXX 호연 : 961231-1111111'"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-16-04. Python Basic (5).html#basic",
    "href": "posts/DX/00. 데이터 다루기/2023-08-16-04. Python Basic (5).html#basic",
    "title": "04. Python Basic (5)",
    "section": "basic",
    "text": "basic\n\nimport re\n\n\nmatch\n- 문자열의 첫 글자가 정규식과 일치하는지 확인\n\ns = \"I am gc\"\n\n\nresult = re.match(\"I\",s)\nif result:\n    print(\"o\")\nelse :\n    print(\"x\")\n\no\n\n\n\n\nsearch\n- 문자열 전체에서 정규식과 일치하는 확인\n\ns = \"I am gcg\"\n\n\nresult = re.search(\"gc\",s)\n\n\nif result:\n    print(\"o\")\nelse :\n    print(\"x\")\n\no\n\n\n- 매치된 결과를 다음과 같은 메소드로 확인할 수 있다.\n\ns = \"I am gc\"\nresult = re.search(\"g\",s)\n\n\ngroup() : 매치된 문자열을 반환\n\n\nresult.group()\n\n'g'\n\n\n\nstart(),end() : 매치된 문자열의 시작과 끝의 위치를 반환\n\nend() : 끝위치-1을 반환한다.\n\n\n\nresult.start()\n\n5\n\n\n\nresult.end()\n\n6\n\n\n\ns[5]\n\n'g'\n\n\n\ns[6]\n\n'c'\n\n\n\nspan() : 매치된 문자열의 시작, 끝을 튜플로 반환\n\n\nresult.span()\n\n(5, 6)\n\n\n\n\nfindall\n- 정규식과 매치되는 모든 문자열을 리스트로 반환\n\ns= 'I am gc gc'\n\n\nre.findall(\"g\",s)\n\n['g', 'g']\n\n\n\nre.findall(\"gc\",s)\n\n['gc', 'gc']\n\n\n\n\nfinditer\n- 정규식과 매치되는 모든 문자열을 반복 가능한 객체로 변환\n\ns\n\n'I am gc gc'\n\n\n\nresult = re.finditer(\"gc\",s)\n\n\nfor i in result : \n    print(i)\n\n&lt;re.Match object; span=(5, 7), match='gc'&gt;\n&lt;re.Match object; span=(8, 10), match='gc'&gt;\n\n\n\n\n나열된 문자 포함여부 확인\n\ns\n\n'I am gc gc'\n\n\n\nre.findall(\"[gc]\",s)\n\n['g', 'c', 'g', 'c']\n\n\n- 같은 문자 한번만 표시\n\nlist(set(re.findall(\"[gc]\",s)))\n\n['c', 'g']\n\n\n\n^ : 해당 문자열을 제외한 문자들을 리스트로 출력\n\n\ns\n\n'I am gc gc'\n\n\n\nlist(set(re.findall(\"[^gc]\",s)))\n\n[' ', 'm', 'a', 'I']\n\n\n\nlist(set(re.findall(\"[^ gc]\",s))) ## 공백 제거\n\n['m', 'a', 'I']\n\n\n\n\n범위안에 문자 포함여부확인\n\ns\n\n'I am gc gc'\n\n\n- 소문자만 출력\n\nre.findall(\"[a-z]\",s)\n\n['a', 'm', 'g', 'c', 'g', 'c']\n\n\n- 대문자만 출력\n\nre.findall(\"[A-Z]\",s)\n\n['I']\n\n\n- 대,소문자 둘다 출력\n\nre.findall(\"[A-Za-z]\",s)\n\n['I', 'a', 'm', 'g', 'c', 'g', 'c']\n\n\n- 숫자 찾기\n\ns = \"gc is 28 years\"\n\n\nre.findall(\"[0-9]\",s)\n\n['2', '8']\n\n\n\n\n두 문자 사이의 포함 여부 확인\n\ns = \"sabe\"\n\n\n.은 한글자를 대체하는 단어로 다음과 같은 패턴으로 활용할 수 있다.\n\n\ns\n\n'sabe'\n\n\n\nre.findall(\"a.e\",s) \n\n['abe']\n\n\n\n\n문자 반복 확인\n- 일정한 패턴이 얼마나 반복되는지 아래와 같은 형식으로 확인할 수 있다.\n\ns = \"gc gcc\"\n\n\ns\n\n'gc gcc'\n\n\n\nc가 0회이상 반복\n\n\nre.findall(\"g*c\",s)\n\n['gc', 'gc', 'c']\n\n\n\nc가 1회 이상 반복\n\n\nre.findall(\"gc{1}\",s)\n\n['gc', 'gc']\n\n\n\nc가 1~2회 반복\n\n\nre.findall(\"gc{1,2}\",s)\n\n['gc', 'gcc']\n\n\n\nc가 0에서 1회 반복\n\n\nre.findall(\"gc?\",s)\n\n['gc', 'gc']"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html",
    "title": "02. Python Basic (3)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#why-use1",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#why-use1",
    "title": "02. Python Basic (3)",
    "section": "why use(1)?",
    "text": "why use(1)?\n- 빠르고 다중 작업에 유리\n- 메모리 관리측면에서도 좋음, 또한 소괄호 생략이 가능하다."
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#예제-1-여러개의-변수를-동시-출력-및-할당",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#예제-1-여러개의-변수를-동시-출력-및-할당",
    "title": "02. Python Basic (3)",
    "section": "예제 1 : 여러개의 변수를 동시 출력 및 할당",
    "text": "예제 1 : 여러개의 변수를 동시 출력 및 할당\n\nn,a,s,h,w = 1,2,3,4,5\n\n\nn,a,s,h,w\n\n(1, 2, 3, 4, 5)\n\n\n\na = 3.1,\na\n\n(3.1,)"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#예제-2.-서로-다른-타입의-원소들로-튜플",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#예제-2.-서로-다른-타입의-원소들로-튜플",
    "title": "02. Python Basic (3)",
    "section": "예제 2. 서로 다른 타입의 원소들로 튜플?",
    "text": "예제 2. 서로 다른 타입의 원소들로 튜플?\n\ns = 90,80,70,\"A\",\"B\"\ns\n\n(90, 80, 70, 'A', 'B')"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#예제-3-튜플-중첩",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#예제-3-튜플-중첩",
    "title": "02. Python Basic (3)",
    "section": "예제 3: 튜플 중첩",
    "text": "예제 3: 튜플 중첩\n\ns = 90,80,70,(\"A\",\"B\")\ns\n\n(90, 80, 70, ('A', 'B'))"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#예제-4-두-변수의-값을-교환",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#예제-4-두-변수의-값을-교환",
    "title": "02. Python Basic (3)",
    "section": "예제 4 : 두 변수의 값을 교환",
    "text": "예제 4 : 두 변수의 값을 교환\n\na,b = 1,2\n\n\na,b\n\n(1, 2)\n\n\n\na,b = b,a\n\n\na,b\n\n(2, 1)"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#예제-5-for문",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#예제-5-for문",
    "title": "02. Python Basic (3)",
    "section": "예제 5: for문",
    "text": "예제 5: for문\n\nlst = [['gc', 2021502565, 'M'],\n       ['iu',202254321, 'F'],\n       ['hodong', 202011223, 'M']]\nlst\n\n[['gc', 2021502565, 'M'], ['iu', 202254321, 'F'], ['hodong', 202011223, 'M']]\n\n\n\nfor name, s, sex in lst :\n    print(name,s,sex)\n\ngc 2021502565 M\niu 202254321 F\nhodong 202011223 M"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#예제-6-range-함수를-사용해-만들기",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#예제-6-range-함수를-사용해-만들기",
    "title": "02. Python Basic (3)",
    "section": "예제 6 : range 함수를 사용해 만들기",
    "text": "예제 6 : range 함수를 사용해 만들기\n\ntest = tuple(range(1,11))\ntest\n\n(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#예제-7-for-tuple-_",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#예제-7-for-tuple-_",
    "title": "02. Python Basic (3)",
    "section": "예제 7 : for +  tuple + \"_\"",
    "text": "예제 7 : for +  tuple + \"_\"\n\nlst\n\n[['gc', 2021502565, 'M'], ['iu', 202254321, 'F'], ['hodong', 202011223, 'M']]\n\n\n\nfor _,s,_ in lst :\n    print(s)\n\n2021502565\n202254321\n202011223\n\n\n\nfor _,_,s in lst :\n    print(s)\n\nM\nF\nM"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#예제-8-언패킹-연산자-for-starstar",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#예제-8-언패킹-연산자-for-starstar",
    "title": "02. Python Basic (3)",
    "section": "예제 8 : 언패킹 연산자(*) + for (\\(\\star\\star\\))",
    "text": "예제 8 : 언패킹 연산자(*) + for (\\(\\star\\star\\))\n\n*는 특정하지 않은 여러개의 인자를 튜플 형태로 받는다.\n\n\nfor n,*a in lst :\n    print(n,*a)\n\ngc 2021502565 M\niu 202254321 F\nhodong 202011223 M\n\n\n\nfor n,*a in lst :\n    print(n,a)\n\ngc [2021502565, 'M']\niu [202254321, 'F']\nhodong [202011223, 'M']\n\n\n\nfor n,*a in lst :\n    print(n)\n\ngc\niu\nhodong\n\n\n\nh,b,*t = range(1,11)\n\n\nh,b,t\n\n(1, 2, [3, 4, 5, 6, 7, 8, 9, 10])\n\n\n\nh,b,*t\n\n(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\n\n\nt\n\n[3, 4, 5, 6, 7, 8, 9, 10]\n\n\n- 언패킹 연산자를 아래처럼 단일값으로 쓸수는 없다. 대신 print문을 이욯아여 출력!\n\n*t\n\nSyntaxError: can't use starred expression here (3801933867.py, line 1)\n\n\n\nprint(*t)\n\n3 4 5 6 7 8 9 10\n\n\n\ntemp = h,b,t\n\n\ntemp\n\n(1, 2, [3, 5, 5, 6, 7, 8, 9, 10])\n\n\n- 튜플안에 선언한 리스트는 수정할 수 있다!\n\ntemp[2][1]=4\n\n\ntemp\n\n(1, 2, [3, 4, 5, 6, 7, 8, 9, 10])\n\n\n- 단, 튜플안에 선언된 리스트 통째?로는 바꿀 수 없음\n\ntemp[2] = d\n\nNameError: name 'd' is not defined\n\n\n\ntemp[2] = [1,2,3]\n\nTypeError: 'tuple' object does not support item assignment"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#예제-9-함수",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#예제-9-함수",
    "title": "02. Python Basic (3)",
    "section": "예제 9 : 함수",
    "text": "예제 9 : 함수\n\n아래의 함수는 여러개의 값을 리턴하는 것처럼 보이나 사실은 길이가 4인, 튜플 1개만을 return한다."
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#왜-튜플만이-괄호를-생략",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#왜-튜플만이-괄호를-생략",
    "title": "02. Python Basic (3)",
    "section": "왜 튜플만이 괄호를 생략?",
    "text": "왜 튜플만이 괄호를 생략?\n\n튜플을 먼저 만들고, 괄호를 생략하는 문법을 추가한 것은 아닐것이다.\n원래는 괄호없이 벡터를 만들고 싶었을 것임\n차피 벡터는 한,두번 쓰고 버리는 경우가 많고, 대부분 이름도 필요없음 \\(\\to\\) (즉, 원소에 접근해서 sorting하고… 순서를 바꿀 필요가 없다는 것임)\n데이터를 분석하면서 우리에게 필요한것은, 데이터가 벡터 형태로 모여, 하나이 DataFrame을 구축하기만 하면된다."
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#원소-추가",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#원소-추가",
    "title": "02. Python Basic (3)",
    "section": "원소 추가",
    "text": "원소 추가\n\ns = {\"a\",\"b\"}\n\n\ns.add(\"c\")\ns\n\n{'a', 'b', 'c'}\n\n\n\ns.update([\"c\",\"d\"])\ns\n\n{'a', 'b', 'c', 'd'}"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#원소-삭제",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#원소-삭제",
    "title": "02. Python Basic (3)",
    "section": "원소 삭제",
    "text": "원소 삭제\n\ns\n\n{'a', 'b', 'c', 'd'}\n\n\n\ns.remove(\"c\")\n\n\ns\n\n{'a', 'b', 'd'}\n\n\n- discard로 삭제시 에러메세지를 반환하지 않음\n\ns.remove(\"z\")\n\nKeyError: 'z'\n\n\n\ns.discard(\"z\")\n\n- 모든 원소 삭제\n\ns.clear()\n\n\ns\n\nset()"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#in",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#in",
    "title": "02. Python Basic (3)",
    "section": "in",
    "text": "in\n\n\"a\" in s\n\nTrue\n\n\n\n합,교,차 집합\n\nset1 = {\"a\",\"b\",\"c\"}\nset2 = {\"b\",\"c\",\"d\"}\n\n- 합집합\n\nset1|set2\n\n{'a', 'b', 'c', 'd'}\n\n\n\nset1.union(set2)\n\n{'a', 'b', 'c', 'd'}\n\n\n- 교집합\n\nset1 & set2\n\n{'b', 'c'}\n\n\n\nset1.intersection(set2)\n\n{'b', 'c'}\n\n\n- 차집합\n\nset1,set2\n\n({'a', 'b', 'c'}, {'b', 'c', 'd'})\n\n\n\nset2-set1\n\n{'d'}\n\n\n\n\n부분 집합\n\nset3 = set1|set2\n\n\nset2&lt;set3\n\nTrue"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#for문",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#for문",
    "title": "02. Python Basic (3)",
    "section": "for문",
    "text": "for문\n- 다음에 txt에서 각 알파벳이 몇 번 사용됬는지 구하기\n\ntxt = 'asdkflkjahsdlkjfhlaksglkjdhflkgjhlskdfjhglkajhsdlkfjhalsdkf'\ntxt\n\n'asdkflkjahsdlkjfhlaksglkjdhflkgjhlskdfjhglkajhsdlkfjhalsdkf'\n\n\n\n{i : list(txt).count(i) for i in set(txt)}\n\n{'l': 9, 'h': 7, 's': 6, 'd': 6, 'a': 5, 'g': 3, 'j': 7, 'f': 6, 'k': 10}"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#선언",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#선언",
    "title": "02. Python Basic (3)",
    "section": "선언",
    "text": "선언\n\n방법 1. 가장 일반적\n\ndct=  {\"a\" : 1, \"b\":2}\ndct\n\n{'a': 1, 'b': 2}\n\n\n\n\n방법 2. dict(\\(\\star\\star\\))\n\ndct = dict(a=1, b=2)\ndct\n\n{'a': 1, 'b': 2}\n\n\n\n\n방법 3. 중첩된 리스트\n\ndct = dict([[\"a\",1],[\"b\",2]])\ndct\n\n{'a': 1, 'b': 2}\n\n\n\n\n방법 4. 튜플\n\ndct = (\"a\",1),(\"b\",2)\ndict(dct)\n\n{'a': 1, 'b': 2}"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#원소-추출",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#원소-추출",
    "title": "02. Python Basic (3)",
    "section": "원소 추출",
    "text": "원소 추출\n- 딕셔너리는 key값을 이용해서 추출해야한다. (value로 key를 찾는 것은 불가!)\n\ndct =dict(dct)\n\n\ndct[\"a\"], dct[\"b\"]\n\n(1, 2)\n\n\n\nex1. 딕셔너리를 쓰는 이유?\n- 아래와 같은 구조가 있다고 하자.\n\nlst = [[\"a\",1],[\"b\",2]]\nlst\n\n[['a', 1], ['b', 2]]\n\n\n- 여기에서 a의 값을 알려면?\n\n[s for n,s in lst if n==\"a\"]\n\n[1]\n\n\n- 딕셔너리를 이용한 풀이\n\ndict(lst)[\"a\"]\n\n1"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#원소-추가-1",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#원소-추가-1",
    "title": "02. Python Basic (3)",
    "section": "원소 추가",
    "text": "원소 추가\n\ndct\n\n{'a': 1, 'b': 2}\n\n\n\ndct[\"c\"]=3\n\n\ndct\n\n{'a': 1, 'b': 2, 'c': 3}"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#원소-삭제-1",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#원소-삭제-1",
    "title": "02. Python Basic (3)",
    "section": "원소 삭제",
    "text": "원소 삭제\n- 방법 1\n\ndel dct[\"a\"]\n\n\ndct\n\n{'b': 2, 'c': 3}\n\n\n- 방법 2\n\ndct.pop(\"b\")\n\n2\n\n\n\ndct\n\n{'c': 3}"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#연산",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#연산",
    "title": "02. Python Basic (3)",
    "section": "연산",
    "text": "연산\n\ndct = dict(a=1,b=2,c=3)\n\n\ndct\n\n{'a': 1, 'b': 2, 'c': 3}\n\n\n\n\"a\" in dct\n\nTrue"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#특수-기능",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#특수-기능",
    "title": "02. Python Basic (3)",
    "section": "특수 기능",
    "text": "특수 기능\n\ndct\n\n{'a': 1, 'b': 2, 'c': 3}\n\n\n\ndct.keys()\n\ndict_keys(['a', 'b', 'c'])\n\n\n\ndct.values()\n\ndict_values([1, 2, 3])\n\n\n\ndct.items()\n\ndict_items([('a', 1), ('b', 2), ('c', 3)])"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#for-dictstarstar",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#for-dictstarstar",
    "title": "02. Python Basic (3)",
    "section": "for + dict(\\(\\star\\star\\))",
    "text": "for + dict(\\(\\star\\star\\))\n\nfor k in dct.keys() :\n    print(k)\n\na\nb\nc\n\n\n- 아 딕셔너리는 루프에서 객체 자체로 전달시 key값을 반환한다.\n\nfor k in dct:\n    print(k)\n\na\nb\nc\n\n\n- value 접근\n\nfor i in dct.values():\n    print(i)\n\n1\n2\n3\n\n\n- key,value 동시 접근\n\nfor i,j in dct.items():\n    print(i,j)\n\na 1\nb 2\nc 3"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#list-to-dict.values",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#list-to-dict.values",
    "title": "02. Python Basic (3)",
    "section": "list \\(\\to\\) dict.values",
    "text": "list \\(\\to\\) dict.values\n- 아래와 같은 원핫인코딩을 생각\n\n\n\n변환전\n변환후\n\n\n\n\na\n[1,0,0,0]\n\n\nb\n[0,1,0,0]\n\n\nc\n[0,0,1,0]\n\n\nd\n[0,0,0,1]\n\n\n\n- 주어진 리스트를 원핫인코딩하여라\n\nlst = list(\"abcd\")*2\nlst\n\n['a', 'b', 'c', 'd', 'a', 'b', 'c', 'd']\n\n\n\nsol\n\ndct = {\"a\": [1,0,0,0],\n      \"b\": [0,1,0,0],\n      \"c\": [0,0,1,0],\n      \"d\": [0,0,0,1]}\n\n\ndct\n\n{'a': [1, 0, 0, 0], 'b': [0, 1, 0, 0], 'c': [0, 0, 1, 0], 'd': [0, 0, 0, 1]}"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#dict.values-to-list",
    "href": "posts/DX/00. 데이터 다루기/2023-08-11-02. Python Basic (3) .html#dict.values-to-list",
    "title": "02. Python Basic (3)",
    "section": "dict.values \\(\\to\\) list",
    "text": "dict.values \\(\\to\\) list\n- 위 원핫인코딩을 다시 변환\n\nsol\n\nvalue = [v for v in dct.values()] *2\nvalue\n\n[[1, 0, 0, 0],\n [0, 1, 0, 0],\n [0, 0, 1, 0],\n [0, 0, 0, 1],\n [1, 0, 0, 0],\n [0, 1, 0, 0],\n [0, 0, 1, 0],\n [0, 0, 0, 1]]\n\n\n\n[i for v in value for i,j in dct.items() if j == v]\n\n['a', 'b', 'c', 'd', 'a', 'b', 'c', 'd']"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-09-00. Python Basic (1).html",
    "href": "posts/DX/00. 데이터 다루기/2023-08-09-00. Python Basic (1).html",
    "title": "00. Python Basic (1)",
    "section": "",
    "text": "from google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\ncd /content/drive/MyDrive/Colab Notebooks/DX/1wk\n\n/content/drive/MyDrive/Colab Notebooks/DX/1wk"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-09-00. Python Basic (1).html#ex1-홀수-짝수-구분",
    "href": "posts/DX/00. 데이터 다루기/2023-08-09-00. Python Basic (1).html#ex1-홀수-짝수-구분",
    "title": "00. Python Basic (1)",
    "section": "ex1) 홀수 짝수 구분",
    "text": "ex1) 홀수 짝수 구분\n\nlst = list(range(1,10))\nlst\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nsol\n\nodd = [i for i in lst if i % 2 == 1]\nevn = [i for i in lst if i % 2 == 0]\n\n\nodd\n\n[1, 3, 5, 7, 9]\n\n\n\nevn\n\n[2, 4, 6, 8]"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-09-00. Python Basic (1).html#ex2-특정-숫자가-입력되었을-때-각-자리의-숫자를-구하기",
    "href": "posts/DX/00. 데이터 다루기/2023-08-09-00. Python Basic (1).html#ex2-특정-숫자가-입력되었을-때-각-자리의-숫자를-구하기",
    "title": "00. Python Basic (1)",
    "section": "ex2) 특정 숫자가 입력되었을 때 각 자리의 숫자를 구하기",
    "text": "ex2) 특정 숫자가 입력되었을 때 각 자리의 숫자를 구하기\n\nsol\n\nnum  = int(input())\n\na = num // 100\n\nb = (num - a*100) // 10\n\nc = num-(a*100+b*10)\n\nprint(\"백의자리는 {}, 십의자리는 {}, 일의 자리는 {}\".format(a,b,c))\n\n254\n백의자리는 2, 십의자리는 5, 일의 자리는 4"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-09-00. Python Basic (1).html#ex3-합과-평균",
    "href": "posts/DX/00. 데이터 다루기/2023-08-09-00. Python Basic (1).html#ex3-합과-평균",
    "title": "00. Python Basic (1)",
    "section": "ex3) 합과 평균",
    "text": "ex3) 합과 평균\n\nsol\n\nimport numpy as np\n\n\ns = list(range(1,10))\n\n\nsumm = sum(s)\n\nave = np.mean(s)\n\n\nprint(\"합 : {}, 평균: {}\".format(summ,ave))\n\n합 : 45, 평균: 5.0"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-09-00. Python Basic (1).html#ex4-비교-연산자",
    "href": "posts/DX/00. 데이터 다루기/2023-08-09-00. Python Basic (1).html#ex4-비교-연산자",
    "title": "00. Python Basic (1)",
    "section": "ex4) 비교 연산자",
    "text": "ex4) 비교 연산자\n- 숫자 비교\n\na = list(range(1,11))\nb = list(range(11,21))\n\n\na, b\n\n([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n\n\n\n[a[i] == b[i] for i in range(10)]\n\n[False, False, False, False, False, False, False, False, False, False]\n\n\n- 문자 비교\n\n(\"A\" == \"a\"), (\"A\" &gt;\"a\")\n\n(False, False)\n\n\n\nord(\"A\"), ord(\"a\")\n\n(65, 97)"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-09-00. Python Basic (1).html#대표적인-자료형",
    "href": "posts/DX/00. 데이터 다루기/2023-08-09-00. Python Basic (1).html#대표적인-자료형",
    "title": "00. Python Basic (1)",
    "section": "대표적인 자료형",
    "text": "대표적인 자료형\n\n\n\nint\nfloat\nbool\nstr\ncomplex\n\n\n\n\n3\n3.14\nTrue\n“강철”\n3+2j\n\n\n5\n3.141551\nFalse\n“이강철”\n2-2j"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-09-00. Python Basic (1).html#형태변환",
    "href": "posts/DX/00. 데이터 다루기/2023-08-09-00. Python Basic (1).html#형태변환",
    "title": "00. Python Basic (1)",
    "section": "형태변환",
    "text": "형태변환\n\nfloat \\(\\to\\) int\n- 아래의 경우는 형태변환이 되었으나 정보의 손실이 발생한 것임\n\na = 3.4\n\n_a = int(a)\n\n\ntype(a), type(_a)\n\n(float, int)\n\n\n\n\n\\(\\text{bool} \\to \\text{(float, int)}\\,\\text{and}\\, \\text{(float, int)} \\to \\text{bool}\\)\n- bool \\(\\to\\) int, float\n\na = True\n_a1 = float(a)\n_a2 = int(a)\n\n\ntype(a), type(_a1), type(_a2)\n\n(bool, float, int)\n\n\n- int, float \\(\\to\\) bool\n\na1 = 1\na2 = 1.0\n\n_a1 = bool(a1)\n_a2 = bool(a2)\n\n\n_a1, _a2, type(_a1), type(_a2)\n\n(True, True, bool, bool)\n\n\n- str \\(\\to\\) bool\n\nbool(\"강철\")\n\nTrue\n\n\n\nbool(\"\")\n\nFalse\n\n\n\n\n암묵적 형변환\n\nTrue +1\n\n2\n\n\n\n1*1.0\n\n1.0\n\n\n\nTrue + True\n\n2\n\n\n\n\n형태변환이 불가능한 경우\n\ncomplex1 = 3+ 0j\ncomplex1\n\n(3+0j)\n\n\nfloat(complex1) ## 에러 발생\n\n\n문자열의 사칙연산(O)\n\n\"X\" + \"2\"\n\n'X2'\n\n\n\n\"X\"*2\n\n'XX'\n\n\n\n\n문자열의 사칙연산(x)\n\"X\" * \"Y\"\n\n\"X\" - \"2\"\n\n\"X\" / \"y\"\n\n- 즉 더하기를 제외한 나머지 사칙연산은 문자형 변수에 적용되지 않는다."
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-09-00. Python Basic (1).html#날짜형-자료",
    "href": "posts/DX/00. 데이터 다루기/2023-08-09-00. Python Basic (1).html#날짜형-자료",
    "title": "00. Python Basic (1)",
    "section": "날짜형 자료",
    "text": "날짜형 자료\n\nfrom datetime import datetime\n\n\nnow = datetime.now()\n\n\nnow\n\ndatetime.datetime(2023, 8, 9, 5, 45, 24, 616085)\n\n\n\nprint(now)\n\n2023-08-09 05:45:24.616085\n\n\n\nprint(\"{}년 {}월 {}일 {}시 {}분 {}초\".format(now.year,now.month,now.day,now.hour,now.minute,now.second))\n\n2023년 8월 9일 5시 45분 24초"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-09-00. Python Basic (1).html#기타-연산",
    "href": "posts/DX/00. 데이터 다루기/2023-08-09-00. Python Basic (1).html#기타-연산",
    "title": "00. Python Basic (1)",
    "section": "기타 연산",
    "text": "기타 연산\n\na = \"ABCD\"\nb = \"efgh\"\n\n\na.lower(),b.upper()\n\n('abcd', 'EFGH')"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-09-00. Python Basic (1).html#f-string요즘-쓰는-방식",
    "href": "posts/DX/00. 데이터 다루기/2023-08-09-00. Python Basic (1).html#f-string요즘-쓰는-방식",
    "title": "00. Python Basic (1)",
    "section": "f-string(요즘 쓰는 방식)",
    "text": "f-string(요즘 쓰는 방식)\n\nex1\n\nn = \"이강철\"\na = 28\ns = 100.213141\n\n\nprint(f\"{a}살의 {n}의 점수는 {s:.2f}입니다. \")\n\n28살의 이강철의 점수는 100.00입니다. \n\n\n\n\nex2\n\nn = \"이강철\"\na = 28\ns1 = 100.2\ns2 = 100.3\ns3 = 100.4\ns4 = 100.5\ns5 = 100.6\n\n\ntxt= f'''\n이름 : {n}\n\n연령대 : {age}\n\n점수 : {s1},{s2},{s3},{s4},{s5}\n\n합계 : {ts:,.0f} /평균 : {avg:,.2f}\n'''`\n\nprint(txt)\n\n\n이름 : 이강철\n\n연령대 : 20\n\n점수 : 100.2,100.3,100.4,100.5,100.6\n\n합계 : 502 /평균 : 100.40"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n- 전북대학교 통계학과 학사(부전공: 컴퓨터공학) 졸업 | 3.67 / 4.50 | 2015. 03 ~ 2021. 02\n- 전북대학교 통계학과 석사 졸업 | 4.44 / 4.50 | 2021. 03 ~ 2023. 02"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\n- 국민연금공단 빅데이터부 현장실습 | 2020. 03 ~ 2020. 06\n- 지역 문화산업 융복합 데이터 전문가 과정 | 과학기술정보통신부, 한국데이터산업진흥원 | 2021. 06 ~ 2021. 08\n- 빅데이터 혁신공유대학사업 서포터즈 |전북대학교 빅데이터 현신공유대학사업| 2021. 07. 01 ~ 2021. 10. 31\n- KT AIVLE School DX Consultant Track | KT | 2023. 08 ~"
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "About Me",
    "section": "Publications",
    "text": "Publications\n- 데이터 분석을 통한 지역별 고령친화도 시각화\n`-` 김영선, 강민구, 이강철 등  | 문화융복합아카이빙연구소 | 2021. 10 | 기록관리/보존 \n- 핵심어 추출 및 데이터 증강기법을 이용한 텍스트 분류 모델 성능 개선\n`-` 이강철, 안정용 | 한국자료분석학회 | 한국자료분석학회 | 2022. 10 | 통계학"
  },
  {
    "objectID": "about.html#certificate",
    "href": "about.html#certificate",
    "title": "About Me",
    "section": "Certificate",
    "text": "Certificate\n- 워드프로세서 | 대한상공회의소 | 19-19-017981 | 2019. 08. 30\n- 데이터분석준전문가(ADsP) | 한국데이터진흥원 | ADsP-0223898 | 2019. 10. 01\n- 사회조사분석사 2급 | 한국산업인력공단 | 19201142418N | 2019. 10. 01"
  },
  {
    "objectID": "about.html#conctact",
    "href": "about.html#conctact",
    "title": "About Me",
    "section": "Conctact",
    "text": "Conctact\n- rkdcjf8232@gmail.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DX track",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 22, 2024\n\n\n00. 텍스트 분류\n\n\nGC \n\n\n\n\nJan 21, 2024\n\n\n00. Associate Review\n\n\nGC \n\n\n\n\nJan 15, 2024\n\n\n00. CA (1)\n\n\nGC \n\n\n\n\nJan 8, 2024\n\n\n04. MNIST\n\n\ngc \n\n\n\n\nJan 1, 2024\n\n\n03. model theory\n\n\nGC \n\n\n\n\nDec 28, 2023\n\n\n00. model train (1)\n\n\nGC \n\n\n\n\nDec 28, 2023\n\n\n01. model train (2)\n\n\nGC \n\n\n\n\nDec 28, 2023\n\n\n02. model fit & save\n\n\nGC \n\n\n\n\nNov 30, 2023\n\n\n05. 제안서 작성 (1)\n\n\ngc \n\n\n\n\nNov 29, 2023\n\n\n04. 사업정의\n\n\ngc \n\n\n\n\nNov 28, 2023\n\n\n03. 실행계획 수립\n\n\ngc \n\n\n\n\nNov 27, 2023\n\n\n02. 고객가치정의 및 아이디어 도출\n\n\ngc \n\n\n\n\nNov 24, 2023\n\n\n01. 환경분석 기법의 이해\n\n\ngc \n\n\n\n\nNov 23, 2023\n\n\n00. 제안전략수립\n\n\ngc \n\n\n\n\nNov 6, 2023\n\n\n00. IT 인프라 (1)\n\n\nGC \n\n\n\n\nNov 2, 2023\n\n\n03. Titanic\n\n\ngc \n\n\n\n\nNov 1, 2023\n\n\n02. iris\n\n\ngc \n\n\n\n\nOct 31, 2023\n\n\n01. VOC\n\n\ngc \n\n\n\n\nOct 30, 2023\n\n\n00. Churn\n\n\ngc \n\n\n\n\nOct 26, 2023\n\n\n02. BERTopic\n\n\ngc \n\n\n\n\nOct 23, 2023\n\n\n00. streamlit\n\n\ngc \n\n\n\n\nOct 18, 2023\n\n\nsummary 01. RNN\n\n\nGC \n\n\n\n\nOct 18, 2023\n\n\nsummary 02. LSTM\n\n\nGC \n\n\n\n\nOct 18, 2023\n\n\nsummary 03. Transformer\n\n\nGC \n\n\n\n\nOct 12, 2023\n\n\nExtra 02. 시각지능\n\n\nGC \n\n\n\n\nOct 10, 2023\n\n\nExtra 01. 언어지능\n\n\nGC \n\n\n\n\nOct 6, 2023\n\n\n03. 딥러닝 (4)\n\n\nGC \n\n\n\n\nOct 5, 2023\n\n\n02. 딥러닝 (3)\n\n\nGC \n\n\n\n\nOct 4, 2023\n\n\n01. 딥러닝 (2)\n\n\nGC \n\n\n\n\nOct 3, 2023\n\n\n00. 딥러닝 (1)\n\n\nGC \n\n\n\n\nSep 28, 2023\n\n\n08. summary (1)\n\n\nGC \n\n\n\n\nSep 19, 2023\n\n\n07. 머신러닝 (6)\n\n\nGC \n\n\n\n\nSep 18, 2023\n\n\n06. 머신러닝 (5)\n\n\nGC \n\n\n\n\nSep 15, 2023\n\n\n04. 머신러닝 (4)\n\n\nGC \n\n\n\n\nSep 15, 2023\n\n\n05. 종합실습\n\n\nGC \n\n\n\n\nSep 14, 2023\n\n\n03. 머신러닝 (3)\n\n\nGC \n\n\n\n\nSep 13, 2023\n\n\n02. 머신러닝 (2)\n\n\nGC \n\n\n\n\nSep 12, 2023\n\n\n01. 머신러닝 (1)\n\n\nGC \n\n\n\n\nSep 11, 2023\n\n\n00. Intro\n\n\nGC \n\n\n\n\nSep 5, 2023\n\n\n01. 데이터 수집 (2)\n\n\nGC \n\n\n\n\nSep 4, 2023\n\n\n00. 데이터 수집 (1)\n\n\nGC \n\n\n\n\nSep 1, 2023\n\n\n02. 데이터 분석 (3)\n\n\nGC \n\n\n\n\nAug 31, 2023\n\n\n01. 데이터 분석 (2)\n\n\nGC \n\n\n\n\nAug 28, 2023\n\n\n00. 데이터 분석 (1)\n\n\nGC \n\n\n\n\nAug 24, 2023\n\n\n00. MP (1)\n\n\nGC \n\n\n\n\nAug 23, 2023\n\n\n04. numpy & pandas (5)\n\n\nGC \n\n\n\n\nAug 22, 2023\n\n\n03. numpy & pandas (4)\n\n\nGC \n\n\n\n\nAug 21, 2023\n\n\n02. numpy & pandas (3)\n\n\nGC \n\n\n\n\nAug 19, 2023\n\n\n01. Plotly test\n\n\nGC \n\n\n\n\nAug 18, 2023\n\n\n00. numpy & pandas (1)\n\n\nGC \n\n\n\n\nAug 18, 2023\n\n\n01. numpy & pandas (2)\n\n\nGC \n\n\n\n\nAug 17, 2023\n\n\n05. Python Basic (6)\n\n\nGC \n\n\n\n\nAug 16, 2023\n\n\n04. Python Basic (5)\n\n\nGC \n\n\n\n\nAug 15, 2023\n\n\nExtra 04. 클래스 탐구 (3)\n\n\nGC \n\n\n\n\nAug 14, 2023\n\n\n03. Python Basic (4)\n\n\nGC \n\n\n\n\nAug 14, 2023\n\n\nExtra 03. 클래스 탐구 (2)\n\n\nGC \n\n\n\n\nAug 11, 2023\n\n\n02. Python Basic (3)\n\n\nGC \n\n\n\n\nAug 10, 2023\n\n\n01. Python Basic (2)\n\n\nGC \n\n\n\n\nAug 10, 2023\n\n\nExtra 02. 클래스 탐구 (1)\n\n\nGC \n\n\n\n\nAug 9, 2023\n\n\n00. Python Basic (1)\n\n\nGC \n\n\n\n\nAug 9, 2023\n\n\nExtra 01. 클래스\n\n\nGC \n\n\n\n\nAug 8, 2023\n\n\nExtra 00. 밈\n\n\nGC \n\n\n\n\nJul 31, 2023\n\n\n00. Intro & setting\n\n\nGC \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html",
    "href": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html",
    "title": "01. Python Basic (2)",
    "section": "",
    "text": "from google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\ncd /content/drive/MyDrive/Colab Notebooks/DX/1wk\n\n/content/drive/MyDrive/Colab Notebooks/DX/1wk\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#슬라이싱-1",
    "href": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#슬라이싱-1",
    "title": "01. Python Basic (2)",
    "section": "슬라이싱 1",
    "text": "슬라이싱 1\n\nn = len(txt)\ntxt[2:n]\n\n' 이름은 이강철입니다.'\n\n\n- 참고 역슬레시는 인데싱에서 제외된다.\n\nprint(\"he's\")\nprint(\"he\\'s\")\n\nhe's\nhe's"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#슬라이싱-2",
    "href": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#슬라이싱-2",
    "title": "01. Python Basic (2)",
    "section": "슬라이싱 2",
    "text": "슬라이싱 2\n- 구조 : txt[start : end : stride]\n\ntxt\n\n'나의 이름은 이강철입니다.'\n\n\n\nprint(txt[0:n:2])\nprint(txt[::2])\nprint(txt[::3])\nprint(txt[::-1])\nprint(txt[::-2])\n\n나 름 강입다\n나 름 강입다\n나이 철다\n.다니입철강이 은름이 의나\n.니철이은이의"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#count",
    "href": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#count",
    "title": "01. Python Basic (2)",
    "section": "Count",
    "text": "Count\n\ntxt\n\n'나의 이름은 이강철입니다.'\n\n\n- 공백 세기\n\ntxt.count(\" \")\n\n2\n\n\n- 특장 문자 세기\n\ntxt.count(\"이\")\n\n2\n\n\n- 리스트나 튜플로 넘겨주면 에러가 발생\n\ntxt.count((\"이\",\" \"))\n\nTypeError: ignored\n\n\n- 문자가 아닌 문자열 카운트\n\nprint(txt)\nprint(txt.count(\"이강철\"))\n\n나의 이름은 이강철입니다.\n1"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#find",
    "href": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#find",
    "title": "01. Python Basic (2)",
    "section": "find",
    "text": "find\n- 특정 문자의 인덱싱을 반환\n\nprint(txt)\nprint(txt.find(\"이\"))\nprint(txt.find(\"이강철\"))\n\n나의 이름은 이강철입니다.\n3\n7\n\n\n- find함수에 문제점은 범위를 지정해주지 않으면 찾고자 하는 문자의 첫 인덱스만 반환한다.\n- 특정 문자열을 찾을때 문자열 시작지점과 끝지점을 정해서 인덱싱\n\nprint(txt)\nprint(txt.find(\"이\",6,n))\n\n나의 이름은 이강철입니다.\n7"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#잠깐-다른-이야기",
    "href": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#잠깐-다른-이야기",
    "title": "01. Python Basic (2)",
    "section": "잠깐 다른 이야기!",
    "text": "잠깐 다른 이야기!\n- 아래를 잘 살펴보자.\n\na = 3\nb = 3\nprint(id(a),id(b))\na +=1\nprint(id(a))\na *=2\nprint(id(a))\na =a-5\nprint(id(a))\n\n138123241308464 138123241308464\n138123241308496\n138123241308624\n138123241308464\n\n\n- a변수와 b변수에 3이라는 값을 할당하고, 각 메모리 주소를 출력하였다.\n- 질문 1 : 사칙 연산시 메모리 주소가 바뀐다?\n- 질문 2 : 그런데 동일한 값을 할당하면 같은 주소가 할당된다?\n- 질문 3 : 그런데 마지막에 a에서 5를 빼서 처음에 할당한 3과 동일하게 했더니 처음과 동일한 주소가 나왔다?\n- 내 생각 : 각각의 값(할당한 정수)들은 이미 각자의 고유한 주소를 가지고 있고, 변수들은 그 값들을 참조하는 포인터 역할을 하는 것 같다. 또한, 이는 정수 뿐이 아니라 단일 값을 가지는 모든 형태의 해당된다.\n\n문자열 주소확인\n\ns = \"apple\"\nprint(s)\nprint(id(s))\n\nprint(s.upper())\nprint(id(s.upper()))\n\napple\n138121951987056\nAPPLE\n138121951989872\n\n\n\n\n그렇다면 리스트도?\n\na = list(range(5))\nprint(a)\nprint(id(a))\na[1] = 100\nprint(a)\nprint(id(a))\n\n[0, 1, 2, 3, 4]\n138121952117504\n[0, 100, 2, 3, 4]\n138121952117504\n\n\n- 리스트는 바뀌지 않았다!!"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#capitalize-title",
    "href": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#capitalize-title",
    "title": "01. Python Basic (2)",
    "section": "capitalize & title",
    "text": "capitalize & title\n- 첫글자만 대문자로 변환\n\na=\"i like apple\"\n\nprint(a.capitalize())\nprint(a.title())\n\nI like apple\nI Like Apple"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#rjust-ljust-center",
    "href": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#rjust-ljust-center",
    "title": "01. Python Basic (2)",
    "section": "rjust, ljust, center",
    "text": "rjust, ljust, center\n- 각각, 오른쪽, 왼쪽, 중앙정렬을 하고 공백을 만든다\n\na = \"apple\"\nprint(\"[\"+a.rjust(7)+\"]\",sep=\"\")\nprint(\"[\",a.ljust(7),\"]\",sep=\"\")\nprint(\"[\",a.center(7),\"]\",sep=\"\")\n\n[  apple]\n[apple  ]\n[ apple ]\n\n\n- 근데 문장 단위로는 적용되지 않는다….\n\na = \"I like apple\"\nprint(\"[\"+a.rjust(7)+\"]\",sep=\"\")\nprint(\"[\",a.ljust(7),\"]\",sep=\"\")\nprint(\"[\",a.center(7),\"]\",sep=\"\")\n\n[I like apple]\n[I like apple]\n[I like apple]\n\n\n- 핳 근데 문장길이가 7보다 커서 그런거였당\n\na = \"I like apple\"\nprint(\"[\"+a.rjust(19)+\"]\",sep=\"\")\nprint(\"[\",a.ljust(19),\"]\",sep=\"\")\nprint(\"[\",a.center(19),\"]\",sep=\"\")\n\n[       I like apple]\n[I like apple       ]\n[    I like apple   ]\n\n\n- 즉, 전달되는 매개변수는 전체공간을 의미하고, 각각의 메소드는 어디로 정렬할지 정해준다!"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#replace",
    "href": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#replace",
    "title": "01. Python Basic (2)",
    "section": "replace",
    "text": "replace\n- 값을 바꾼다고 객체가 변환되지는 않는다.\n\ntxt = \"사과\"\nprint(txt.replace(\"사과\",\"바나나\"))\nprint(txt)\n\n바나나\n사과\n\n\n- replace 함수를 자주 사용하는 이유!\n- 실제 데이터 전처리 시 아래와 같은 구조가 많다.\nnumber = \"123,456\"\n이러한 상황이 발생했을 때 replace 함수를 사용\n\nnumber =\"123,456\"\n\nint(number.replace(\",\",\"\"))\n\n123456"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#strip",
    "href": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#strip",
    "title": "01. Python Basic (2)",
    "section": "strip",
    "text": "strip\n- 텍스트 문자열에서 양쪽 끝에 특정 문자(공백포함)를 제거\n\ntxt = \"  ###사과%%%%  \"\nprint(txt)\nprint(txt.strip(\" \"))\nprint(txt.strip(\"#%\"))  ## 공백이 포함되어 있어서 지워지지 않는 것 같다.\nprint(txt.strip(\" #%\")) ## 공백을 포함하니 깔끔하게 지워졌다.\n\n  ###사과%%%%  \n###사과%%%%\n  ###사과%%%%  \n사과"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#split",
    "href": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#split",
    "title": "01. Python Basic (2)",
    "section": "split",
    "text": "split\n\n\nCode\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/covid19_20211202.csv\").iloc[1:,:]\ndf = df.set_index(\"일자\").iloc[:,1:18]\n\ndf = df.reset_index()\ndt = df[\"일자\"].tolist()\n\n\n\ndt[:5]\n\n['2020-01-20', '2020-01-21', '2020-01-22', '2020-01-23', '2020-01-24']\n\n\n\nyear = [i.split(\"-\")[0] for i in dt]\nmonth = [i.split(\"-\")[1] for i in dt]\nday = [i.split(\"-\")[2] for i in dt]\n\n\nnew_df = pd.DataFrame({\"year\": year,\n                       \"month\": month,\n                       \"day\" : day})\nnew_df.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nyear\nmonth\nday\n\n\n\n\n0\n2020\n01\n20\n\n\n1\n2020\n01\n21\n\n\n2\n2020\n01\n22\n\n\n3\n2020\n01\n23\n\n\n4\n2020\n01\n24"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#join",
    "href": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#join",
    "title": "01. Python Basic (2)",
    "section": "join",
    "text": "join\n\ndate = [(\"-\").join([year[i],month[i],day[i]]) for i in range(len(dt))]\n\n\ndate[:5]\n\n['2020-01-20', '2020-01-21', '2020-01-22', '2020-01-23', '2020-01-24']"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#리스트-basic",
    "href": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#리스트-basic",
    "title": "01. Python Basic (2)",
    "section": "리스트 Basic",
    "text": "리스트 Basic\n\n[], list(), list(range(start,end)) 방법으로 리스트를 선언할 수 있다.\n\n\n리스트 선언 & 기본 method\n\na = [1,2,3,4,5]\nb = list((1,2,3,4,5))\nc = list(range(1,6))\n\n\nprint(f'''\n  a= {a}\n  b= {b}\n  c= {c}\n''')\n\n\n  a= [1, 2, 3, 4, 5]\n  b= [1, 2, 3, 4, 5]\n  c= [1, 2, 3, 4, 5]    \n\n\n\n- 선언한 리스트의 합을 구하기\n\nsum(a)\n\n15\n\n\n- 최대값과 최소값\n\nmin(a), max(a)\n\n(1, 5)\n\n\n- 특정 요소 카운트\n\nlst =  np.concatenate([np.ones(2), np.zeros(2)]).tolist()\nlst\n\n[1.0, 1.0, 0.0, 0.0]\n\n\n\nlst.count(0),lst.count(1)\n\n(2, 2)\n\n\n- 인덱스 반환\n\n리스트는 인덱스 반환 시, 맨 처음 인덱스만 반환한다 \\(\\to\\) 문자열.find 함수와 동일!\n\n\nlst.index(0)\n\n2\n\n\n\n그럼 다른 범위에 있는 “원소” 찾을때도 방법이 같지 않을까…???\n\n\nlst.index(0,3,5)\n\n3\n\n\n- 또한, 리스트는 자료형이 같지 않아도 다양한 자료형을 가질 수 있다.\n\n_lst = [True, 3.14, 1, \"에이블\"]\n\n\n[type(i) for i in _lst]\n\n[bool, float, int, str]\n\n\n\n\n리스트 중첩\n- 리스트안에 리스트를 집어넣을 수 있다.\n\nimport numpy as np\n\n\nX = np.arange(1,16).reshape(5,-1)\nprint(\n    f'''\n    X.shape = {X.shape}\n    listX = {X.tolist()}\n   '''\n)\n\n\n    X.shape = (5, 3)\n    listX = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15]]\n   \n\n\n\n_X = X.tolist()\n\n\nlen(_X) ## 길이가 5인리스트로 인식됌\n\n5\n\n\n\n\n중첩된 리스트 flatten\n1 리스트 컴프리헨션\n\n[ j for i in _X for j in i ]\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n\n\n2 numpy 이용\n\nnp.array(_X).flatten().tolist()\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#리스트-연산",
    "href": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#리스트-연산",
    "title": "01. Python Basic (2)",
    "section": "리스트 연산",
    "text": "리스트 연산\n- 리스트의 연산은 더하기와 곱하기만 지원한다.\n\n+ : 2개의 리스트를 이어붙임, R에서 처럼 브로드캐스팅이 수행되지 않음\n\n\na = [1,2]\nb = [3,4]\n\n\na+b\n\n[1, 2, 3, 4]\n\n\n\n\\(\\times\\) 는 특정 리스트를 얼마나 반복할지 결정해줌\n\n\na*2\n\n[1, 2, 1, 2]\n\n\n\na*0\n\n[]\n\n\n\na*(-2)\n\n[]"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#리스트-원소-추가",
    "href": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#리스트-원소-추가",
    "title": "01. Python Basic (2)",
    "section": "리스트 원소 추가",
    "text": "리스트 원소 추가\n\nappend\n\na=[]\n\nfor i in range(3):\n    a += [i]\n\na\n\n[0, 1, 2]\n\n\n\nappend 주의!\n- 아래와 같은 연산은 수행되지 않는다.\n\na.append(0).append(1).append(2)\n\nAttributeError: ignored\n\n\n- 또한, 매개변수로 리스트를 전달 시 아래처럼 수행된다.\n\na = [1,2,3]\nb = [4,5]\na.append(b)\na\n\n[1, 2, 3, [4, 5]]\n\n\n\n\nappend를 쓰지않고…\n\na=[]\n\nfor i in range(3):\n    a += [i]\na\n\n[0, 1, 2]\n\n\n\n\n+ 와 .append의 차이\n- append 함수의 경우 연산 수행 후 연산 대상 객체가 변화한다.\n\na = []\na.append(1)\na\n\n[1]\n\n\n- +는 그렇지 않음\n\na = []\na + [1]\na\n\n[]\n\n\n\n\n\nextend\n- 두개의 리스트를 더할 때 extend함수를 사용한다. (append는 리스트 오브 리스트로 붙여준다는점에서 차이가 명확함)\n- append처럼 +와의 차이가 같음.\n\na = [1,2]\nb = [3,4]\na.extend(b)\nprint(f'''\n      a.extend(b) = {a}\n      a = {a}\n      ''')\n\n\n      a.extend(b) = [1, 2, 3, 4]\n      a = [1, 2, 3, 4]\n      \n\n\n\na = [1,2]\nb = [3,4]\na+b\nprint(f'''\n      a + b = {a}\n      a = {a}\n      ''')\n\n\n      a + b = [1, 2]\n      a = [1, 2]\n      \n\n\n\n\ninsert\n- 원하는 인덱스의 요소를 추가한다.\n\na = list(np.round(np.random.normal(size=10),2))\na\n\n[0.51, -0.52, -0.57, -1.48, 0.57, -1.32, 1.71, 0.03, 0.18, 0.97]\n\n\n- 0과 2인덱스의 해당 값을 추가\n\na.insert(0,100)\na.insert(2,77)\n\n\nfor i in range(len(a)) :\n    print(f\"index : {i},  value : {a[i]}\")\n\nindex : 0,  value : 100\nindex : 1,  value : 0.51\nindex : 2,  value : 77\nindex : 3,  value : -0.52\nindex : 4,  value : -0.57\nindex : 5,  value : -1.48\nindex : 6,  value : 0.57\nindex : 7,  value : -1.32\nindex : 8,  value : 1.71\nindex : 9,  value : 0.03\nindex : 10,  value : 0.18\nindex : 11,  value : 0.97"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#리스트-원소-삭제",
    "href": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#리스트-원소-삭제",
    "title": "01. Python Basic (2)",
    "section": "리스트 원소 삭제",
    "text": "리스트 원소 삭제\n\na = list(range(5))\na\n\n[0, 1, 2, 3, 4]\n\n\n\n단일 원소삭제\n\na = list(range(5))\na\n\ndel a[0]\n\n\na\n\n[1, 2, 3, 4]\n\n\n\n\n범위 삭제\n\na = list(range(5))\na\n\ndel a[0:2]\na\n\n[2, 3, 4]\n\n\n\n\nremove\n- 특정 원소를 삭제\n\na = list(range(5))\na\n\n[0, 1, 2, 3, 4]\n\n\n\na.remove(3)\n\n\na\n\n[0, 1, 2, 4]\n\n\n\n\npop\n- 특정 index값을 받아 해당 인덱스의 값을 출력하고, 리스트에서 제거된다. \\(\\to\\) 나머지 원소들은 자동으로 인덱스가 앞으로 땅겨짐\n\na = [1,2,3,4]\n\n\na.pop(0)\n\n1\n\n\n\na.pop(0)\n\n2\n\n\n\n\nclear\n- 전체 원소 삭제\n\na = [1,2,3,4]\na\n\n[1, 2, 3, 4]\n\n\n\na.clear()\na\n\n[]"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#리스트-정렬",
    "href": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#리스트-정렬",
    "title": "01. Python Basic (2)",
    "section": "리스트 정렬",
    "text": "리스트 정렬\n\na = [1,3,2,4]\na\n\n[1, 3, 2, 4]\n\n\n\na.sort()\na\n\n[1, 2, 3, 4]\n\n\n\na.sort(reverse= True)\na\n\n[4, 3, 2, 1]\n\n\n\na.reverse()\na\n\n[1, 2, 3, 4]\n\n\n리스트 카피\n- 카피함수를 사용하는 이유는 아래와 같은 경우를 방지하기 위함이다.\n\na = list(range(4))\nb = a\n\n\nid(a) == id(b)\n\nTrue\n\n\n- 위와 같이 같은 메모리 공간을 참조하고 있으면….\n\na.append(5)\n\n\na\n\n[0, 1, 2, 3, 5]\n\n\n\nb\n\n[0, 1, 2, 3, 5]\n\n\n- 이러한 경우를 방지하기 위해 copy함수를 사용\n\na = list(range(4))\nb = a.copy()\n\nid(a) == id(b)\n\nFalse\n\n\n\na.append(5)\n\n\nprint(a,b)\n\n[0, 1, 2, 3, 5] [0, 1, 2, 3]"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#exercise-link",
    "href": "posts/DX/00. 데이터 다루기/2023-08-10-01. Python Basic (2) .html#exercise-link",
    "title": "01. Python Basic (2)",
    "section": "exercise Link",
    "text": "exercise Link\n- exercise 1\n- exercise 2"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html",
    "href": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html",
    "title": "03. Python Basic (4)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex1.-특정-수보다-크거나-같으면-10-더하기",
    "href": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex1.-특정-수보다-크거나-같으면-10-더하기",
    "title": "03. Python Basic (4)",
    "section": "ex1. 특정 수보다 크거나 같으면 10 더하기",
    "text": "ex1. 특정 수보다 크거나 같으면 10 더하기\n\nv = int(input(\"기준값을 입력하시오 : \"))\ns = int(input(\"비교값을 입력하시오 : \"))\nif s&gt;= v:\n    s1 = s+10\n    print(f\"입력값 {v}보다 크거나 같은 숫자가 입력되었습니다. {s}에 10을 더한값은 {s1}입니다.\")\nelse:\n    print(f\"입력값 {v}보다 작은 숫자입니다.\")\n\n기준값을 입력하시오 :  70\n비교값을 입력하시오 :  60\n\n\n입력값 70보다 작은 숫자입니다.\n\n\n\nv = int(input(\"기준값을 입력하시오 : \"))\ns = int(input(\"비교값을 입력하시오 : \"))\nif s&gt;= v:\n    s1 = s+10\n    print(f\"입력값 {v}보다 크거나 같은 숫자가 입력되었습니다. {s}에 10을 더한값은 {s1}입니다.\")\nelse:\n    print(f\"입력값 {v}보다 작은 숫자입니다.\")\n\n기준값을 입력하시오 :  70\n비교값을 입력하시오 :  70\n\n\n입력값 70보다 크거나 같은 숫자가 입력되었습니다. 70에 10을 더한값은 80입니다."
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex2.-리스트-요소-확인",
    "href": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex2.-리스트-요소-확인",
    "title": "03. Python Basic (4)",
    "section": "ex2. 리스트 요소 확인",
    "text": "ex2. 리스트 요소 확인\n\ns = [1,2,3,4,5]\nif s :\n    print(s)\nelse :\n    print(\"요소 없음\")\n\n[1, 2, 3, 4, 5]\n\n\n\ns = []\nif s :\n    print(s)\nelse :\n    print(\"요소 없음\")\n\n요소 없음\n\n\n\ns = [1,2,3,4,5]\nif s :\n    pass ## 나중에 먼가 내가 조건문일 때 정의를 남기려고 `pass`!\nelse :\n    print(\"요소 없음\")"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex1.-학점",
    "href": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex1.-학점",
    "title": "03. Python Basic (4)",
    "section": "ex1. 학점",
    "text": "ex1. 학점\n\nimport numpy as np\n\n\ns = list(range(0,100,10))\n\n\ndef grade(s) :\n    if s &gt;= 90:\n        return \"A\"\n    elif s&gt;=80:\n        return \"B\"\n    elif s&gt;=70:\n        return \"C\"\n    elif s&gt;=60:\n        return \"D\"\n    else :\n        return \"F\"        \n\n\ns\n\n[0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\n\n\n\n[grade(i) for i in s]\n\n['F', 'F', 'F', 'F', 'F', 'F', 'D', 'C', 'B', 'A']"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex2.-아래-인코딩을-수행",
    "href": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex2.-아래-인코딩을-수행",
    "title": "03. Python Basic (4)",
    "section": "ex2. 아래 인코딩을 수행",
    "text": "ex2. 아래 인코딩을 수행\n\n\n\n숫자\n요일\n\n\n\n\n0\n월요일\n\n\n1\n화요일\n\n\n2\n수요일\n\n\n3\n목요일\n\n\n4\n금요일\n\n\n5\n토요일\n\n\n6\n일요일\n\n\n\n\nnum = list(np.random.randint(0,7,20))\n\n\ndef days(n) :\n    if n == 0 :\n        return \"월\"\n    elif n == 1 :\n        return \"화\"\n    elif n == 2 :\n        return \"수\"\n    elif n == 3 :\n        return \"목\"\n    elif n == 4 :\n        return \"금\"\n    elif n == 5 :\n        return \"토\"\n    else :\n        return \"일\"\n\n\nweek = [days(n) for n in num]\n\n\nprint(week)\n\n['월', '일', '토', '수', '월', '금', '일', '목', '월', '월', '토', '금', '일', '수', '화', '월', '수', '목', '일', '일']"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex3.-60점-이하-과락",
    "href": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex3.-60점-이하-과락",
    "title": "03. Python Basic (4)",
    "section": "ex3. 60점 이하 과락",
    "text": "ex3. 60점 이하 과락\n\ns = [62,59,70]\n\n\nfor i in s :\n    if i &lt;60 :\n        print (i,\"과락\")\n    else :\n        print(i)\n\n62\n59 과락\n70"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex1-for-if-to-짝수만-출력",
    "href": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex1-for-if-to-짝수만-출력",
    "title": "03. Python Basic (4)",
    "section": "ex1) for + if \\(\\to\\) 짝수만 출력",
    "text": "ex1) for + if \\(\\to\\) 짝수만 출력\n\ns = list(np.random.randint(1,100,size=10))\ns\n\n[37, 10, 35, 31, 70, 28, 40, 74, 69, 55]\n\n\n\ns2 = [i for i in s if i%2==0]\n\n\nprint(f\"짝수 list는 {s2}, 합은 {sum(s2)}\")\n\n짝수 list는 [10, 70, 28, 40, 74], 합은 222"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex2-리스트안에-원소-출력",
    "href": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex2-리스트안에-원소-출력",
    "title": "03. Python Basic (4)",
    "section": "ex2) 리스트안에 원소 출력",
    "text": "ex2) 리스트안에 원소 출력\n\nlst = [1,2, \"강철\",True, 1+2j]\n\n\n[i for i in lst]\n\n[1, 2, '강철', True, (1+2j)]"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex3-문자열-리스트-출력",
    "href": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex3-문자열-리스트-출력",
    "title": "03. Python Basic (4)",
    "section": "ex3) 문자열 리스트 출력",
    "text": "ex3) 문자열 리스트 출력\n\nl = list(\"abc\")\n\nfor i in l:\n    print(i)\n\na\nb\nc"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex4-구구단-구현",
    "href": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex4-구구단-구현",
    "title": "03. Python Basic (4)",
    "section": "ex4) 구구단 구현",
    "text": "ex4) 구구단 구현\n\nlst = list(range(2,10))\n\n\nfor i in lst :\n    for j in range(1,10) : \n        print (f\"{i} x {j} = {i*j}\")\n\n2 x 1 = 2\n2 x 2 = 4\n2 x 3 = 6\n2 x 4 = 8\n2 x 5 = 10\n2 x 6 = 12\n2 x 7 = 14\n2 x 8 = 16\n2 x 9 = 18\n3 x 1 = 3\n3 x 2 = 6\n3 x 3 = 9\n3 x 4 = 12\n3 x 5 = 15\n3 x 6 = 18\n3 x 7 = 21\n3 x 8 = 24\n3 x 9 = 27\n4 x 1 = 4\n4 x 2 = 8\n4 x 3 = 12\n4 x 4 = 16\n4 x 5 = 20\n4 x 6 = 24\n4 x 7 = 28\n4 x 8 = 32\n4 x 9 = 36\n5 x 1 = 5\n5 x 2 = 10\n5 x 3 = 15\n5 x 4 = 20\n5 x 5 = 25\n5 x 6 = 30\n5 x 7 = 35\n5 x 8 = 40\n5 x 9 = 45\n6 x 1 = 6\n6 x 2 = 12\n6 x 3 = 18\n6 x 4 = 24\n6 x 5 = 30\n6 x 6 = 36\n6 x 7 = 42\n6 x 8 = 48\n6 x 9 = 54\n7 x 1 = 7\n7 x 2 = 14\n7 x 3 = 21\n7 x 4 = 28\n7 x 5 = 35\n7 x 6 = 42\n7 x 7 = 49\n7 x 8 = 56\n7 x 9 = 63\n8 x 1 = 8\n8 x 2 = 16\n8 x 3 = 24\n8 x 4 = 32\n8 x 5 = 40\n8 x 6 = 48\n8 x 7 = 56\n8 x 8 = 64\n8 x 9 = 72\n9 x 1 = 9\n9 x 2 = 18\n9 x 3 = 27\n9 x 4 = 36\n9 x 5 = 45\n9 x 6 = 54\n9 x 7 = 63\n9 x 8 = 72\n9 x 9 = 81"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex5-enumerate-to-짝수번째-인덱스만-출력",
    "href": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex5-enumerate-to-짝수번째-인덱스만-출력",
    "title": "03. Python Basic (4)",
    "section": "ex5) enumerate \\(\\to\\) 짝수번째 인덱스만 출력",
    "text": "ex5) enumerate \\(\\to\\) 짝수번째 인덱스만 출력\n- 컨테이너 자료형(문자열(str), 튜플(tuple), 리스트(list), 딕셔터리(dictionary), 집합(set))을 입력받아, 순번과 요소를 포함하는 오브젝트를 출력\n\nlst = list(\"abcdefghi\")\n\n\nlist(enumerate(lst))\n\n[(0, 'a'),\n (1, 'b'),\n (2, 'c'),\n (3, 'd'),\n (4, 'e'),\n (5, 'f'),\n (6, 'g'),\n (7, 'h'),\n (8, 'i')]\n\n\n\nfor i,j in enumerate(lst) :\n    if i%2==0 :\n        print(f\"index : {i}, str = {j}\")\n\nindex : 0, str = a\nindex : 2, str = c\nindex : 4, str = e\nindex : 6, str = g\nindex : 8, str = i"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex6-여러개의-값을-입력받아-짝수들의-제곱을-구하기",
    "href": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex6-여러개의-값을-입력받아-짝수들의-제곱을-구하기",
    "title": "03. Python Basic (4)",
    "section": "ex6) 여러개의 값을 입력받아 짝수들의 제곱을 구하기",
    "text": "ex6) 여러개의 값을 입력받아 짝수들의 제곱을 구하기\n\nnum_list = list(map(int, input(\"숫자를 입력하세요 : \").split()))\n\n\ns_list = [i**2 for i in num_list if i % 2 == 0]\n\ns_list\n\n숫자를 입력하세요 :  1 2 3 4 5 6\n\n\n[4, 16, 36]"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex1-기본-반복문",
    "href": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex1-기본-반복문",
    "title": "03. Python Basic (4)",
    "section": "ex1) 기본 반복문",
    "text": "ex1) 기본 반복문\n\ns = dict(a=100,b=55,c=74,d=87)\ns.items()\n\ndict_items([('a', 100), ('b', 55), ('c', 74), ('d', 87)])\n\n\n\nfor i,j in s.items() :\n    print(i,j)\n\na 100\nb 55\nc 74\nd 87"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex2-기준값을-입력받은후-특정값보다-큰-keyvalue를-추출",
    "href": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex2-기준값을-입력받은후-특정값보다-큰-keyvalue를-추출",
    "title": "03. Python Basic (4)",
    "section": "ex2) 기준값을 입력받은후 특정값보다 큰 key,value를 추출",
    "text": "ex2) 기준값을 입력받은후 특정값보다 큰 key,value를 추출\n\nnum = int(input(\"숫자를 입력하시오 : \"))\n\nfor i,j in s.items() :\n    if j&gt;num :\n        print (i,j)\n\n숫자를 입력하시오 :  60\n\n\na 100\nc 74\nd 87"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex3-합구하기",
    "href": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex3-합구하기",
    "title": "03. Python Basic (4)",
    "section": "ex3) 합구하기",
    "text": "ex3) 합구하기\n\ns = {\"a\" : [1,2,3,4],\n     \"b\" : [5,6,7,8],\n     \"c\" : [6,7,8,9]}\n\n\n{i : sum(j) for i,j in s.items()}\n\n{'a': 10, 'b': 26, 'c': 30}"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex2.-break",
    "href": "posts/DX/00. 데이터 다루기/2023-08-14-03. Python Basic (4).html#ex2.-break",
    "title": "03. Python Basic (4)",
    "section": "ex2. break",
    "text": "ex2. break\n- 짝수합 구하기\n\ns,i =0,0\n\nwhile True :\n    i+=1\n    if i&gt;100 :\n        break ## 100d을 넘어가면 break\n    if i%2==0 :\n        s += i\n    else :\n        continue ## 다시 처음으로"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html",
    "title": "05. Python Basic (6)",
    "section": "",
    "text": "- 홈 디렉토리를 불러오는 명령어\n\nfrom pathlib import Path\nprint(Path.home())\n\nC:\\Users\\user\n\n\n- 현재 작업 디렉토리 확인\n\nprint(Path.cwd())\n\nC:\\projects\\mysite2\\posts\\DX\\0. 데이터 다루기\n\n\n\nPath.cwd().glob() \\(\\to\\) 해당 경로안에 있는 파일들의 목록을 확인\n\n\nfrom pathlib import Path\n\nfiles = Path.cwd().glob('*')\n\nfor f in files:\n     print(f)\n\nC:\\projects\\mysite2\\posts\\DX\\0. 데이터 다루기\\.ipynb_checkpoints\nC:\\projects\\mysite2\\posts\\DX\\0. 데이터 다루기\\2023-08-09-00. Python Basic (1).ipynb\nC:\\projects\\mysite2\\posts\\DX\\0. 데이터 다루기\\2023-08-10-01. Python Basic (2) .ipynb\nC:\\projects\\mysite2\\posts\\DX\\0. 데이터 다루기\\2023-08-11-02. Python Basic (3) .ipynb\nC:\\projects\\mysite2\\posts\\DX\\0. 데이터 다루기\\2023-08-14-03. Python Basic (4).ipynb\nC:\\projects\\mysite2\\posts\\DX\\0. 데이터 다루기\\2023-08-16-04. Python Basic (5).ipynb\nC:\\projects\\mysite2\\posts\\DX\\0. 데이터 다루기\\2023-08-17-05. Python Basic (6).ipynb\nC:\\projects\\mysite2\\posts\\DX\\0. 데이터 다루기\\extra\nC:\\projects\\mysite2\\posts\\DX\\0. 데이터 다루기\\repr.png\n\n\n\n\n\n\n\nf = open(\"gc.txt\", \"w\")\n\nf.write(\"gc test\\n\")\n\nf.close()\n\n- 파일 생성 확인\n\n- 디렉토리 만들기\n\nexist_ok = True : 아래와 같이 Files이라는 폴더가 존재하면 있는 폴더를 쓰고 없을 경우 만들어서 씀\n\n\nPath(\"Files\").mkdir(exist_ok = True)\n\n- 에러문 확인!\n\nPath(\"Files\").mkdir(exist_ok = False)\n\nFileExistsError: [WinError 183] 파일이 이미 있으므로 만들 수 없습니다: 'Files'\n\n\n- 생성한 디렉토리에 파일을 쓰기\n\nf = open(\"Files/gc.txt\", \"w\")\n\nf.write(\"gc test\\n\")\n\nf.close()\n\n\n\n\n\nf = open(\"Files/gc.txt\",\"r\")\n\nprint(f.read())\n\nf.close()\n\ngc test\n\n\n\n\n\n\n- 지정한 파일이 없으면 새로운 파일을 생성한다.\n\nf = open(\"Files/gc.txt\",\"a\")\n\nf.write(\"test를 확인 중입니다.\")\n\nf.close()\n\n\nf = open('Files/gc.txt', 'r')\nprint(f.read())\nf.close()\n\ngc test\ntest를 확인 중입니다.\n\n\n\n\n\n- [Errno 17] File exists: ‘Files/gc.txt’\n\nf = open('Files/gc.txt', 'x')\n\nFileExistsError: [Errno 17] File exists: 'Files/gc.txt'\n\n\n\n\n\ntry : \n    f = open('Files/gc.txt', 'x')\nexcept FileExistsError :\n    print(\"이미 파일이 존재합니다.\")\nelse : \n    print (\"파일 쓰기 성공!\")\n\n이미 파일이 존재합니다.\n\n\n\n\n\n\n\n\n\nf = open(\"test.txt\",\"w\")\nf.close()\n\n\n\n\n\n\nf = open(\"test.txt\",\"a\")\n\nf.write(\"이름 : 이강철\\n 나이 : 28세\")\n\nf.close()\n\n\n\n\n\nf = open(\"test.txt\",\"r\")\n\nprint(f.read())\n\n이름 : 이강철\n 나이 : 28세\n\n\n\n\n\n\n\n\nl = [\"이강철\\n\",\"28세\\n\", \"통계학 전공\\n\"]\n\nf = open(\"test2.txt\",\"w\")\n\nf.writelines(l)\nf.close()\n\n\nf = open(\"test2.txt\",\"r\")\nprint(f.read())\n\n이강철\n28세\n통계학 전공\n\n\n\n\n\n\n\nf = open(\"test2.txt\",\"r\")\nresult = f.readlines() \nprint(result)\n\n['이강철\\n', '28세\\n', '통계학 전공\\n']\n\n\n\nfor txt in result:\n    print(txt,end=\"\")\n\n이강철\n28세\n통계학 전공\n\n\n\n\n\n\nf = open(\"test2.txt\",\"r\")\n\nresult = f.readline()\nwhile result : \n    print(result,end=\"\")\n    result = f.readline()\nf.close()\n\n이강철\n28세\n통계학 전공"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#경로-설정",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#경로-설정",
    "title": "05. Python Basic (6)",
    "section": "",
    "text": "- 홈 디렉토리를 불러오는 명령어\n\nfrom pathlib import Path\nprint(Path.home())\n\nC:\\Users\\user\n\n\n- 현재 작업 디렉토리 확인\n\nprint(Path.cwd())\n\nC:\\projects\\mysite2\\posts\\DX\\0. 데이터 다루기\n\n\n\nPath.cwd().glob() \\(\\to\\) 해당 경로안에 있는 파일들의 목록을 확인\n\n\nfrom pathlib import Path\n\nfiles = Path.cwd().glob('*')\n\nfor f in files:\n     print(f)\n\nC:\\projects\\mysite2\\posts\\DX\\0. 데이터 다루기\\.ipynb_checkpoints\nC:\\projects\\mysite2\\posts\\DX\\0. 데이터 다루기\\2023-08-09-00. Python Basic (1).ipynb\nC:\\projects\\mysite2\\posts\\DX\\0. 데이터 다루기\\2023-08-10-01. Python Basic (2) .ipynb\nC:\\projects\\mysite2\\posts\\DX\\0. 데이터 다루기\\2023-08-11-02. Python Basic (3) .ipynb\nC:\\projects\\mysite2\\posts\\DX\\0. 데이터 다루기\\2023-08-14-03. Python Basic (4).ipynb\nC:\\projects\\mysite2\\posts\\DX\\0. 데이터 다루기\\2023-08-16-04. Python Basic (5).ipynb\nC:\\projects\\mysite2\\posts\\DX\\0. 데이터 다루기\\2023-08-17-05. Python Basic (6).ipynb\nC:\\projects\\mysite2\\posts\\DX\\0. 데이터 다루기\\extra\nC:\\projects\\mysite2\\posts\\DX\\0. 데이터 다루기\\repr.png"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#파일-쓰기w-write",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#파일-쓰기w-write",
    "title": "05. Python Basic (6)",
    "section": "",
    "text": "f = open(\"gc.txt\", \"w\")\n\nf.write(\"gc test\\n\")\n\nf.close()\n\n- 파일 생성 확인\n\n- 디렉토리 만들기\n\nexist_ok = True : 아래와 같이 Files이라는 폴더가 존재하면 있는 폴더를 쓰고 없을 경우 만들어서 씀\n\n\nPath(\"Files\").mkdir(exist_ok = True)\n\n- 에러문 확인!\n\nPath(\"Files\").mkdir(exist_ok = False)\n\nFileExistsError: [WinError 183] 파일이 이미 있으므로 만들 수 없습니다: 'Files'\n\n\n- 생성한 디렉토리에 파일을 쓰기\n\nf = open(\"Files/gc.txt\", \"w\")\n\nf.write(\"gc test\\n\")\n\nf.close()"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#파일-읽기-r-read",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#파일-읽기-r-read",
    "title": "05. Python Basic (6)",
    "section": "",
    "text": "f = open(\"Files/gc.txt\",\"r\")\n\nprint(f.read())\n\nf.close()\n\ngc test"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#파일-추가-a-append",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#파일-추가-a-append",
    "title": "05. Python Basic (6)",
    "section": "",
    "text": "- 지정한 파일이 없으면 새로운 파일을 생성한다.\n\nf = open(\"Files/gc.txt\",\"a\")\n\nf.write(\"test를 확인 중입니다.\")\n\nf.close()\n\n\nf = open('Files/gc.txt', 'r')\nprint(f.read())\nf.close()\n\ngc test\ntest를 확인 중입니다."
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#파일-존재-유무-확인-x",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#파일-존재-유무-확인-x",
    "title": "05. Python Basic (6)",
    "section": "",
    "text": "- [Errno 17] File exists: ‘Files/gc.txt’\n\nf = open('Files/gc.txt', 'x')\n\nFileExistsError: [Errno 17] File exists: 'Files/gc.txt'\n\n\n\n\n\ntry : \n    f = open('Files/gc.txt', 'x')\nexcept FileExistsError :\n    print(\"이미 파일이 존재합니다.\")\nelse : \n    print (\"파일 쓰기 성공!\")\n\n이미 파일이 존재합니다."
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#excercise",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#excercise",
    "title": "05. Python Basic (6)",
    "section": "",
    "text": "f = open(\"test.txt\",\"w\")\nf.close()\n\n\n\n\n\n\nf = open(\"test.txt\",\"a\")\n\nf.write(\"이름 : 이강철\\n 나이 : 28세\")\n\nf.close()\n\n\n\n\n\nf = open(\"test.txt\",\"r\")\n\nprint(f.read())\n\n이름 : 이강철\n 나이 : 28세"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#파일-쓰기-writelines",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#파일-쓰기-writelines",
    "title": "05. Python Basic (6)",
    "section": "",
    "text": "l = [\"이강철\\n\",\"28세\\n\", \"통계학 전공\\n\"]\n\nf = open(\"test2.txt\",\"w\")\n\nf.writelines(l)\nf.close()\n\n\nf = open(\"test2.txt\",\"r\")\nprint(f.read())\n\n이강철\n28세\n통계학 전공"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#파일-읽기readlines",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#파일-읽기readlines",
    "title": "05. Python Basic (6)",
    "section": "",
    "text": "f = open(\"test2.txt\",\"r\")\nresult = f.readlines() \nprint(result)\n\n['이강철\\n', '28세\\n', '통계학 전공\\n']\n\n\n\nfor txt in result:\n    print(txt,end=\"\")\n\n이강철\n28세\n통계학 전공"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#파일-읽기-readline-한-행씩",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#파일-읽기-readline-한-행씩",
    "title": "05. Python Basic (6)",
    "section": "",
    "text": "f = open(\"test2.txt\",\"r\")\n\nresult = f.readline()\nwhile result : \n    print(result,end=\"\")\n    result = f.readline()\nf.close()\n\n이강철\n28세\n통계학 전공"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#패키지-설치",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#패키지-설치",
    "title": "05. Python Basic (6)",
    "section": "패키지 설치",
    "text": "패키지 설치\n\n#!pip install wordcloud\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#워드클라우드-생성",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#워드클라우드-생성",
    "title": "05. Python Basic (6)",
    "section": "워드클라우드 생성",
    "text": "워드클라우드 생성\n\n%config InlineBackend.figure_format='retina' ## 이미지 포맷\n\n# 워드 클라우드 만들기\nwordcloud = WordCloud(font_path = 'C:/Windiws/fonts/HMKMRHD.TTF',  ## 글씨 폰트\n                      width=2000,\n                      height=1000,\n                      background_color='white').generate_from_frequencies(w_c) ##우리가 만든 워드 카운트를전달\n\n# 표시하기\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#extra-이미지-파일",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#extra-이미지-파일",
    "title": "05. Python Basic (6)",
    "section": "extra : 이미지 파일",
    "text": "extra : 이미지 파일\n- 이미지 파일을 사용해 워드클라우드의 프레임 변셩\n\nimport\n\n#pip install opencv-python\n\n\nimport numpy as np\nfrom PIL import Image ## 이미지를 불러오기 위한 모듈\nimport cv2 as cv\n\n\n\n해당 이미지 확인\n\nh = cv.imread(\"human.jpg\")\nplt.imshow(h)\n\n&lt;matplotlib.image.AxesImage at 0x1701cf10bd0&gt;\n\n\n\n\n\n\n\n워드클라우드와 함께 그리기\n\nm_image = np.array(Image.open(\"human.jpg\"))\n\nwordcloud = WordCloud(font_path = 'C:/Windiws/fonts/HMKMRHD.TTF',\n                      width=2000, \n                      height=1000, \n                      mask=m_image,\n                      background_color='white').generate_from_frequencies(w_c)\n\n\nfig, axes = plt.subplots(1,2,figsize=(12,8))\n\nax1,ax2 = axes\n\nax1.imshow(h)\nax2.imshow(wordcloud)\nfig.tight_layout()\n\n\n\n\n- 원래 강아지 사진으로 하려고 했는데 강아지 모양이 안나왔다.\n\n아마 해당 사진에 위 사진처럼 흰색의 공간이 없어서 그런것 같다..ㅜㅜ\n개인적으로 워드클라우드를 선호하지 않으니 쓰는 방법만 기록해두자!"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#꽃-사진으로-워드클라우드",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#꽃-사진으로-워드클라우드",
    "title": "05. Python Basic (6)",
    "section": "꽃 사진으로 워드클라우드!",
    "text": "꽃 사진으로 워드클라우드!\n\nflower=  cv.imread(\"flower.png\")\n\n\nm_image = np.array(Image.open(\"flower.png\"))\n\nwordcloud = WordCloud(font_path = 'C:/Windiws/fonts/HMKMRHD.TTF',\n                      width=2000, \n                      height=1000, \n                      mask=m_image,\n                      background_color='white').generate_from_frequencies(w_c)\n\n\nfig, axes = plt.subplots(1,2,figsize=(12,8))\n\nax1,ax2 = axes\n\nax1.imshow(flower)\nax2.imshow(wordcloud)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#파일-load",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#파일-load",
    "title": "05. Python Basic (6)",
    "section": "파일 Load",
    "text": "파일 Load\n\ns1 = wb[\"Sheet1\"]\n\n\n셀이름으로 값 확인\n\ns1[\"A1\"].value\n\n'date'\n\n\n\n\n행과 열 변호로 셀 값확인\n\ns1.cell(row=1,column =1).value\n\n'date'\n\n\n\ns1.cell(row=2,column =6).value\n\n'F'\n\n\n\n\n데이터 영역 확인\n\nprint(s1.min_row,s1.max_row)\nprint(s1.min_column,s1.max_column)\n\n1 21\n1 6\n\n\n\n\nex1. 반복문을 이용해서 데이터를 출력\n\nfor i in range(s1.min_row,s1.max_row+1) :\n    for j in range(s1.min_column,s1.max_column+1) :\n        print(s1.cell(row = i, column=j).value,end=\" | \")\n    print(\"\\n\")\n\ndate | A | B | C | D | sex | \n\n2020-12-25 | 1.624345363663242 | -0.6117564136500754 | -0.5281717522634557 | -1.072968622156171 | F | \n\n2020-12-26 | 0.8654076293246785 | -2.301538696880283 | 1.74481176421648 | -0.7612069008951028 | M | \n\n2020-12-27 | 0.3190390960570985 | -0.2493703754774101 | 1.462107937044974 | -2.060140709497654 | F | \n\n2020-12-28 | -0.3224172040135075 | -0.3840543546684156 | 1.133769442335437 | -1.099891267314031 | M | \n\n2020-12-29 | -0.1724282075504357 | -0.8778584179213718 | 0.0422137467155928 | 0.5828152137158222 | F | \n\n2020-12-30 | -1.100619177212921 | 1.144723709839614 | 0.9015907205927955 | 0.5024943389018682 | M | \n\n2020-12-31 | 0.9008559492644118 | -0.6837278591743331 | -0.1228902255186481 | -0.9357694342590688 | F | \n\n2021-01-01 | -0.2678880796260159 | 0.530355466738186 | -0.691660751725309 | -0.3967535268559773 | M | \n\n2021-01-02 | -0.6871727001195994 | -0.8452056414987196 | -0.671246130836819 | -0.0126645989189013 | F | \n\n2021-01-03 | -1.117310348635278 | 0.2344156978170921 | 1.65980217710987 | 0.7420441605773356 | M | \n\n2021-01-04 | -0.1918355523616149 | -0.8876289640848363 | -0.7471582937508376 | 1.692454601027747 | F | \n\n2021-01-05 | 0.0508077547760289 | -0.6369956465693534 | 0.190915484667466 | 2.100255136478842 | M | \n\n2021-01-06 | 0.1201589524816291 | 0.6172031097074192 | 0.3001703199558275 | -0.3522498464935186 | F | \n\n2021-01-07 | -1.14251819802214 | -0.3493427224128775 | -0.2088942333747781 | 0.5866231911821976 | M | \n\n2021-01-08 | 0.8389834138745049 | 0.9311020813035572 | 0.2855873252542588 | 0.8851411642707281 | F | \n\n2021-01-09 | -0.7543979409966528 | 1.252868155233288 | 0.5129298204180088 | -0.2980928351027156 | M | \n\n2021-01-10 | 0.488518146537497 | -0.0755717130210557 | 1.131629387451427 | 1.519816816422199 | F | \n\n2021-01-11 | 2.185575406533161 | -1.396496335488138 | -1.444113805429589 | -0.5044658629464512 | M | \n\n2021-01-12 | 0.1600370694478304 | 0.8761689211162249 | 0.3156349472416052 | -2.022201215824003 | F | \n\n2021-01-13 | -0.3062040126283718 | 0.8279746426072462 | 0.2300947353643834 | 0.7620111803120247 | M | \n\n\n\n- 원데이터 확인\n\n_df = pd.read_excel(\"df.xlsx\")\n_df\n\n\n\n\n\n\n\n\ndate\nA\nB\nC\nD\nsex\n\n\n\n\n0\n2020-12-25\n1.624345\n-0.611756\n-0.528172\n-1.072969\nF\n\n\n1\n2020-12-26\n0.865408\n-2.301539\n1.744812\n-0.761207\nM\n\n\n2\n2020-12-27\n0.319039\n-0.249370\n1.462108\n-2.060141\nF\n\n\n3\n2020-12-28\n-0.322417\n-0.384054\n1.133769\n-1.099891\nM\n\n\n4\n2020-12-29\n-0.172428\n-0.877858\n0.042214\n0.582815\nF\n\n\n5\n2020-12-30\n-1.100619\n1.144724\n0.901591\n0.502494\nM\n\n\n6\n2020-12-31\n0.900856\n-0.683728\n-0.122890\n-0.935769\nF\n\n\n7\n2021-01-01\n-0.267888\n0.530355\n-0.691661\n-0.396754\nM\n\n\n8\n2021-01-02\n-0.687173\n-0.845206\n-0.671246\n-0.012665\nF\n\n\n9\n2021-01-03\n-1.117310\n0.234416\n1.659802\n0.742044\nM\n\n\n10\n2021-01-04\n-0.191836\n-0.887629\n-0.747158\n1.692455\nF\n\n\n11\n2021-01-05\n0.050808\n-0.636996\n0.190915\n2.100255\nM\n\n\n12\n2021-01-06\n0.120159\n0.617203\n0.300170\n-0.352250\nF\n\n\n13\n2021-01-07\n-1.142518\n-0.349343\n-0.208894\n0.586623\nM\n\n\n14\n2021-01-08\n0.838983\n0.931102\n0.285587\n0.885141\nF\n\n\n15\n2021-01-09\n-0.754398\n1.252868\n0.512930\n-0.298093\nM\n\n\n16\n2021-01-10\n0.488518\n-0.075572\n1.131629\n1.519817\nF\n\n\n17\n2021-01-11\n2.185575\n-1.396496\n-1.444114\n-0.504466\nM\n\n\n18\n2021-01-12\n0.160037\n0.876169\n0.315635\n-2.022201\nF\n\n\n19\n2021-01-13\n-0.306204\n0.827975\n0.230095\n0.762011\nM"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#엑셀값-수정",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#엑셀값-수정",
    "title": "05. Python Basic (6)",
    "section": "엑셀값 수정",
    "text": "엑셀값 수정\n\ndate \\(\\to\\) 날짜로 ㄹ바꿔보자\n\n\ns1[\"A1\"].value = \"날짜\"\n\n\ns1.cell(row=1,column=1).value\n\n'날짜'\n\n\n- 그러나 아직 파일에는 반영이 되지 않았음\n\n원본에 파일 반영하여 저장\n\nwb.save(\"df.xlsx\")\n\n- 다시 load후 결과 확인\n\nwb = xl.load_workbook(\"df.xlsx\")\n\n\ns1 = wb[\"Sheet1\"]\n\n\ns1.cell(row=1,column=1).value\n\n'날짜'\n\n\n\n\nex1. sex \\(\\to\\) 성별\n\ns1.cell(row=1,column=6).value\n\n'sex'\n\n\n\ns1[\"F1\"].value = \"성별\"\n\n\nwb.save(\"df.xlsx\")\n\n\nwb = xl.load_workbook(\"df.xlsx\")\ns1 = wb[\"Sheet1\"]\ns1.cell(row=1,column=6).value\n\n'성별'\n\n\n\n\n행 추가\n- 워크북 오브젝트 생성\n\nwb = xl.load_workbook(\"df.xlsx\")\n\n\ns1 = wb[\"Sheet1\"]\n\n- 두 번째 행에 빈 행 추가\n\ns1.insert_rows(2)\n\n- 빈 값이 들어간 것을 확인\n\nprint(s1.cell(2, 1).value, s1.cell(2, 2).value)b\n\nNone None\n\n\n\n\n열 추가\n\ns1.insert_cols(7)\n\n- 빈 값이 들어간 것을 확인\n\nprint(s1.cell(1, 7).value, s1.cell(2, 7).value)\n\nNone None\n\n\n- 전체 데이터 확인\n\nfor i in range(s1.min_row,s1.max_row+1) :\n    for j in range(s1.min_column,s1.max_column+1) :\n        print(s1.cell(row = i, column=j).value,end=\" | \")\n    print(\"\\n\")\n\n날짜 | A | B | C | D | 성별 | None | \n\nNone | None | None | None | None | None | None | \n\n2020-12-25 | 1.624345363663242 | -0.6117564136500754 | -0.5281717522634557 | -1.072968622156171 | F | None | \n\n2020-12-26 | 0.8654076293246785 | -2.301538696880283 | 1.74481176421648 | -0.7612069008951028 | M | None | \n\n2020-12-27 | 0.3190390960570985 | -0.2493703754774101 | 1.462107937044974 | -2.060140709497654 | F | None | \n\n2020-12-28 | -0.3224172040135075 | -0.3840543546684156 | 1.133769442335437 | -1.099891267314031 | M | None | \n\n성별 | -0.1724282075504357 | -0.8778584179213718 | 0.0422137467155928 | 0.5828152137158222 | F | None | \n\n2020-12-30 | -1.100619177212921 | 1.144723709839614 | 0.9015907205927955 | 0.5024943389018682 | M | None | \n\n2020-12-31 | 0.9008559492644118 | -0.6837278591743331 | -0.1228902255186481 | -0.9357694342590688 | F | None | \n\n2021-01-01 | -0.2678880796260159 | 0.530355466738186 | -0.691660751725309 | -0.3967535268559773 | M | None | \n\n2021-01-02 | -0.6871727001195994 | -0.8452056414987196 | -0.671246130836819 | -0.0126645989189013 | F | None | \n\n2021-01-03 | -1.117310348635278 | 0.2344156978170921 | 1.65980217710987 | 0.7420441605773356 | M | None | \n\n2021-01-04 | -0.1918355523616149 | -0.8876289640848363 | -0.7471582937508376 | 1.692454601027747 | F | None | \n\n2021-01-05 | 0.0508077547760289 | -0.6369956465693534 | 0.190915484667466 | 2.100255136478842 | M | None | \n\n2021-01-06 | 0.1201589524816291 | 0.6172031097074192 | 0.3001703199558275 | -0.3522498464935186 | F | None | \n\n2021-01-07 | -1.14251819802214 | -0.3493427224128775 | -0.2088942333747781 | 0.5866231911821976 | M | None | \n\n2021-01-08 | 0.8389834138745049 | 0.9311020813035572 | 0.2855873252542588 | 0.8851411642707281 | F | None | \n\n2021-01-09 | -0.7543979409966528 | 1.252868155233288 | 0.5129298204180088 | -0.2980928351027156 | M | None | \n\n2021-01-10 | 0.488518146537497 | -0.0755717130210557 | 1.131629387451427 | 1.519816816422199 | F | None | \n\n2021-01-11 | 2.185575406533161 | -1.396496335488138 | -1.444113805429589 | -0.5044658629464512 | M | None | \n\n2021-01-12 | 0.1600370694478304 | 0.8761689211162249 | 0.3156349472416052 | -2.022201215824003 | F | None | \n\n2021-01-13 | -0.3062040126283718 | 0.8279746426072462 | 0.2300947353643834 | 0.7620111803120247 | M | None | \n\n\n\n\n\n위에서 추가한 빈 행과 열을 삭제\n\ns1.delete_rows(2)\ns1.delete_cols(7)\n\n- 삭제 후 확인\n\nfor i in range(s1.min_row,s1.max_row+1) :\n    for j in range(s1.min_column,s1.max_column+1) :\n        print(s1.cell(row = i, column=j).value,end=\" | \")\n    print(\"\\n\")\n\n날짜 | A | B | C | D | 성별 | \n\n2020-12-25 | 1.624345363663242 | -0.6117564136500754 | -0.5281717522634557 | -1.072968622156171 | F | \n\n2020-12-26 | 0.8654076293246785 | -2.301538696880283 | 1.74481176421648 | -0.7612069008951028 | M | \n\n2020-12-27 | 0.3190390960570985 | -0.2493703754774101 | 1.462107937044974 | -2.060140709497654 | F | \n\n2020-12-28 | -0.3224172040135075 | -0.3840543546684156 | 1.133769442335437 | -1.099891267314031 | M | \n\n성별 | -0.1724282075504357 | -0.8778584179213718 | 0.0422137467155928 | 0.5828152137158222 | F | \n\n2020-12-30 | -1.100619177212921 | 1.144723709839614 | 0.9015907205927955 | 0.5024943389018682 | M | \n\n2020-12-31 | 0.9008559492644118 | -0.6837278591743331 | -0.1228902255186481 | -0.9357694342590688 | F | \n\n2021-01-01 | -0.2678880796260159 | 0.530355466738186 | -0.691660751725309 | -0.3967535268559773 | M | \n\n2021-01-02 | -0.6871727001195994 | -0.8452056414987196 | -0.671246130836819 | -0.0126645989189013 | F | \n\n2021-01-03 | -1.117310348635278 | 0.2344156978170921 | 1.65980217710987 | 0.7420441605773356 | M | \n\n2021-01-04 | -0.1918355523616149 | -0.8876289640848363 | -0.7471582937508376 | 1.692454601027747 | F | \n\n2021-01-05 | 0.0508077547760289 | -0.6369956465693534 | 0.190915484667466 | 2.100255136478842 | M | \n\n2021-01-06 | 0.1201589524816291 | 0.6172031097074192 | 0.3001703199558275 | -0.3522498464935186 | F | \n\n2021-01-07 | -1.14251819802214 | -0.3493427224128775 | -0.2088942333747781 | 0.5866231911821976 | M | \n\n2021-01-08 | 0.8389834138745049 | 0.9311020813035572 | 0.2855873252542588 | 0.8851411642707281 | F | \n\n2021-01-09 | -0.7543979409966528 | 1.252868155233288 | 0.5129298204180088 | -0.2980928351027156 | M | \n\n2021-01-10 | 0.488518146537497 | -0.0755717130210557 | 1.131629387451427 | 1.519816816422199 | F | \n\n2021-01-11 | 2.185575406533161 | -1.396496335488138 | -1.444113805429589 | -0.5044658629464512 | M | \n\n2021-01-12 | 0.1600370694478304 | 0.8761689211162249 | 0.3156349472416052 | -2.022201215824003 | F | \n\n2021-01-13 | -0.3062040126283718 | 0.8279746426072462 | 0.2300947353643834 | 0.7620111803120247 | M |"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#import-1",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#import-1",
    "title": "05. Python Basic (6)",
    "section": "import",
    "text": "import\n\nimport smtplib\nfrom email.mime.text import MIMEText"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#step1.-기본-셋팅",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#step1.-기본-셋팅",
    "title": "05. Python Basic (6)",
    "section": "step1. 기본 셋팅",
    "text": "step1. 기본 셋팅\n\n지메일, 모든 설정 \\(\\to\\) 전달 및 POP/IMAP \\(\\to\\) IMAP 엑세스(IMAP 사용 체크) \\(\\to\\) 변경사항 저장\n구글 계정 괸리 클랙 \\(\\to\\) 보안 \\(\\to\\) 2단계 인증 클릭 \\(\\to\\) 앱 비밀번호 \\(\\to\\) windows, 컴퓨터 메일 오픈\n컴퓨터용 앱 비밀번호 복사\n복사 후 첫 번째 인자에는 나의 이메일을, 두 번째 인자에는 복사한 비밀번호를 기입 후 실행\n\n\n# smtp 주소\ns = smtplib.SMTP('smtp.gmail.com', 587)\n\n# TLS 보안 시작\ns.starttls() \n\n## 로그인\ns.login('rkdcjf8232@gmail.com', 'zfkdrzytshqsucln') \n\n(235, b'2.7.0 Accepted')"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#step2.-메일-내용구성",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#step2.-메일-내용구성",
    "title": "05. Python Basic (6)",
    "section": "step2. 메일 내용구성",
    "text": "step2. 메일 내용구성\n\n## 본문\nm = MIMEText('''\n메일 보내기 테스트 테스트\n'''\n)\n\n## 제목\nm[\"Subject\"] = \"gc test\""
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#step3.-메일-보내기",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#step3.-메일-보내기",
    "title": "05. Python Basic (6)",
    "section": "step3. 메일 보내기",
    "text": "step3. 메일 보내기\n- s.sendmail(발신주소, 수신주소, 메일)\n\ns.sendmail(\"rkdcjf8232@gmail.com\",\"rkdcjf202150256@gmail.com\",m.as_string())\ns.quit()\n\n(221,\n b'2.0.0 closing connection s11-20020a62e70b000000b006888029fd63sm2245220pfh.9 - gsmtp')"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#step4.-메일-확인",
    "href": "posts/DX/00. 데이터 다루기/2023-08-17-05. Python Basic (6).html#step4.-메일-확인",
    "title": "05. Python Basic (6)",
    "section": "step4. 메일 확인",
    "text": "step4. 메일 확인"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html",
    "title": "Extra 01. 클래스",
    "section": "",
    "text": "from IPython.core.display import HTML\n\n\n\n\nclass jkm :\n     pass\n\n\ntest = jkm()\ntest\n\n&lt;__main__.jkm at 0x18dda40e3d0&gt;\n\n\n\n\n\n\ntest.title = \"중요한건 꺽이지 않는 마음\"\n\ntest.url = \"https://github.com/guebin/PP2023/blob/main/posts/03_Class/JungGGuckMa.jpg?raw=true\"\n\ntest.Q = \"Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\"\n\ntest.A = \"A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\"\n\ntest.h1 = \"마음\"\n\ntest.html_str =  '''\n&lt;style&gt;\n    .title {{\n        font-family: \"Times New Roman\", serif;\n        font-size: 30px;\n        font-weight: 900;\n    }}\n    .text {{\n        font-family: \"Arial\", sans-serif;\n        font-size: 20px;\n        font-style: italic;\n    }}\n    .highlight {{\n        font-family: \"Montserrat\", monospace;\n        font-size: 35px;\n        font-weight: 900;\n        text-decoration: underline; ## 밑줄\n        font-style: normal;\n        color: darkblue;\n        background-color: #FFFF00;\n    }}\n&lt;/style&gt;\n\n&lt;p class=\"title\"&gt;{tt1}&lt;/p&gt;\n&lt;img src={url} width=\"600\"&gt;\n&lt;p&gt; \\n &lt;/p&gt;\n&lt;p class=\"text\"&gt; {Q}&lt;/p&gt;\n&lt;p class=\"text\"&gt; {A}: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n&lt;p class=\"title\"&gt;중요한 것은 꺾이지 않는 &lt;span class=\"highlight\"&gt; {h1} &lt;/span&gt;&lt;/p&gt;\n'''\n\n\n\n\n\ndef show(test):\n    _str = test.html_str.format(\n        tt1 = test.title,\n        url = test.url,\n        Q = test.Q,\n        A = test.A,\n        h1 = test.h1\n    )\n    display(HTML(_str))\n\n\nshow(test)\n\n\n\n\n중요한건 꺽이지 않는 마음\n\n \n \n Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\n A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n중요한 것은 꺾이지 않는  마음"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#단계-도화지-생성",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#단계-도화지-생성",
    "title": "Extra 01. 클래스",
    "section": "",
    "text": "class jkm :\n     pass\n\n\ntest = jkm()\ntest\n\n&lt;__main__.jkm at 0x18dda40e3d0&gt;"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#단계-뼈대-생성",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#단계-뼈대-생성",
    "title": "Extra 01. 클래스",
    "section": "",
    "text": "test.title = \"중요한건 꺽이지 않는 마음\"\n\ntest.url = \"https://github.com/guebin/PP2023/blob/main/posts/03_Class/JungGGuckMa.jpg?raw=true\"\n\ntest.Q = \"Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\"\n\ntest.A = \"A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\"\n\ntest.h1 = \"마음\"\n\ntest.html_str =  '''\n&lt;style&gt;\n    .title {{\n        font-family: \"Times New Roman\", serif;\n        font-size: 30px;\n        font-weight: 900;\n    }}\n    .text {{\n        font-family: \"Arial\", sans-serif;\n        font-size: 20px;\n        font-style: italic;\n    }}\n    .highlight {{\n        font-family: \"Montserrat\", monospace;\n        font-size: 35px;\n        font-weight: 900;\n        text-decoration: underline; ## 밑줄\n        font-style: normal;\n        color: darkblue;\n        background-color: #FFFF00;\n    }}\n&lt;/style&gt;\n\n&lt;p class=\"title\"&gt;{tt1}&lt;/p&gt;\n&lt;img src={url} width=\"600\"&gt;\n&lt;p&gt; \\n &lt;/p&gt;\n&lt;p class=\"text\"&gt; {Q}&lt;/p&gt;\n&lt;p class=\"text\"&gt; {A}: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n&lt;p class=\"title\"&gt;중요한 것은 꺾이지 않는 &lt;span class=\"highlight\"&gt; {h1} &lt;/span&gt;&lt;/p&gt;\n'''"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#단계-show-함수-작성",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#단계-show-함수-작성",
    "title": "Extra 01. 클래스",
    "section": "",
    "text": "def show(test):\n    _str = test.html_str.format(\n        tt1 = test.title,\n        url = test.url,\n        Q = test.Q,\n        A = test.A,\n        h1 = test.h1\n    )\n    display(HTML(_str))\n\n\nshow(test)\n\n\n\n\n중요한건 꺽이지 않는 마음\n\n \n \n Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\n A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n중요한 것은 꺾이지 않는  마음"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#self",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#self",
    "title": "Extra 01. 클래스",
    "section": "self",
    "text": "self\n- self는 밈 클래스에서 생성할 인스턴스의 이름을 대신한다.\n- jkm이라는 클래스에 다음과 같이 show함수를 종속 시키자\n\nclass jkm:\n    def show(self):\n        _str = self.html_str.format(\n            tt1 = self.title,\n            url = self.url,\n            Q = self.Q,\n            A = self.A,\n            h1 = self.h1 )\n        display(HTML(_str))\n\n\ntest  = jkm()\n\n\ntest.title = \"중요한건 꺽이지 않는 마음\"\n\ntest.url = \"https://github.com/guebin/PP2023/blob/main/posts/03_Class/JungGGuckMa.jpg?raw=true\"\n\ntest.Q = \"Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\"\n\ntest.A = \"A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\"\n\ntest.h1 = \"마음\"\n\ntest.html_str =  '''\n&lt;style&gt;\n    .title {{\n        font-family: \"Times New Roman\", serif;\n        font-size: 30px;\n        font-weight: 900;\n    }}\n    .text {{\n        font-family: \"Arial\", sans-serif;\n        font-size: 20px;\n        font-style: italic;\n    }}\n    .highlight {{\n        font-family: \"Montserrat\", monospace;\n        font-size: 35px;\n        font-weight: 900;\n        text-decoration: underline; ## 밑줄\n        font-style: normal;\n        color: darkblue;\n        background-color: #FFFF00;\n    }}\n&lt;/style&gt;\n\n&lt;p class=\"title\"&gt;{tt1}&lt;/p&gt;\n&lt;img src={url} width=\"600\"&gt;\n&lt;p&gt; \\n &lt;/p&gt;\n&lt;p class=\"text\"&gt; {Q}&lt;/p&gt;\n&lt;p class=\"text\"&gt; {A}: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n&lt;p class=\"title\"&gt;중요한 것은 꺾이지 않는 &lt;span class=\"highlight\"&gt; {h1} &lt;/span&gt;&lt;/p&gt;\n'''\n\n\ntest.show()\n\n\n\n\n중요한건 꺽이지 않는 마음\n\n \n \n Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\n A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n중요한 것은 꺾이지 않는  마음"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#init",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#init",
    "title": "Extra 01. 클래스",
    "section": "init",
    "text": "init\n\n모티브\n- 인스턴스를 생성할 떄마다 변수를 선언하는 것이 귀찮음….\n- 초기값을 정의하는 함수 또한 클래스안에 정의해주자\n\n\n뼈대 작성\n\nclass jkm:\n    def __init__(self) :\n        self.title = \"중요한건 꺽이지 않는 마음\"\n\n        self.url = \"https://github.com/guebin/PP2023/blob/main/posts/03_Class/JungGGuckMa.jpg?raw=true\"\n\n        self.Q = \"Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\"\n\n        self.A = \"A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\"\n\n        self.h1 = \"마음\"\n\n        self.html_str =  '''\n                    &lt;style&gt;\n                        .title {{\n                            font-family: \"Times New Roman\", serif;\n                            font-size: 30px;\n                            font-weight: 900;\n                        }}\n                        .text {{\n                            font-family: \"Arial\", sans-serif;\n                            font-size: 20px;\n                            font-style: italic;\n                        }}\n                        .highlight {{\n                            font-family: \"Montserrat\", monospace;\n                            font-size: 35px;\n                            font-weight: 900;\n                            text-decoration: underline; ## 밑줄\n                            font-style: normal;\n                            color: darkblue;\n                            background-color: #FFFF00;\n                        }}\n                    &lt;/style&gt;\n\n                    &lt;p class=\"title\"&gt;{tt1}&lt;/p&gt;\n                    &lt;img src={url} width=\"600\"&gt;\n                    &lt;p&gt; \\n &lt;/p&gt;\n                    &lt;p class=\"text\"&gt; {Q}&lt;/p&gt;\n                    &lt;p class=\"text\"&gt; {A}: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n                    &lt;p class=\"title\"&gt;중요한 것은 꺾이지 않는 &lt;span class=\"highlight\"&gt; {h1} &lt;/span&gt;&lt;/p&gt;\n                    '''\n        \n    def show(self):\n        _str = self.html_str.format(\n            tt1 = self.title,\n            url = self.url,\n            Q = self.Q,\n            A = self.A,\n            h1 = self.h1 )\n        display(HTML(_str))\n\n\ntest = jkm()\n\n\n\nSHOW\n\ntest.show()\n\n\n                    \n\n                    중요한건 꺽이지 않는 마음\n                    \n                     \n \n                     Q: 로그와 2라운드 재대결, 어떤 점에 유의해야 할까\n                     A: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.: 상대팀에 대해서 더 분석할 건 없는 것 같고, 저희가 저희 플레이 잘하는 게 제일 중요한 것 같고 오늘 지긴 했지만 저희끼리만 안 무너지면 충분히 이길 수 있을 것 같아요.\n                    중요한 것은 꺾이지 않는  마음"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex1.클래스-내에-hello라는-메소드를-정의하고-출력하라",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex1.클래스-내에-hello라는-메소드를-정의하고-출력하라",
    "title": "Extra 01. 클래스",
    "section": "ex1.클래스 내에 hello라는 메소드를 정의하고 출력하라",
    "text": "ex1.클래스 내에 hello라는 메소드를 정의하고 출력하라\n\nclass ex1 :\n    def hello(self):\n        print(\"안녕하세요\")\n        \nex1 = ex1()\n\n\nex1.hello()\n\n안녕하세요"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex2.-아래의-조건에-맞는-클래스를-구현",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex2.-아래의-조건에-맞는-클래스를-구현",
    "title": "Extra 01. 클래스",
    "section": "ex2. 아래의 조건에 맞는 클래스를 구현",
    "text": "ex2. 아래의 조건에 맞는 클래스를 구현\n1 “클래스 \\(\\to\\) 인스턴스”의 과정에서 변수 a가 True로 초기설정된다.\n2 클래스에는 show()라는 메소드가 정의되어 있으며, show()의 기능은 a의 값을 print하는 기능을 한다.\n\nclass ex2 :\n     def __init__(self) :\n            self.a = True\n     def show(self) :\n         print(self.a)\n\n\nex2 = ex2()\n\n\nex2.show()\n\nTrue"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex3.-아래에-조건에-맞는-클래스를-구현하라.",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex3.-아래에-조건에-맞는-클래스를-구현하라.",
    "title": "Extra 01. 클래스",
    "section": "ex3. 아래에 조건에 맞는 클래스를 구현하라.",
    "text": "ex3. 아래에 조건에 맞는 클래스를 구현하라.\n1 “클래스 \\(\\to\\)인스턴스”의 과정에서 변수 a가 True 로 초기설정된다.\n2. 클래스에는 toggle() 이라는 메소드가 정의되어 있다. 이 기능은 변수 a의 값이 True 이면 False 로, False 이면 True 로 바꾸는 역할을 한다.\n3. 클래스에는 show()라는 메소드가 정의되어 있다. 이 기능은 a의 값을 print 하는 기능을 한다.\n\nclass ex3: \n    def __init__(self) :\n        self.a = True\n    def toggle(self):\n        self.a = not self.a\n    def show(self) :\n        print(self.a)\n\n\ntest = ex3()\n\n\ntest.a\n\nTrue\n\n\n\ntest.toggle()\n\n\ntest.a\n\nFalse\n\n\n\ntest.show()\n\nFalse"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex4.-아래-조건에-맞는-클래스를-구현",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex4.-아래-조건에-맞는-클래스를-구현",
    "title": "Extra 01. 클래스",
    "section": "ex4. 아래 조건에 맞는 클래스를 구현",
    "text": "ex4. 아래 조건에 맞는 클래스를 구현\n1 “클래스 \\(\\to\\) 인스턴스”의 과정에서 변수 a가 0으로 초기설정된다.\n2 클래스에는 up()이라는 메소드가 정의되어 있따. up()의 기능은 a의 값을 1증가 시키는 기능을 한다.\n3 클래스에는 show()라는 메소드가 정의되어 있다. show()의 기능은 a의값을 print하다.\n\nclass ex4: \n    def __init__(self) :\n        self.a = 1\n    def up(self):\n        self.a += 1\n    def show(self) :\n        print(self.a)\n\n\nex4 = ex4()\n\n\nex4.up()\n\n\nex4.a\n\n2\n\n\n\nex4.show()\n\n1"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex5.-아래-조건에-맞는-클래스를-구현하라.",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex5.-아래-조건에-맞는-클래스를-구현하라.",
    "title": "Extra 01. 클래스",
    "section": "ex5. 아래 조건에 맞는 클래스를 구현하라.",
    "text": "ex5. 아래 조건에 맞는 클래스를 구현하라.\n1 클래스 \\(\\to\\) 인스턴스” 과정에서 변수 a의 값이 사용자가 입력한 값으로 초기 설정된다.\n2 클래스에는 show()라는 메소드가 정의되어 있다. show()의 기능은 a의 값을 print한다.\n\nclass ex5 :\n    def __init__(self,value):\n         self.a = value\n    def show(self) :\n         print(self.a)\n\n\nex5 = ex5(1)\n\n\nex5.show()\n\n1"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex6.-다음-조건을-만족하는-클래스를-구현하라.",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex6.-다음-조건을-만족하는-클래스를-구현하라.",
    "title": "Extra 01. 클래스",
    "section": "ex6. 다음 조건을 만족하는 클래스를 구현하라.",
    "text": "ex6. 다음 조건을 만족하는 클래스를 구현하라.\n\n“클래스 인스턴스”의 과정에서 변수 a가 0으로 초기설정된다.\n클래스에는 up()라는 메소드가 정의되어 있다. up()의 기능은 a의 값을 1증가시키는 기능을 한다.\n클래스에는 jump()라는 메소드가 정의되어 있다. jump()는 jump_size 를 입력으로 받으며 a의 값을 jump_size 만큼 증가시키는 기능을 한다.\n클래스에는 show()라는 메소드가 정의되어 있다. show()의 기능은 a의 값을 print 하는 기능을 한다.\n\n\nclass ex6:\n    def __init__(self):\n        self.a = 0\n    def up(self):\n        self.a += 1\n    def jump(self,js) :\n        self.a += js\n    def show(self) :\n        print(self.a)\n\n\nex6 = ex6()\n\n\nex6.up()\n\n\nex6.a\n\n1\n\n\n\nex6.jump(100)\n\n\nex6.a\n\n101\n\n\n\nex6.show()\n\n101"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex7.-아래-조건에-맞는-클래스를-구현",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex7.-아래-조건에-맞는-클래스를-구현",
    "title": "Extra 01. 클래스",
    "section": "ex7. 아래 조건에 맞는 클래스를 구현",
    "text": "ex7. 아래 조건에 맞는 클래스를 구현\n\n“클래스 인스턴스”의 과정에서 변수 a가 0으로 초기설정된다.\n클래스에는 up()라는 메소드가 정의되어 있다. up()의 기능은 a의 값을 1증가시키는 기능을 한다.\n클래스에는 jump()라는 메소드가 정의되어 있다. jump()는 jump_size 를 입력으로 받으며 a의 값을 jump_size 만큼 증가시키는 기능을 한다.\n클래스에는 reset()이라는 메소드가 정의되어 있다. reset()는 a의 값을 0으로 초기화하는 역할을 한다.\n클래스에는 show()라는 메소드가 정의되어 있다. show()의 기능은 a의 값을 print 하는 기능을 한다.\n\n\nclass ex7:\n    def __init__(self):\n        self.a = 0\n    def up(self):\n        self.a += 1\n    def jump(self,js) :\n        self.a += js\n    def reset(self) :\n        self.a = 0\n    def show(self) :\n        print(self.a)\n\n\nex7 = ex7()\n\n\nex7.up()\n\n\nex7.a\n\n1\n\n\n\nex7.reset()\n\n\nex7.a\n\n0"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex8.-이미지-출력",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex8.-이미지-출력",
    "title": "Extra 01. 클래스",
    "section": "ex8. 이미지 출력",
    "text": "ex8. 이미지 출력\n\nurl = 'https://github.com/guebin/PP2023/blob/main/posts/03_Class/burgerking.png?raw=true'\nhtml_str = '&lt;img src={url} width=\"600\"&gt;'.format(url=url)\ndisplay(HTML(html_str))\n\n\n\n\n\n“클래스 \\(\\to\\) 인스턴스”의 과정에서 변수 url이 위에서 제시된 값으로 초기화된다.\n클래스에는 show()라는 함수가 있어서 url에 해당하는 이미지를 출력해주는 기능을 가진다.\n\n\nclass ex8:\n    def __init_(self) :\n        self.url = 'https://github.com/guebin/PP2023/blob/main/posts/03_Class/burgerking.png?raw=true'\n    def show(self):\n        self.html_str = '&lt;img src={url} width=\"600\"&gt;'.format(url=url)\n        display(HTML(self.html_str))\n\n\nex8= ex8()\n\n\nex8.show()"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex9.-이미지-출력-횟수기록",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex9.-이미지-출력-횟수기록",
    "title": "Extra 01. 클래스",
    "section": "ex9. 이미지 출력 + 횟수기록",
    "text": "ex9. 이미지 출력 + 횟수기록\n\n“클래스 \\(\\to\\)인스턴스”의 과정에서 변수 url이 위에서 제시된 값으로 초기화된다.\n클래스에는 show()라는 함수가 있어서 (1) url에 해당하는 이미지를 출력하고 (2) “당신은 이 그림을 \\(n\\)번 보았습니다” 를 출력하는 기능을 한다. 여기에서 \\(n\\)은 그림을 본 횟수\n\n\nclass ex9:\n    def __init__(self) :\n        self.n = 0\n        self.url = 'https://github.com/guebin/PP2023/blob/main/posts/03_Class/burgerking.png?raw=true'\n    def show(self):\n        self.html_str = '&lt;img src={url} width=\"600\"&gt;'.format(url=self.url)\n        display(HTML(self.html_str))\n        self.n += 1\n        print(\"당신은 이 그림을 {}번 보았습니다.\".format(self.n))\n\n\nex9 = ex9()\n\n\nex9.show()\n\n\n\n\n당신은 이 그림을 1번 보았습니다."
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex10.-예제-9에서-만든-클래스를-이용하여-아래의-url에-해당하는-이미지를-출려가하라.",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex10.-예제-9에서-만든-클래스를-이용하여-아래의-url에-해당하는-이미지를-출려가하라.",
    "title": "Extra 01. 클래스",
    "section": "ex10. 예제 9에서 만든 클래스를 이용하여 아래의 url에 해당하는 이미지를 출려가하라.",
    "text": "ex10. 예제 9에서 만든 클래스를 이용하여 아래의 url에 해당하는 이미지를 출려가하라.\n\ni1 =  ex9()\ni2 =  ex9()\n\n\ni2.url = \"https://github.com/guebin/PP2023/blob/main/posts/03_Class/JungGGuckMa.jpg?raw=true\"\n\n\ni1.show()\n\n\n\n\n당신은 이 그림을 1번 보았습니다.\n\n\n\ni2.show()\n\n\n\n\n당신은 이 그림을 1번 보았습니다."
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex11.-stock-이라는-이름의-클래스를-만들고-아래의-기능을-넣어라",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex11.-stock-이라는-이름의-클래스를-만들고-아래의-기능을-넣어라",
    "title": "Extra 01. 클래스",
    "section": "ex11. Stock 이라는 이름의 클래스를 만들고 아래의 기능을 넣어라",
    "text": "ex11. Stock 이라는 이름의 클래스를 만들고 아래의 기능을 넣어라\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport yfinance as yf\n\n(1) crawling 메소드: crawling 메소드는 start_date, end_date, code 를 입력으로 받는 함수이며, code 에 대응하는 주식의 주가를 크롤링하는 기능을 가진다. 크롤링된 주식의 가격은 numpy array 형태로 저장되어 있다.\nhint\n\nstart_date = \"2023-01-01\"\nend_date = \"2023-05-02\"\ncode = \"005930.KS\"\ny = yf.download(code, start=start_date, end=end_date)['Adj Close'].to_numpy()\n\n[*********************100%***********************]  1 of 1 completed\n\n\n(2) smoothing 메소드 : smoothing는 크롤링된 주가를 아래의 수식을 통하여 \\(n\\)회 변환하는 기능을 한다.\n\n\\(\\overset{\\sim}{y_1} : \\frac 14(3y_1 + y_2)\\)\n\\(\\overset{\\sim}{y_i} : \\frac 14(y_{i-1} + 2y_i + y_{i+1})\\quad i=2,3 \\dots, n-1\\)\n\\(\\overset{\\sim}{y_n} : \\frac 14(y_{n-1} + 3y_n)\\)\n\nhint\n\nT = len(y)\nM = (np.eye(T) + np.array([abs(i-j)&lt;2 for i in range(T) for j in range(T)]).reshape(T,T))/4\nM[0,0] = 3/4; M[-1,-1]= 3/4 \n#np.linalg.matrix_power(M,50)@y\n\n\n풀이\n\nclass stock :\n    def __init__(self) :\n        self.y = None\n        self.sy = None\n    def crawling(self,code, start_date,end_date):\n        self.y = yf.download(code, start_date,end_date)['Adj Close'].to_numpy()\n    def smoothing(self,n):\n        T = len(self.y)\n        self.n = n\n        M = (np.eye(T) + np.array([abs(i-j)&lt;2 for i in range(T) for j in range(T)]).reshape(T,T))/4\n        M[0,0] = 3/4; M[-1,-1]= 3/4\n        self.sy = np.linalg.matrix_power(M,50)@self.y\n    def plot(self) : \n        plt.plot(self.y,label=r\"$y$\")\n        plt.plot(self.sy,\"--\",label =f\"$M^{self.n}@y$\")\n        plt.legend()\n\n\nex11 = stock()\n\n\nstart_date = \"2023-01-01\"\nend_date = \"2023-05-02\"\ncode = \"005930.KS\"\nex11.crawling(code, start_date,end_date)\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\nex11.smoothing(50)\n\n\nex11.plot()"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex12.-ex11에서-만든-stock-클래스에서-kakao-인스턴스를-생성하라.-생성된-kakao-인스턴스에서-crawling-메소드를-이용하여-아래의-조건에-맞는-주식을-긁어오라.",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex12.-ex11에서-만든-stock-클래스에서-kakao-인스턴스를-생성하라.-생성된-kakao-인스턴스에서-crawling-메소드를-이용하여-아래의-조건에-맞는-주식을-긁어오라.",
    "title": "Extra 01. 클래스",
    "section": "ex12. ex11에서 만든 Stock 클래스에서 kakao 인스턴스를 생성하라. 생성된 kakao 인스턴스에서 crawling 메소드를 이용하여 아래의 조건에 맞는 주식을 긁어오라.",
    "text": "ex12. ex11에서 만든 Stock 클래스에서 kakao 인스턴스를 생성하라. 생성된 kakao 인스턴스에서 crawling 메소드를 이용하여 아래의 조건에 맞는 주식을 긁어오라.\n\ncode: ‘035720.KS’ (카카오)\nstart_date = “2023-01-01”\nend_date = “2023-05-26”\n\n이후 .smoothing 메소드를 이용하여 \\(n=50\\)회 스무딩하고 .plot 메소드를 이용하여 결과를 시각화하라.\n\n풀이\n\nkakao = stock()\n\n\ncode = \"035720.KS\"\nstart_date = \"2023-01-01\"\nend_date = \"2023-05-26\"\n\n\nkakao.crawling(code,start_date, end_date)\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\nkakao.smoothing(50)\n\n\nkakao.plot()"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex13.-linearregression이라는-이름의-클래스를-만들고-아래의-기능을-넣어라",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex13.-linearregression이라는-이름의-클래스를-만들고-아래의-기능을-넣어라",
    "title": "Extra 01. 클래스",
    "section": "ex13. LinearRegression이라는 이름의 클래스를 만들고 아래의 기능을 넣어라",
    "text": "ex13. LinearRegression이라는 이름의 클래스를 만들고 아래의 기능을 넣어라\n(1) “클래스 \\(\\to\\) 인스턴스” 인 시점에 길이가 \\(n\\)인 numpy array \\(\\bf{x} = (x_1\\dots, x_n),\\bf{y} = (y_1\\dots, y_n)\\) 을 입력으로 받아 내부에 저장한다.\n(2) fit 메소드 : fit은 내부에 저장된 \\(\\bf{x,y}\\)를 이용하여 아래의 수식을 계산한다.\n\\[\\bf{\\hat{y}} = X(X^{T}X)^{-1}X^{T}y,\\quad X = \\begin{bmatrix}1 & x_1 \\\\ \\dots & \\dots \\\\ 1 & x_n \\end{bmatrix}\\]\n(3) plot 메소드 작성\n\n풀이\n\nclass LinearRegression:\n        def __init__(self,x,y):\n            self.x = x\n            self.y = y\n        def fit(self):\n            n = len(self.x)\n            self.X = np.stack([np.ones(n),self.x],axis=1)\n            self.yhat = self.X@np.linalg.inv(self.X.T@self.X)@self.X.T@self.y     \n        \n        def plot(self) :\n            plt.plot(self.x,self.y,\"o\",label = r\"$(x,y)$\")\n            plt.plot(self.x,self.yhat,\"--\",label = r\"$(x,\\hat y)$\")\n            plt.legend()\n\n\n\nCode\nx =  np.array([0.00983, 0.01098, 0.02951, 0.0384 , 0.03973, 0.04178, 0.0533 ,\n               0.058  , 0.09454, 0.1103 , 0.1328 , 0.1412 , 0.1497 , 0.1664 ,\n               0.1906 , 0.1923 , 0.198  , 0.2141 , 0.2393 , 0.2433 , 0.3157 ,\n               0.3228 , 0.3418 , 0.3552 , 0.3918 , 0.3962 , 0.4    , 0.4482 ,\n               0.496  , 0.507  , 0.53   , 0.5654 , 0.582  , 0.5854 , 0.5854 ,\n               0.6606 , 0.7007 , 0.723  , 0.7305 , 0.7383 , 0.7656 , 0.7725 ,\n               0.831  , 0.8896 , 0.9053 , 0.914  , 0.949  , 0.952  , 0.9727 ,\n               0.982  ])\ny =  np.array([0.7381, 0.7043, 0.3937, 0.1365, 0.3784, 0.3028, 0.1037, 0.3846,\n               0.706 , 0.7572, 0.2421, 0.232 , 0.9855, 1.162 , 0.4653, 0.6791,\n               0.6905, 0.6865, 0.9757, 0.7665, 0.9522, 0.4641, 0.5498, 1.1509,\n               0.5288, 1.1195, 1.1659, 1.4341, 1.2779, 1.1648, 1.4002, 0.7472,\n               0.9142, 0.9658, 1.0707, 1.4501, 1.6758, 0.8778, 1.3384, 0.7476,\n               1.3086, 1.7537, 1.5559, 1.2928, 1.3832, 1.3115, 1.3382, 1.536 ,           \n               1.9177, 1.2069])\n\n\n\nex13 = LinearRegression(x,y)\n\n\nex13.fit()\n\n\nex13.plot()"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex14.-ex13에서-작성한-클래스를-이용하여-아래의-자료를-분석하고-시각화해라.",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-09-Extra 01. 클래스.html#ex14.-ex13에서-작성한-클래스를-이용하여-아래의-자료를-분석하고-시각화해라.",
    "title": "Extra 01. 클래스",
    "section": "ex14. ex13에서 작성한 클래스를 이용하여 아래의 자료를 분석하고 시각화해라.",
    "text": "ex14. ex13에서 작성한 클래스를 이용하여 아래의 자료를 분석하고 시각화해라.\n\n\nCode\nx = np.array(\n    [0.007, 0.008, 0.008, 0.011, 0.037, 0.047, 0.059, 0.07 , 0.072,\n     0.075, 0.078, 0.08 , 0.082, 0.11 , 0.114, 0.117, 0.133, 0.15 ,\n     0.161, 0.163, 0.172, 0.208, 0.209, 0.221, 0.229, 0.231, 0.234,\n     0.235, 0.249, 0.251, 0.256, 0.269, 0.269, 0.273, 0.275, 0.298,\n     0.305, 0.309, 0.34 , 0.362, 0.371, 0.374, 0.382, 0.387, 0.388,\n     0.394, 0.395, 0.397, 0.401, 0.404, 0.419, 0.433, 0.436, 0.466,\n     0.481, 0.492, 0.495, 0.508, 0.511, 0.512, 0.554, 0.57 , 0.574,\n     0.575, 0.584, 0.6  , 0.601, 0.615, 0.618, 0.623, 0.629, 0.633,\n     0.646, 0.65 , 0.654, 0.662, 0.673, 0.686, 0.702, 0.744, 0.754,\n     0.766, 0.772, 0.781, 0.798, 0.8  , 0.807, 0.836, 0.837, 0.871,\n     0.873, 0.877, 0.879, 0.889, 0.891, 0.902, 0.904, 0.923, 0.952,\n     0.981]\n)\n\n\ny = np.array(\n    [4.004, 4.189, 5.483, 4.902, 5.174, 4.468, 4.95 , 4.463, 5.476,\n     4.446, 4.764, 5.244, 4.357, 4.796, 5.464, 4.196, 5.244, 4.868,\n     5.358, 4.493, 4.831, 4.716, 4.929, 4.588, 4.718, 4.389, 4.985,\n     4.266, 4.291, 3.697, 4.248, 4.88 , 5.126, 4.563, 4.131, 4.728,\n     4.168, 4.584, 3.953, 4.747, 3.592, 5.023, 4.601, 3.904, 4.092,\n     4.37 , 3.922, 4.145, 4.576, 4.25 , 4.051, 3.616, 4.634, 3.496,\n     4.631, 4.025, 4.197, 4.226, 4.808, 3.676, 3.834, 3.197, 4.36 ,\n     3.547, 3.956, 3.522, 4.26 , 3.443, 3.97 , 4.068, 4.186, 3.262,\n     3.452, 3.946, 3.875, 3.444, 3.501, 3.959, 3.843, 2.679, 3.266,\n     3.506, 2.916, 3.714, 4.007, 2.795, 3.329, 2.756, 3.72 , 2.381,\n     2.798, 3.035, 3.492, 3.22 , 3.073, 3.85 , 3.233, 3.396, 3.264,\n     2.986]\n)    \n\n\n\nex14 = LinearRegression(x,y)\nex14.fit()\nex14.plot()"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#모티블-클래스를-매번-수정하기-불편해",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#모티블-클래스를-매번-수정하기-불편해",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "모티블 : 클래스를 매번 수정하기 불편해",
    "text": "모티블 : 클래스를 매번 수정하기 불편해"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#upjump---ver1",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#upjump---ver1",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "UPjump - Ver1",
    "text": "UPjump - Ver1\n\nclass UpJump:\n    def __init__(self):\n        self.value = 0\n    def up(self):\n        self.value = self.value + 1  \n    def jump(self,jump_size):\n        self.value = self.value + jump_size\n    def __repr__(self):\n        return str(self.value)        \n\n\na = UpJump()\na.jump(2)\n\n\na\n\n2"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#upjump---ver2",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#upjump---ver2",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "Upjump - Ver2",
    "text": "Upjump - Ver2\n\nclass UpJump_Ver2(UpJump):\n    def jump(self,jump_size):\n        self.value = self.value + jump_size\n\n\na = UpJump_Ver2()\n\n\na.jump(2)\n\n\na\n\n2"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#tip",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#tip",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "tip",
    "text": "tip\n- 클래스를 조금 수정하고 싶을때, 아래와 같은 문법을 이용하면 편리하다.\nclass 새로운 클래스(수정할 클래스):\n    def 수정 및 추가할 함수(self,parameter):\n        return ...\n- 사용에시\n\nclass temp(UpJump) :\n      def __repr__(self) :\n        return f\"현재 value는 {self.value}입니다.\"\n\n\na = temp()\n\n\na.jump(50)\n\n\na\n\n현재 value는 50입니다."
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#motive",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#motive",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "motive",
    "text": "motive\n\n클래스를 배우기 전 : int 자료형의 +는 “정수의 덧셈”을 의미하고 list자료형의 +는 “자료의 추가” 를 의미한다.\n클래스를 배운 후 : 아마 int, list 클래스의 + 라는 연산을 정의하는 숨겨진 메소드가 있을 것이다.\n\n\na,b= 1,2\n\n- int\n\nprint(f\"a = {a}, b = {b}, a+b = {a+b}\")\nprint(f\"a = {a}, b = {b}, a.__add__(b) = {a.__add__(b)}\")\n\na = 1, b = 2, a+b = 3\na = 1, b = 2, a.__add__(b) = 3\n\n\n- list\n\na,b = [1,2],[3,4]\n\n\nprint(f\"a = {a}, b = {b}, a+b = {a+b}\")\nprint(f\"a = {a}, b = {b}, a.__add__(b) = {a.__add__(b)}\")\n\na = [1, 2], b = [3, 4], a+b = [1, 2, 3, 4]\na = [1, 2], b = [3, 4], a.__add__(b) = [1, 2, 3, 4]\n\n\n- 확인 : a+b는 사실 내부적으로 a.__add__(b)의 축약구문이다.\n- 추측 : 따라서 만약 : a.__add__(b)의 기능을 재정의 하면 a+b의 기능도 바뀔것이다."
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#ex1.-__add__",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#ex1.-__add__",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "ex1. __add__()",
    "text": "ex1. __add__()\n- 클래스 선언\n\n한 학기를 등록할 때마다. 학생의 나이가 0.5, 학기가 1 증가하는 __add__ 함수를 클래스 내부에 구현\n\n\nclass s:\n    def __init__(self, age=20.0, semester=0): \n        self.age = age\n        self.semester = semester\n        print(f\"입학을 축하합니다. 당신의 나이는 {self.age}이고 현재 학기는 {self.semester}학기 입니다.\")\n    def __add__(self,registration_status):  \n        if registration_status=='휴학': \n            self.age=self.age+0.5 \n        elif registration_status=='등록':\n            self.age=self.age+0.5 \n            self.semester= self.semester+1\n    def _repr_html_(self): ## 코드 출력 정의\n        html_str = \"\"\"\n        나이: {} &lt;br/&gt;\n        학기: {} &lt;br/&gt;\n        \"\"\"\n        return html_str.format(self.age,self.semester)\n\n\ngc = s()\n\n입학을 축하합니다. 당신의 나이는 20.0이고 현재 학기는 0학기 입니다.\n\n\n\ngc + \"등록\"\n\n\ngc\n\n\n        나이: 20.5 \n        학기: 1 \n        \n\n\n\ngc + \"휴학\"\n\n\ngc\n\n\n        나이: 21.0 \n        학기: 1"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#ex2.-__add__-return",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#ex2.-__add__-return",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "ex2. __add__ + return",
    "text": "ex2. __add__ + return\n- 잘못된 사용\n\ngc + '등록'+ '휴학' + '등록' + '휴학'\n\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\n\n\n- Ver 1\n\nclass s:\n    def __init__(self, age=20.0, semester=0): \n        self.age = age\n        self.semester = semester\n        print(f\"입학을 축하합니다. 당신의 나이는 {self.age}이고 현재 학기는 {self.semester}학기 입니다.\")\n    def __add__(self,registration_status):  \n        if registration_status=='휴학': \n            self.age=self.age+0.5 \n        elif registration_status=='등록':\n            self.age=self.age+0.5 \n            self.semester= self.semester+1 \n        return self  ## return 추가\n    def _repr_html_(self): ## 코드 출력 정의\n        html_str = \"\"\"\n        나이: {} &lt;br/&gt;\n        학기: {} &lt;br/&gt;\n        \"\"\"\n        return html_str.format(self.age,self.semester)\n\n\ngc = s()\n\n입학을 축하합니다. 당신의 나이는 20.0이고 현재 학기는 0학기 입니다.\n\n\n\ngc + '등록'+ '휴학' + '등록' + '휴학'\n\n\n        나이: 22.0 \n        학기: 2 \n        \n\n\n- Ver2"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#ex2.-__add__-super",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#ex2.-__add__-super",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "ex2. __add__ + super",
    "text": "ex2. __add__ + super\n\nclass s_2(s):\n    def __add__(self,registration_status): \n        if registration_status=='휴학': \n            self.age = self.age+0.5\n        elif registration_status=='등록':\n            self.age = self.age+0.5 \n            self.semester = self.semester+1 \n        return self\n\n\ngc = s_2()\n\n입학을 축하합니다. 당신의 나이는 20.0이고 현재 학기는 0학기 입니다.\n\n\n\ngc + '등록'+ '휴학' + '등록' + '휴학'\n\n\n        나이: 22.0 \n        학기: 2"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#motive-1",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#motive-1",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "motive",
    "text": "motive\n\na = [1,2,3]\na\n\n[1, 2, 3]\n\n\n\na[0]\n\n1\n\n\n\na.__getitem__(0)\n\n1"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#ex1.-rps가위바위보",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#ex1.-rps가위바위보",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "ex1. RPS(가위바위보)",
    "text": "ex1. RPS(가위바위보)\n\nclass RPS:\n    def __init__(self,candidate):\n        self.candidate = candidate\n        self.actions = list() \n    def pick(self):\n        self.actions.append(np.random.choice(self.candidate))        \n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt;\n        기록: {}\n        \"\"\"        \n        return html_str.format(self.candidate,self.actions)\n\n\na = RPS([\"가위\",\"바위\",\"보\"])\n\n\na.pick()\na.pick()\n\n\na.actions\n\n['바위', '보']\n\n\n\na[0], a[1]\n\nTypeError: 'RPS' object is not subscriptable\n\n\n- 위를 에러가 나지않고 리스트처럼 인덱싱 했으면 좋겠음"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#ex2.-리스트-인덱싱-기능-추가",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#ex2.-리스트-인덱싱-기능-추가",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "ex2. 리스트 인덱싱 기능 추가",
    "text": "ex2. 리스트 인덱싱 기능 추가\n\nclass RPS_Ver2(RPS):\n    def __getitem__(self,item):\n        return self.actions[item]\n\n\na = RPS_Ver2(['가위','바위','보'])\n\n\na.pick()\na.pick()\na.pick()\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        기록: ['가위', '가위', '가위']\n        \n\n\n\na[0]\n\n'가위'"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#motive-2",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#motive-2",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "motive",
    "text": "motive\n\na = RPS_Ver2(['가위','바위'])\na\n\n\n        낼 수 있는 패: ['가위', '바위'] \n        기록: []\n        \n\n\n\na.pick()\na.pick()\na\n\n\n        낼 수 있는 패: ['가위', '바위'] \n        기록: ['가위', '바위']\n        \n\n\n\na[0]\n\n'가위'\n\n\n\na[0] = '보' \n\nTypeError: 'RPS_Ver2' object does not support item assignment\n\n\n- 만약 실수로 기록할 때와 같은 경우가 있을경우 위처럼 수정이 필요할 수도 있겠다.\n- 리스트 예제 관찰\n\nl = [1,2]\nl\n\n[1, 2]\n\n\n\nl[0]=2\n\n\nl\n\n[2, 2]\n\n\n\nl.__setitem__(0,1)\n\n\nl\n\n[1, 2]"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#ex1.-값을-수정하는-메소드-구현",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#ex1.-값을-수정하는-메소드-구현",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "ex1. 값을 수정하는 메소드 구현",
    "text": "ex1. 값을 수정하는 메소드 구현\n\nclass RPS_Ver3(RPS_Ver2):\n    def __setitem__(self,index,val):\n        self.actions[index] = val\n\n\na=RPS_Ver3(['가위','바위','보'])\n\n\na.pick()\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        기록: ['보']\n        \n\n\n\na[0] = \"가위\"\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        기록: ['가위']"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#motive1",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#motive1",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "motive1",
    "text": "motive1\n\n가위===가위이면 True가 나왔으면 좋겠음.\n\n\n관찰 : __eq__\n\na = 1\na\n\n1\n\n\n\na==1\n\nTrue\n\n\n\na.__eq__(1)\n\nTrue\n\n\n\na==1은 a.__eq__(1)의 축약버전이다.\n\n\n\n구현 : __eq__\n\nclass RPS_Ver5(RPS_Ver4):\n    def __eq__(self,other):\n        return self[-1] == other[-1]\n\n\na = RPS_Ver5(['가위','바위'])\nb = RPS_Ver5(['가위','바위'])\n\n- 1회 대결\n\na.pick()\nb.pick()\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위'] \n        기록: ['가위']\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['가위', '바위'] \n        기록: ['바위']\n        \n\n\n\na == b\n\nFalse\n\n\n- 2회 대결\n\na.pick()\nb.pick()\n\n\na == b\n\nFalse\n\n\n- 3회 대결\n\na.pick()\nb.pick()\n\n\na == b\n\nTrue"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#motive2",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#motive2",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "motive2",
    "text": "motive2\n\na[-1], b[-1]\n\n('가위', '가위')\n\n\n\na &gt; b\n\nTypeError: '&gt;' not supported between instances of 'RPS_Ver5' and 'RPS_Ver5'\n\n\n- 목표 : False가 나왔으면 좋겠음\n\n관찰 : __gt__\n\na = 1\nb = 2\n\n\na&gt;1, a.__gt__(1), b&gt;1, b.__gt__(1)\n\n(False, False, True, True)\n\n\n\na.__gt__(1)\n\nFalse\n\n\n\n\n예비학습\n\na = RPS_Ver5(['가위','바위'])\nb = RPS_Ver5(['가위','바위'])\n\n\nfor i in range(3) :\n    a.pick()\n    b.pick()\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위'] \n        기록: ['가위', '바위', '바위']\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['가위', '바위'] \n        기록: ['가위', '가위', '바위']\n        \n\n\n\na[-1], b[-1]\n\n('바위', '바위')\n\n\n- 이기는 요소를 각 리스트 원소에 왼쪽에 배치\n\n[a[-1],b[-1]] in [['가위','보'],['바위','가위'],['보','바위']]\n\nFalse\n\n\n\n\n구현 : __gt__\n\nclass RPS_Ver6(RPS_Ver5):\n    def __gt__(self,other):\n        return [self[-1],other[-1]] in [['가위','보'],['바위','가위'],['보','바위']]\n\n\na = RPS_Ver6(['가위','바위','보'])\nb = RPS_Ver6(['가위','바위','보'])\n\n- 1회 대결\n\na.pick()\nb.pick()\n\n\na[-1],b[-1]\n\n('바위', '가위')\n\n\n\na&gt;b\n\nTrue\n\n\n- 2회 대결\n\na.pick()\nb.pick()\n\n\na[-1],b[-1]\n\n('바위', '바위')\n\n\n\na&gt;b\n\nFalse"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#motive3",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#motive3",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "motive3",
    "text": "motive3\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        기록: ['바위', '바위']\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        기록: ['가위', '바위']\n        \n\n\n- True로 나왔으면 좋겠음\n\na&gt;=b\n\nTypeError: '&gt;=' not supported between instances of 'RPS_Ver6' and 'RPS_Ver6'\n\n\n\n구현\n- 비교 연산자 정리\n\n\n\n특수 메소드\n의미\n\n\n\n\n__eq__\nself == other\n\n\n__gt__\nself &gt; other\n\n\n__lt__\nself &lt; other\n\n\n__ge__\nself &gt;= other\n\n\n__le__\nself &lt;= other\n\n\n\n\nclass RPS_Ver7(RPS_Ver6):\n    def __ge__(self,other):\n        return (self == other) or (self &gt; other)\n    def __lt__(self,other):\n        return not (self &gt;= other)\n    def __le__(self,other):\n        return (self == other) or (self &lt; other)\n\n\na = RPS_Ver7(['가위','바위','보'])\nb = RPS_Ver7(['가위','바위','보'])\n\n\na.pick()\nb.pick()\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        기록: ['가위']\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        기록: ['가위']\n        \n\n\n\na==b, a&gt;b, a&lt;b, a&gt;=b, a&lt;=b \n\n(True, False, False, True, True)"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#함수",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#함수",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "함수",
    "text": "함수\n\ndef f() :\n    return \"test\"\n\n\nf()\n\n'test'\n\n\n\n?f\n\n\nSignature: f()\nDocstring: &lt;no docstring&gt;\nFile:      c:\\users\\user\\appdata\\local\\temp\\ipykernel_12160\\3092864090.py\nType:      function\n\n\n\n- 함수 f의 type은 function이다.\n- 즉, 우리가 만든 f는 function이라는 클래스의 오브젝트에 불과하다."
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#ex1-x-a2",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#ex1-x-a2",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "ex1) \\((x-a)^2\\)",
    "text": "ex1) \\((x-a)^2\\)\n\n풀이 1. 중첩\n\ndef f(a) :\n    def _f(x) :\n        return (x-a)**2\n    return _f\n\n\ng = f(10)\n\n\ng(2)\n\n64\n\n\n\n\n풀이 2. lambda\n\ndef f(a) :\n    _f = lambda x : (x-a)**2\n    return _f\n\n\ng = f(10)\n\n\ng(2)\n\n64"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#ex2-fprimex",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#ex2-fprimex",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "ex2) \\(f\\prime(x)\\)",
    "text": "ex2) \\(f\\prime(x)\\)\n\n풀이 1. 기본 함수\n\ndef f(x): \n    return x**2 \n\n\ndef d(f,x): \n    h=0.000000000001\n    return (f(x+h)-f(x))/h \n\n\nd(f,4) # f'(4) = 2*4 = 8\n\n8.000711204658728\n\n\n\n\n풀이 2. 함수 중첩\n\ndef f(x): \n    return x**2 \n\n\ndef d(f): \n    def df(x):  \n        h=0.000000000001\n        return (f(x+h)-f(x))/h \n    return df\n\n\nff = d(f)\n\n\nff(4)\n\n8.000711204658728\n\n\n\n\n풀이3. lambda\n\ndef f(x): \n    return x**2 \n\n\ndef dl(f) :\n    h=0.000000000001\n    return lambda x :  (f(x+h)-f(x))/h\n\n\ndd = dl(f)\n\n\ndd(4)\n\n8.000711204658728\n\n\n\n\n시각화\n\nx = np.linspace(-1,1,100)\n\n\nplt.plot(x,f(x),label = r\"$f(x)=x^2$\")\nplt.plot(x,ff(x),label = r\"$f^{\\,\\prime}(x)=2x$\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x1e2d16cbdd0&gt;"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#ex3-함수들의-리스트",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#ex3-함수들의-리스트",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "ex3) 함수들의 리스트",
    "text": "ex3) 함수들의 리스트\n\nflst = [lambda x: x, lambda x: x**2, lambda x: x**3] \nflst\n\n[&lt;function __main__.&lt;lambda&gt;(x)&gt;,\n &lt;function __main__.&lt;lambda&gt;(x)&gt;,\n &lt;function __main__.&lt;lambda&gt;(x)&gt;]\n\n\n\nfor f in flst : \n    print(f(2))\n\n2\n4\n8\n\n\n\nfor f in flst : \n    plt.plot(x,f(x),\"--\")"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#call__",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#call__",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "__call__",
    "text": "__call__\n\nf = lambda x : x+1\n\n\nf(3)\n\n4\n\n\n\nf.__call__(3)\n\n4\n\n\n\nf(3)은 f__call__(3)의 축약버전이다.\n\n\n관찰\n- 함수처럼 쓸 수없는 인스턴스는 단지 __call__이 없을뿐이다. \\(\\to\\) ‘A’ object is not callable\n\nclass A():\n    def __init__(self) :\n        self.n = \"강철\"\n\n\na = A()\n\n\na()\n\nTypeError: 'A' object is not callable\n\n\n- 위 코드 수정\n\nclass A2(A):\n    def __call__(self) :\n        print(self.n)\n\n\na = A2()\n\n\na()\n\n강철"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#구현-self.상수-상수",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#구현-self.상수-상수",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "구현 : self.상수 + 상수",
    "text": "구현 : self.상수 + 상수\n\nclass add1:\n    def __init__(self,c) :\n        self.c = c\n    def __call__(self,x) :\n        return self.c + x\n\n\na = add1(3)\n\n\na(5)\n\n8\n\n\n\na(10)\n\n13"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#관찰-1.-iter",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#관찰-1.-iter",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "관찰 1. iter",
    "text": "관찰 1. iter\n- 아래 ???의 자리에 올수 있는것은 dir(?)하여 set(dir(lst)) & {'__iter__'} 가 있는 오브젝트이다\nfor i in ???:\n    print(i)\n\nlst = [1,2,3]\n\n\nlst = [1,2,3]\nset(dir(lst)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\ntpl = 1,2,3\nset(dir(tpl)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\nstring = '123'\nset(dir(string)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\na = 5\nset(dir(a)) & {'__iter__'}\n\nset()"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#관찰2.-next",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#관찰2.-next",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "관찰2. next",
    "text": "관찰2. next\n\n__next__의 기능은 \\(\\to\\) iterable 객체들의 원소들을 차례대로 꺼내준다\n더이상 꺼낼 원소가 없으면 Stopiteration Error를 발생시킴\n\n\nlst = [1,2,3].__iter__()\n\n\nnext(lst)\n\n1\n\n\n\nnext(lst)\n\n2\n\n\n\nnext(lst)\n\n3\n\n\n- 원소가 3개이기 때문에 3번이상 명령어 입력시 에러 발생\n\nnext(lst)\n\nStopIteration:"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#관찰-3.-데이터-프레임",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#관찰-3.-데이터-프레임",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "관찰 3. 데이터 프레임",
    "text": "관찰 3. 데이터 프레임\n\ndf = pd.DataFrame({'x':[1,2,3],'y':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\nset(dir(df)) & {'__iter__'}\n\n{'__iter__'}\n\n\n- 데이터 프레임의 경우는 루프문 수행 시 컬럼 네임을 출력해준다.\n\nfor i in df :\n    print(i)\n\nx\ny\n\n\n- next 함수 적용\n\n_df = iter(df)\n\n\nnext(_df)\n\n'x'\n\n\n\nnext(_df)\n\n'y'\n\n\n- 컬럼이 2개 밖에 없어 에러 발생\n\nnext(_df)\n\nStopIteration:"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#구현-1",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#구현-1",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "구현",
    "text": "구현\n- 가찌를 내는 순간 for문이 멈추도록 하는 이터레이터를 만들자\n\nclass RPS_ITERATOR: # 찌를 내는순간 for문이 멈추도록 하는 이터레이터를 만들자\n    def __init__(self): \n        self.candidate = [\"묵\",\"찌\",\"빠\"] \n    def __iter__(self):\n        return self \n    def __next__(self):\n        action = np.random.choice(self.candidate) ## 묵찌빠에서 랜덤 선택\n        if action == \"찌\":\n            print(\"찌가 나와서 for문을 멈춥니다\")\n            raise StopIteration\n        else:\n            return action\n\n\na = RPS_ITERATOR()\n\n\nnext(a)\n\n찌가 나와서 for문을 멈춥니다\n\n\nStopIteration: \n\n\n\nnext(a)\n\n'묵'\n\n\n\nnext(a)\n\n'빠'\n\n\n\nfor i in a :\n    print (i)\n\n묵\n빠\n빠\n빠\n찌가 나와서 for문을 멈춥니다"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#range-생략",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#range-생략",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "range (생략)",
    "text": "range (생략)"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#zip",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#zip",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "zip",
    "text": "zip\n- 이터레이터엔 개념을 알면 for문에 대한 이해도가 대폭 상승한다.\n\n관찰 1. zip?\n- 일단 zip으로 만든 객체가 iterable object인지 확인하자\n\ntemp = zip([1,2,3],\"abc\")\n\n\ntemp\n\n&lt;zip at 0x1e2d53d2f00&gt;\n\n\n\nset(dir(temp)) & {\"__iter__\",\"__next__\"}\n\n{'__iter__', '__next__'}\n\n\n- 오 2가지 다있는 것을 보니 temp는 iterable 오브젝트이다.\n\nnext(temp)\n\n(1, 'a')\n\n\n\nnext(temp)\n\n(2, 'b')\n\n\n\nnext(temp)\n\n(3, 'c')\n\n\n- 훗, 무슨 느낌인지 알겠음\n\nnext(temp)\n\nStopIteration: \n\n\n\n\n관찰 2. 그래서 뭐하는 문법인가?\n\nzip?\n\n\nInit signature: zip(self, /, *args, **kwargs)\nDocstring:     \nzip(*iterables, strict=False) --&gt; Yield tuples until an input is exhausted.\n   &gt;&gt;&gt; list(zip('abcdefg', range(3), range(4)))\n   [('a', 0, 0), ('b', 1, 1), ('c', 2, 2)]\nThe zip object yields n-length tuples, where n is the number of iterables\npassed as positional arguments to zip().  The i-th element in every tuple\ncomes from the i-th iterable argument to zip().  This continues until the\nshortest argument is exhausted.\nIf strict is true and one of the arguments is exhausted before the others,\nraise a ValueError.\nType:           type\nSubclasses:     \n\n\n\n- 음, 위에 나온 예제를 싱행해보니 무슨 느낌인지 알겠음\n\nlist(zip('abcdefg', range(3), range(4)))\n\n[('a', 0, 0), ('b', 1, 1), ('c', 2, 2)]\n\n\n\n\n관찰 3. enumerate\n- 이녀석도 이터러블 객체이다.\n\nset(dir(enumerate('abc'))) & {'__iter__', '__next__'}\n\n{'__iter__', '__next__'}\n\n\n- enumerate는 원소와 인덱스를 반환해준다.\n\nfor i in enumerate(list(\"abc\")) :\n    print(i)\n\n(0, 'a')\n(1, 'b')\n(2, 'c')"
  },
  {
    "objectID": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#summary",
    "href": "posts/DX/00. 데이터 다루기/extra/2023-08-14-Extra 03. 클래스 탐구 (2).html#summary",
    "title": "Extra 03. 클래스 탐구 (2)",
    "section": "summary",
    "text": "summary\n- for문을 사용하려면 “iterable object” 여야만 한다."
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html",
    "title": "00. numpy & pandas (1)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html#차원-배열의-선언",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html#차원-배열의-선언",
    "title": "00. numpy & pandas (1)",
    "section": "(1) 1차원 배열의 선언",
    "text": "(1) 1차원 배열의 선언\n- 튜플\n\nnp.array((1,2,3))\n\narray([1, 2, 3])\n\n\n- 리스트\n\nnp.array([1,2,3])\n\narray([1, 2, 3])\n\n\n- range\n\nnp.array(range(10))\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\n알아두면 좋은 1차원 배열 선언\n\nnp.zeros(3)\n\narray([0., 0., 0.])\n\n\n\nnp.ones(3)\n\narray([1., 1., 1.])\n\n\n- np.linspace(2,8,4) : 2~8까지, 4개의 원소를 생성\n\nnp.linspace(2,8,4)\n\narray([2., 4., 6., 8.])\n\n\n- np.arange(start,end) : nump버전 range라고 생각하자!\n\nnp.arange(5), np.arange(1,6)\n\n(array([0, 1, 2, 3, 4]), array([1, 2, 3, 4, 5]))\n\n\n\n\n주의 1. 배열의 자료형\n- 숫자 \\(\\to\\) 문자\n\na = np.array([1,\"A\",3.0])\na\n\narray(['1', 'A', '3'], dtype='&lt;U11')\n\n\n- 정수 \\(\\to\\) 실수\n\na = np.array([1,2,3.0])\na\n\narray([1., 2., 3.])"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html#차원-배열의-선언-1",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html#차원-배열의-선언-1",
    "title": "00. numpy & pandas (1)",
    "section": "(2) 2차원 배열의 선언",
    "text": "(2) 2차원 배열의 선언\n\nlist\n\na1 = [[1,2,3],\n     [4,5,6]]\n\nnp.array(a1)\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\n\n\nstack\n\na1 = np.random.randint(0,9,3)\na2 = np.random.randint(0,9,3)\n\nA1 = np.stack([a1,a2],axis=0) \nA1, A1.shape\n\n(array([[0, 4, 2],\n        [3, 2, 3]]),\n (2, 3))\n\n\n\nA2 = np.stack([a1,a2],axis=1) \nA2, A2.shape\n\n(array([[0, 3],\n        [4, 2],\n        [2, 3]]),\n (3, 2))"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html#차원-배열의-선언-2",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html#차원-배열의-선언-2",
    "title": "00. numpy & pandas (1)",
    "section": "(3) 3차원 배열의 선언",
    "text": "(3) 3차원 배열의 선언\n\nlist\n\na = [[[1,2],[3,4]],[[5,6],[7,8]]]\nnp.array(a), np.array(a).shape\n\n(array([[[1, 2],\n         [3, 4]],\n \n        [[5, 6],\n         [7, 8]]]),\n (2, 2, 2))\n\n\n\n\nstack + reshape\n\na1 = np.random.randint(0,9,4)\na2 = np.random.randint(0,9,4)\na3 = np.random.randint(0,9,4)\n\nA1 = np.stack([a1,a2,a3],axis=0).reshape(2,3,2)\nA1, A1.shape\n\n(array([[[8, 8],\n         [4, 0],\n         [6, 8]],\n \n        [[8, 4],\n         [0, 1],\n         [1, 6]]]),\n (2, 3, 2))\n\n\n\nA1.dtype\n\ndtype('int32')"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html#행-조회",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html#행-조회",
    "title": "00. numpy & pandas (1)",
    "section": "행 조회",
    "text": "행 조회\n\nA1[0:2,]\n\narray([[8, 8, 7],\n       [3, 1, 1]])\n\n\n\nA1[[0,1,2]]\n\narray([[8, 8, 7],\n       [3, 1, 1],\n       [3, 5, 7]])\n\n\n\nA1[[0,1,2], : ]\n\narray([[8, 8, 7],\n       [3, 1, 1],\n       [3, 5, 7]])"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html#열-조회",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html#열-조회",
    "title": "00. numpy & pandas (1)",
    "section": "열 조회",
    "text": "열 조회\n\nA1[:,0:2]\n\narray([[8, 8],\n       [3, 1],\n       [3, 5],\n       [7, 8]])\n\n\n\nA1[:,[0,1,2]]\n\narray([[8, 8, 7],\n       [3, 1, 1],\n       [3, 5, 7],\n       [7, 8, 6]])"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html#행열-조회",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html#행열-조회",
    "title": "00. numpy & pandas (1)",
    "section": "행,열 조회",
    "text": "행,열 조회\n\nA1[0:2,0:2]\n\narray([[8, 8],\n       [3, 1]])"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html#stride-start-end-interval",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html#stride-start-end-interval",
    "title": "00. numpy & pandas (1)",
    "section": "stride( start :end : interval)",
    "text": "stride( start :end : interval)\n\nA1\n\narray([[8, 8, 7],\n       [3, 1, 1],\n       [3, 5, 7],\n       [7, 8, 6]])\n\n\n\nA1[::2, ::2]\n\narray([[8, 7],\n       [3, 7]])"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html#특정값을-지정하여-접근",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html#특정값을-지정하여-접근",
    "title": "00. numpy & pandas (1)",
    "section": "특정값을 지정하여 접근",
    "text": "특정값을 지정하여 접근\n\nA1\n\narray([[8, 8, 7],\n       [3, 1, 1],\n       [3, 5, 7],\n       [7, 8, 6]])\n\n\n\nA1[A1==1]\n\narray([1, 1])\n\n\n\nA1[A1==7]\n\narray([7, 7, 7])"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html#기본-연산",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html#기본-연산",
    "title": "00. numpy & pandas (1)",
    "section": "기본 연산",
    "text": "기본 연산\n\na = np.array([1,2,3])\n\n\n\nCode\nprint(f'''\n1. 사칙연산 \n\na + 1 = {a+1}\na - 2 = {a-2}\na x 2 = {a*2}\na / 2 = {a/2}\na // 2 = {a//2}\na % 2 = {a % 2}\n\n=======================================\n\n2. 거듭제곱, 로그, 지수, 삼각함수\n\na^2 = {a**2}\nsqrt(a) = {np.round(np.sqrt(a),2)}\nlog(a) = {np.round(np.log(a),2)}\nexp(a) = {np.round(np.exp(a),2)}\nsin(a),cos(a) = {np.round(np.sin(a),2),np.round(np.cos(a),2)}\n''')\n\n\n\n1. 사칙연산 \n\na + 1 = [2 3 4]\na - 2 = [-1  0  1]\na x 2 = [2 4 6]\na / 2 = [0.5 1.  1.5]\na // 2 = [0 1 1]\na % 2 = [1 0 1]\n\n=======================================\n\n2. 거듭제곱, 로그, 지수, 삼각함수\n\na^2 = [1 4 9]\nsqrt(a) = [1.   1.41 1.73]\nlog(a) = [0.   0.69 1.1 ]\nexp(a) = [ 2.72  7.39 20.09]\nsin(a),cos(a) = (array([0.84, 0.91, 0.14]), array([ 0.54, -0.42, -0.99]))"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html#기타-연산전치행렬역행렬",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-18-00. numpy & pandas (1).html#기타-연산전치행렬역행렬",
    "title": "00. numpy & pandas (1)",
    "section": "기타 연산(전치행렬,역행렬)",
    "text": "기타 연산(전치행렬,역행렬)\n\na1 = np.random.randint(0,9,3)\na2 = np.random.randint(0,9,3)\na3 = np.random.randint(0,9,3)\n\nA1 = np.stack([a1,a2,a3],axis=0) \nA1\n\narray([[8, 1, 1],\n       [2, 1, 4],\n       [4, 4, 1]])\n\n\n- 전치행렬\n\nA1.T\n\narray([[8, 2, 4],\n       [1, 1, 4],\n       [1, 4, 1]])\n\n\n- 역행렬\n\nnp.linalg.inv(A1)\n\narray([[ 0.14705882, -0.02941176, -0.02941176],\n       [-0.1372549 , -0.03921569,  0.29411765],\n       [-0.03921569,  0.2745098 , -0.05882353]])\n\n\n- 행렬연산\n\\[\\bf A^{-1} \\times A = {I}\\]\n\\[\\bf {I} = \\begin{bmatrix} 1 & 0 & \\dots & 0 \\\\\n                                                     0 & 1 & \\dots & 0 \\\\\n                                                     \\dots &\\dots &\\dots &\\dots \\\\\n                                                     0 & 0 & \\dots & 1\n                                                        \\end{bmatrix}\\]\n\nnp.round(A1 @ np.linalg.inv(A1),2)\n\narray([[ 1.,  0.,  0.],\n       [ 0.,  1., -0.],\n       [ 0.,  0.,  1.]])"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-21-02. numpy & pandas (3).html",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-21-02. numpy & pandas (3).html",
    "title": "02. numpy & pandas (3)",
    "section": "",
    "text": "import plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-21-02. numpy & pandas (3).html#plotly-test",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-21-02. numpy & pandas (3).html#plotly-test",
    "title": "02. numpy & pandas (3)",
    "section": "plotly test",
    "text": "plotly test\n\ntip.plot.box(backend = \"plotly\",\n            x = \"time\",\n            y = \"tip\", color = \"sex\")"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-21-02. numpy & pandas (3).html#열-조회",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-21-02. numpy & pandas (3).html#열-조회",
    "title": "02. numpy & pandas (3)",
    "section": "열 조회",
    "text": "열 조회\n\ntip.loc[:,[\"total_bill\"]].head()\n\n\n\n\n\n\n\n\ntotal_bill\n\n\n\n\n0\n16.99\n\n\n1\n10.34\n\n\n2\n21.01\n\n\n3\n23.68\n\n\n4\n24.59\n\n\n\n\n\n\n\n\ntip.loc[:,[\"tip\",\"total_bill\"]].head()\n\n\n\n\n\n\n\n\ntip\ntotal_bill\n\n\n\n\n0\n1.01\n16.99\n\n\n1\n1.66\n10.34\n\n\n2\n3.50\n21.01\n\n\n3\n3.31\n23.68\n\n\n4\n3.61\n24.59\n\n\n\n\n\n\n\n\ntip.loc[:,[\"tip\",\"day\",\"total_bill\"]].sort_values(\"tip\",ascending=False)\n\n\n\n\n\n\n\n\ntip\nday\ntotal_bill\n\n\n\n\n170\n10.00\nSat\n50.81\n\n\n212\n9.00\nSat\n48.33\n\n\n23\n7.58\nSat\n39.42\n\n\n59\n6.73\nSat\n48.27\n\n\n141\n6.70\nThur\n34.30\n\n\n...\n...\n...\n...\n\n\n0\n1.01\nSun\n16.99\n\n\n236\n1.00\nSat\n12.60\n\n\n111\n1.00\nSat\n7.25\n\n\n67\n1.00\nSat\n3.07\n\n\n92\n1.00\nFri\n5.75\n\n\n\n\n244 rows × 3 columns\n\n\n\n- 인덱스 제거\n\ntip.loc[:,[\"tip\",\"day\",\"total_bill\"]].\\\n        sort_values(\"tip\",ascending=False).\\\n            reset_index(drop=True)\n\n\n\n\n\n\n\n\ntip\nday\ntotal_bill\n\n\n\n\n0\n10.00\nSat\n50.81\n\n\n1\n9.00\nSat\n48.33\n\n\n2\n7.58\nSat\n39.42\n\n\n3\n6.73\nSat\n48.27\n\n\n4\n6.70\nThur\n34.30\n\n\n...\n...\n...\n...\n\n\n239\n1.01\nSun\n16.99\n\n\n240\n1.00\nSat\n12.60\n\n\n241\n1.00\nSat\n7.25\n\n\n242\n1.00\nSat\n3.07\n\n\n243\n1.00\nFri\n5.75\n\n\n\n\n244 rows × 3 columns\n\n\n\n- 열 범위 조회\n\ntip.head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\ntip.loc[:,\"tip\":\"day\"].head()\n\n\n\n\n\n\n\n\ntip\nsex\nsmoker\nday\n\n\n\n\n0\n1.01\nFemale\nNo\nSun\n\n\n1\n1.66\nMale\nNo\nSun\n\n\n2\n3.50\nMale\nNo\nSun\n\n\n3\n3.31\nMale\nNo\nSun\n\n\n4\n3.61\nFemale\nNo\nSun"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-21-02. numpy & pandas (3).html#행-조회",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-21-02. numpy & pandas (3).html#행-조회",
    "title": "02. numpy & pandas (3)",
    "section": "행 조회",
    "text": "행 조회\n\n단일 조건 조회\n\ntip.loc[tip.tip&gt;6.0,:]\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n23\n39.42\n7.58\nMale\nNo\nSat\nDinner\n4\n\n\n59\n48.27\n6.73\nMale\nNo\nSat\nDinner\n4\n\n\n141\n34.30\n6.70\nMale\nNo\nThur\nLunch\n6\n\n\n170\n50.81\n10.00\nMale\nYes\nSat\nDinner\n3\n\n\n183\n23.17\n6.50\nMale\nYes\nSun\nDinner\n4\n\n\n212\n48.33\n9.00\nMale\nNo\nSat\nDinner\n4\n\n\n214\n28.17\n6.50\nFemale\nYes\nSat\nDinner\n3\n\n\n\n\n\n\n\n\ntip.loc[map(lambda x : x &gt;6.0,tip.tip),:]\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n23\n39.42\n7.58\nMale\nNo\nSat\nDinner\n4\n\n\n59\n48.27\n6.73\nMale\nNo\nSat\nDinner\n4\n\n\n141\n34.30\n6.70\nMale\nNo\nThur\nLunch\n6\n\n\n170\n50.81\n10.00\nMale\nYes\nSat\nDinner\n3\n\n\n183\n23.17\n6.50\nMale\nYes\nSun\nDinner\n4\n\n\n212\n48.33\n9.00\nMale\nNo\nSat\nDinner\n4\n\n\n214\n28.17\n6.50\nFemale\nYes\nSat\nDinner\n3\n\n\n\n\n\n\n\n\n\n여러 조건 조회\n- map(lambda)를 사용하지 않고 데이터 프레임을 필터링 할 때는 and 대신 &연산자를 사용해야함.\n\nor 도 | 으로 사용해야 한다.\n\n\ntip.loc[(tip.tip&gt;6.0) & (tip.day == \"Sat\"),:]\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n23\n39.42\n7.58\nMale\nNo\nSat\nDinner\n4\n\n\n59\n48.27\n6.73\nMale\nNo\nSat\nDinner\n4\n\n\n170\n50.81\n10.00\nMale\nYes\nSat\nDinner\n3\n\n\n212\n48.33\n9.00\nMale\nNo\nSat\nDinner\n4\n\n\n214\n28.17\n6.50\nFemale\nYes\nSat\nDinner\n3\n\n\n\n\n\n\n\n\ntip.loc[(tip.tip&gt;6.0) & (tip.day == \"Sat\")]\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n23\n39.42\n7.58\nMale\nNo\nSat\nDinner\n4\n\n\n59\n48.27\n6.73\nMale\nNo\nSat\nDinner\n4\n\n\n170\n50.81\n10.00\nMale\nYes\nSat\nDinner\n3\n\n\n212\n48.33\n9.00\nMale\nNo\nSat\nDinner\n4\n\n\n214\n28.17\n6.50\nFemale\nYes\nSat\nDinner\n3\n\n\n\n\n\n\n\n\ntip.loc[(tip.tip&gt;6.0) | (tip.day == \"Sat\")]\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n19\n20.65\n3.35\nMale\nNo\nSat\nDinner\n3\n\n\n20\n17.92\n4.08\nMale\nNo\nSat\nDinner\n2\n\n\n21\n20.29\n2.75\nFemale\nNo\nSat\nDinner\n2\n\n\n22\n15.77\n2.23\nFemale\nNo\nSat\nDinner\n2\n\n\n23\n39.42\n7.58\nMale\nNo\nSat\nDinner\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n238\n35.83\n4.67\nFemale\nNo\nSat\nDinner\n3\n\n\n239\n29.03\n5.92\nMale\nNo\nSat\nDinner\n3\n\n\n240\n27.18\n2.00\nFemale\nYes\nSat\nDinner\n2\n\n\n241\n22.67\n2.00\nMale\nYes\nSat\nDinner\n2\n\n\n242\n17.82\n1.75\nMale\nNo\nSat\nDinner\n2\n\n\n\n\n89 rows × 7 columns\n\n\n\n\ntip.loc[map(lambda x,y : (x&gt;6.0) and (y == \"Sat\"),tip.tip, tip.day)]\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n23\n39.42\n7.58\nMale\nNo\nSat\nDinner\n4\n\n\n59\n48.27\n6.73\nMale\nNo\nSat\nDinner\n4\n\n\n170\n50.81\n10.00\nMale\nYes\nSat\nDinner\n3\n\n\n212\n48.33\n9.00\nMale\nNo\nSat\nDinner\n4\n\n\n214\n28.17\n6.50\nFemale\nYes\nSat\nDinner\n3\n\n\n\n\n\n\n\n\ntip.loc[map(lambda x,y : (x&gt;6.0) or (y == \"Sat\"),tip.tip, tip.day)]\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n19\n20.65\n3.35\nMale\nNo\nSat\nDinner\n3\n\n\n20\n17.92\n4.08\nMale\nNo\nSat\nDinner\n2\n\n\n21\n20.29\n2.75\nFemale\nNo\nSat\nDinner\n2\n\n\n22\n15.77\n2.23\nFemale\nNo\nSat\nDinner\n2\n\n\n23\n39.42\n7.58\nMale\nNo\nSat\nDinner\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n238\n35.83\n4.67\nFemale\nNo\nSat\nDinner\n3\n\n\n239\n29.03\n5.92\nMale\nNo\nSat\nDinner\n3\n\n\n240\n27.18\n2.00\nFemale\nYes\nSat\nDinner\n2\n\n\n241\n22.67\n2.00\nMale\nYes\nSat\nDinner\n2\n\n\n242\n17.82\n1.75\nMale\nNo\nSat\nDinner\n2\n\n\n\n\n89 rows × 7 columns\n\n\n\n\n\nisin(),between()\n\ntip.loc[tip.day.isin([\"Sat\",\"Sun\"]), :].head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n- between(\\(x_1,x_2\\)) \\(\\to x_1 \\leq X \\leq x_2\\)\n\ntip.loc[tip.tip.between(1,2)].head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n6\n8.77\n2.00\nMale\nNo\nSun\nDinner\n2\n\n\n8\n15.04\n1.96\nMale\nNo\nSun\nDinner\n2\n\n\n10\n10.27\n1.71\nMale\nNo\nSun\nDinner\n2"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-21-02. numpy & pandas (3).html#특정-열의-합-구하기",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-21-02. numpy & pandas (3).html#특정-열의-합-구하기",
    "title": "02. numpy & pandas (3)",
    "section": "특정 열의 합 구하기",
    "text": "특정 열의 합 구하기\n- total_bill의 합계 구하기\n\ntip.total_bill.sum()\n\n4827.77\n\n\n- 두 열의 합계\n\ntip[[\"total_bill\",\"tip\"]].sum()\n\ntotal_bill    4827.77\ntip            731.58\ndtype: float64"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-21-02. numpy & pandas (3).html#특정-범주형-변수를-기준으로-합을-구하기",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-21-02. numpy & pandas (3).html#특정-범주형-변수를-기준으로-합을-구하기",
    "title": "02. numpy & pandas (3)",
    "section": "특정 범주형 변수를 기준으로 합을 구하기",
    "text": "특정 범주형 변수를 기준으로 합을 구하기\n- 방법 1\n\nas_index =True \\(\\to\\) 집계 기준이 되는 열이 인덱스로 설정됨\n\n\ntip.groupby(\"day\",as_index = True)[\"tip\"].sum()\n\nday\nFri      51.96\nSat     260.40\nSun     247.39\nThur    171.83\nName: tip, dtype: float64\n\n\n- 방법 2\n\ntip.groupby(\"day\",as_index = False)[\"tip\"].sum()\n\n\n\n\n\n\n\n\nday\ntip\n\n\n\n\n0\nFri\n51.96\n\n\n1\nSat\n260.40\n\n\n2\nSun\n247.39\n\n\n3\nThur\n171.83\n\n\n\n\n\n\n\n- 방법 3\n\ntip.groupby(\"day\").agg({\"tip\" : sum}).reset_index()\n\n\n\n\n\n\n\n\nday\ntip\n\n\n\n\n0\nFri\n51.96\n\n\n1\nSat\n260.40\n\n\n2\nSun\n247.39\n\n\n3\nThur\n171.83\n\n\n\n\n\n\n\n\n데이터 프레임으로 선언\n\ntip_sum = tip.groupby(\"day\").agg({\"tip\" : sum}).reset_index()\n\n\ntip_sum\n\n\n\n\n\n\n\n\nday\ntip\n\n\n\n\n0\nFri\n51.96\n\n\n1\nSat\n260.40\n\n\n2\nSun\n247.39\n\n\n3\nThur\n171.83\n\n\n\n\n\n\n\n\n\n집계 결과 시각화\n\nplt.figure(figsize=(4,2))\nplt.bar(x=tip_sum.day, height =tip_sum.tip)\nplt.title(\"Tips by day\",size =15,fontweight=\"bold\")\nplt.xlabel(\"Day\")\nplt.ylabel(\"tip\")\nplt.grid(axis=\"y\")\n\n\n\n\n- 가로 막대\n\nplt.figure(figsize=(4,2))\nplt.barh(y=tip_sum.day, width =tip_sum.tip)\nplt.title(\"Tips by day\",size =15,fontweight=\"bold\")\nplt.xlabel(\"Day\")\nplt.ylabel(\"tip\")\nplt.grid(axis=\"y\")\n\n\n\n\n- 색상 변경\n\nplt.figure(figsize=(4,2))\nplt.barh(y=tip_sum.day, width =tip_sum.tip,color= \"tab:orange\")\nplt.title(\"Tips by day\",size =15,fontweight=\"bold\")\nplt.xlabel(\"Day\")\nplt.ylabel(\"tip\")\n\nText(0, 0.5, 'tip')\n\n\n\n\n\n\n\nlineplot\n\nfig, axes =plt.subplots(1,2,figsize=(4,2))\n\nax1,ax2=axes\nax1.plot(tip.tip)\nax2.plot(tip[[\"total_bill\",\"tip\"]])\nax1.legend([\"tip\",\"tip & total_bill\"],loc=\"upper left\")\nax2.legend([\"tip\",\"tip & total_bill\"],loc=\"upper left\")\nfig.tight_layout()\n\n\n\n\n\n\nhist\n\nplt.figure(figsize=(4,2))\nplt.hist(tip.tip,alpha=0.3,bins=20)\nplt.title(\"hist of tip\")\nplt.axvline(tip.tip.median(),color=\"r\")\nplt.text(tip.tip.median()+1,30,tip.tip.median(),color=\"r\")\n\nText(3.9, 30, '2.9')"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-21-02. numpy & pandas (3).html#여러-열-집계",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-21-02. numpy & pandas (3).html#여러-열-집계",
    "title": "02. numpy & pandas (3)",
    "section": "여러 열 집계",
    "text": "여러 열 집계\n1. 특정 열을 지정 후 집계\n\n_ts = tip.groupby(\"day\").agg({\"total_bill\":sum,\n                         \"tip\" : sum  }).reset_index()\n_ts\n\n\n\n\n\n\n\n\nday\ntotal_bill\ntip\n\n\n\n\n0\nFri\n325.88\n51.96\n\n\n1\nSat\n1778.40\n260.40\n\n\n2\nSun\n1627.16\n247.39\n\n\n3\nThur\n1096.33\n171.83\n\n\n\n\n\n\n\n2 컬럼이 numeric인 열만 집계\n\ntip.groupby(\"day\",as_index= False).sum(numeric_only=True)\n\n\n\n\n\n\n\n\nday\ntotal_bill\ntip\nsize\n\n\n\n\n0\nFri\n325.88\n51.96\n40\n\n\n1\nSat\n1778.40\n260.40\n219\n\n\n2\nSun\n1627.16\n247.39\n216\n\n\n3\nThur\n1096.33\n171.83\n152\n\n\n\n\n\n\n\n3 일별, 흡연 여부를 포함하여 집계\n\n_ts = tip.groupby([\"day\",\"smoker\"],as_index=False).sum(numeric_only=True)\n_ts\n\n\n\n\n\n\n\n\nday\nsmoker\ntotal_bill\ntip\nsize\n\n\n\n\n0\nFri\nNo\n73.68\n11.25\n9\n\n\n1\nFri\nYes\n252.20\n40.71\n31\n\n\n2\nSat\nNo\n884.78\n139.63\n115\n\n\n3\nSat\nYes\n893.62\n120.77\n104\n\n\n4\nSun\nNo\n1168.88\n180.57\n167\n\n\n5\nSun\nYes\n458.28\n66.82\n49\n\n\n6\nThur\nNo\n770.09\n120.32\n112\n\n\n7\nThur\nYes\n326.24\n51.51\n40\n\n\n\n\n\n\n\n4 요일별 팁의 평균\n\ntip.groupby([\"day\"],as_index=False).agg({\"tip\" : np.mean})\n\n\n\n\n\n\n\n\nday\ntip\n\n\n\n\n0\nFri\n2.734737\n\n\n1\nSat\n2.993103\n\n\n2\nSun\n3.255132\n\n\n3\nThur\n2.771452\n\n\n\n\n\n\n\n5 day, sex별 모든 숫자 열의 평균\n\ntip.groupby([\"day\",\"sex\"],as_index=False).mean(numeric_only=True)\n\n\n\n\n\n\n\n\nday\nsex\ntotal_bill\ntip\nsize\n\n\n\n\n0\nFri\nFemale\n14.145556\n2.781111\n2.111111\n\n\n1\nFri\nMale\n19.857000\n2.693000\n2.100000\n\n\n2\nSat\nFemale\n19.680357\n2.801786\n2.250000\n\n\n3\nSat\nMale\n20.802542\n3.083898\n2.644068\n\n\n4\nSun\nFemale\n19.872222\n3.367222\n2.944444\n\n\n5\nSun\nMale\n21.887241\n3.220345\n2.810345\n\n\n6\nThur\nFemale\n16.715312\n2.575625\n2.468750\n\n\n7\nThur\nMale\n18.714667\n2.980333\n2.433333\n\n\n\n\n\n\n\n6 요일별 tip의 최대값 구하기\n\ntip.groupby([\"day\"],as_index=False).tip.max()\n\n\n\n\n\n\n\n\nday\ntip\n\n\n\n\n0\nFri\n4.73\n\n\n1\nSat\n10.00\n\n\n2\nSun\n6.50\n\n\n3\nThur\n6.70"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-23-04. numpy & pandas (5).html",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-23-04. numpy & pandas (5).html",
    "title": "04. numpy & pandas (5)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-23-04. numpy & pandas (5).html#데이터-프레임-합치기-axis1",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-23-04. numpy & pandas (5).html#데이터-프레임-합치기-axis1",
    "title": "04. numpy & pandas (5)",
    "section": "데이터 프레임 합치기 (axis=1)",
    "text": "데이터 프레임 합치기 (axis=1)\n- axis=1 \\(\\to\\) 옆으로 붙임\n\n_df = pd.concat([pop1,pop2],axis=1,join=\"outer\")\n\n\n_df.head()\n\n\n\n\n\n\n\n\nk_male\nk_female\nf_male\nf_female\n\n\n\n\n1981\n4160\n4191\nNaN\nNaN\n\n\n1982\n4160\n4191\nNaN\nNaN\n\n\n1983\n4160\n4191\nNaN\nNaN\n\n\n1984\n4160\n4191\nNaN\nNaN\n\n\n1985\n4160\n4191\n7.0\n6.0\n\n\n\n\n\n\n\n- 데이터를 합친 결과 : pop02 에는 1981~1984년 데이터가 없어 결측치가 생긴다.\n\njoin = \"inner\"옵션을 주면 매핑이 되지 않은 데이터는 제거해서 보여준다.\n\n\n_df = pd.concat([pop1,pop2],axis=1,join=\"inner\")\n\n\n_df.head()\n\n\n\n\n\n\n\n\nk_male\nk_female\nf_male\nf_female\n\n\n\n\n1985\n4160\n4191\n7\n6\n\n\n1986\n4899\n4888\n7\n5\n\n\n1987\n5000\n4979\n6\n5\n\n\n1988\n5156\n5120\n5\n5\n\n\n1989\n5305\n5261\n6\n5"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-23-04. numpy & pandas (5).html#데이터-프레임-합치기-axis0",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-23-04. numpy & pandas (5).html#데이터-프레임-합치기-axis0",
    "title": "04. numpy & pandas (5)",
    "section": "데이터 프레임 합치기 (axis=0)",
    "text": "데이터 프레임 합치기 (axis=0)\n- axis = 0은 생략가능하다. \\(\\to\\) 세로로 합치기\n\n# 서울 인구정보 읽어오기 #1\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/seoul_pop_v01.csv'\npop1 = pd.read_csv(path)\n\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/seoul_pop_v02.csv'\npop2 = pd.read_csv(path)\n\n\npop1.head()\n\n\n\n\n\n\n\n\nyear\nk_male\nk_female\nf_male\nf_female\n\n\n\n\n0\n1985\n4788\n4838\n7\n6\n\n\n1\n1986\n4899\n4888\n7\n5\n\n\n2\n1987\n5000\n4979\n6\n5\n\n\n3\n1988\n5156\n5120\n5\n5\n\n\n4\n1989\n5305\n5261\n6\n5\n\n\n\n\n\n\n\n\npop2.head()\n\n\n\n\n\n\n\n\nyear\nk_male\nk_female\nf_male\nf_female\n\n\n\n\n0\n2001\n5142\n5122\n34\n34\n\n\n1\n2002\n5109\n5098\n36\n37\n\n\n2\n2003\n5085\n5089\n49\n54\n\n\n3\n2004\n5075\n5098\n54\n61\n\n\n4\n2005\n5062\n5105\n61\n68\n\n\n\n\n\n\n\n\n pd.concat([pop1,pop2],axis=0,join=\"outer\").head()\n\n\n\n\n\n\n\n\nyear\nk_male\nk_female\nf_male\nf_female\n\n\n\n\n0\n1985\n4788\n4838\n7\n6\n\n\n1\n1986\n4899\n4888\n7\n5\n\n\n2\n1987\n5000\n4979\n6\n5\n\n\n3\n1988\n5156\n5120\n5\n5\n\n\n4\n1989\n5305\n5261\n6\n5\n\n\n\n\n\n\n\n\n pd.concat([pop1,pop2],axis=0,join=\"inner\").head()\n\n\n\n\n\n\n\n\nyear\nk_male\nk_female\nf_male\nf_female\n\n\n\n\n0\n1985\n4788\n4838\n7\n6\n\n\n1\n1986\n4899\n4888\n7\n5\n\n\n2\n1987\n5000\n4979\n6\n5\n\n\n3\n1988\n5156\n5120\n5\n5\n\n\n4\n1989\n5305\n5261\n6\n5"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-23-04. numpy & pandas (5).html#inner-join",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-23-04. numpy & pandas (5).html#inner-join",
    "title": "04. numpy & pandas (5)",
    "section": "inner join",
    "text": "inner join\n\n같은 이름의 열이 있으면 on옵션을 지정하지 않아도 해당 열을 기준으로 합침\nhow=\"inner\" 옵션은 디폴트 옵션!\n\n\npop = pd.merge(pop1,pop2, on =\"year\",how = \"inner\")\n\n\npop.head()\n\n\n\n\n\n\n\n\nyear\nk_male\nk_female\nf_male\nf_female\n\n\n\n\n0\n1985\n4160\n4191\n7\n6\n\n\n1\n1986\n4899\n4888\n7\n5\n\n\n2\n1987\n5000\n4979\n6\n5\n\n\n3\n1988\n5156\n5120\n5\n5\n\n\n4\n1989\n5305\n5261\n6\n5"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-23-04. numpy & pandas (5).html#outer-join",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-23-04. numpy & pandas (5).html#outer-join",
    "title": "04. numpy & pandas (5)",
    "section": "outer join",
    "text": "outer join\n\nhow=\"outer\" 와 how=\"left\"의 조인 결과는 동일하다.\n\n\npop = pd.merge(pop1,pop2, on =\"year\",how = \"outer\")\n\n\npop.head()\n\n\n\n\n\n\n\n\nyear\nk_male\nk_female\nf_male\nf_female\n\n\n\n\n0\n1981\n4160\n4191\nNaN\nNaN\n\n\n1\n1982\n4160\n4191\nNaN\nNaN\n\n\n2\n1983\n4160\n4191\nNaN\nNaN\n\n\n3\n1984\n4160\n4191\nNaN\nNaN\n\n\n4\n1985\n4160\n4191\n7.0\n6.0\n\n\n\n\n\n\n\n\npop = pd.merge(pop1,pop2, on =\"year\",how = \"left\")\n\n\npop.head()\n\n\n\n\n\n\n\n\nyear\nk_male\nk_female\nf_male\nf_female\n\n\n\n\n0\n1981\n4160\n4191\nNaN\nNaN\n\n\n1\n1982\n4160\n4191\nNaN\nNaN\n\n\n2\n1983\n4160\n4191\nNaN\nNaN\n\n\n3\n1984\n4160\n4191\nNaN\nNaN\n\n\n4\n1985\n4160\n4191\n7.0\n6.0"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-23-04. numpy & pandas (5).html#join의-또-다른-방법",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-23-04. numpy & pandas (5).html#join의-또-다른-방법",
    "title": "04. numpy & pandas (5)",
    "section": "join의 또 다른 방법",
    "text": "join의 또 다른 방법\n\npop1.join(pop2.set_index(\"year\"),on=\"year\",how=\"outer\").head()\n\n\n\n\n\n\n\n\nyear\nk_male\nk_female\nf_male\nf_female\n\n\n\n\n0\n1981\n4160\n4191\nNaN\nNaN\n\n\n1\n1982\n4160\n4191\nNaN\nNaN\n\n\n2\n1983\n4160\n4191\nNaN\nNaN\n\n\n3\n1984\n4160\n4191\nNaN\nNaN\n\n\n4\n1985\n4160\n4191\n7.0\n6.0\n\n\n\n\n\n\n\n\npop1.join(pop2.set_index(\"year\"),on=\"year\",how=\"inner\").head()\n\n\n\n\n\n\n\n\nyear\nk_male\nk_female\nf_male\nf_female\n\n\n\n\n4\n1985\n4160\n4191\n7\n6\n\n\n5\n1986\n4899\n4888\n7\n5\n\n\n6\n1987\n5000\n4979\n6\n5\n\n\n7\n1988\n5156\n5120\n5\n5\n\n\n8\n1989\n5305\n5261\n6\n5"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-23-04. numpy & pandas (5).html#setting",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-23-04. numpy & pandas (5).html#setting",
    "title": "04. numpy & pandas (5)",
    "section": "setting",
    "text": "setting\n\n# 폰트설정\nplt.rcParams['font.family'] = 'Malgun Gothic'\nplt.rcParams['axes.unicode_minus'] = False"
  },
  {
    "objectID": "posts/DX/01. 데이터 다듬기/2023-08-23-04. numpy & pandas (5).html#test",
    "href": "posts/DX/01. 데이터 다듬기/2023-08-23-04. numpy & pandas (5).html#test",
    "title": "04. numpy & pandas (5)",
    "section": "test",
    "text": "test\n\nx = [1,2,3,4]\ny = [1,2,4,4]\n\n\nplt.plot(x,y)\nplt.xlabel(\"x축\")\nplt.ylabel(\"y축\")\n\nText(0, 0.5, 'y축')"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html",
    "href": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html",
    "title": "01. 데이터 분석 (2)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams['font.family'] = 'Malgun Gothic'\nplt.rcParams['axes.unicode_minus'] = False\n\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nimport scipy.stats as spst"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#pairplot좋아하는-plot은-아님",
    "href": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#pairplot좋아하는-plot은-아님",
    "title": "01. 데이터 분석 (2)",
    "section": "pairplot(좋아하는 plot은 아님)",
    "text": "pairplot(좋아하는 plot은 아님)\n\nsns.pairplot(air, kind='reg' )\nplt.show()\n\nC:\\Users\\rkdcj\\anaconda3\\envs\\dx\\Lib\\site-packages\\seaborn\\axisgrid.py:118: UserWarning:\n\nThe figure layout has changed to tight"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#joinplot",
    "href": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#joinplot",
    "title": "01. 데이터 분석 (2)",
    "section": "joinplot",
    "text": "joinplot\n\nsns.jointplot(x='Temp', y='Ozone', data = air)\nplt.show()\n\n\n\n\n\nsns.jointplot(x='Wind', y='Ozone', data = air)\nplt.show()"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#regplot",
    "href": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#regplot",
    "title": "01. 데이터 분석 (2)",
    "section": "regplot",
    "text": "regplot\n\nsns.regplot(x='Solar.R', y='Ozone', data = air)\nplt.show()"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#상관계수-계산",
    "href": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#상관계수-계산",
    "title": "01. 데이터 분석 (2)",
    "section": "상관계수 계산",
    "text": "상관계수 계산\n\nimport scipy.stats as spst\n\n\nspst.pearsonr(air.Temp,air.Ozone)\n\nPearsonRResult(statistic=0.6833717861490115, pvalue=2.197769800200287e-22)\n\n\n\nspst.pearsonr(air.Wind,air.Ozone)\n\nPearsonRResult(statistic=-0.6054782354684076, pvalue=1.1255146087637929e-16)\n\n\n\nair.corr()\n\n\n\n\n\n\n\n\nOzone\nSolar.R\nWind\nTemp\nDate\n\n\n\n\nOzone\n1.000000\n0.280068\n-0.605478\n0.683372\n0.170271\n\n\nSolar.R\n0.280068\n1.000000\n-0.056792\n0.275840\n-0.104682\n\n\nWind\n-0.605478\n-0.056792\n1.000000\n-0.457988\n-0.168683\n\n\nTemp\n0.683372\n0.275840\n-0.457988\n1.000000\n0.385605\n\n\nDate\n0.170271\n-0.104682\n-0.168683\n0.385605\n1.000000"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#heatmap",
    "href": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#heatmap",
    "title": "01. 데이터 분석 (2)",
    "section": "heatmap",
    "text": "heatmap\n\nplt.figure(figsize = (4, 4))\nsns.heatmap(air.corr(),\n            annot = True,            # 숫자(상관계수) 표기 여부\n            fmt = '.3f',             # 숫자 포멧 : 소수점 3자리까지 표기\n            cmap = 'RdYlBu_r',       # 칼라맵\n            vmin = -1, vmax = 1)     # 값의 최소, 최대값\nplt.show()"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#import-1",
    "href": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#import-1",
    "title": "01. 데이터 분석 (2)",
    "section": "import",
    "text": "import\n\nimport pandas as pd\nimport numpy as np\nimport random as rd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport scipy.stats as spst"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#분산과-표준편차",
    "href": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#분산과-표준편차",
    "title": "01. 데이터 분석 (2)",
    "section": "분산과 표준편차",
    "text": "분산과 표준편차\n\n값들이 평균으로부터 얼마나 벗어나 있느지를 나타내는 값.\n위 값들은 집단간 평균 비교시 사용되는 값들이다."
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#표준오차",
    "href": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#표준오차",
    "title": "01. 데이터 분석 (2)",
    "section": "표준오차",
    "text": "표준오차\n\\[\\text{표준오차} = \\frac{s}{\\sqrt n},\\quad \\text{표본분산}=  s^2 = \\frac{\\sum (x-\\bar x )^2}{n-1}\\]\n- 표본의 분산을 \\(n-1\\)로 나누는 이유"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#신뢰구간",
    "href": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#신뢰구간",
    "title": "01. 데이터 분석 (2)",
    "section": "95%신뢰구간",
    "text": "95%신뢰구간\n\nrd.normalvariate?\n\n1 평균이 172, 표준편차가 1인 표본을 800,000개 추출\n\npop = [round(rd.normalvariate(172, 7),1) for i in range(800000)]\n\n\nsns.histplot(pop, bins = 100)\nplt.axvline(np.mean(pop), color = 'r')\nplt.text(np.mean(pop)+1, 30000, f'pop mean : {np.mean(pop).round(3)}', color = 'r')\nplt.show()\n\n\n\n\n2 . 50개씩 데이터를 추출 후 표본 평균을 계산\n\nx_mean = []\nfor i in range(100):\n    s1 = rd.sample(pop, 50)\n    s1 = pd.Series(s1)\n    x_mean.append(round(s1.mean(),3))\n\n\nplt.figure(figsize=(10,6))\nsns.kdeplot(x_mean)\nplt.axvline(np.mean(x_mean), color = 'r')\nplt.text(np.mean(x_mean)+1, 0.1, f'pop mean : {round(np.mean(x_mean),3)}', color = 'r')\n\nText(172.92226, 0.1, 'pop mean : 171.922')"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#중심극한-정리",
    "href": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#중심극한-정리",
    "title": "01. 데이터 분석 (2)",
    "section": "중심극한 정리",
    "text": "중심극한 정리\n- \\(n\\)의 크기가 충분히 크면 표본평균의 분포는 정규분포로 수렴한다.\n\n조건 1 : 데이터의 수가 충분히 커야함\n조건 2 : 추출된 표본들은 서로 독립이어야 한다.\n\n\n표본평균의 분포\n\npop = [round(rd.expovariate(.3)+165,2) for i in range(10001)]\n\n\n# 표본의 크기\nn = 100\n\n# 표본의 갯수\nm = 200\n\nsample_mean = [np.mean(rd.sample(pop,n)) for i in range(m)]\n\nplt.figure(figsize=(10,6))\nsns.kdeplot(sample_mean)\n\nplt.axvline(x=np.mean(sample_mean), color = 'red') #표본평균들의 평균\nplt.axvline(x=np.mean(pop), color = 'grey') # 모평균\n\nplt.text(np.mean(sample_mean)-.2, 0.02, round(np.mean(sample_mean),3), color = 'red') #표본평균들의 평균\nplt.text(np.mean(pop)+.1,0.02, round(np.mean(pop),3), color = 'grey') #모평균\n\nText(168.4570502949705, 0.02, '168.357')\n\n\n\n\n\n\n\n모집단의 분포\n\nplt.figure(figsize=(10,6)) #설정\nsns.histplot(pop, bins = 100)\nplt.axvline(x=np.mean(pop), color = 'grey') # 모평균\nplt.text(np.mean(pop)+.5, 800, round(np.mean(pop),2), color = 'grey')\nplt.show()"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#데이터-로드",
    "href": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#데이터-로드",
    "title": "01. 데이터 분석 (2)",
    "section": "1. 데이터 로드",
    "text": "1. 데이터 로드\n\ntitanic = pd.read_csv('https://raw.githubusercontent.com/DA4BAM/dataset/master/titanic.1.csv')"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#교차표-생성",
    "href": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#교차표-생성",
    "title": "01. 데이터 분석 (2)",
    "section": "2. 교차표 생성",
    "text": "2. 교차표 생성\n- 생존과 성별 변수로 교차표를 생성\n\n# 두 범주별 빈도수를 교차표로 만들어 봅시다.\npd.crosstab(titanic['Survived'], titanic['Sex'])\n\n\n\n\n\n\n\nSex\nfemale\nmale\n\n\nSurvived\n\n\n\n\n\n\n0\n81\n468\n\n\n1\n233\n109\n\n\n\n\n\n\n\n- 열별 비율\n\npd.crosstab(titanic['Survived'], titanic['Sex'], normalize = 'columns')\n\n\n\n\n\n\n\nSex\nfemale\nmale\n\n\nSurvived\n\n\n\n\n\n\n0\n0.257962\n0.811092\n\n\n1\n0.742038\n0.188908\n\n\n\n\n\n\n\n- index별 비율\n\npd.crosstab(titanic['Survived'], titanic['Sex'], normalize = 'index')\n\n\n\n\n\n\n\nSex\nfemale\nmale\n\n\nSurvived\n\n\n\n\n\n\n0\n0.147541\n0.852459\n\n\n1\n0.681287\n0.318713\n\n\n\n\n\n\n\n- (행,열)별 전체 비율\n\npd.crosstab(titanic['Survived'], titanic['Embarked'], normalize = 'all')\n\n\n\n\n\n\n\nEmbarked\nC\nQ\nS\n\n\nSurvived\n\n\n\n\n\n\n\n0\n0.084175\n0.05275\n0.479237\n\n\n1\n0.104377\n0.03367\n0.245791\n\n\n\n\n\n\n\n\nx축의 길이는 각 객실 등급별 승객 비율을 표시\ny축의 길이는 각 객실 등급별 사망 및 생존 비율을 의미한다.\n\n\n# Pclass별 생존여부를 mosaic plot으로 그려 봅시다.\nmosaic(titanic, [ 'Pclass','Survived'])\nplt.axhline(1- titanic['Survived'].mean(), color = 'r')\n\n&lt;matplotlib.lines.Line2D at 0x2c073331350&gt;\n\n\n\n\n\n\ntemp = pd.crosstab(titanic['Pclass'], titanic['Survived'], normalize = 'index')\nprint(temp)\ntemp.plot.bar(stacked=True)\nplt.axhline(1-titanic['Survived'].mean(), color = 'r')\n\nSurvived         0         1\nPclass                      \n1         0.370370  0.629630\n2         0.527174  0.472826\n3         0.757637  0.242363\n\n\n&lt;matplotlib.lines.Line2D at 0x2c0729818d0&gt;"
  },
  {
    "objectID": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#카이제곱검정",
    "href": "posts/DX/02. 데이터 분석/2023-08-29-01. 데이터 분석 (2).html#카이제곱검정",
    "title": "01. 데이터 분석 (2)",
    "section": "3. 카이제곱검정",
    "text": "3. 카이제곱검정\n- A와 B라는 두 개의 범주형 변수가 존재할 때\n\n두 변수들 사이의 관계가 독립인지 아닌지 검정\n\n\\[H_0 : \\text{두 변수는 독립이다.}\\quad H_1 : \\text{두 변수는 독립이 아니다.}\\]\n\\[\\text {카이제곱 통계량} : \\chi^2 [(p-1)(q-1),\\alpha]= \\sum_{p=1}^P\\sum_{q=1}^Q\\frac {(\\text{관측빈도-기대빈도)}^2}{기대빈도}\\]\n\\[p,q  = \\text {각 범주의 클래스의 수}\\]\n\\[n=(p-1)(q-1)\\to \\text{자유도}\\]\n\\[\\text{기대빈도}= \\frac{\\text{sum}(p=i) \\times \\text{sum}(q=i)}{\\text{total sum}}\\]\n\n카이제곱통계량 계산(손계산)\n1. 기대빈도 계산\n\ntem = titanic.groupby([\"Pclass\",\"Survived\"],as_index=False)[[\"Pclass\"]].value_counts()\ntem.rename(columns = {\"count\": \"관측빈도\"},inplace=True)\n\n\ntem\n\n\n\n\n\n\n\n\nPclass\nSurvived\n관측빈도\n\n\n\n\n0\n1\n0\n80\n\n\n1\n1\n1\n136\n\n\n2\n2\n0\n97\n\n\n3\n2\n1\n87\n\n\n4\n3\n0\n372\n\n\n5\n3\n1\n119\n\n\n\n\n\n\n\n\ntem1  = tem.groupby([\"Pclass\"],as_index=False)[[\"관측빈도\"]].sum().rename(columns ={\"관측빈도\" : \"P_total\"})\ntem2  = tem.groupby([\"Survived\"],as_index=False)[[\"관측빈도\"]].sum().rename(columns ={\"관측빈도\" : \"S_total\"})\n\n\ntotal = pd.merge(pd.merge(tem,tem1),tem2)\n\n\ntotal[\"total\"] =sum(total.S_total.unique())\n\n\ntotal[\"기대빈도\"] = total.P_total*total.S_total/total.total\n\n\ntotal\n\n\n\n\n\n\n\n\nPclass\nSurvived\n관측빈도\nP_total\nS_total\ntotal\n기대빈도\n\n\n\n\n0\n1\n0\n80\n216\n549\n891\n133.090909\n\n\n1\n2\n0\n97\n184\n549\n891\n113.373737\n\n\n2\n3\n0\n372\n491\n549\n891\n302.535354\n\n\n3\n1\n1\n136\n216\n342\n891\n82.909091\n\n\n4\n2\n1\n87\n184\n342\n891\n70.626263\n\n\n5\n3\n1\n119\n491\n342\n891\n188.464646\n\n\n\n\n\n\n\n2. 카이제곱 통계량 계산\n\npd.crosstab(titanic['Survived'], titanic['Pclass'])\n\n\n\n\n\n\n\nPclass\n1\n2\n3\n\n\nSurvived\n\n\n\n\n\n\n\n0\n80\n97\n372\n\n\n1\n136\n87\n119\n\n\n\n\n\n\n\n\nchi_s = sum(((total[\"관측빈도\"] - total[\"기대빈도\"])**2)/total[\"기대빈도\"])\nchi_s\n\n102.88898875696056\n\n\n\nalpha = 0.05 ## 유의수준\n\n\na2 = spst.chi2.ppf(1-alpha/2,2) ## 기각영역1\na1 = spst.chi2.ppf(alpha/2,2) ## 기각영역 2\n\n(a1,a2)\n\n(0.05063561596857975, 7.377758908227871)\n\n\n\nx = np.linspace(0,110,10000) ## 임의의 x생성\ns = spst.chi2.pdf(x,2) ## x에 따른 카이제곱 분포 생성\n\n\n\nCode\nplt.plot(x,s,\".r\",alpha=0.2)\nplt.plot(x[(x&gt;a1) & (x&lt;a2)],s[(x&gt;a1) & (x&lt;a2)],\".b\",alpha=0.3)\nplt.axvline(x=chi_s,color=\"b\",linestyle = \":\")\nplt.text(chi_s-15,.4, r\"계산된 $\\chi$ 의 위치 \"+\" \\n = \"+str(round(chi_s,2)))\nplt.legend([\"빨강(기각영역)\",\"파랑(신뢰구간)\"])\n\n\n&lt;matplotlib.legend.Legend at 0x2c072976190&gt;\n\n\n\n\n\n\n\n카이제곱통계량 계산(함수이용)\n\ntable = pd.crosstab(titanic['Survived'], titanic['Pclass'])\nprint(table)\nprint(\"\\n\",'-' * 50)\n\n# 2) 카이제곱검정\nspst.chi2_contingency(table)\n\nPclass      1   2    3\nSurvived              \n0          80  97  372\n1         136  87  119\n\n --------------------------------------------------\n\n\nChi2ContingencyResult(statistic=102.88898875696056, pvalue=4.549251711298793e-23, dof=2, expected_freq=array([[133.09090909, 113.37373737, 302.53535354],\n       [ 82.90909091,  70.62626263, 188.46464646]]))"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html",
    "href": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html",
    "title": "00. 데이터 수집 (1)",
    "section": "",
    "text": "a = 1\n\nb,c = 2,3\n\nd = e = 4\n\n\n%whos\n\nVariable   Type    Data/Info\n----------------------------\na          int     1\nb          int     2\nc          int     3\nd          int     4\ne          int     4\n\n\n- 생성한 변수 reset\n\n%reset\n\nOnce deleted, variables cannot be recovered. Proceed (y/[n])?  y\n\n\n\n%whos\n\nInteractive namespace is empty.\n\n\n\n\n\n- 기본 : int, float, bool, str\n\na,b,c,d = -3,1.2,True, \"py\"\n%whos\n\nVariable   Type     Data/Info\n-----------------------------\na          int      -3\nb          float    1.2\nc          bool     True\nd          str      py\n\n\n- 컬렉션 : list, tuple, dict, set \\(\\to\\) 변수 1개가 여러 개의 데이터를 가지고 있는 형태\n\n데이터를 다룰 때 중요 개념 : create, read, update, delete (CRUD)\n\n1 create\n\nd5, d6, d7 = [1, 2, \"A\", \"B\"], (1, 2, \"A\", \"B\"), {\"A\" : 1, 2 : \"B\"}\n%whos\n\nVariable   Type     Data/Info\n-----------------------------\na          int      -3\nb          float    1.2\nc          bool     True\nd          str      py\nd5         list     n=4\nd6         tuple    n=4\nd7         dict     n=2\n\n\n2 read : masking 문법(:)\n\nd5[2] ## data[idx]\n\n'A'\n\n\n\nd7[\"A\"] ## data[key]\n\n1\n\n\n\nd5[1:3] ## slice\n\n[2, 'A']\n\n\n\nd5[::2] ## stride\n\n[1, 'A']\n\n\n3 update : 데이터 선택 = 수정할 데이터\n\nd5[2] = \"C\" \nd5\n\n[1, 2, 'C', 'B']\n\n\n4 delete : del 데이터 선택\n\ndel d5[2]\n\nd5\n\n[1, 2, 'B']\n\n\n\n\n\n\nd1, d2 = [1, 2, 3], (1, 2, 4)\n\n\nprint(d1,d2)\n\n[1, 2, 3] (1, 2, 4)\n\n\n\nprint(type(d1),type(d2)) ## type 출력\n\n&lt;class 'list'&gt; &lt;class 'tuple'&gt;\n\n\n\nprint(id(d1),id(d2)) ## 주소값 출력\n\n2444525537280 2444526172864\n\n\n\nimport sys\n\n- 수정할 필요가 없는 데이터는 리스트보다 튜플이 메모리 효율적으로 좋다.\n\nprint(sys.getsizeof(d1),sys.getsizeof(d2))\n\n88 64\n\n\n\n\n\n- 산술 : (+, -) &lt; (*, /, //, %) &lt;&lt; **\n- 비교 : ==, !=, &gt;, &lt;, &gt;=, &lt;= : 조건 1개\n- 논리 : not, and, or\n\n\n\n\nfor _ in range(3) : \n    print(\"py\")\n\npy\npy\npy\n\n\n\n%whos\n\nVariable   Type      Data/Info\n------------------------------\na          int       -3\nb          float     1.2\nc          bool      True\nd          str       py\nd1         list      n=3\nd2         tuple     n=3\nd5         list      n=3\nd6         tuple     n=4\nd7         dict      n=2\nsys        module    &lt;module 'sys' (built-in)&gt;\n\n\n\n\n\n\n\n\ndef plus(a = 1, b = 2) :\n    print(a + b)\n\n- data type이 function이다.\n\n%whos\n\nVariable   Type        Data/Info\n--------------------------------\na          int         -3\nb          float       1.2\nc          bool        True\nd          str         py\nd1         list        n=3\nd2         tuple       n=3\nd5         list        n=3\nd6         tuple       n=4\nd7         dict        n=2\nplus       function    &lt;function plus at 0x00000239291E2B60&gt;\nsys        module      &lt;module 'sys' (built-in)&gt;\n\n\n\n\n\n\nplus() ## 디퐅트값\nplus(1,3) ## 값을 전달\n\n3\n4\n\n\n\n\n\n\nreturn을 추가하지 않아 아래처럼 None으로 결과를 뱉는다.\n\n\ndef plus(a,b = 1, c = 2) :\n    print(a + b + c)\n    \nresult = plus(1, c = 50)\nprint(\"result\", result)\n\n52\nresult None\n\n\n\ndef plus(a,b = 1, c = 2) :\n    return(a + b + c)\n    \nresult = plus(1, c = 50)\nprint(\"result\", result)\n\nresult 52\n\n\n\n\n\n\n함수를 작성한 사람이, 함수를 어떻게 사용하면 되는지 남겨둔 것!\n\n\n?print\n\n\nSignature: print(*args, sep=' ', end='\\n', file=None, flush=False)\nDocstring:\nPrints the values to a stream, or to sys.stdout by default.\nsep\n  string inserted between values, default a space.\nend\n  string appended after the last value, default a newline.\nfile\n  a file-like object (stream); defaults to the current sys.stdout.\nflush\n  whether to forcibly flush the stream.\nType:      builtin_function_or_method\n\n\n\n\nprint(\"A\",\"B\")\nprint(\"C\")\n\nA B\nC\n\n\n\nprint(\"A\",\"B\",sep=\",\", end = \":\")\nprint(\"C\")\n\nA,B:C\n\n\n\n\n\n\n\n- 변수, 함수를 묶어서 코드 작성 실행\n- 객체 지향 문법을 구현한 방법 : 실제 세계를 모델링하여 프로그램을 개발하는 방법론 (사전적 의미)\n\n교수님 생각 : 클래스는 복제, 변형, 재생산을 용이하게 하기 위해 만들어진 확장가능한 프로그램이 코드 단위(extensible program-code-template)이다.\n\n- 사용법 : 클래스 선언 (코드 작성) -&gt; 객체 생성(메모리에 탑재(사용)) -&gt; 메소드 호출 (코드 실행)\n- 클래스 식별자 컨벤션 : PascalCase(O), snake_case(X)\n\n\n\nclass Marine : \n    health, ap = 40, 5\n    def attack(self, target) :\n        target.health -=self.ap\n\n\n\n\n\nm1, m2 = Marine(), Marine()\nm1.health, m1.ap, m2.health, m2.ap\n\n(40, 5, 40, 5)\n\n\n- dir() : 객체에 저장되어 있는 변수(함수포함) 출력\n\ndir(m1)[-3 : ]\n\n['ap', 'attack', 'health']\n\n\n- 값 변경\n\nm2.health, m2.ap =  60, 7\n\nm2.health, m2.ap\n\n(60, 7)\n\n\n\n\n\n\nm2.attack(m1)\n\n\nm1.health\n\n33\n\n\n- m1의 피가 깍여버린 원리\nclass Marine : \n    health, ap = 40, 5\n    def attack(self, target) : ## self : 생성된 객체 자신\n        target.health -=self.ap ## 여기에서 target이 공격당한 대상이 m1, 따라서 m2의 공격력 7만큼 감소\n\n\n\n- 스페셜 메서드 중 하나 (__init__())\n\nclass Marine : \n    #health, ap = 40, 5\n    def attack(self, target) : ## self : 생성된 객체 자신\n        target.health -=self.ap ## 여기에서 target이 공격당한 대상이 m1, 따라서 m2의 공격력 7만큼 감소\n\n\nm1, m2 = Marine(), Marine()\n\n- 객체는 생성이 되었으나 피와 공격력이 없어서 메소드 호출 시 에러가 발생한다.\n\n'Marine' object has no attribute 'health'\n\n\nm2.attack(m1)\n\nAttributeError: 'Marine' object has no attribute 'health'\n\n\n- 위를 방지하는 방법 중 하나가 생성자 메소드이다.\n\n제품을 만들 때, 메소드에서 사용하는 변수들이 있는지 한번 다 확인하는 것!\n\n\nclass Marine : \n    #health, ap = 40, 5\n    def __init__(self,health,ap) : ## 생성자 작성\n        self.health, self.ap = health, ap\n    def attack(self, target) : ## self : 생성된 객체 자신\n        target.health -=self.ap ## 여기에서 target이 공격당한 대상이 m1, 따라서 m2의 공격력 7만큼 감소\n\n- 이제 메서드가 제대로 실행된다.\n\nm1, m2 = Marine(40,5), Marine(40,5)\nm2.attack(m1)\nm2.health, m1.health\n\n(40, 35)\n\n\n- 요약 : 생성자는 객체 생성시 변수의 초기값을 검사 및 설정한다.\n\n디폴트 값도 사용가능\n\n\nclass Marine : \n    #health, ap = 40, 5\n    def __init__(self,health,ap = 5) : ## 생성자 작성\n        self.health, self.ap = health, ap\n    def attack(self, target) : ## self : 생성된 객체 자신\n        target.health -=self.ap ## 여기에서 target이 공격당한 대상이 m1, 따라서 m2의 공격력 7만큼 감소\n\n\nm1, m2 = Marine(40,5), Marine(40)\nm2.attack(m1)\nm2.health, m1.health\n\n(40, 35)\n\n\n\n\n\n\nimport pandas as pd \ndf = pd.DataFrame({\"kospi\" : [3, 4, 5], \"usd\" : [9, 2, 1]})\ndf\n\n\n\n\n\n\n\n\nkospi\nusd\n\n\n\n\n0\n3\n9\n\n\n1\n4\n2\n\n\n2\n5\n1\n\n\n\n\n\n\n\n1 아래 코드는 다른사람이 만든 클래스(모듈) 코드를 우리가 불러오는 것!\nimport pandas as pd \n2 생성자 메서드를 이용하여 초기값을 설정하는 것! (메모리가 사용됨)\ndf = pd.DataFrame({\"kospi\" : [3, 4, 5], \"usd\" : [9, 2, 1]})\n3 객체(df) 안에는 pandas안에 작성된 다양한 메서드들이 존재하며 다음과 같이 사용할 수 있다.\n\ndf.corr()\n\n\n\n\n\n\n\n\nkospi\nusd\n\n\n\n\nkospi\n1.000000\n-0.917663\n\n\nusd\n-0.917663\n1.000000\n\n\n\n\n\n\n\n4. 가장 중요 : dir을 통해 객체안에서 쓸 수 있는 method를 잘 살펴보자!\n5 클래스는 데이터 타입이다!! (\\(\\star\\star\\))\n\nm1 객체는 우리가 만든 marine이라는 클래스의 객체이다.\n\n커스터마이징, 즉, 클래스는 사용자 정의 데이터 타입이다.\n\n\n\nm1 = Marine(40)\ntype(m1)\n\n__main__.Marine\n\n\n6 d1, d2 객체에서 사용가능한 변수와 메서드는 str, list 클래스, 정의되어 있다.\n\n파이썬 문법으로 만들어진 내장 클래스는 앞글자를 소문자로 쓴다.\n\n\nd1 = \"python\"\nd2 = [1, 3, 2]\n\n\ntype(d1), type(d2)\n\n(str, list)\n\n\n7 최종 결론\n\n사실 우리가 하는 변수 생성은 객체를 생성, 즉 클래스 객체를 생성한 것이다.\n따라서, 데이터 타입에 따라서 변수에서 사용가능한 변수, 메서드가 다르다."
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#변수-선언-및-삭제",
    "href": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#변수-선언-및-삭제",
    "title": "00. 데이터 수집 (1)",
    "section": "",
    "text": "a = 1\n\nb,c = 2,3\n\nd = e = 4\n\n\n%whos\n\nVariable   Type    Data/Info\n----------------------------\na          int     1\nb          int     2\nc          int     3\nd          int     4\ne          int     4\n\n\n- 생성한 변수 reset\n\n%reset\n\nOnce deleted, variables cannot be recovered. Proceed (y/[n])?  y\n\n\n\n%whos\n\nInteractive namespace is empty."
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#데이터-타입",
    "href": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#데이터-타입",
    "title": "00. 데이터 수집 (1)",
    "section": "",
    "text": "- 기본 : int, float, bool, str\n\na,b,c,d = -3,1.2,True, \"py\"\n%whos\n\nVariable   Type     Data/Info\n-----------------------------\na          int      -3\nb          float    1.2\nc          bool     True\nd          str      py\n\n\n- 컬렉션 : list, tuple, dict, set \\(\\to\\) 변수 1개가 여러 개의 데이터를 가지고 있는 형태\n\n데이터를 다룰 때 중요 개념 : create, read, update, delete (CRUD)\n\n1 create\n\nd5, d6, d7 = [1, 2, \"A\", \"B\"], (1, 2, \"A\", \"B\"), {\"A\" : 1, 2 : \"B\"}\n%whos\n\nVariable   Type     Data/Info\n-----------------------------\na          int      -3\nb          float    1.2\nc          bool     True\nd          str      py\nd5         list     n=4\nd6         tuple    n=4\nd7         dict     n=2\n\n\n2 read : masking 문법(:)\n\nd5[2] ## data[idx]\n\n'A'\n\n\n\nd7[\"A\"] ## data[key]\n\n1\n\n\n\nd5[1:3] ## slice\n\n[2, 'A']\n\n\n\nd5[::2] ## stride\n\n[1, 'A']\n\n\n3 update : 데이터 선택 = 수정할 데이터\n\nd5[2] = \"C\" \nd5\n\n[1, 2, 'C', 'B']\n\n\n4 delete : del 데이터 선택\n\ndel d5[2]\n\nd5\n\n[1, 2, 'B']"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#변수-속성값-함수",
    "href": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#변수-속성값-함수",
    "title": "00. 데이터 수집 (1)",
    "section": "",
    "text": "d1, d2 = [1, 2, 3], (1, 2, 4)\n\n\nprint(d1,d2)\n\n[1, 2, 3] (1, 2, 4)\n\n\n\nprint(type(d1),type(d2)) ## type 출력\n\n&lt;class 'list'&gt; &lt;class 'tuple'&gt;\n\n\n\nprint(id(d1),id(d2)) ## 주소값 출력\n\n2444525537280 2444526172864\n\n\n\nimport sys\n\n- 수정할 필요가 없는 데이터는 리스트보다 튜플이 메모리 효율적으로 좋다.\n\nprint(sys.getsizeof(d1),sys.getsizeof(d2))\n\n88 64"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#연산자",
    "href": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#연산자",
    "title": "00. 데이터 수집 (1)",
    "section": "",
    "text": "- 산술 : (+, -) &lt; (*, /, //, %) &lt;&lt; **\n- 비교 : ==, !=, &gt;, &lt;, &gt;=, &lt;= : 조건 1개\n- 논리 : not, and, or"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#row-dash-변수를-사용하지-않음",
    "href": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#row-dash-변수를-사용하지-않음",
    "title": "00. 데이터 수집 (1)",
    "section": "",
    "text": "for _ in range(3) : \n    print(\"py\")\n\npy\npy\npy\n\n\n\n%whos\n\nVariable   Type      Data/Info\n------------------------------\na          int       -3\nb          float     1.2\nc          bool      True\nd          str       py\nd1         list      n=3\nd2         tuple     n=3\nd5         list      n=3\nd6         tuple     n=4\nd7         dict      n=2\nsys        module    &lt;module 'sys' (built-in)&gt;"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#함수-코드-작성의-효율을-높임",
    "href": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#함수-코드-작성의-효율을-높임",
    "title": "00. 데이터 수집 (1)",
    "section": "",
    "text": "def plus(a = 1, b = 2) :\n    print(a + b)\n\n- data type이 function이다.\n\n%whos\n\nVariable   Type        Data/Info\n--------------------------------\na          int         -3\nb          float       1.2\nc          bool        True\nd          str         py\nd1         list        n=3\nd2         tuple       n=3\nd5         list        n=3\nd6         tuple       n=4\nd7         dict        n=2\nplus       function    &lt;function plus at 0x00000239291E2B60&gt;\nsys        module      &lt;module 'sys' (built-in)&gt;\n\n\n\n\n\n\nplus() ## 디퐅트값\nplus(1,3) ## 값을 전달\n\n3\n4\n\n\n\n\n\n\nreturn을 추가하지 않아 아래처럼 None으로 결과를 뱉는다.\n\n\ndef plus(a,b = 1, c = 2) :\n    print(a + b + c)\n    \nresult = plus(1, c = 50)\nprint(\"result\", result)\n\n52\nresult None\n\n\n\ndef plus(a,b = 1, c = 2) :\n    return(a + b + c)\n    \nresult = plus(1, c = 50)\nprint(\"result\", result)\n\nresult 52\n\n\n\n\n\n\n함수를 작성한 사람이, 함수를 어떻게 사용하면 되는지 남겨둔 것!\n\n\n?print\n\n\nSignature: print(*args, sep=' ', end='\\n', file=None, flush=False)\nDocstring:\nPrints the values to a stream, or to sys.stdout by default.\nsep\n  string inserted between values, default a space.\nend\n  string appended after the last value, default a newline.\nfile\n  a file-like object (stream); defaults to the current sys.stdout.\nflush\n  whether to forcibly flush the stream.\nType:      builtin_function_or_method\n\n\n\n\nprint(\"A\",\"B\")\nprint(\"C\")\n\nA B\nC\n\n\n\nprint(\"A\",\"B\",sep=\",\", end = \":\")\nprint(\"C\")\n\nA,B:C"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#클래스",
    "href": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#클래스",
    "title": "00. 데이터 수집 (1)",
    "section": "",
    "text": "- 변수, 함수를 묶어서 코드 작성 실행\n- 객체 지향 문법을 구현한 방법 : 실제 세계를 모델링하여 프로그램을 개발하는 방법론 (사전적 의미)\n\n교수님 생각 : 클래스는 복제, 변형, 재생산을 용이하게 하기 위해 만들어진 확장가능한 프로그램이 코드 단위(extensible program-code-template)이다.\n\n- 사용법 : 클래스 선언 (코드 작성) -&gt; 객체 생성(메모리에 탑재(사용)) -&gt; 메소드 호출 (코드 실행)\n- 클래스 식별자 컨벤션 : PascalCase(O), snake_case(X)\n\n\n\nclass Marine : \n    health, ap = 40, 5\n    def attack(self, target) :\n        target.health -=self.ap\n\n\n\n\n\nm1, m2 = Marine(), Marine()\nm1.health, m1.ap, m2.health, m2.ap\n\n(40, 5, 40, 5)\n\n\n- dir() : 객체에 저장되어 있는 변수(함수포함) 출력\n\ndir(m1)[-3 : ]\n\n['ap', 'attack', 'health']\n\n\n- 값 변경\n\nm2.health, m2.ap =  60, 7\n\nm2.health, m2.ap\n\n(60, 7)\n\n\n\n\n\n\nm2.attack(m1)\n\n\nm1.health\n\n33\n\n\n- m1의 피가 깍여버린 원리\nclass Marine : \n    health, ap = 40, 5\n    def attack(self, target) : ## self : 생성된 객체 자신\n        target.health -=self.ap ## 여기에서 target이 공격당한 대상이 m1, 따라서 m2의 공격력 7만큼 감소\n\n\n\n- 스페셜 메서드 중 하나 (__init__())\n\nclass Marine : \n    #health, ap = 40, 5\n    def attack(self, target) : ## self : 생성된 객체 자신\n        target.health -=self.ap ## 여기에서 target이 공격당한 대상이 m1, 따라서 m2의 공격력 7만큼 감소\n\n\nm1, m2 = Marine(), Marine()\n\n- 객체는 생성이 되었으나 피와 공격력이 없어서 메소드 호출 시 에러가 발생한다.\n\n'Marine' object has no attribute 'health'\n\n\nm2.attack(m1)\n\nAttributeError: 'Marine' object has no attribute 'health'\n\n\n- 위를 방지하는 방법 중 하나가 생성자 메소드이다.\n\n제품을 만들 때, 메소드에서 사용하는 변수들이 있는지 한번 다 확인하는 것!\n\n\nclass Marine : \n    #health, ap = 40, 5\n    def __init__(self,health,ap) : ## 생성자 작성\n        self.health, self.ap = health, ap\n    def attack(self, target) : ## self : 생성된 객체 자신\n        target.health -=self.ap ## 여기에서 target이 공격당한 대상이 m1, 따라서 m2의 공격력 7만큼 감소\n\n- 이제 메서드가 제대로 실행된다.\n\nm1, m2 = Marine(40,5), Marine(40,5)\nm2.attack(m1)\nm2.health, m1.health\n\n(40, 35)\n\n\n- 요약 : 생성자는 객체 생성시 변수의 초기값을 검사 및 설정한다.\n\n디폴트 값도 사용가능\n\n\nclass Marine : \n    #health, ap = 40, 5\n    def __init__(self,health,ap = 5) : ## 생성자 작성\n        self.health, self.ap = health, ap\n    def attack(self, target) : ## self : 생성된 객체 자신\n        target.health -=self.ap ## 여기에서 target이 공격당한 대상이 m1, 따라서 m2의 공격력 7만큼 감소\n\n\nm1, m2 = Marine(40,5), Marine(40)\nm2.attack(m1)\nm2.health, m1.health\n\n(40, 35)\n\n\n\n\n\n\nimport pandas as pd \ndf = pd.DataFrame({\"kospi\" : [3, 4, 5], \"usd\" : [9, 2, 1]})\ndf\n\n\n\n\n\n\n\n\nkospi\nusd\n\n\n\n\n0\n3\n9\n\n\n1\n4\n2\n\n\n2\n5\n1\n\n\n\n\n\n\n\n1 아래 코드는 다른사람이 만든 클래스(모듈) 코드를 우리가 불러오는 것!\nimport pandas as pd \n2 생성자 메서드를 이용하여 초기값을 설정하는 것! (메모리가 사용됨)\ndf = pd.DataFrame({\"kospi\" : [3, 4, 5], \"usd\" : [9, 2, 1]})\n3 객체(df) 안에는 pandas안에 작성된 다양한 메서드들이 존재하며 다음과 같이 사용할 수 있다.\n\ndf.corr()\n\n\n\n\n\n\n\n\nkospi\nusd\n\n\n\n\nkospi\n1.000000\n-0.917663\n\n\nusd\n-0.917663\n1.000000\n\n\n\n\n\n\n\n4. 가장 중요 : dir을 통해 객체안에서 쓸 수 있는 method를 잘 살펴보자!\n5 클래스는 데이터 타입이다!! (\\(\\star\\star\\))\n\nm1 객체는 우리가 만든 marine이라는 클래스의 객체이다.\n\n커스터마이징, 즉, 클래스는 사용자 정의 데이터 타입이다.\n\n\n\nm1 = Marine(40)\ntype(m1)\n\n__main__.Marine\n\n\n6 d1, d2 객체에서 사용가능한 변수와 메서드는 str, list 클래스, 정의되어 있다.\n\n파이썬 문법으로 만들어진 내장 클래스는 앞글자를 소문자로 쓴다.\n\n\nd1 = \"python\"\nd2 = [1, 3, 2]\n\n\ntype(d1), type(d2)\n\n(str, list)\n\n\n7 최종 결론\n\n사실 우리가 하는 변수 생성은 객체를 생성, 즉 클래스 객체를 생성한 것이다.\n따라서, 데이터 타입에 따라서 변수에서 사용가능한 변수, 메서드가 다르다."
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#종류",
    "href": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#종류",
    "title": "00. 데이터 수집 (1)",
    "section": "종류",
    "text": "종류\n- 동적 페이지 : 웹 브라우저에서 이벤트가 발생하면 서버에서 데이터를 가져와 화면을 변경하는 페이지 (url 변경 x)\n\n동적 페이지는 데이터를 가져올 때 json 포맷에서 데이터를 가져온다.\n\n위 경우는 리스트나 딕셔너리로 가져오기 편함\n데이터를 가져오기 위해 추가적인 request, response가 발생함.\n이 때, json 포맷의 데이터를 가져온다!!\n이말은 우리가 파이썬 코드로 해당 url을 요청했을 때 json포맷에 데이터를 가져오는 것이다!\n\n\n- 정적 페이지 : url이 바뀌어야만 화면이 바뀌는 페이지\n\n정적 페이지는 html 포맷에서 데이터를 가져옴\n\n이 경우는 리스트나 딕셔너리로 가져오기 어려움\nrequest, response 시 html 포맷의 데이터를 가져옴\n파이썬 코드로 데이터 수집 시 해당 이벤트의 url을 찾고 requset, response를 한다!\n\n\n\\(\\divideontimes\\) 즉, 페이지 방식에 따라 url 접근 방식이 달라지고, 이에 따라 format이 달라져 크롤링 방식이 달라진다."
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#import",
    "href": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#import",
    "title": "00. 데이터 수집 (1)",
    "section": "import",
    "text": "import\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport requests ## 크롤링을 위한 모듈"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#step-1.-url-찾기-to-웹-페이지-분석",
    "href": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#step-1.-url-찾기-to-웹-페이지-분석",
    "title": "00. 데이터 수집 (1)",
    "section": "step 1. url 찾기 \\(\\to\\) 웹 페이지 분석",
    "text": "step 1. url 찾기 \\(\\to\\) 웹 페이지 분석\n- 크롬 개발자 도구를 이용하여 url을 찾기\n1 네이버의 코스피 주가데이터를 크롤링 (크롤링 할때는 모바일을 이용하자!)\n네이버 코스피 주가 데이터 : 해당 페이지는 동적 페이지\n\npage, pagesize = 1, 20 \nurl = f'https://m.stock.naver.com/api/index/KOSPI/price?pageSize={pagesize}&page={page}' \nprint(url)\n\nhttps://m.stock.naver.com/api/index/KOSPI/price?pageSize=20&page=1"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#step-2.-response",
    "href": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#step-2.-response",
    "title": "00. 데이터 수집 (1)",
    "section": "step 2. response",
    "text": "step 2. response\n2 response\n\nresponse = requests.get(url)\nresponse\n\n&lt;Response [200]&gt;\n\n\n\nresponse.text[:200]\n\n'[{\"localTradedAt\":\"2023-09-04\",\"closePrice\":\"2,579.39\",\"compareToPreviousClosePrice\":\"15.68\",\"compareToPreviousPrice\":{\"code\":\"2\",\"text\":\"상승\",\"name\":\"RISING\"},\"fluctuationsRatio\":\"0.61\",\"openPrice\":\"2'\n\n\n\ntype(response)\n\nrequests.models.Response"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#step-3.-데이터-파싱",
    "href": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#step-3.-데이터-파싱",
    "title": "00. 데이터 수집 (1)",
    "section": "step 3. 데이터 파싱",
    "text": "step 3. 데이터 파싱\n3. data(json(str)) \\(\\to\\) list,dict \\(\\to\\) dataframe : 파싱을 수행\n\ntype(response.text), type(response.json())\n\n(str, list)\n\n\n\ndata = response.json()\n\nkospi = pd.DataFrame(data)[[\"localTradedAt\",\"closePrice\"]]\nkospi.head()\n\n\n\n\n\n\n\n\nlocalTradedAt\nclosePrice\n\n\n\n\n0\n2023-09-04\n2,579.39\n\n\n1\n2023-09-01\n2,563.71\n\n\n2\n2023-08-31\n2,556.27\n\n\n3\n2023-08-30\n2,561.22\n\n\n4\n2023-08-29\n2,552.16"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#step-4.-위-과정을-함수로-작성",
    "href": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#step-4.-위-과정을-함수로-작성",
    "title": "00. 데이터 수집 (1)",
    "section": "step 4. 위 과정을 함수로 작성",
    "text": "step 4. 위 과정을 함수로 작성\n\ndef stock_crawling(page=1, pagesize=60) :\n    url = f'https://m.stock.naver.com/api/index/KOSPI/price?pageSize={pagesize}&page={page}'\n    response = requests.get(url)\n    data = response.json()\n    kospi = pd.DataFrame(data)[[\"localTradedAt\",\"closePrice\"]]\n    return kospi\n\n\nkospi = stock_crawling()"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#extra-코스닥-데이터-가져오기",
    "href": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#extra-코스닥-데이터-가져오기",
    "title": "00. 데이터 수집 (1)",
    "section": "Extra : 코스닥 데이터 가져오기",
    "text": "Extra : 코스닥 데이터 가져오기\n\ndef stock_crawling2(code = \"KOSPI\",page=1, pagesize=60) :\n    url = f'https://m.stock.naver.com/api/index/{code}/price?pageSize={pagesize}&page={page}'\n    response = requests.get(url)\n    data = response.json()\n    kospi = pd.DataFrame(data)[[\"localTradedAt\",\"closePrice\"]]\n    return kospi\n\n\nkosdaq = stock_crawling2(code = \"KOSDAQ\")"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#step-5.-원달러-환율-데이터-수집하기",
    "href": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#step-5.-원달러-환율-데이터-수집하기",
    "title": "00. 데이터 수집 (1)",
    "section": "step 5. 원달러 환율 데이터 수집하기",
    "text": "step 5. 원달러 환율 데이터 수집하기\n- 뼈대\n\npage = 1\nurl = f\"https://m.stock.naver.com/front-api/v1/marketIndex/prices?page={page}&category=exchange&reutersCode=FX_USDKRW\"\n\nresponse = requests.get(url)\n\nusd = pd.DataFrame(response.json()[\"result\"])[[\"localTradedAt\",\"closePrice\"]]\nusd.tail()\n\n\n\n\n\n\n\n\nlocalTradedAt\nclosePrice\n\n\n\n\n5\n2023-08-28\n1,326.00\n\n\n6\n2023-08-25\n1,327.00\n\n\n7\n2023-08-24\n1,325.00\n\n\n8\n2023-08-23\n1,335.00\n\n\n9\n2023-08-22\n1,339.50\n\n\n\n\n\n\n\n- pagesize = 60 설정\n\npage = 1\npagesize = 60\nurl = f\"https://m.stock.naver.com/front-api/v1/marketIndex/prices?page={page}&category=exchange&reutersCode=FX_USDKRW&pageSize={pagesize}\"\n\nresponse = requests.get(url)\n\nusd = pd.DataFrame(response.json()[\"result\"])[[\"localTradedAt\",\"closePrice\"]]\nusd.tail()\n\n\n\n\n\n\n\n\nlocalTradedAt\nclosePrice\n\n\n\n\n55\n2023-06-16\n1,280.00\n\n\n56\n2023-06-15\n1,277.00\n\n\n57\n2023-06-14\n1,275.00\n\n\n58\n2023-06-13\n1,272.00\n\n\n59\n2023-06-12\n1,290.00"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#step-6.-시각화",
    "href": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#step-6.-시각화",
    "title": "00. 데이터 수집 (1)",
    "section": "step 6. 시각화",
    "text": "step 6. 시각화\n\n(1) 데이터 전처리\n\ndf = kospi.copy()\ndf.columns = [\"date\",\"kospi\"]\ndf[\"kosdaq\"] = kosdaq[\"closePrice\"]\ndf[\"usd\"] = usd[\"closePrice\"]\ndf.head()\n\n\n\n\n\n\n\n\ndate\nkospi\nkosdaq\nusd\n\n\n\n\n0\n2023-09-04\n2,584.55\n919.16\n1,320.50\n\n\n1\n2023-09-01\n2,563.71\n919.74\n1,321.50\n\n\n2\n2023-08-31\n2,556.27\n928.40\n1,325.00\n\n\n3\n2023-08-30\n2,561.22\n923.81\n1,322.00\n\n\n4\n2023-08-29\n2,552.16\n916.24\n1,325.00\n\n\n\n\n\n\n\n\ntemp = df.iloc[:,1:].applymap(lambda x : float(x.replace(\",\",\"\")))\n_df = pd.concat([df[\"date\"],temp],axis=1)\n_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 60 entries, 0 to 59\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   date    60 non-null     object \n 1   kospi   60 non-null     float64\n 2   kosdaq  60 non-null     float64\n 3   usd     60 non-null     float64\ndtypes: float64(3), object(1)\nmemory usage: 2.0+ KB\n\n\n\n_df.head()\n\n\n\n\n\n\n\n\ndate\nkospi\nkosdaq\nusd\n\n\n\n\n0\n2023-09-04\n2584.55\n919.16\n1320.5\n\n\n1\n2023-09-01\n2563.71\n919.74\n1321.5\n\n\n2\n2023-08-31\n2556.27\n928.40\n1325.0\n\n\n3\n2023-08-30\n2561.22\n923.81\n1322.0\n\n\n4\n2023-08-29\n2552.16\n916.24\n1325.0\n\n\n\n\n\n\n\n\n\n(2) 시각화\n\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(14,5))\nplt.plot(_df[\"date\"], _df.kospi)\nplt.plot(_df[\"date\"], _df.kosdaq)\nplt.plot(_df[\"date\"], _df.usd)\nplt.xticks(_df[\"date\"].values[::5])\nplt.show()\n\n\n\n\n\n\n(3) 데이터 스케일링 + 시각화\n\nfrom sklearn.preprocessing import minmax_scale\n\n\nplt.figure(figsize=(14,5))\nplt.plot(_df[\"date\"], minmax_scale(_df.kospi))\nplt.plot(_df[\"date\"], minmax_scale(_df.kosdaq))\nplt.plot(_df[\"date\"], minmax_scale(_df.usd))\nplt.xticks(_df[\"date\"].values[::5])\nplt.show()"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#step-7.-상관분석",
    "href": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#step-7.-상관분석",
    "title": "00. 데이터 수집 (1)",
    "section": "step 7. 상관분석",
    "text": "step 7. 상관분석\n\n_df.corr()\n\n\n\n\n\n\n\n\nkospi\nkosdaq\nusd\n\n\n\n\nkospi\n1.000000\n0.435544\n-0.777496\n\n\nkosdaq\n0.435544\n1.000000\n-0.163426\n\n\nusd\n-0.777496\n-0.163426\n1.000000"
  },
  {
    "objectID": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#다음-환율-조사",
    "href": "posts/DX/03. 데이터 수집/2023-09-04-00. 데이터 수집 (1).html#다음-환율-조사",
    "title": "00. 데이터 수집 (1)",
    "section": "다음 환율 조사",
    "text": "다음 환율 조사\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport requests\n\n\n1. 웹서비스 분석 : URL 찾기\n\nurl = \"https://finance.daum.net/api/exchanges/summaries\"\n\n\n\n2. response\n- 403 error가 뜬다? 현재 user-agent가 python으로 찍혀서 그럼\nresponse = requests.get(url) response\n- User-Agent를 같이 보내주어야 한다! (크롤링 소스를 잘보자….)\n- 근데도 403 error가 뜬다….\n\nheaders = {\n\"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)\\\n                Chrome/116.0.0.0 Safari/537.36\",}\n\nresponse = requests.get(url, headers = headers)\nresponse\n\n&lt;Response [403]&gt;\n\n\n- Referer도 추가해야한다. 근데 이런 방법을 알려면 노가다로 다 넣어서 찾아야한다.\n\nheaders = {\n\"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)\\\n                Chrome/116.0.0.0 Safari/537.36\",\n\"Referer\" : \"https://finance.daum.net/exchanges\" }\n\nresponse = requests.get(url, headers = headers)\nresponse\n\n&lt;Response [200]&gt;\n\n\n- 드디어 성공 !\n\nresponse.text[:500]\n\n'{\"data\":[{\"symbolCode\":\"FRX.KRWUSD\",\"date\":\"2023-09-04 16:53:55\",\"currencyCode\":\"USD\",\"currencyName\":\"달러\",\"currencyUnit\":1,\"country\":\"미국\",\"region\":{\"korName\":\"아메리카\",\"engName\":\"America\"},\"name\":\"미국 (USD/KRW)\",\"recurrenceCount\":372,\"basePrice\":1320.5,\"change\":\"FALL\",\"changePrice\":1.0,\"changeRate\":0.0007567159,\"cashBuyingPrice\":1343.6,\"cashSellingPrice\":1297.4,\"ttBuyingPrice\":1307.6,\"ttSellingPrice\":1333.4,\"tcBuyingPrice\":null,\"fcSellingPrice\":null,\"exchangeCommission\":7.1789,\"usDollarRate\":1.0,\"ch'\n\n\n\n\n3. 데이터 파싱\n\ndf = pd.DataFrame(response.json()[\"data\"])[[\"symbolCode\",\"currencyCode\",\"basePrice\"]]\ndf.head()\n\n\n\n\n\n\n\n\nsymbolCode\ncurrencyCode\nbasePrice\n\n\n\n\n0\nFRX.KRWUSD\nUSD\n1320.50\n\n\n1\nFRX.KRWJPY\nJPY\n901.70\n\n\n2\nFRX.KRWCNY\nCNY\n181.52\n\n\n3\nFRX.KRWEUR\nEUR\n1426.14\n\n\n4\nFRX.KRWGBP\nGBP\n1667.00"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-11-00. intro.html",
    "href": "posts/DX/04. 머신러닝/2023-09-11-00. intro.html",
    "title": "00. Intro",
    "section": "",
    "text": "- AirQuality 데이터를 대상으로 모델링을 수행 후 오존 농도를 예측해보자\n\n\n\n# 라이브러리 불러오기\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings(action='ignore')\n%config InlineBackend.figure_format = 'retina'\n\n\n\n\n\n\n- 데이터의 정보 및 EDA 수행\n\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/airquality_simple.csv'\ndata = pd.read_csv(path)\n\n\ndata.head()\n\n\n\n\n\n\n\n\nOzone\nSolar.R\nWind\nTemp\nMonth\nDay\n\n\n\n\n0\n41\n190.0\n7.4\n67\n5\n1\n\n\n1\n36\n118.0\n8.0\n72\n5\n2\n\n\n2\n12\n149.0\n12.6\n74\n5\n3\n\n\n3\n18\n313.0\n11.5\n62\n5\n4\n\n\n4\n19\nNaN\n14.3\n56\n5\n5\n\n\n\n\n\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 153 entries, 0 to 152\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    153 non-null    int64  \n 1   Solar.R  146 non-null    float64\n 2   Wind     153 non-null    float64\n 3   Temp     153 non-null    int64  \n 4   Month    153 non-null    int64  \n 5   Day      153 non-null    int64  \ndtypes: float64(2), int64(4)\nmemory usage: 7.3 KB\n\n\n- 기술통계 확인\n\ndata.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nOzone\n153.0\n42.052288\n30.156127\n1.0\n20.00\n34.0\n59.00\n168.0\n\n\nSolar.R\n146.0\n185.931507\n90.058422\n7.0\n115.75\n205.0\n258.75\n334.0\n\n\nWind\n153.0\n9.957516\n3.523001\n1.7\n7.40\n9.7\n11.50\n20.7\n\n\nTemp\n153.0\n77.882353\n9.465270\n56.0\n72.00\n79.0\n85.00\n97.0\n\n\nMonth\n153.0\n6.993464\n1.416522\n5.0\n6.00\n7.0\n8.00\n9.0\n\n\nDay\n153.0\n15.803922\n8.864520\n1.0\n8.00\n16.0\n23.00\n31.0\n\n\n\n\n\n\n\n- 상관관계 확인\n\ndata.corr()\n\n\n\n\n\n\n\n\nOzone\nSolar.R\nWind\nTemp\nMonth\nDay\n\n\n\n\nOzone\n1.000000\n0.280068\n-0.605478\n0.683372\n0.174197\n0.004419\n\n\nSolar.R\n0.280068\n1.000000\n-0.056792\n0.275840\n-0.075301\n-0.150275\n\n\nWind\n-0.605478\n-0.056792\n1.000000\n-0.457988\n-0.178293\n0.027181\n\n\nTemp\n0.683372\n0.275840\n-0.457988\n1.000000\n0.420947\n-0.130593\n\n\nMonth\n0.174197\n-0.075301\n-0.178293\n0.420947\n1.000000\n-0.007962\n\n\nDay\n0.004419\n-0.150275\n0.027181\n-0.130593\n-0.007962\n1.000000\n\n\n\n\n\n\n\n\nsns.heatmap(data.corr(),\n                   fmt = \".3f\",\n                    annot = True,\n                   square =True,\n                    cbar = False,\n                   annot_kws = {\"size\"  : 8})\n\n&lt;Axes: &gt;\n\n\n\n\n\n- temp \\(\\to\\) ozone의 관계\n\nplt.figure(figsize=(4,4))\nplt.scatter(data.Temp, data.Ozone,alpha=0.3)\n\n&lt;matplotlib.collections.PathCollection at 0x1c510169b10&gt;\n\n\n\n\n\n\n\n\n\n\n\n\ndata.isna().sum()\n\nOzone      0\nSolar.R    7\nWind       0\nTemp       0\nMonth      0\nDay        0\ndtype: int64\n\n\n\ndata.fillna(method= \"ffill\",inplace=True)\n\n\ndata.isna().sum()\n\nOzone      0\nSolar.R    0\nWind       0\nTemp       0\nMonth      0\nDay        0\ndtype: int64\n\n\n\n\n\n\nd_cols = data.columns.drop([\"Month\", \"Day\"])\n\n_data = data[d_cols].copy()\n\n\n_data\n\n\n\n\n\n\n\n\nOzone\nSolar.R\nWind\nTemp\n\n\n\n\n0\n41\n190.0\n7.4\n67\n\n\n1\n36\n118.0\n8.0\n72\n\n\n2\n12\n149.0\n12.6\n74\n\n\n3\n18\n313.0\n11.5\n62\n\n\n4\n19\n313.0\n14.3\n56\n\n\n...\n...\n...\n...\n...\n\n\n148\n30\n193.0\n6.9\n70\n\n\n149\n23\n145.0\n13.2\n77\n\n\n150\n14\n191.0\n14.3\n75\n\n\n151\n18\n131.0\n8.0\n76\n\n\n152\n20\n223.0\n11.5\n68\n\n\n\n\n153 rows × 4 columns\n\n\n\n\n\n\n\nx = _data.iloc[:,1:]\ny = _data[\"Ozone\"]\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=7)\nx_train.head()\n\n\n\n\n\n\n\n\nSolar.R\nWind\nTemp\n\n\n\n\n62\n248.0\n9.2\n85\n\n\n51\n150.0\n6.3\n77\n\n\n141\n238.0\n10.3\n68\n\n\n118\n153.0\n5.7\n88\n\n\n74\n291.0\n14.9\n91\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error \n\n\n\n\n\nmodel = LinearRegression()\n\n\n\n\n\nmodel.fit(x_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\n\n\ny_pred = model.predict(x_test)\n\n\n\n\n\nmean_absolute_error(y_pred,y_test)\n\n12.930971421482479\n\n\n- 시각화\n\nplt.plot(y_pred)\nplt.plot(y_test.values)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-11-00. intro.html#import",
    "href": "posts/DX/04. 머신러닝/2023-09-11-00. intro.html#import",
    "title": "00. Intro",
    "section": "",
    "text": "# 라이브러리 불러오기\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings(action='ignore')\n%config InlineBackend.figure_format = 'retina'"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-11-00. intro.html#데이터-이해",
    "href": "posts/DX/04. 머신러닝/2023-09-11-00. intro.html#데이터-이해",
    "title": "00. Intro",
    "section": "",
    "text": "- 데이터의 정보 및 EDA 수행\n\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/airquality_simple.csv'\ndata = pd.read_csv(path)\n\n\ndata.head()\n\n\n\n\n\n\n\n\nOzone\nSolar.R\nWind\nTemp\nMonth\nDay\n\n\n\n\n0\n41\n190.0\n7.4\n67\n5\n1\n\n\n1\n36\n118.0\n8.0\n72\n5\n2\n\n\n2\n12\n149.0\n12.6\n74\n5\n3\n\n\n3\n18\n313.0\n11.5\n62\n5\n4\n\n\n4\n19\nNaN\n14.3\n56\n5\n5\n\n\n\n\n\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 153 entries, 0 to 152\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    153 non-null    int64  \n 1   Solar.R  146 non-null    float64\n 2   Wind     153 non-null    float64\n 3   Temp     153 non-null    int64  \n 4   Month    153 non-null    int64  \n 5   Day      153 non-null    int64  \ndtypes: float64(2), int64(4)\nmemory usage: 7.3 KB\n\n\n- 기술통계 확인\n\ndata.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nOzone\n153.0\n42.052288\n30.156127\n1.0\n20.00\n34.0\n59.00\n168.0\n\n\nSolar.R\n146.0\n185.931507\n90.058422\n7.0\n115.75\n205.0\n258.75\n334.0\n\n\nWind\n153.0\n9.957516\n3.523001\n1.7\n7.40\n9.7\n11.50\n20.7\n\n\nTemp\n153.0\n77.882353\n9.465270\n56.0\n72.00\n79.0\n85.00\n97.0\n\n\nMonth\n153.0\n6.993464\n1.416522\n5.0\n6.00\n7.0\n8.00\n9.0\n\n\nDay\n153.0\n15.803922\n8.864520\n1.0\n8.00\n16.0\n23.00\n31.0\n\n\n\n\n\n\n\n- 상관관계 확인\n\ndata.corr()\n\n\n\n\n\n\n\n\nOzone\nSolar.R\nWind\nTemp\nMonth\nDay\n\n\n\n\nOzone\n1.000000\n0.280068\n-0.605478\n0.683372\n0.174197\n0.004419\n\n\nSolar.R\n0.280068\n1.000000\n-0.056792\n0.275840\n-0.075301\n-0.150275\n\n\nWind\n-0.605478\n-0.056792\n1.000000\n-0.457988\n-0.178293\n0.027181\n\n\nTemp\n0.683372\n0.275840\n-0.457988\n1.000000\n0.420947\n-0.130593\n\n\nMonth\n0.174197\n-0.075301\n-0.178293\n0.420947\n1.000000\n-0.007962\n\n\nDay\n0.004419\n-0.150275\n0.027181\n-0.130593\n-0.007962\n1.000000\n\n\n\n\n\n\n\n\nsns.heatmap(data.corr(),\n                   fmt = \".3f\",\n                    annot = True,\n                   square =True,\n                    cbar = False,\n                   annot_kws = {\"size\"  : 8})\n\n&lt;Axes: &gt;\n\n\n\n\n\n- temp \\(\\to\\) ozone의 관계\n\nplt.figure(figsize=(4,4))\nplt.scatter(data.Temp, data.Ozone,alpha=0.3)\n\n&lt;matplotlib.collections.PathCollection at 0x1c510169b10&gt;"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-11-00. intro.html#데이터-준비",
    "href": "posts/DX/04. 머신러닝/2023-09-11-00. intro.html#데이터-준비",
    "title": "00. Intro",
    "section": "",
    "text": "data.isna().sum()\n\nOzone      0\nSolar.R    7\nWind       0\nTemp       0\nMonth      0\nDay        0\ndtype: int64\n\n\n\ndata.fillna(method= \"ffill\",inplace=True)\n\n\ndata.isna().sum()\n\nOzone      0\nSolar.R    0\nWind       0\nTemp       0\nMonth      0\nDay        0\ndtype: int64\n\n\n\n\n\n\nd_cols = data.columns.drop([\"Month\", \"Day\"])\n\n_data = data[d_cols].copy()\n\n\n_data\n\n\n\n\n\n\n\n\nOzone\nSolar.R\nWind\nTemp\n\n\n\n\n0\n41\n190.0\n7.4\n67\n\n\n1\n36\n118.0\n8.0\n72\n\n\n2\n12\n149.0\n12.6\n74\n\n\n3\n18\n313.0\n11.5\n62\n\n\n4\n19\n313.0\n14.3\n56\n\n\n...\n...\n...\n...\n...\n\n\n148\n30\n193.0\n6.9\n70\n\n\n149\n23\n145.0\n13.2\n77\n\n\n150\n14\n191.0\n14.3\n75\n\n\n151\n18\n131.0\n8.0\n76\n\n\n152\n20\n223.0\n11.5\n68\n\n\n\n\n153 rows × 4 columns\n\n\n\n\n\n\n\nx = _data.iloc[:,1:]\ny = _data[\"Ozone\"]\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=7)\nx_train.head()\n\n\n\n\n\n\n\n\nSolar.R\nWind\nTemp\n\n\n\n\n62\n248.0\n9.2\n85\n\n\n51\n150.0\n6.3\n77\n\n\n141\n238.0\n10.3\n68\n\n\n118\n153.0\n5.7\n88\n\n\n74\n291.0\n14.9\n91"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-11-00. intro.html#모델링",
    "href": "posts/DX/04. 머신러닝/2023-09-11-00. intro.html#모델링",
    "title": "00. Intro",
    "section": "",
    "text": "from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error \n\n\n\n\n\nmodel = LinearRegression()\n\n\n\n\n\nmodel.fit(x_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\n\n\ny_pred = model.predict(x_test)\n\n\n\n\n\nmean_absolute_error(y_pred,y_test)\n\n12.930971421482479\n\n\n- 시각화\n\nplt.plot(y_pred)\nplt.plot(y_test.values)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html",
    "href": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html",
    "title": "02. 머신러닝 (2)",
    "section": "",
    "text": "# 라이브러리 불러오기\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\nwarnings.filterwarnings(action='ignore')\n%config InlineBackend.figure_format='retina'"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#데이터-로드-및-탐색",
    "href": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#데이터-로드-및-탐색",
    "title": "02. 머신러닝 (2)",
    "section": "(1) 데이터 로드 및 탐색",
    "text": "(1) 데이터 로드 및 탐색\n\n# 데이터 읽어오기\npath = 'https://bit.ly/CarsFile'\ndata = pd.read_csv(path)\n\n\ndata.head()\n\n\n\n\n\n\n\n\nspeed\ndist\n\n\n\n\n0\n4\n2\n\n\n1\n4\n10\n\n\n2\n7\n4\n\n\n3\n7\n22\n\n\n4\n8\n16\n\n\n\n\n\n\n\n\ndata.describe()\n\n\n\n\n\n\n\n\nspeed\ndist\n\n\n\n\ncount\n50.000000\n50.000000\n\n\nmean\n15.400000\n42.980000\n\n\nstd\n5.287644\n25.769377\n\n\nmin\n4.000000\n2.000000\n\n\n25%\n12.000000\n26.000000\n\n\n50%\n15.000000\n36.000000\n\n\n75%\n19.000000\n56.000000\n\n\nmax\n25.000000\n120.000000\n\n\n\n\n\n\n\n\ndata.isnull().sum()\n\nspeed    0\ndist     0\ndtype: int64\n\n\n- \\((x,y)\\) 시각화\n\nplt.figure(figsize = (4, 4))\nplt.plot(data.speed, data.dist, \".r\", label = r\"$(x, y)$\",alpha = 0.3)\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x1c94b19fb90&gt;"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#모델링",
    "href": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#모델링",
    "title": "02. 머신러닝 (2)",
    "section": "(2) 모델링",
    "text": "(2) 모델링\n\n\nCode\ntarget = \"dist\"\n\n## step 1. x,y 부리\nx  = data.drop(target, axis = 1)\ny  = data[target]\n\n## step 2. 훈련 데이터와 평가 데이터 분리\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 1, test_size = 0.3)\n\n## step 3 . 모델 불러오기\nfrom sklearn.linear_model import  LinearRegression\nmodel1 = LinearRegression()\n\n## step 4.  모델 fit\nmodel1.fit(x_train, y_train)\n\n## step 5. 예측\ny_pred = model1.predict(x_test)\n\n\n- 회귀계수 확인\n\nprint(model1.coef_)\nprint(model1.intercept_)\n\n[3.91046344]\n-16.37336414935769\n\n\n- 산출된 회귀식\n\\[\\text{dist} \\approx 3.9 \\times \\text{speed} -16.37\\]\n- 결과 시각화\n\n\nCode\nplt.figure(figsize = (4,4))\nplt.plot(x_test, y_test, \".r\", label =r\"$(x,y)$\", alpha = 0.3)\nplt.plot(x_test, y_pred, label =r\"$(x,\\hat {y})$\", alpha = 0.5)\nplt.axhline(y_train.mean(),linestyle = \"--\", color = \"g\", label = r\"$\\bar {y}_{train}$\")\nplt.axhline(y_test.mean(),linestyle = \"--\", color = \"y\", label = r\"$\\bar {y}_{test}$\")\nplt.legend()\n\n\n&lt;matplotlib.legend.Legend at 0x1c94e0b02d0&gt;\n\n\n\n\n\n- 예측성능확인\n\nfrom sklearn.metrics import *\n\nprint(f\"MAE :  {mean_absolute_error(y_test, y_pred) : .2f}\" )\nprint(f\"R2 : {r2_score(y_test, y_pred) : .2f}\")\n\nMAE :   15.11\nR2 :  0.55\n\n\n- 학습 데이터의 평균 성능은?\n\ny_mean = y_train.mean()\ny_mean = [y_mean]*len(y_pred)\n\nprint(f\"MAE :  {mean_absolute_error(y_test, y_mean) : .2f}\" )\nprint(f\"R2 : {r2_score(y_test, y_mean) : .2f}\")\n\nMAE :   18.34\nR2 : -0.06"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#excercise.-1",
    "href": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#excercise.-1",
    "title": "02. 머신러닝 (2)",
    "section": "excercise. 1",
    "text": "excercise. 1\n\n(1). 데이터 로드 및 전처리\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/income_happy.csv'\ndata = pd.read_csv(path)\n\n\ndata.isna().sum()\n\nincome       0\nhappiness    0\ndtype: int64\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 498 entries, 0 to 497\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   income     498 non-null    float64\n 1   happiness  498 non-null    float64\ndtypes: float64(2)\nmemory usage: 7.9 KB\n\n\n\n\n(2). 모델링\n\n\nCode\n# step 1.  데이터 분리\ntarget = \"income\"\n\nx = data.drop(target, axis = 1)\ny = data[target]\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, random_state = 1, test_size = 0.3)\n\n# step 2.  model 선언 \nmodel2 = LinearRegression()\n\n# step 3. fit\nmodel2.fit(x_train,y_train)\n\n# step 4. predict\ny_pred = model2.predict(x_test)\n\nr2 = r2_score(y_test, y_pred)\nMAE = mean_absolute_error(y_test, y_pred)\n\nplt.figure(figsize=(4,4))\nplt.plot(x_test, y_test, \".r\", label = r\"$(x,y)$\", alpha = 0.3)\nplt.plot(x_test, y_pred, \".b\", label = r\"$(x,\\hat {y})$\", alpha = 0.3)\nplt.title(r\"$income = 1.03 \\times happiness + 0.963,\\,   R^{2} = 0.75, MAE = 0.69$\",fontsize = 10)\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#excercise.-2",
    "href": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#excercise.-2",
    "title": "02. 머신러닝 (2)",
    "section": "excercise. 2",
    "text": "excercise. 2\n데이터 설명\n\nSales: 각 지역 판매량(단위: 1,000개)\nCompPrice: 경쟁사 가격 (단위: 달러)\nIncome: 지역 평균 소득 (단위: 1,000달러)\nAdvertising: 각 지역, 회사의 광고 예산 (단위: 1,000달러)\nPopulation: 지역 인구 수 (단위: 1,000명)\nPrice: 자사 지역별 판매 가격 (단위: 달러)\nShelveLoc: 진열 상태\nAge: 지역 인구의 평균 연령\nEducation: 각 지역 교육 수준\nUrban: 도심 지역 여부 (Yes,No)\nUS: 매장이 미국에 있는지 여부 (Yes, No)\n\n\n(1) 데이터 탐색 및 전처리\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/Carseats.csv'\ndata = pd.read_csv(path)\n\ndata.isna().sum()\n\nSales          0\nCompPrice      0\nIncome         0\nAdvertising    0\nPopulation     0\nPrice          0\nShelveLoc      0\nAge            0\nEducation      0\nUrban          0\nUS             0\ndtype: int64\n\n\n\ndata.select_dtypes(\"number\").corr()\n\n\n\n\n\n\n\n\nSales\nCompPrice\nIncome\nAdvertising\nPopulation\nPrice\nAge\nEducation\n\n\n\n\nSales\n1.000000\n0.064079\n0.151951\n0.269507\n0.050471\n-0.444951\n-0.231815\n-0.051955\n\n\nCompPrice\n0.064079\n1.000000\n-0.080653\n-0.024199\n-0.094707\n0.584848\n-0.100239\n0.025197\n\n\nIncome\n0.151951\n-0.080653\n1.000000\n0.058995\n-0.007877\n-0.056698\n-0.004670\n-0.056855\n\n\nAdvertising\n0.269507\n-0.024199\n0.058995\n1.000000\n0.265652\n0.044537\n-0.004557\n-0.033594\n\n\nPopulation\n0.050471\n-0.094707\n-0.007877\n0.265652\n1.000000\n-0.012144\n-0.042663\n-0.106378\n\n\nPrice\n-0.444951\n0.584848\n-0.056698\n0.044537\n-0.012144\n1.000000\n-0.102177\n0.011747\n\n\nAge\n-0.231815\n-0.100239\n-0.004670\n-0.004557\n-0.042663\n-0.102177\n1.000000\n0.006488\n\n\nEducation\n-0.051955\n0.025197\n-0.056855\n-0.033594\n-0.106378\n0.011747\n0.006488\n1.000000\n\n\n\n\n\n\n\n- 컬럼 가변수화\n\nd_cols = [\"ShelveLoc\", \"Education\", \"Urban\", \"US\"]\n\n\n_data = pd.get_dummies(data, columns= d_cols, drop_first=True, dtype = \"float\")\n_data.head()\n\n\n\n\n\n\n\n\nSales\nCompPrice\nIncome\nAdvertising\nPopulation\nPrice\nAge\nShelveLoc_Good\nShelveLoc_Medium\nEducation_11\nEducation_12\nEducation_13\nEducation_14\nEducation_15\nEducation_16\nEducation_17\nEducation_18\nUrban_Yes\nUS_Yes\n\n\n\n\n0\n9.50\n138\n73\n11\n276\n120\n42\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n\n\n1\n11.22\n111\n48\n16\n260\n83\n65\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n2\n10.06\n113\n35\n10\n269\n80\n59\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n3\n7.40\n117\n100\n4\n466\n97\n55\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n4\n4.15\n141\n64\n3\n340\n128\n38\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\n- 학습용, 평가용 데이터 분리\n\ntarget  = \"Sales\"\nx = _data.drop(target, axis = 1)\ny = _data[target]\n\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 1, test_size = 0.3)\n\n\n\n(2) 모델링\n\n## 1. 모델 선언\nmodel3 = LinearRegression()\n\n## 2. model fit\nmodel3.fit(x_train, y_train)\n\n## 3. predict\ny_pred = model3.predict(x_test)\n\n## 5. 결정계수 및 MAE 값 계산\nr2 = r2_score(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\n\n\n\n(3) 시각화 1 : 회귀계수\n\n\nCode\nx_cols = x_train.columns.values\ncoef = model3.coef_\n\nfig = pd.DataFrame({\"X\" : x_cols,\n                            \"coef\" : coef}).\\\n                                sort_values(\"coef\",ascending=False).\\\n                                                        plot(x=\"coef\", y=\"X\",\n                                                               color= \"X\",kind= \"barh\",\n                                                                backend = \"plotly\",width = 700, height = 450)\nfig.update_layout(showlegend=False)\n\n\n\n                                                \n\n\n\n\n(4) 시각화 2 : 잔차 시각화\n- 잔차를 살펴보니 잔차의 평균으로부터 분포가 일정하다. (회귀선이 안정적으로 추정된 것 같다!)\n\n\nCode\ne = y_test - y_pred\ne_m = e.mean()\n\nplt.figure(figsize=(5,4))\nplt.plot(e,'.',label = r\"$\\varepsilon$\")\nplt.axhline(e_m, linestyle = \"--\", color = \"red\", label = r\"$E(\\varepsilon)$\")\nplt.title(r\"$\\varepsilon \\sim  N(0, \\sigma^2)$\")\nplt.legend()\nplt.ylim(-5,5)\nplt.show()\n\n\n\n\n\n\n\n(4) 시각화 3 : \\((y, \\hat y)\\)\n\nplt.plot(y_test.reset_index(drop=True), \"--r\", label = r\"$y$\",alpha = 0.3)\nplt.plot(y_pred, \"b\", label = r\"$\\hat {y}$\",alpha=0.5)\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x1c94e3cf890&gt;"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#데이터-로드-및-탐색-1",
    "href": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#데이터-로드-및-탐색-1",
    "title": "02. 머신러닝 (2)",
    "section": "(1) 데이터 로드 및 탐색",
    "text": "(1) 데이터 로드 및 탐색\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/airquality_simple.csv'\ndata = pd.read_csv(path)\n\n\ndata.head()\n\n\n\n\n\n\n\n\nOzone\nSolar.R\nWind\nTemp\nMonth\nDay\n\n\n\n\n0\n41\n190.0\n7.4\n67\n5\n1\n\n\n1\n36\n118.0\n8.0\n72\n5\n2\n\n\n2\n12\n149.0\n12.6\n74\n5\n3\n\n\n3\n18\n313.0\n11.5\n62\n5\n4\n\n\n4\n19\nNaN\n14.3\n56\n5\n5\n\n\n\n\n\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 153 entries, 0 to 152\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Ozone    153 non-null    int64  \n 1   Solar.R  146 non-null    float64\n 2   Wind     153 non-null    float64\n 3   Temp     153 non-null    int64  \n 4   Month    153 non-null    int64  \n 5   Day      153 non-null    int64  \ndtypes: float64(2), int64(4)\nmemory usage: 7.3 KB\n\n\n\ndata.isna().sum()\n\nOzone      0\nSolar.R    7\nWind       0\nTemp       0\nMonth      0\nDay        0\ndtype: int64\n\n\n\ndata.corr()\n\n\n\n\n\n\n\n\nOzone\nSolar.R\nWind\nTemp\nMonth\nDay\n\n\n\n\nOzone\n1.000000\n0.280068\n-0.605478\n0.683372\n0.174197\n0.004419\n\n\nSolar.R\n0.280068\n1.000000\n-0.056792\n0.275840\n-0.075301\n-0.150275\n\n\nWind\n-0.605478\n-0.056792\n1.000000\n-0.457988\n-0.178293\n0.027181\n\n\nTemp\n0.683372\n0.275840\n-0.457988\n1.000000\n0.420947\n-0.130593\n\n\nMonth\n0.174197\n-0.075301\n-0.178293\n0.420947\n1.000000\n-0.007962\n\n\nDay\n0.004419\n-0.150275\n0.027181\n-0.130593\n-0.007962\n1.000000"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#결측치-처리",
    "href": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#결측치-처리",
    "title": "02. 머신러닝 (2)",
    "section": "(2) 결측치 처리",
    "text": "(2) 결측치 처리\n- 시계열 데이터이므로 선형보간법으로 채움\n\ndata[\"Solar.R\"].interpolate(method = \"linear\", inplace = True)\n\n\ndata.isna().sum()\n\nOzone      0\nSolar.R    0\nWind       0\nTemp       0\nMonth      0\nDay        0\ndtype: int64"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#변수-제거",
    "href": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#변수-제거",
    "title": "02. 머신러닝 (2)",
    "section": "(3) 변수 제거",
    "text": "(3) 변수 제거\n\n#  변수 제거\ndrop_cols = ['Month', 'Day']\ndata.drop(drop_cols, axis=1, inplace=True)\n\n# 확인\ndata.head()\n\n\n\n\n\n\n\n\nOzone\nSolar.R\nWind\nTemp\n\n\n\n\n0\n41\n190.000000\n7.4\n67\n\n\n1\n36\n118.000000\n8.0\n72\n\n\n2\n12\n149.000000\n12.6\n74\n\n\n3\n18\n313.000000\n11.5\n62\n\n\n4\n19\n308.333333\n14.3\n56"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#모델링-3",
    "href": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#모델링-3",
    "title": "02. 머신러닝 (2)",
    "section": "(4) 모델링",
    "text": "(4) 모델링\n\n# target 확인\ntarget = 'Ozone'\n\n# 데이터 분리\nx = data.drop(target, axis=1)\ny = data.loc[:, target]\n\n# 모듈 불러오기\nfrom sklearn.model_selection import train_test_split\n\n# 데이터 분리\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n\n- 스케일링\n\n# 1.  손계산\nx_train = x_train.apply(lambda x : \n                                            (x-min(x))/(max(x)-min(x)),axis=0)\n\nx_test = x_test.apply(lambda x : \n                                            (x-min(x))/(max(x)-min(x)),axis=0)\n\n# 2. 모듈 이용\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n- 모델링 수행\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_absolute_error, r2_score\n\nmodel = KNeighborsRegressor()\n\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\n\nprint(f\"MAE : {mean_absolute_error(y_test, y_pred ): .2f}, R2 = {r2_score(y_test, y_pred): .2f} \")\n\nMAE :  13.68, R2 =  0.57"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#결과-시각화",
    "href": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#결과-시각화",
    "title": "02. 머신러닝 (2)",
    "section": "(5) 결과 시각화",
    "text": "(5) 결과 시각화\n\nplt.figure(figsize = (5,4))\nplt.plot(y_test.reset_index(drop=True),\"--r\",label = r\"$y$\",alpha = .3)\nplt.plot(y_pred,\"--b\",label = r\"$\\hat y$\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x1c94e3cfb90&gt;"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#exercise.-1",
    "href": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#exercise.-1",
    "title": "02. 머신러닝 (2)",
    "section": "exercise. 1",
    "text": "exercise. 1\n\n(1) 데이터 탐색 및 전처리\n데이터설명\n피마 인디언 당뇨 데이터셋은 몇 명의 여성 피마 인디언의 진료 자료와 진단 후 5년 내 당뇨 발병 여부로 구성됨\n\nPregnancies: 임신 횟수\nGlucose: 포도당 부하 검사 수치\nBloodPressure: 혈압(mm Hg)\nSkinThickness: 팔 삼두근 뒤쪽의 피하지방 측정값(mm)\nInsulin: 혈청 인슐린(mu U/ml)\nBMI: 체질량지수(체중(kg)/키(m))^2\nDiabetesPedigreeFunction: 당뇨 내력 가중치 값\nAge: 나이\nOutcome: 클래스 결정 값(0 또는 1)\n\ndiabetes\n\n당뇨병(糖尿病, diabetes)은 높은 혈당 수치가 오랜 기간 지속되는 대사 질환이다.\n혈당이 높을 때의 증상으로는 소변이 잦아지고, 갈증과 배고픔이 심해진다.\n이를 치료하지 않으면 다른 합병증을 유발할 수 있다. (출처: 위키백과)\n\n\npath = 'https://raw.githubusercontent.com/jangrae/csv/master/diabetes.csv'\ndata = pd.read_csv(path)\ndata.head()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 768 entries, 0 to 767\nData columns (total 9 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Pregnancies               768 non-null    int64  \n 1   Glucose                   768 non-null    int64  \n 2   BloodPressure             768 non-null    int64  \n 3   SkinThickness             768 non-null    int64  \n 4   Insulin                   768 non-null    int64  \n 5   BMI                       768 non-null    float64\n 6   DiabetesPedigreeFunction  768 non-null    float64\n 7   Age                       768 non-null    int64  \n 8   Outcome                   768 non-null    int64  \ndtypes: float64(2), int64(7)\nmemory usage: 54.1 KB\n\n\n\ndata.isna().sum()\n\nPregnancies                 0\nGlucose                     0\nBloodPressure               0\nSkinThickness               0\nInsulin                     0\nBMI                         0\nDiabetesPedigreeFunction    0\nAge                         0\nOutcome                     0\ndtype: int64\n\n\n\ndata.Outcome.value_counts()\n\nOutcome\n0    500\n1    268\nName: count, dtype: int64\n\n\n\n\n(2) 모델링\n\n# step 1. 데이터 분리\ntarget = \"Outcome\"\n\nx = data.drop(target, axis=1)\ny = data[target]\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 1, test_size = 0.3)\n\n# step 2.  정규화\n\nscale = lambda  x : (x-min(x)) /(max(x)-min(x))\n\nx_train = x_train.apply(scale, axis=0)\nx_test = x_test.apply(scale, axis=0)\n\n# step 3. 모델 호출 및 예측\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import  accuracy_score, precision_score, recall_score, f1_score\n\nmodel = KNeighborsClassifier()\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\n\nprint(f\"accuracy : {accuracy_score(y_pred, y_test) : .3}\")\nprint(f\"precision : {precision_score(y_pred, y_test) : .3}\")\nprint(f\"recall : {recall_score(y_pred, y_test) : .3}\")\nprint(f\"F1-score : {f1_score(y_pred, y_test) : .3}\")\n\naccuracy :  0.779\nprecision :  0.506\nrecall :  0.827\nF1-score :  0.628\n\n\n\n\n(3) 결과 시각화 1 : \\((y, \\hat y)\\) 클래스 비교\n\npd.DataFrame({\"y_test\" : y_test.astype(str),\n                             \"y_pred\" : y_pred.astype(str)}).\\\n                                    melt(var_name = \"label\", \n                                     value_name= \"Outcome\").\\\n                                     groupby(\"label\",as_index=False)[[\"Outcome\"]].value_counts().\\\n                                            plot(x = \"label\",y = \"count\", \n                                                    kind =\"bar\", backend = \"plotly\",\n                                                     color = \"label\", facet_col = \"Outcome\",width = 500, height = 400)\n\n\n                                                \n\n\n\n\n(4) 결과 시각화 2 : 평가지표\n\nacc = accuracy_score(y_pred, y_test)\npre = precision_score(y_pred, y_test)\nrecall = recall_score(y_pred, y_test)\nf1 = f1_score(y_pred, y_test)\n\nresult = [acc, pre, recall, f1]\ncol = [\"accuracy\", \"precision\", \"recall\", \"F1-score\"]\n\npd.DataFrame(result,columns=[\"value\"],index=col).reset_index().\\\n                plot(x=\"index\", y= \"value\",kind=\"bar\",color= \"index\", backend = \"plotly\",\n                        height = 400, width = 600)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#excercise.-2-불균형-클래스",
    "href": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#excercise.-2-불균형-클래스",
    "title": "02. 머신러닝 (2)",
    "section": "excercise. 2 (불균형 클래스)",
    "text": "excercise. 2 (불균형 클래스)\n데이터 설명\n\nAttrition: 이직 여부 (1: 이직, 0: 잔류)\nAge: 나이\nDistanceFromHome: 집-직장 거리 (단위: 마일)\nEmployeeNumber: 사번\nGender: 성별 (Male, Female)\nJobSatisfaction: 직무 만족도(1: Low, 2: Medium, 3: High, 4: Very High)\nMaritalStatus: 결혼 상태 (Single, Married, Divorced)\nMonthlyIncome: 월급 (단위: 달러)\nOverTime: 야근 여부 (Yes, No)\nPercentSalaryHike: 전년 대비 급여 인상율(단위: %)\nTotalWorkingYears: 총 경력 연수\n\n\n(1) 데이터 로드 및 탐색\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/jangrae/csv/master/Attrition_simple2.csv'\ndata = pd.read_csv(path)\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1196 entries, 0 to 1195\nData columns (total 11 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   Attrition          1196 non-null   int64 \n 1   Age                1196 non-null   int64 \n 2   DistanceFromHome   1196 non-null   int64 \n 3   EmployeeNumber     1196 non-null   int64 \n 4   Gender             1196 non-null   object\n 5   JobSatisfaction    1196 non-null   int64 \n 6   MaritalStatus      1196 non-null   object\n 7   MonthlyIncome      1196 non-null   int64 \n 8   OverTime           1196 non-null   object\n 9   PercentSalaryHike  1196 non-null   int64 \n 10  TotalWorkingYears  1196 non-null   int64 \ndtypes: int64(8), object(3)\nmemory usage: 102.9+ KB\n\n\n\ndata.isna().sum()\n\nAttrition            0\nAge                  0\nDistanceFromHome     0\nEmployeeNumber       0\nGender               0\nJobSatisfaction      0\nMaritalStatus        0\nMonthlyIncome        0\nOverTime             0\nPercentSalaryHike    0\nTotalWorkingYears    0\ndtype: int64\n\n\n\ndata.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nAttrition\n1196.0\n0.163043\n0.369560\n0.0\n0.00\n0.0\n0.00\n1.0\n\n\nAge\n1196.0\n36.943980\n9.092700\n18.0\n30.00\n36.0\n43.00\n60.0\n\n\nDistanceFromHome\n1196.0\n9.258361\n8.166016\n1.0\n2.00\n7.0\n14.00\n29.0\n\n\nEmployeeNumber\n1196.0\n1035.629599\n604.340130\n1.0\n507.75\n1028.0\n1581.25\n2068.0\n\n\nJobSatisfaction\n1196.0\n2.716555\n1.110962\n1.0\n2.00\n3.0\n4.00\n4.0\n\n\nMonthlyIncome\n1196.0\n6520.104515\n4665.902253\n1009.0\n2928.25\n4973.5\n8420.50\n19999.0\n\n\nPercentSalaryHike\n1196.0\n15.251672\n3.625946\n11.0\n12.00\n14.0\n18.00\n25.0\n\n\nTotalWorkingYears\n1196.0\n11.330268\n7.823821\n0.0\n6.00\n10.0\n15.00\n40.0\n\n\n\n\n\n\n\n\ndata.select_dtypes(\"number\").corr()\n\n\n\n\n\n\n\n\nAttrition\nAge\nDistanceFromHome\nEmployeeNumber\nJobSatisfaction\nMonthlyIncome\nPercentSalaryHike\nTotalWorkingYears\n\n\n\n\nAttrition\n1.000000\n-0.167866\n0.081973\n-0.008707\n-0.078936\n-0.163572\n-0.000048\n-0.182162\n\n\nAge\n-0.167866\n1.000000\n-0.010917\n-0.023786\n-0.012425\n0.490107\n-0.008303\n0.674331\n\n\nDistanceFromHome\n0.081973\n-0.010917\n1.000000\n0.054948\n-0.021623\n-0.012803\n0.052348\n0.002606\n\n\nEmployeeNumber\n-0.008707\n-0.023786\n0.054948\n1.000000\n-0.022863\n-0.014032\n-0.009514\n-0.016317\n\n\nJobSatisfaction\n-0.078936\n-0.012425\n-0.021623\n-0.022863\n1.000000\n-0.025082\n0.030811\n-0.039380\n\n\nMonthlyIncome\n-0.163572\n0.490107\n-0.012803\n-0.014032\n-0.025082\n1.000000\n-0.021334\n0.768437\n\n\nPercentSalaryHike\n-0.000048\n-0.008303\n0.052348\n-0.009514\n0.030811\n-0.021334\n1.000000\n-0.021988\n\n\nTotalWorkingYears\n-0.182162\n0.674331\n0.002606\n-0.016317\n-0.039380\n0.768437\n-0.021988\n1.000000\n\n\n\n\n\n\n\n\n\n(2) 데이터 준비\n- 변수 제거\n\ndata.drop(\"EmployeeNumber\", axis=1, inplace =True)\ndata.head()\n\n\n\n\n\n\n\n\nAttrition\nAge\nDistanceFromHome\nGender\nJobSatisfaction\nMaritalStatus\nMonthlyIncome\nOverTime\nPercentSalaryHike\nTotalWorkingYears\n\n\n\n\n0\n0\n33\n7\nMale\n3\nMarried\n11691\nNo\n11\n14\n\n\n1\n0\n35\n18\nMale\n4\nSingle\n9362\nNo\n11\n10\n\n\n2\n0\n42\n6\nMale\n1\nMarried\n13348\nNo\n13\n18\n\n\n3\n0\n46\n2\nFemale\n1\nMarried\n17048\nNo\n23\n28\n\n\n4\n1\n22\n4\nMale\n3\nSingle\n3894\nNo\n16\n4\n\n\n\n\n\n\n\n- x, y 분리\n\ntarget = \"Attrition\"\n\nx = data.drop(target, axis=1)\ny = data[target]\n\n- 가변수화\n\ndum_col = [\"Gender\", \"JobSatisfaction\", \"MaritalStatus\", \"OverTime\"]\n\nx = pd.get_dummies(x, columns = dum_col, drop_first= True, dtype = float)\n\n\ndata.head()\n\n\n\n\n\n\n\n\nAttrition\nAge\nDistanceFromHome\nGender\nJobSatisfaction\nMaritalStatus\nMonthlyIncome\nOverTime\nPercentSalaryHike\nTotalWorkingYears\n\n\n\n\n0\n0\n33\n7\nMale\n3\nMarried\n11691\nNo\n11\n14\n\n\n1\n0\n35\n18\nMale\n4\nSingle\n9362\nNo\n11\n10\n\n\n2\n0\n42\n6\nMale\n1\nMarried\n13348\nNo\n13\n18\n\n\n3\n0\n46\n2\nFemale\n1\nMarried\n17048\nNo\n23\n28\n\n\n4\n1\n22\n4\nMale\n3\nSingle\n3894\nNo\n16\n4\n\n\n\n\n\n\n\n- 학슴용 평가 데이터 분리\n\nx_train, x_test, y_train, y_test = train_test_split(x, y.astype(str), random_state = 1, test_size = 0.3)\n\n- 정규화\n\nscale = lambda  x : (x - min(x))/(max(x)-min(x))\n\nx_train = x_train.apply(scale, axis =  0)\nx_test = x_test.apply(scale, axis =  0)\nx_train.head()\n\n\n\n\n\n\n\n\nAge\nDistanceFromHome\nMonthlyIncome\nPercentSalaryHike\nTotalWorkingYears\nGender_Male\nJobSatisfaction_2\nJobSatisfaction_3\nJobSatisfaction_4\nMaritalStatus_Married\nMaritalStatus_Single\nOverTime_Yes\n\n\n\n\n567\n0.214286\n0.321429\n0.248997\n0.000000\n0.150\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n\n\n330\n0.547619\n0.071429\n0.416244\n0.000000\n0.350\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n785\n0.285714\n0.071429\n0.084758\n0.000000\n0.200\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n11\n0.357143\n0.500000\n0.662814\n0.071429\n0.375\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n885\n0.380952\n0.071429\n0.353863\n0.428571\n0.175\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\n\n\n(3) 모델링\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n\nmodel = KNeighborsClassifier()\n\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\n\n\n\n(4) 결과 시각화 1 : \\((y, \\hat y)\\) 클래스 비교\n\nfig = pd.DataFrame({\"y_test\" : y_test, \"y_pred\" : y_pred}).\\\n                        melt(var_name = \"label\",value_name = \"Attrition\").\\\n                            groupby(\"label\",as_index=False)[[\"Attrition\"]].value_counts().\\\n                                plot(x=\"label\",y= \"count\", backend = \"plotly\",\n                                        facet_col = \"Attrition\", kind=\"bar\",color = \"label\",height = 400, width = 600)\nfig\n\n\n                                                \n\n\n\n\n(5) 결과 시각화 2 : 평가지표\n\nacc = accuracy_score(y_pred, y_test)\npre = precision_score(y_pred, y_test, pos_label = \"1\")\nrecall = recall_score(y_pred, y_test,pos_label = \"1\")\nf1 = f1_score(y_pred, y_test,pos_label = \"1\")\n\nresult = [acc, pre, recall, f1]\ncol = [\"accuracy\", \"precision\", \"recall\", \"F1-score\"]\n\npd.DataFrame(result,columns=[\"value\"],index=col).reset_index().\\\n                plot(x=\"index\", y= \"value\",kind=\"bar\",color= \"index\", backend = \"plotly\",\n                        height = 400, width = 600)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#실습",
    "href": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#실습",
    "title": "02. 머신러닝 (2)",
    "section": "실습",
    "text": "실습\n\n(1) 데이터 로드 및 탐색\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/jangrae/csv/master/titanic_train.csv'\ndata = pd.read_csv(path)\n\n\ndata.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\n\ndata[\"Survived\"].value_counts()\n\nSurvived\n0    549\n1    342\nName: count, dtype: int64\n\n\n\ndata.isna().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\ndata.select_dtypes(\"number\").corr()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\nPassengerId\n1.000000\n-0.005007\n-0.035144\n0.036847\n-0.057527\n-0.001652\n0.012658\n\n\nSurvived\n-0.005007\n1.000000\n-0.338481\n-0.077221\n-0.035322\n0.081629\n0.257307\n\n\nPclass\n-0.035144\n-0.338481\n1.000000\n-0.369226\n0.083081\n0.018443\n-0.549500\n\n\nAge\n0.036847\n-0.077221\n-0.369226\n1.000000\n-0.308247\n-0.189119\n0.096067\n\n\nSibSp\n-0.057527\n-0.035322\n0.083081\n-0.308247\n1.000000\n0.414838\n0.159651\n\n\nParch\n-0.001652\n0.081629\n0.018443\n-0.189119\n0.414838\n1.000000\n0.216225\n\n\nFare\n0.012658\n0.257307\n-0.549500\n0.096067\n0.159651\n0.216225\n1.000000\n\n\n\n\n\n\n\n\n# 제거 대상: PassengerId, Name, Ticket, Cabin\ndrop_cols = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n\n# 변수 제거\ndata.drop(drop_cols, axis=1, inplace=True)\n\n# 확인\ndata.head()\n\n\n\n\n\n\n\n\nSurvived\nPclass\nSex\nAge\nSibSp\nParch\nFare\nEmbarked\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\n\n\n\n\n\n\n\n\n# Age 결측치를 중앙값으로 채우기\nage_median = data['Age'].median()\ndata['Age'].fillna(age_median, inplace=True)\n\n\n# Embarked 최빈값으로 채우기\nemb_freq = data['Embarked'].mode()[0]\ndata['Embarked'].fillna(emb_freq, inplace=True)\n\n\n# target 확인\ntarget = 'Survived'\n\n# 데이터 분리\nx = data.drop(target, axis=1)\ny = data.loc[:, target]\n\n\n# 가변수화 대상: Pclass, Sex, Embarked\ndumm_cols = ['Pclass', 'Sex', 'Embarked']\n\n# 가변수화\nx = pd.get_dummies(x, columns=dumm_cols, drop_first=True)\n\n# 확인\nx.head()\n\n\n\n\n\n\n\n\nAge\nSibSp\nParch\nFare\nPclass_2\nPclass_3\nSex_male\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n22.0\n1\n0\n7.2500\nFalse\nTrue\nTrue\nFalse\nTrue\n\n\n1\n38.0\n1\n0\n71.2833\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n26.0\n0\n0\n7.9250\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n3\n35.0\n1\n0\n53.1000\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n4\n35.0\n0\n0\n8.0500\nFalse\nTrue\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n# 모듈 불러오기\nfrom sklearn.model_selection import train_test_split\n\n# 7:3으로 분리\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n\n\n\n(2) 모델링\n\n# step 1.  모듈 로드\nfrom sklearn.tree import DecisionTreeClassifier\n\n# step 2.  선언하기\n\nmodel = DecisionTreeClassifier(max_depth = 5)\n\n# step 3. 학습하기\n\nmodel.fit(x_train, y_train)\n\n# step 4.  예측하기\ny_pred = model.predict(x_test)\n\n# step 5. 평가하기\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nacc = accuracy_score(y_test, y_pred)\npre = precision_score(y_test, y_pred) \nrecall = recall_score(y_test, y_pred)\nf1_score = f1_score(y_test, y_pred)\n\n\n\n(3) 시각화 1 : 평가지표\n\nfig = pd.DataFrame({\"score\" : [acc,pre,recall,f1_score],\n                            \"measure\" : [\"acc\",\"precision\",\"recall\",\"f1_score\"]}).\\\n                                    plot(x = \"measure\", y = \"score\",  color = \"measure\",\n                                            backend = \"plotly\", kind =  \"bar\",height = 500, width = 600)\nfig.update_yaxes(range = [0.5, 0.85])\n\n\n                                                \n\n\n\n\n(4) 시각화 2 : tree 모델\n\n\nCode\n# 시각화 모듈 불러오기\nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image\n\n# 이미지 파일 만들기\nexport_graphviz(model,                                 # 모델 이름\n                out_file='tree.dot',                   # 파일 이름\n                feature_names=x.columns,               # Feature 이름\n                class_names=['die', 'survived'],       # Target Class 이름\n                rounded=True,                          # 둥근 테두리\n                precision=2,                           # 불순도 소숫점 자리수\n                filled=True,                              # 박스 내부 채우기\n                max_depth=3)                         # 그래프에 표시할 트리의 깊이\n\n# 파일 변환\n!dot tree.dot -Tpng -otree.png -Gdpi=300\n\n# 이미지 파일 표시\nImage(filename='tree.png')\n\n\n\n\n\n\n\n(5) 시각화 3 : 변수 중요도\n\nipt = model.feature_importances_\npd.DataFrame({\"importance\" : ipt, \"var\" : list(x)}).\\\n                            sort_values(\"importance\", ascending = False).\\\n                            plot( x = \"importance\", y= \"var\", \n                                     kind = \"barh\", backend = \"plotly\",\n                                     height = 400, width = 600,color = \"var\")"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#excercise.-1-1",
    "href": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#excercise.-1-1",
    "title": "02. 머신러닝 (2)",
    "section": "excercise. 1",
    "text": "excercise. 1\n\n(1) 데이터 로드 및 탐색\n데이터 설명\n\nCOLLEGE: 대학 졸업여부\nINCOME: 연수입\nOVERAGE: 월평균 초과사용 시간(분)\nLEFTOVER: 월평균 잔여시간비율(%)\nHOUSE: 집값\nHANDSET_PRICE: 스마트폰 가격\nOVER_15MINS_CALLS_PER_MONTH: 월평균 장기통화(15분이상) 횟수\nAVERAGE_CALL_DURATION: 평균 통화 시간\nREPORTED_SATISFACTION: 만족도 설문조사 결과\nREPORTED_USAGE_LEVEL: 사용도 자가진단 결과\nCONSIDERING_CHANGE_OF_PLAN: 향후 변경계획 설문조사 결과\nCHURN: 이탈(번호이동) 여부 (Target 변수)\n\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/jangrae/csv/master/mobile_cust_churn.csv'\ndata = pd.read_csv(path)\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20000 entries, 0 to 19999\nData columns (total 13 columns):\n #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   id                           20000 non-null  int64 \n 1   COLLEGE                      20000 non-null  int64 \n 2   INCOME                       20000 non-null  int64 \n 3   OVERAGE                      20000 non-null  int64 \n 4   LEFTOVER                     20000 non-null  int64 \n 5   HOUSE                        20000 non-null  int64 \n 6   HANDSET_PRICE                20000 non-null  int64 \n 7   OVER_15MINS_CALLS_PER_MONTH  20000 non-null  int64 \n 8   AVERAGE_CALL_DURATION        20000 non-null  int64 \n 9   REPORTED_SATISFACTION        20000 non-null  object\n 10  REPORTED_USAGE_LEVEL         20000 non-null  object\n 11  CONSIDERING_CHANGE_OF_PLAN   20000 non-null  object\n 12  CHURN                        20000 non-null  object\ndtypes: int64(9), object(4)\nmemory usage: 2.0+ MB\n\n\n\ndata[\"CHURN\"].value_counts()\n\nCHURN\nSTAY     10148\nLEAVE     9852\nName: count, dtype: int64\n\n\n\ndata.select_dtypes(\"number\").corr()\n\n\n\n\n\n\n\n\nid\nCOLLEGE\nINCOME\nOVERAGE\nLEFTOVER\nHOUSE\nHANDSET_PRICE\nOVER_15MINS_CALLS_PER_MONTH\nAVERAGE_CALL_DURATION\n\n\n\n\nid\n1.000000\n-0.005557\n0.003686\n-0.006050\n0.006069\n0.011347\n-0.007838\n0.001254\n-0.005830\n\n\nCOLLEGE\n-0.005557\n1.000000\n0.011122\n-0.003091\n-0.003925\n-0.000217\n0.009950\n-0.007205\n-0.001490\n\n\nINCOME\n0.003686\n0.011122\n1.000000\n0.000458\n0.006515\n-0.010964\n0.727200\n0.002136\n-0.007219\n\n\nOVERAGE\n-0.006050\n-0.003091\n0.000458\n1.000000\n-0.003123\n0.002412\n0.000324\n0.770557\n0.000653\n\n\nLEFTOVER\n0.006069\n-0.003925\n0.006515\n-0.003123\n1.000000\n0.006530\n0.004004\n-0.010411\n-0.660285\n\n\nHOUSE\n0.011347\n-0.000217\n-0.010964\n0.002412\n0.006530\n1.000000\n-0.007756\n0.007410\n-0.009359\n\n\nHANDSET_PRICE\n-0.007838\n0.009950\n0.727200\n0.000324\n0.004004\n-0.007756\n1.000000\n0.002680\n-0.005190\n\n\nOVER_15MINS_CALLS_PER_MONTH\n0.001254\n-0.007205\n0.002136\n0.770557\n-0.010411\n0.007410\n0.002680\n1.000000\n0.007769\n\n\nAVERAGE_CALL_DURATION\n-0.005830\n-0.001490\n-0.007219\n0.000653\n-0.660285\n-0.009359\n-0.005190\n0.007769\n1.000000\n\n\n\n\n\n\n\n\ndata.isna().sum()\n\nid                             0\nCOLLEGE                        0\nINCOME                         0\nOVERAGE                        0\nLEFTOVER                       0\nHOUSE                          0\nHANDSET_PRICE                  0\nOVER_15MINS_CALLS_PER_MONTH    0\nAVERAGE_CALL_DURATION          0\nREPORTED_SATISFACTION          0\nREPORTED_USAGE_LEVEL           0\nCONSIDERING_CHANGE_OF_PLAN     0\nCHURN                          0\ndtype: int64\n\n\n\n(a). 데이터 준비\n- 변수 제거\n\n_data = data.drop(\"id\", axis = 1).copy()\n_data.head()\n\n\n\n\n\n\n\n\nCOLLEGE\nINCOME\nOVERAGE\nLEFTOVER\nHOUSE\nHANDSET_PRICE\nOVER_15MINS_CALLS_PER_MONTH\nAVERAGE_CALL_DURATION\nREPORTED_SATISFACTION\nREPORTED_USAGE_LEVEL\nCONSIDERING_CHANGE_OF_PLAN\nCHURN\n\n\n\n\n0\n0\n31953\n0\n6\n313378\n161\n0\n4\nunsat\nlittle\nno\nSTAY\n\n\n1\n1\n36147\n0\n13\n800586\n244\n0\n6\nunsat\nlittle\nconsidering\nSTAY\n\n\n2\n1\n27273\n230\n0\n305049\n201\n16\n15\nunsat\nvery_little\nperhaps\nSTAY\n\n\n3\n0\n120070\n38\n33\n788235\n780\n3\n2\nunsat\nvery_high\nconsidering\nLEAVE\n\n\n4\n1\n29215\n208\n85\n224784\n241\n21\n1\nvery_unsat\nlittle\nnever_thought\nSTAY\n\n\n\n\n\n\n\n= x, y 분리\n\ntarget = \"CHURN\"\n\nx = _data.drop(target, axis = 1)\ny = _data[target]\n\n- 가변수화\n\nd_cols = [\"REPORTED_SATISFACTION\", \"REPORTED_USAGE_LEVEL\", \"CONSIDERING_CHANGE_OF_PLAN\"]\n\nx = pd.get_dummies(x, columns = d_cols, drop_first = True, dtype = float)\n\n- 훈련, 평가 데이터 분리\n\nx_train, x_test, y_train , y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)\n\n\n\n\n(2) 모델링\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n# step 1. 모델 선언\nmodel  = DecisionTreeClassifier(max_depth = 5,random_state=1)\n# step 2.  모델 fit\nmodel.fit(x_train,y_train)\n\n# step 3. 모델 pred\ny_pred = model.predict(x_test)\n\n# step 4.  평가지표 산출\nfrom sklearn import metrics\nacc = accuracy_score(y_pred, y_test)\npre = precision_score(y_pred, y_test, pos_label = \"LEAVE\")\nrec =  recall_score(y_pred, y_test,  pos_label = \"LEAVE\")\nf1 = metrics.f1_score(y_test,y_pred, pos_label = \"LEAVE\")\n\n\n\n(3) 시각화 1 : 평가지표\n\nfig = pd.DataFrame({\"score\" : [acc,pre,recall,f1_score],\n                            \"measure\" : [\"acc\",\"precision\",\"recall\",\"f1_score\"]}).\\\n                                    plot(x = \"measure\", y = \"score\",  color = \"measure\",\n                                            backend = \"plotly\", kind =  \"bar\",height = 500, width = 600)\nfig.update_yaxes(range = [0.5, 0.71])\n\n\n                                                \n\n\n\n\n(4) 시각화 2 : tree 모델\n\n\nCode\n# 시각화 모듈 불러오기\nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image\n\n# 이미지 파일 만들기\nexport_graphviz(model,                                 # 모델 이름\n                out_file='tree.dot',                   # 파일 이름\n                feature_names=x.columns,               # Feature 이름\n                class_names=['Leave', 'Stay'],       # Target Class 이름\n                rounded=True,                          # 둥근 테두리\n                precision=2,                           # 불순도 소숫점 자리수\n                filled=True,                              # 박스 내부 채우기\n                max_depth=3)                         # 그래프에 표시할 트리의 깊이\n\n# 파일 변환\n!dot tree.dot -Tpng -otree.png -Gdpi=300\n\n# 이미지 파일 표시\nImage(filename='tree.png')\n\n\n\n\n\n\n\n(5) 시각화 3 : 변수 중요도\n\nipt = model.feature_importances_\nfig = pd.DataFrame({\"importance\" : ipt, \"var\" : list(x)}).\\\n                            sort_values(\"importance\", ascending = False).\\\n                            plot( y = \"importance\", x= \"var\", \n                                     kind = \"bar\", backend = \"plotly\",\n                                     height = 400, width = 600,color = \"var\")\nfig.update_layout(showlegend = False)\nfig.update_xaxes(showticklabels=False)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#excercise.-2-1",
    "href": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#excercise.-2-1",
    "title": "02. 머신러닝 (2)",
    "section": "excercise. 2",
    "text": "excercise. 2\n\n(1) 데이터 로드 및 탐색\n데이터설명\n피마 인디언 당뇨 데이터셋은 몇 명의 여성 피마 인디언의 진료 자료와 진단 후 5년 내 당뇨 발병 여부로 구성됨\n\nPregnancies: 임신 횟수\nGlucose: 포도당 부하 검사 수치\nBloodPressure: 혈압(mm Hg)\nSkinThickness: 팔 삼두근 뒤쪽의 피하지방 측정값(mm)\nInsulin: 혈청 인슐린(mu U/ml)\nBMI: 체질량지수(체중(kg)/키(m))^2\nDiabetesPedigreeFunction: 당뇨 내력 가중치 값\nAge: 나이\nOutcome: 클래스 결정 값(0 또는 1)\n\ndiabetes\n\n당뇨병(糖尿病, diabetes)은 높은 혈당 수치가 오랜 기간 지속되는 대사 질환이다.\n혈당이 높을 때의 증상으로는 소변이 잦아지고, 갈증과 배고픔이 심해진다.\n이를 치료하지 않으면 다른 합병증을 유발할 수 있다. (출처: 위키백과)\n\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/jangrae/csv/master/diabetes.csv'\ndata = pd.read_csv(path)\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 768 entries, 0 to 767\nData columns (total 9 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Pregnancies               768 non-null    int64  \n 1   Glucose                   768 non-null    int64  \n 2   BloodPressure             768 non-null    int64  \n 3   SkinThickness             768 non-null    int64  \n 4   Insulin                   768 non-null    int64  \n 5   BMI                       768 non-null    float64\n 6   DiabetesPedigreeFunction  768 non-null    float64\n 7   Age                       768 non-null    int64  \n 8   Outcome                   768 non-null    int64  \ndtypes: float64(2), int64(7)\nmemory usage: 54.1 KB\n\n\n\ndata.isna().sum()\n\nPregnancies                 0\nGlucose                     0\nBloodPressure               0\nSkinThickness               0\nInsulin                     0\nBMI                         0\nDiabetesPedigreeFunction    0\nAge                         0\nOutcome                     0\ndtype: int64\n\n\n\ndata.head()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\n\ndata.corr()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\nPregnancies\n1.000000\n0.129459\n0.141282\n-0.081672\n-0.073535\n0.017683\n-0.033523\n0.544341\n0.221898\n\n\nGlucose\n0.129459\n1.000000\n0.152590\n0.057328\n0.331357\n0.221071\n0.137337\n0.263514\n0.466581\n\n\nBloodPressure\n0.141282\n0.152590\n1.000000\n0.207371\n0.088933\n0.281805\n0.041265\n0.239528\n0.065068\n\n\nSkinThickness\n-0.081672\n0.057328\n0.207371\n1.000000\n0.436783\n0.392573\n0.183928\n-0.113970\n0.074752\n\n\nInsulin\n-0.073535\n0.331357\n0.088933\n0.436783\n1.000000\n0.197859\n0.185071\n-0.042163\n0.130548\n\n\nBMI\n0.017683\n0.221071\n0.281805\n0.392573\n0.197859\n1.000000\n0.140647\n0.036242\n0.292695\n\n\nDiabetesPedigreeFunction\n-0.033523\n0.137337\n0.041265\n0.183928\n0.185071\n0.140647\n1.000000\n0.033561\n0.173844\n\n\nAge\n0.544341\n0.263514\n0.239528\n-0.113970\n-0.042163\n0.036242\n0.033561\n1.000000\n0.238356\n\n\nOutcome\n0.221898\n0.466581\n0.065068\n0.074752\n0.130548\n0.292695\n0.173844\n0.238356\n1.000000\n\n\n\n\n\n\n\n\ndata.Outcome.value_counts()\n\nOutcome\n0    500\n1    268\nName: count, dtype: int64\n\n\n\n\n(2) 데이터 분리\n\ntarget = \"Outcome\"\n\nx = data.drop(target, axis = 1)\ny = data[target]\n\nx_train, x_test, y_train, y_test = train_test_split(x, y,  random_state = 1, test_size = 0.3)\n\n\n\n(3) 모델링\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier()\n\nmodel.fit(x_train,y_train)\n\ny_pred = model.predict(x_test)\n\nacc, pre, re, f1 = accuracy_score(y_test,y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), metrics.f1_score(y_test, y_pred)\n\n\n\n(4) 시각화 1. 평가지표\n- 걍함수로 만들자…\n\ndef score(range = [0.5, 0.71]) : \n            fig = pd.DataFrame({\"score\" : [acc,pre,recall,f1_score],\n                            \"measure\" : [\"acc\",\"precision\",\"recall\",\"f1_score\"]}).\\\n                                    plot(x = \"measure\", y = \"score\",  color = \"measure\",\n                                            backend = \"plotly\", kind =  \"bar\",height = 500, width = 600)\n            fig.update_yaxes(range = range)\n            return fig\n\n\nscore()\n\n\n                                                \n\n\n\n\n(5) 시각화 2 : tree 모델\n\n\nCode\n# 시각화 모듈 불러오기\nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image\n\n# 이미지 파일 만들기\nexport_graphviz(model,                                 # 모델 이름\n                out_file='tree.dot',                   # 파일 이름\n                feature_names=x.columns,               # Feature 이름\n                class_names=['0', '1'],       # Target Class 이름\n                rounded=True,                          # 둥근 테두리\n                precision=2,                           # 불순도 소숫점 자리수\n                filled=True,                              # 박스 내부 채우기\n                max_depth=3)                         # 그래프에 표시할 트리의 깊이\n\n# 파일 변환\n!dot tree.dot -Tpng -otree.png -Gdpi=300\n\n# 이미지 파일 표시\nImage(filename='tree.png')\n\n\n\n\n\n\n\n(5) 시각화 3 : 변수 중요도\n\ndef importance(x,model) :\n        ipt = model.feature_importances_\n        fig = pd.DataFrame({\"importance\" : ipt, \"var\" : list(x)}).\\\n                                sort_values(\"importance\", ascending = False).\\\n                                plot( y = \"importance\", x= \"var\", \n                                        kind = \"bar\", backend = \"plotly\",\n                                         height = 400, width = 600,color = \"var\", title=\"\".join([i for i in str(model) if i.isalpha()]))\n        fig.update_layout(showlegend = False)\n        fig.update_xaxes(showticklabels=True)\n        return fig\n\n\nimportance(x,model)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#데이터-로드-및-탐색-6",
    "href": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#데이터-로드-및-탐색-6",
    "title": "02. 머신러닝 (2)",
    "section": "(1) 데이터 로드 및 탐색",
    "text": "(1) 데이터 로드 및 탐색\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/jangrae/csv/master/admission_simple.csv'\ndata = pd.read_csv(path)\n\n\n# 상위 몇 개 행 확인\ndata.head()\n\n\n\n\n\n\n\n\nGRE\nTOEFL\nRANK\nSOP\nLOR\nGPA\nRESEARCH\nADMIT\n\n\n\n\n0\n337\n118\n4\n4.5\n4.5\n9.65\n1\n1\n\n\n1\n324\n107\n4\n4.0\n4.5\n8.87\n1\n1\n\n\n2\n316\n104\n3\n3.0\n3.5\n8.00\n1\n0\n\n\n3\n322\n110\n3\n3.5\n2.5\n8.67\n1\n1\n\n\n4\n314\n103\n2\n2.0\n3.0\n8.21\n0\n0\n\n\n\n\n\n\n\n\n# 기술통계 확인\ndata.describe()\n\n\n\n\n\n\n\n\nGRE\nTOEFL\nRANK\nSOP\nLOR\nGPA\nRESEARCH\nADMIT\n\n\n\n\ncount\n500.000000\n500.000000\n500.000000\n500.000000\n500.00000\n500.000000\n500.000000\n500.000000\n\n\nmean\n316.472000\n107.192000\n3.114000\n3.374000\n3.48400\n8.576440\n0.560000\n0.436000\n\n\nstd\n11.295148\n6.081868\n1.143512\n0.991004\n0.92545\n0.604813\n0.496884\n0.496384\n\n\nmin\n290.000000\n92.000000\n1.000000\n1.000000\n1.00000\n6.800000\n0.000000\n0.000000\n\n\n25%\n308.000000\n103.000000\n2.000000\n2.500000\n3.00000\n8.127500\n0.000000\n0.000000\n\n\n50%\n317.000000\n107.000000\n3.000000\n3.500000\n3.50000\n8.560000\n1.000000\n0.000000\n\n\n75%\n325.000000\n112.000000\n4.000000\n4.000000\n4.00000\n9.040000\n1.000000\n1.000000\n\n\nmax\n340.000000\n120.000000\n5.000000\n5.000000\n5.00000\n9.920000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n# 범주값 개수 확인\ndata['ADMIT'].value_counts()\n\nADMIT\n0    282\n1    218\nName: count, dtype: int64\n\n\n\n# 상관관계 확인\ndata.corr()\n\n\n\n\n\n\n\n\nGRE\nTOEFL\nRANK\nSOP\nLOR\nGPA\nRESEARCH\nADMIT\n\n\n\n\nGRE\n1.000000\n0.827200\n0.635376\n0.613498\n0.524679\n0.825878\n0.563398\n0.701671\n\n\nTOEFL\n0.827200\n1.000000\n0.649799\n0.644410\n0.541563\n0.810574\n0.467012\n0.680503\n\n\nRANK\n0.635376\n0.649799\n1.000000\n0.728024\n0.608651\n0.705254\n0.427047\n0.618367\n\n\nSOP\n0.613498\n0.644410\n0.728024\n1.000000\n0.663707\n0.712154\n0.408116\n0.606876\n\n\nLOR\n0.524679\n0.541563\n0.608651\n0.663707\n1.000000\n0.637469\n0.372526\n0.536527\n\n\nGPA\n0.825878\n0.810574\n0.705254\n0.712154\n0.637469\n1.000000\n0.501311\n0.752196\n\n\nRESEARCH\n0.563398\n0.467012\n0.427047\n0.408116\n0.372526\n0.501311\n1.000000\n0.503104\n\n\nADMIT\n0.701671\n0.680503\n0.618367\n0.606876\n0.536527\n0.752196\n0.503104\n1.000000"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#데이터-준비-1",
    "href": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#데이터-준비-1",
    "title": "02. 머신러닝 (2)",
    "section": "(2) 데이터 준비",
    "text": "(2) 데이터 준비\n\n# target 확인\ntarget = 'ADMIT'\n\n# 데이터 분리\nx = data.drop(target, axis=1)\ny = data.loc[:, target]\n\n\n# 모듈 불러오기\nfrom sklearn.model_selection import train_test_split\n\n# 7:3으로 분리\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#모델링-9",
    "href": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#모델링-9",
    "title": "02. 머신러닝 (2)",
    "section": "(3) 모델링",
    "text": "(3) 모델링\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n\n# step1. model 선언\nmodel  = LogisticRegression()\n\n# step2. fit\nmodel.fit(x_train, y_train)\n\n# step3 . 얘측\ny_pred = model.predict(x_test)\n\n# step 4. 성능 지표 산출\nacc, pre, recall, f1_score = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), f1_score(y_test, y_pred)\n\n\nscore(range = [0.7, 0.87])"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#extra-확률값-구한-후-결과-계산",
    "href": "posts/DX/04. 머신러닝/2023-09-13-02. 머신러닝 (2).html#extra-확률값-구한-후-결과-계산",
    "title": "02. 머신러닝 (2)",
    "section": "(4) extra : 확률값 구한 후 결과 계산",
    "text": "(4) extra : 확률값 구한 후 결과 계산\n\np = model.predict_proba(x_test)\n\nmy_pred = [1 if x &gt; 0.5 else 0 for x in p[:,1]]"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-15-04. 머신러닝 (4).html",
    "href": "posts/DX/04. 머신러닝/2023-09-15-04. 머신러닝 (4).html",
    "title": "04. 머신러닝 (4)",
    "section": "",
    "text": "- 알고리즘을 사영해 모델링을 수행할 때 모델 성능을 최적화하기 위해 조절할 수 있는 매개변수\n\nKNN의 n_neighbors, tree의 max_depth 등\n\n- 모델의 성능 향상을 위해선 최선의 하이퍼파라미터 값을 찾는 다양한 시도를 해야함\n\n\n- 모든 경우의 수를 고려\n\n\n- 1부터 \\(n\\)까지 주변 이웃수를 바꾸어구며 가장 최적의 모델을 찾음 \\(\\to\\) \\(n\\)이 커질수록 엄청난 시간이 소요됨\n\nKNN은 주변의 이웃수인 \\(k\\)값이 작을수록 복잡한 모델, 클수록 단순한 모델이된다.\n보통 데이터 건수의 제곱근으로 결정하는 경우가 종종 있음\n\n- 또한, 거리 계산법에 따라 성능이 달라질 수 있으나. 참고만 해두자\n\n\n\n- max_depth : 트리의 최대 깊이로 작을수록 모형이 단순해짐\n- min_samples_leaf * leaf가 되기 위한 최소한의 샘플 데이터 수 * 이 값이 클 수록 모델이 단순해 짐\n- min_ samples_split : 분리하려면 최소 몇 명은 되어야한다.~~\n\n노드를 분할하기 위한 최소한의 샘플 데이터 수\n해당 값이 클 수록 모델이 단순해 짐\n\n- 위 값을을 적절히 조잘하여 과적합을 막을 수 있다.\n\n\n\n\n- 1부터 n구간의 정수 중 무작위로 \\(m\\)개를 골라 최적의 모델을 선태그\n\n임의의 \\(m\\)개만 골라 수행하니 시간 소모는 적을 것임\n그러나 선택되지 못한 값중에서 더 좋은 성능이 보이는 값이 있을까 걱정됨.\n\n- 근데 우리는 Random Search와 Grid Search를 함께 사용할 수 있음!\n\n일단 random search 수행\n그 다음 산출된 최적의 파라미터 주변 파라미터들에 대해서 grid search를 수행!\n\n\n\n\n\n\n\n# 라이브러리 불러오기\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\nwarnings.filterwarnings(action='ignore')\n%config InlineBackend.figure_format = 'retina'\n\n\n\n\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/jangrae/csv/master/boston.csv'\ndata = pd.read_csv(path)\n\n데이터 설명\n\ncrim: 자치시(Town)별 1인당 범죄율\nzn: 25,000 평방피트를 초과하는 거주지역 비율\nindus: 비소매상업지역이 점유하고 있는 토지 비율\nchas: 찰스강에 대한 더미 변수 (= 1 강 경계에 위치; 0 나머지)\nnox: 10ppm당 농축 일산화질소\nrm: 주택 1가구당 평균 방 개수\nage: 1940년 이전에 건축된 소유주택 비율\ndis: 5개 보스턴 직업센터까지 접근성 지수\nrad: 방사형 도로까지의 접근성 지수\ntax: 10,000달러 당 재산세율\nptratio: 자치시(Town)별 학생/교사 비율\nblack: 1000(Bk - 0.63)^2, 여기서 Bk는 자치시별 흑인의 비율을 의미\nlstat: 모집단 하위 계층의 비율(%)\nmedv: 본인 소유 주택가격(중앙값) (단위:$1,000)\n\n\n\n\n\n# target 확인\ntarget = 'medv'\n\n# 데이터 분리\nx = data.drop(target, axis=1)\ny = data[target]\n\n\n# 모듈 불러오기\nfrom sklearn.model_selection import train_test_split\n\n# 데이터 분리\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error, r2_score\n\ntree = DecisionTreeRegressor(random_state =1)\n\ncv_tree = cross_val_score(tree, x_train, y_train, cv = 5)\n\n\nprint(cv_tree)\nprint(cv_tree.mean())\n\n[0.65873754 0.49225288 0.78163071 0.80327749 0.82834327]\n0.7128483767547819\n\n\n- 튜닝을 해보자. 먼가 튜닝을 하면 더 괜찮은 분석이 될 것 같다.\n1) 모델 튜닝\n\n성능을 확인할 파라미터를 딕셔너리 형태로 선언합니다.\n기존 모델을 기본으로 RandomizedSearchCV 알고리즘을 사용하는 모델을 선언합니다.\n다음 정보를 최종 모델에 파라미터로 전달합니다.\n\n기본 모델 이름\n파라미터 변수\ncv: K-Fold 분할 개수(기본값=5)\nn_iter: 시도 횟수(기본값=10)\nscoring: 평가 방법\n\n\n- step 1. 일단 Random Search를 수행\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# 파라미터 선언\n  # max_depth: 1~50\nparam = {\"max_depth\" : range(1, 51)}\n\ntree =DecisionTreeRegressor(random_state=1)\n# Random Search 선언\n  # cv=5\n  # n_iter=20\n  # scoring='r2'\ntree_model = RandomizedSearchCV(tree,       ## 기번 모델 이름을 전달\n                                                                param,  ## 설정한 파라미터 범위를 전달\n                                                                   cv = 5,  ## k-fold 개수  \n                                                                     n_iter = 20, ## 전체 파라미터 범위 중에서 몇 개만 뽑을 것인지. \n                                                                           scoring = \"r2\")  \ntree_model.fit(x_train, y_train)\n\nRandomizedSearchCV(cv=5, estimator=DecisionTreeRegressor(random_state=1),\n                   n_iter=20, param_distributions={'max_depth': range(1, 51)},\n                   scoring='r2')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=5, estimator=DecisionTreeRegressor(random_state=1),\n                   n_iter=20, param_distributions={'max_depth': range(1, 51)},\n                   scoring='r2')estimator: DecisionTreeRegressorDecisionTreeRegressor(random_state=1)DecisionTreeRegressorDecisionTreeRegressor(random_state=1)\n\n\n- 중요정보 확인\n\n# 중요 정보 확인\nprint('=' * 80)\nprint(tree_model.cv_results_['mean_test_score'])\nprint('-' * 80)\nprint('최적파라미터:',tree_model.best_params_)\nprint('-' * 80)\nprint('최고성능:', tree_model.best_score_)\nprint('=' * 80)\n\n================================================================================\n[0.71284838 0.70905766 0.67646772 0.7383174  0.72240391 0.71284838\n 0.71284838 0.71284838 0.71278394 0.71250015 0.71284838 0.37077174\n 0.71284838 0.70928798 0.71284838 0.70526236 0.71284838 0.71284838\n 0.71284838 0.71284838]\n--------------------------------------------------------------------------------\n최적파라미터: {'max_depth': 5}\n--------------------------------------------------------------------------------\n최고성능: 0.7383174002807829\n================================================================================\n\n\n- 변수 중요도 확인\n\n# 변수 중요도\nplt.figure(figsize=(5, 5))\nplt.barh(y=list(x), width=tree_model.best_estimator_.feature_importances_)\nplt.show()\n\n\n\n\n\n\n\n\ny_pred = tree_model.predict(x_test)\n\n\nprint(\"MAE : \", mean_absolute_error(y_test, y_pred))\nprint(\"r2 : \", r2_score(y_test, y_pred))\n\nMAE :  3.0965698865596964\nr2 :  0.7389100169622292\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, f1_score, r2_score, precision_score, recall_score\n\n\n\n\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/jangrae/csv/master/mobile.csv'\ndata = pd.read_csv(path)\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20000 entries, 0 to 19999\nData columns (total 13 columns):\n #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   id                           20000 non-null  int64 \n 1   COLLEGE                      20000 non-null  int64 \n 2   INCOME                       20000 non-null  int64 \n 3   OVERAGE                      20000 non-null  int64 \n 4   LEFTOVER                     20000 non-null  int64 \n 5   HOUSE                        20000 non-null  int64 \n 6   HANDSET_PRICE                20000 non-null  int64 \n 7   OVER_15MINS_CALLS_PER_MONTH  20000 non-null  int64 \n 8   AVERAGE_CALL_DURATION        20000 non-null  int64 \n 9   REPORTED_SATISFACTION        20000 non-null  object\n 10  REPORTED_USAGE_LEVEL         20000 non-null  object\n 11  CONSIDERING_CHANGE_OF_PLAN   20000 non-null  object\n 12  CHURN                        20000 non-null  int64 \ndtypes: int64(10), object(3)\nmemory usage: 2.0+ MB\n\n\n\ndata.isna().sum()\n\nid                             0\nCOLLEGE                        0\nINCOME                         0\nOVERAGE                        0\nLEFTOVER                       0\nHOUSE                          0\nHANDSET_PRICE                  0\nOVER_15MINS_CALLS_PER_MONTH    0\nAVERAGE_CALL_DURATION          0\nREPORTED_SATISFACTION          0\nREPORTED_USAGE_LEVEL           0\nCONSIDERING_CHANGE_OF_PLAN     0\nCHURN                          0\ndtype: int64\n\n\n\n\n\n- 변수 제거\n\ndata.drop(\"id\", axis = 1, inplace = True)\ndata.head()\n\n\n\n\n\n\n\n\nCOLLEGE\nINCOME\nOVERAGE\nLEFTOVER\nHOUSE\nHANDSET_PRICE\nOVER_15MINS_CALLS_PER_MONTH\nAVERAGE_CALL_DURATION\nREPORTED_SATISFACTION\nREPORTED_USAGE_LEVEL\nCONSIDERING_CHANGE_OF_PLAN\nCHURN\n\n\n\n\n0\n0\n31953\n0\n6\n313378\n161\n0\n4\nunsat\nlittle\nno\n0\n\n\n1\n1\n36147\n0\n13\n800586\n244\n0\n6\nunsat\nlittle\nconsidering\n0\n\n\n2\n1\n27273\n230\n0\n305049\n201\n16\n15\nunsat\nvery_little\nperhaps\n0\n\n\n3\n0\n120070\n38\n33\n788235\n780\n3\n2\nunsat\nvery_high\nconsidering\n1\n\n\n4\n1\n29215\n208\n85\n224784\n241\n21\n1\nvery_unsat\nlittle\nnever_thought\n0\n\n\n\n\n\n\n\n- x, y 분리\n\ntarget = \"CHURN\"\nx = data.drop(target, axis = 1)\ny = data[target]\n\n- 가변수화\n\nd_cols = [\"REPORTED_SATISFACTION\", \"REPORTED_USAGE_LEVEL\", \"CONSIDERING_CHANGE_OF_PLAN\"]\n\nx = pd.get_dummies(x, columns = d_cols, dtype = float, drop_first = True)\n\n\nx.head()\n\n\n\n\n\n\n\n\nCOLLEGE\nINCOME\nOVERAGE\nLEFTOVER\nHOUSE\nHANDSET_PRICE\nOVER_15MINS_CALLS_PER_MONTH\nAVERAGE_CALL_DURATION\nREPORTED_SATISFACTION_sat\nREPORTED_SATISFACTION_unsat\nREPORTED_SATISFACTION_very_sat\nREPORTED_SATISFACTION_very_unsat\nREPORTED_USAGE_LEVEL_high\nREPORTED_USAGE_LEVEL_little\nREPORTED_USAGE_LEVEL_very_high\nREPORTED_USAGE_LEVEL_very_little\nCONSIDERING_CHANGE_OF_PLAN_considering\nCONSIDERING_CHANGE_OF_PLAN_never_thought\nCONSIDERING_CHANGE_OF_PLAN_no\nCONSIDERING_CHANGE_OF_PLAN_perhaps\n\n\n\n\n0\n0\n31953\n0\n6\n313378\n161\n0\n4\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n1\n1\n36147\n0\n13\n800586\n244\n0\n6\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n2\n1\n27273\n230\n0\n305049\n201\n16\n15\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0\n120070\n38\n33\n788235\n780\n3\n2\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n4\n1\n29215\n208\n85\n224784\n241\n21\n1\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n\n- 학습용, 평가용 데이터 분리\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)\n\n\n\n\n\n일단 k-fold로 예측\n\n\ntree1 = DecisionTreeClassifier(max_depth = 5)\n\ncv_tree = cross_val_score(tree1, x_train, y_train, cv = 10)\nprint(cv_tree)\nprint(cv_tree.mean())\n\n[0.69928571 0.69214286 0.69857143 0.69285714 0.69428571 0.695\n 0.70857143 0.69142857 0.70428571 0.70285714]\n0.6979285714285715\n\n\n\n\n\n- 선언\n\nparams = {\"max_depth\" : range(3, 31)}\n\ntree =DecisionTreeClassifier(random_state=1)\ncv_tree2 = RandomizedSearchCV(tree,\n                                                           params, cv = 5, n_iter = 20, scoring = \"accuracy\" )\n\n- 모델 학습\n\ncv_tree2.fit(x_train, y_train)\n\nRandomizedSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=1),\n                   n_iter=20, param_distributions={'max_depth': range(3, 31)},\n                   scoring='accuracy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=1),\n                   n_iter=20, param_distributions={'max_depth': range(3, 31)},\n                   scoring='accuracy')estimator: DecisionTreeClassifierDecisionTreeClassifier(random_state=1)DecisionTreeClassifierDecisionTreeClassifier(random_state=1)\n\n\n\n# 중요 정보 확인\nprint(cv_tree2.cv_results_[\"params\"])\nprint('=' * 80)\nprint(cv_tree2.cv_results_['mean_test_score']) ## 각각의 랜덤서치요소 에서의 cv =5  결과\nprint('-' * 80)\nprint('최적파라미터:',cv_tree2.best_params_)\nprint('-' * 80)\nprint('최고성능:', cv_tree2.best_score_)\nprint('=' * 80)\n\n[{'max_depth': 9}, {'max_depth': 8}, {'max_depth': 4}, {'max_depth': 5}, {'max_depth': 25}, {'max_depth': 21}, {'max_depth': 7}, {'max_depth': 24}, {'max_depth': 23}, {'max_depth': 30}, {'max_depth': 6}, {'max_depth': 11}, {'max_depth': 26}, {'max_depth': 29}, {'max_depth': 3}, {'max_depth': 13}, {'max_depth': 14}, {'max_depth': 28}, {'max_depth': 19}, {'max_depth': 27}]\n================================================================================\n[0.68792857 0.69428571 0.69964286 0.69621429 0.62078571 0.62321429\n 0.69742857 0.61914286 0.61742857 0.61814286 0.69757143 0.67385714\n 0.6205     0.61792857 0.68907143 0.65785714 0.65592857 0.61685714\n 0.63342857 0.6165    ]\n--------------------------------------------------------------------------------\n최적파라미터: {'max_depth': 4}\n--------------------------------------------------------------------------------\n최고성능: 0.6996428571428571\n================================================================================\n\n\n- 변수 중요도 확인\n\n# 변수 중요도\nplt.figure(figsize=(4, 4))\nplt.barh(y=list(x), width=cv_tree2.best_estimator_.feature_importances_)\nplt.show()\n\n\n\n\n- 기존의 튜닝전과 비교\n\nprint(\"튜닝전 : \", cv_tree.mean(), \", 튜닝후 : \", cv_tree2.best_score_)\n\n튜닝전 :  0.6979285714285715 , 튜닝후 :  0.6996428571428571\n\n\n\n\n\n\ny_pred = cv_tree2.predict(x_test)\n\n\nacc, pre, re, f1 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), f1_score(y_test, y_pred)\n\n\nscore = [acc, pre, re, f1]\nmeasure = [\"accuracy\", \"precision\", \"recall\", \"F1-score\"]\n\nfig = pd.DataFrame({\"measure\" : measure, \"score\" : score}).\\\n                                sort_values(\"score\", ascending = False).\\\n                                    plot(y = \"measure\",  x= \"score\", color = \"measure\",\n                                            backend = \"plotly\", kind = \"barh\", width = 600, height = 400)\n\nfig.update_xaxes(range = (0.5, 0.8))"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-15-04. 머신러닝 (4).html#grid-search",
    "href": "posts/DX/04. 머신러닝/2023-09-15-04. 머신러닝 (4).html#grid-search",
    "title": "04. 머신러닝 (4)",
    "section": "",
    "text": "- 모든 경우의 수를 고려\n\n\n- 1부터 \\(n\\)까지 주변 이웃수를 바꾸어구며 가장 최적의 모델을 찾음 \\(\\to\\) \\(n\\)이 커질수록 엄청난 시간이 소요됨\n\nKNN은 주변의 이웃수인 \\(k\\)값이 작을수록 복잡한 모델, 클수록 단순한 모델이된다.\n보통 데이터 건수의 제곱근으로 결정하는 경우가 종종 있음\n\n- 또한, 거리 계산법에 따라 성능이 달라질 수 있으나. 참고만 해두자\n\n\n\n- max_depth : 트리의 최대 깊이로 작을수록 모형이 단순해짐\n- min_samples_leaf * leaf가 되기 위한 최소한의 샘플 데이터 수 * 이 값이 클 수록 모델이 단순해 짐\n- min_ samples_split : 분리하려면 최소 몇 명은 되어야한다.~~\n\n노드를 분할하기 위한 최소한의 샘플 데이터 수\n해당 값이 클 수록 모델이 단순해 짐\n\n- 위 값을을 적절히 조잘하여 과적합을 막을 수 있다."
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-15-04. 머신러닝 (4).html#random-search",
    "href": "posts/DX/04. 머신러닝/2023-09-15-04. 머신러닝 (4).html#random-search",
    "title": "04. 머신러닝 (4)",
    "section": "",
    "text": "- 1부터 n구간의 정수 중 무작위로 \\(m\\)개를 골라 최적의 모델을 선태그\n\n임의의 \\(m\\)개만 골라 수행하니 시간 소모는 적을 것임\n그러나 선택되지 못한 값중에서 더 좋은 성능이 보이는 값이 있을까 걱정됨.\n\n- 근데 우리는 Random Search와 Grid Search를 함께 사용할 수 있음!\n\n일단 random search 수행\n그 다음 산출된 최적의 파라미터 주변 파라미터들에 대해서 grid search를 수행!"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-15-04. 머신러닝 (4).html#실습",
    "href": "posts/DX/04. 머신러닝/2023-09-15-04. 머신러닝 (4).html#실습",
    "title": "04. 머신러닝 (4)",
    "section": "",
    "text": "# 라이브러리 불러오기\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\nwarnings.filterwarnings(action='ignore')\n%config InlineBackend.figure_format = 'retina'\n\n\n\n\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/jangrae/csv/master/boston.csv'\ndata = pd.read_csv(path)\n\n데이터 설명\n\ncrim: 자치시(Town)별 1인당 범죄율\nzn: 25,000 평방피트를 초과하는 거주지역 비율\nindus: 비소매상업지역이 점유하고 있는 토지 비율\nchas: 찰스강에 대한 더미 변수 (= 1 강 경계에 위치; 0 나머지)\nnox: 10ppm당 농축 일산화질소\nrm: 주택 1가구당 평균 방 개수\nage: 1940년 이전에 건축된 소유주택 비율\ndis: 5개 보스턴 직업센터까지 접근성 지수\nrad: 방사형 도로까지의 접근성 지수\ntax: 10,000달러 당 재산세율\nptratio: 자치시(Town)별 학생/교사 비율\nblack: 1000(Bk - 0.63)^2, 여기서 Bk는 자치시별 흑인의 비율을 의미\nlstat: 모집단 하위 계층의 비율(%)\nmedv: 본인 소유 주택가격(중앙값) (단위:$1,000)\n\n\n\n\n\n# target 확인\ntarget = 'medv'\n\n# 데이터 분리\nx = data.drop(target, axis=1)\ny = data[target]\n\n\n# 모듈 불러오기\nfrom sklearn.model_selection import train_test_split\n\n# 데이터 분리\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error, r2_score\n\ntree = DecisionTreeRegressor(random_state =1)\n\ncv_tree = cross_val_score(tree, x_train, y_train, cv = 5)\n\n\nprint(cv_tree)\nprint(cv_tree.mean())\n\n[0.65873754 0.49225288 0.78163071 0.80327749 0.82834327]\n0.7128483767547819\n\n\n- 튜닝을 해보자. 먼가 튜닝을 하면 더 괜찮은 분석이 될 것 같다.\n1) 모델 튜닝\n\n성능을 확인할 파라미터를 딕셔너리 형태로 선언합니다.\n기존 모델을 기본으로 RandomizedSearchCV 알고리즘을 사용하는 모델을 선언합니다.\n다음 정보를 최종 모델에 파라미터로 전달합니다.\n\n기본 모델 이름\n파라미터 변수\ncv: K-Fold 분할 개수(기본값=5)\nn_iter: 시도 횟수(기본값=10)\nscoring: 평가 방법\n\n\n- step 1. 일단 Random Search를 수행\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# 파라미터 선언\n  # max_depth: 1~50\nparam = {\"max_depth\" : range(1, 51)}\n\ntree =DecisionTreeRegressor(random_state=1)\n# Random Search 선언\n  # cv=5\n  # n_iter=20\n  # scoring='r2'\ntree_model = RandomizedSearchCV(tree,       ## 기번 모델 이름을 전달\n                                                                param,  ## 설정한 파라미터 범위를 전달\n                                                                   cv = 5,  ## k-fold 개수  \n                                                                     n_iter = 20, ## 전체 파라미터 범위 중에서 몇 개만 뽑을 것인지. \n                                                                           scoring = \"r2\")  \ntree_model.fit(x_train, y_train)\n\nRandomizedSearchCV(cv=5, estimator=DecisionTreeRegressor(random_state=1),\n                   n_iter=20, param_distributions={'max_depth': range(1, 51)},\n                   scoring='r2')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=5, estimator=DecisionTreeRegressor(random_state=1),\n                   n_iter=20, param_distributions={'max_depth': range(1, 51)},\n                   scoring='r2')estimator: DecisionTreeRegressorDecisionTreeRegressor(random_state=1)DecisionTreeRegressorDecisionTreeRegressor(random_state=1)\n\n\n- 중요정보 확인\n\n# 중요 정보 확인\nprint('=' * 80)\nprint(tree_model.cv_results_['mean_test_score'])\nprint('-' * 80)\nprint('최적파라미터:',tree_model.best_params_)\nprint('-' * 80)\nprint('최고성능:', tree_model.best_score_)\nprint('=' * 80)\n\n================================================================================\n[0.71284838 0.70905766 0.67646772 0.7383174  0.72240391 0.71284838\n 0.71284838 0.71284838 0.71278394 0.71250015 0.71284838 0.37077174\n 0.71284838 0.70928798 0.71284838 0.70526236 0.71284838 0.71284838\n 0.71284838 0.71284838]\n--------------------------------------------------------------------------------\n최적파라미터: {'max_depth': 5}\n--------------------------------------------------------------------------------\n최고성능: 0.7383174002807829\n================================================================================\n\n\n- 변수 중요도 확인\n\n# 변수 중요도\nplt.figure(figsize=(5, 5))\nplt.barh(y=list(x), width=tree_model.best_estimator_.feature_importances_)\nplt.show()\n\n\n\n\n\n\n\n\ny_pred = tree_model.predict(x_test)\n\n\nprint(\"MAE : \", mean_absolute_error(y_test, y_pred))\nprint(\"r2 : \", r2_score(y_test, y_pred))\n\nMAE :  3.0965698865596964\nr2 :  0.7389100169622292"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-15-04. 머신러닝 (4).html#excercise.-1",
    "href": "posts/DX/04. 머신러닝/2023-09-15-04. 머신러닝 (4).html#excercise.-1",
    "title": "04. 머신러닝 (4)",
    "section": "",
    "text": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, f1_score, r2_score, precision_score, recall_score\n\n\n\n\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/jangrae/csv/master/mobile.csv'\ndata = pd.read_csv(path)\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20000 entries, 0 to 19999\nData columns (total 13 columns):\n #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   id                           20000 non-null  int64 \n 1   COLLEGE                      20000 non-null  int64 \n 2   INCOME                       20000 non-null  int64 \n 3   OVERAGE                      20000 non-null  int64 \n 4   LEFTOVER                     20000 non-null  int64 \n 5   HOUSE                        20000 non-null  int64 \n 6   HANDSET_PRICE                20000 non-null  int64 \n 7   OVER_15MINS_CALLS_PER_MONTH  20000 non-null  int64 \n 8   AVERAGE_CALL_DURATION        20000 non-null  int64 \n 9   REPORTED_SATISFACTION        20000 non-null  object\n 10  REPORTED_USAGE_LEVEL         20000 non-null  object\n 11  CONSIDERING_CHANGE_OF_PLAN   20000 non-null  object\n 12  CHURN                        20000 non-null  int64 \ndtypes: int64(10), object(3)\nmemory usage: 2.0+ MB\n\n\n\ndata.isna().sum()\n\nid                             0\nCOLLEGE                        0\nINCOME                         0\nOVERAGE                        0\nLEFTOVER                       0\nHOUSE                          0\nHANDSET_PRICE                  0\nOVER_15MINS_CALLS_PER_MONTH    0\nAVERAGE_CALL_DURATION          0\nREPORTED_SATISFACTION          0\nREPORTED_USAGE_LEVEL           0\nCONSIDERING_CHANGE_OF_PLAN     0\nCHURN                          0\ndtype: int64\n\n\n\n\n\n- 변수 제거\n\ndata.drop(\"id\", axis = 1, inplace = True)\ndata.head()\n\n\n\n\n\n\n\n\nCOLLEGE\nINCOME\nOVERAGE\nLEFTOVER\nHOUSE\nHANDSET_PRICE\nOVER_15MINS_CALLS_PER_MONTH\nAVERAGE_CALL_DURATION\nREPORTED_SATISFACTION\nREPORTED_USAGE_LEVEL\nCONSIDERING_CHANGE_OF_PLAN\nCHURN\n\n\n\n\n0\n0\n31953\n0\n6\n313378\n161\n0\n4\nunsat\nlittle\nno\n0\n\n\n1\n1\n36147\n0\n13\n800586\n244\n0\n6\nunsat\nlittle\nconsidering\n0\n\n\n2\n1\n27273\n230\n0\n305049\n201\n16\n15\nunsat\nvery_little\nperhaps\n0\n\n\n3\n0\n120070\n38\n33\n788235\n780\n3\n2\nunsat\nvery_high\nconsidering\n1\n\n\n4\n1\n29215\n208\n85\n224784\n241\n21\n1\nvery_unsat\nlittle\nnever_thought\n0\n\n\n\n\n\n\n\n- x, y 분리\n\ntarget = \"CHURN\"\nx = data.drop(target, axis = 1)\ny = data[target]\n\n- 가변수화\n\nd_cols = [\"REPORTED_SATISFACTION\", \"REPORTED_USAGE_LEVEL\", \"CONSIDERING_CHANGE_OF_PLAN\"]\n\nx = pd.get_dummies(x, columns = d_cols, dtype = float, drop_first = True)\n\n\nx.head()\n\n\n\n\n\n\n\n\nCOLLEGE\nINCOME\nOVERAGE\nLEFTOVER\nHOUSE\nHANDSET_PRICE\nOVER_15MINS_CALLS_PER_MONTH\nAVERAGE_CALL_DURATION\nREPORTED_SATISFACTION_sat\nREPORTED_SATISFACTION_unsat\nREPORTED_SATISFACTION_very_sat\nREPORTED_SATISFACTION_very_unsat\nREPORTED_USAGE_LEVEL_high\nREPORTED_USAGE_LEVEL_little\nREPORTED_USAGE_LEVEL_very_high\nREPORTED_USAGE_LEVEL_very_little\nCONSIDERING_CHANGE_OF_PLAN_considering\nCONSIDERING_CHANGE_OF_PLAN_never_thought\nCONSIDERING_CHANGE_OF_PLAN_no\nCONSIDERING_CHANGE_OF_PLAN_perhaps\n\n\n\n\n0\n0\n31953\n0\n6\n313378\n161\n0\n4\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n1\n1\n36147\n0\n13\n800586\n244\n0\n6\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n2\n1\n27273\n230\n0\n305049\n201\n16\n15\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0\n120070\n38\n33\n788235\n780\n3\n2\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n4\n1\n29215\n208\n85\n224784\n241\n21\n1\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n\n- 학습용, 평가용 데이터 분리\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)\n\n\n\n\n\n일단 k-fold로 예측\n\n\ntree1 = DecisionTreeClassifier(max_depth = 5)\n\ncv_tree = cross_val_score(tree1, x_train, y_train, cv = 10)\nprint(cv_tree)\nprint(cv_tree.mean())\n\n[0.69928571 0.69214286 0.69857143 0.69285714 0.69428571 0.695\n 0.70857143 0.69142857 0.70428571 0.70285714]\n0.6979285714285715\n\n\n\n\n\n- 선언\n\nparams = {\"max_depth\" : range(3, 31)}\n\ntree =DecisionTreeClassifier(random_state=1)\ncv_tree2 = RandomizedSearchCV(tree,\n                                                           params, cv = 5, n_iter = 20, scoring = \"accuracy\" )\n\n- 모델 학습\n\ncv_tree2.fit(x_train, y_train)\n\nRandomizedSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=1),\n                   n_iter=20, param_distributions={'max_depth': range(3, 31)},\n                   scoring='accuracy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=1),\n                   n_iter=20, param_distributions={'max_depth': range(3, 31)},\n                   scoring='accuracy')estimator: DecisionTreeClassifierDecisionTreeClassifier(random_state=1)DecisionTreeClassifierDecisionTreeClassifier(random_state=1)\n\n\n\n# 중요 정보 확인\nprint(cv_tree2.cv_results_[\"params\"])\nprint('=' * 80)\nprint(cv_tree2.cv_results_['mean_test_score']) ## 각각의 랜덤서치요소 에서의 cv =5  결과\nprint('-' * 80)\nprint('최적파라미터:',cv_tree2.best_params_)\nprint('-' * 80)\nprint('최고성능:', cv_tree2.best_score_)\nprint('=' * 80)\n\n[{'max_depth': 9}, {'max_depth': 8}, {'max_depth': 4}, {'max_depth': 5}, {'max_depth': 25}, {'max_depth': 21}, {'max_depth': 7}, {'max_depth': 24}, {'max_depth': 23}, {'max_depth': 30}, {'max_depth': 6}, {'max_depth': 11}, {'max_depth': 26}, {'max_depth': 29}, {'max_depth': 3}, {'max_depth': 13}, {'max_depth': 14}, {'max_depth': 28}, {'max_depth': 19}, {'max_depth': 27}]\n================================================================================\n[0.68792857 0.69428571 0.69964286 0.69621429 0.62078571 0.62321429\n 0.69742857 0.61914286 0.61742857 0.61814286 0.69757143 0.67385714\n 0.6205     0.61792857 0.68907143 0.65785714 0.65592857 0.61685714\n 0.63342857 0.6165    ]\n--------------------------------------------------------------------------------\n최적파라미터: {'max_depth': 4}\n--------------------------------------------------------------------------------\n최고성능: 0.6996428571428571\n================================================================================\n\n\n- 변수 중요도 확인\n\n# 변수 중요도\nplt.figure(figsize=(4, 4))\nplt.barh(y=list(x), width=cv_tree2.best_estimator_.feature_importances_)\nplt.show()\n\n\n\n\n- 기존의 튜닝전과 비교\n\nprint(\"튜닝전 : \", cv_tree.mean(), \", 튜닝후 : \", cv_tree2.best_score_)\n\n튜닝전 :  0.6979285714285715 , 튜닝후 :  0.6996428571428571\n\n\n\n\n\n\ny_pred = cv_tree2.predict(x_test)\n\n\nacc, pre, re, f1 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), f1_score(y_test, y_pred)\n\n\nscore = [acc, pre, re, f1]\nmeasure = [\"accuracy\", \"precision\", \"recall\", \"F1-score\"]\n\nfig = pd.DataFrame({\"measure\" : measure, \"score\" : score}).\\\n                                sort_values(\"score\", ascending = False).\\\n                                    plot(y = \"measure\",  x= \"score\", color = \"measure\",\n                                            backend = \"plotly\", kind = \"barh\", width = 600, height = 400)\n\nfig.update_xaxes(range = (0.5, 0.8))"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-15-04. 머신러닝 (4).html#실습-review",
    "href": "posts/DX/04. 머신러닝/2023-09-15-04. 머신러닝 (4).html#실습-review",
    "title": "04. 머신러닝 (4)",
    "section": "실습 : review",
    "text": "실습 : review\n\n# 학습용 데이터 불러오기\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/insurance_train.csv'\ndata1 = pd.read_csv(path)\n\n\n# 평가용 데이터 불러오기\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/insurance_test.csv'\ndata2 = pd.read_csv(path)\n\n\n1. 데이터 이해\n- 가변수화\n\n# 가변수화\ndumm_cols = ['sex', 'smoker', 'region']\ndata1 = pd.get_dummies(data1, columns=dumm_cols, drop_first=True)\n\n# 확인\ndata1.head()\n\n\n\n\n\n\n\n\nage\nbmi\nchildren\ncharges\nsex_male\nsmoker_yes\nregion_northwest\nregion_southeast\nregion_southwest\n\n\n\n\n0\n41\n31.600\n0\n6186.1270\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n1\n30\n25.460\n0\n3645.0894\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n2\n18\n30.115\n0\n21344.8467\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n61\n29.920\n3\n30942.1918\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n4\n34\n27.500\n1\n5003.8530\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n\n\n\n\n\n- x, y 분리\n\n# x, y 분리\ntarget = 'charges'\nx = data1.drop(target, axis=1)\ny = data1.loc[:, target]\n\n- 학습 및 평가 데이터 분리\n\n# 학습용, 검증용 분리\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=1)\n\n\n\n2. 모델링\n\n\nCode\n# 불러오기\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error, r2_score\n\n# 선언하기\nmodel = DecisionTreeRegressor(max_depth=5, random_state=1)\n\n# 학습하기\nmodel.fit(x_train, y_train)\n\n# 예측하기\ny_val_pred = model.predict(x_val)\n\n# 평가하기\nprint('MAE:', mean_absolute_error(y_val, y_val_pred))\nprint('R2:', r2_score(y_val, y_val_pred))\n\n\nMAE: 2848.925847322852\nR2: 0.8488510105762039\n\n\n\n\n3. 일반화된 성능을 확인 (K-fold)\n\n# 불러오기\nfrom sklearn.model_selection import cross_val_score\n\n# 성능예측\ncv_score = cross_val_score(model, x_train, y_train, cv=5)\n\n# 결과\nprint(cv_score)\nprint('평균:', cv_score.mean())\n\n[0.80393852 0.87010485 0.80735679 0.84036835 0.81403185]\n평균: 0.8271600698665711\n\n\n\n\n4. 성능 튜닝\n- Grid Search\n\n\nCode\n# 불러오기\nfrom sklearn.model_selection import GridSearchCV\n\n# 기본 모델 선언\nmodel_dt = DecisionTreeRegressor(random_state=1)\n\n# 파라미터 선언\nparams = {'max_depth': range(1, 51)}\n\n# 모델 선언\nmodel = GridSearchCV(model_dt,\n                     params,\n                     cv=5,\n                     scoring='r2')\n\n\n\n# 학습하기\nmodel.fit(x_train, y_train)\n\nGridSearchCV(cv=5, estimator=DecisionTreeRegressor(random_state=1),\n             param_grid={'max_depth': range(1, 51)}, scoring='r2')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=DecisionTreeRegressor(random_state=1),\n             param_grid={'max_depth': range(1, 51)}, scoring='r2')estimator: DecisionTreeRegressorDecisionTreeRegressor(random_state=1)DecisionTreeRegressorDecisionTreeRegressor(random_state=1)\n\n\n\n# 예측 결과 확인\nprint(model.best_params_)\nprint(model.best_score_)\n\n{'max_depth': 4}\n0.83462894168412\n\n\n\n# 성능 검증\ny_val_pred = model.predict(x_val)\nprint(r2_score(y_val, y_val_pred))\n\n0.8577535714634761\n\n\n\n\n5. 최종 평가\n\n# 평가 데이터 확인\ndata2.head()\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n\n\n\n\n\n\n\n\n# 평가 데이터 가변수화\ndumm_cols = ['sex', 'smoker', 'region']\ndata2 = pd.get_dummies(data2, columns=dumm_cols, drop_first=True)\n\n# 확인\ndata2.head()\n\n\n\n\n\n\n\n\nage\nbmi\nchildren\nsex_male\nsmoker_yes\nregion_northwest\nregion_southeast\nregion_southwest\n\n\n\n\n0\n19\n27.900\n0\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n1\n18\n33.770\n1\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n2\n28\n33.000\n3\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n3\n33\n22.705\n0\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n4\n32\n28.880\n0\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n\n\n\n\n\n\n# 예측하기\nx_test = data2\ny_pred = model.predict(x_test)\n\ndata2[\"charges\"] = y_pred\n\n- 파일 저장\n\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/insurance_test.csv'\ndata2 = pd.read_csv(path)\ndata2[\"charges\"] = y_pred\n\n\ndata.to_csv?\n\n\ndata2.to_csv(\"insurance_test.csv\", index= False)\n\n- 다시 불러오기\n\npd.read_csv(\"insurance_test.csv\").head()\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16167.418120\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n6117.634358\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n6117.634358\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n5235.846316\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n5235.846316"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-15-04. 머신러닝 (4).html#클래스-불균형",
    "href": "posts/DX/04. 머신러닝/2023-09-15-04. 머신러닝 (4).html#클래스-불균형",
    "title": "04. 머신러닝 (4)",
    "section": "클래스 불균형",
    "text": "클래스 불균형\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/Attrition2.csv'\ndata = pd.read_csv(path)\n\n\n1. 데이터 이해\n\n# 데이터 읽어오기\npath = 'https://raw.githubusercontent.com/Jangrae/csv/master/Attrition2.csv'\ndata = pd.read_csv(path)\n\n\n# Target 확인\nprint(data['Attrition'].value_counts())\n\n# 시각화\ndata['Attrition'].value_counts().plot(kind='barh')\nplt.show()\n\nAttrition\n0    1050\n1     100\nName: count, dtype: int64\n\n\n\n\n\n\n\n2. 데이터 준비\n\n# 가변수화\ndumm_cols = ['Education', 'Department', 'EducationField', 'Gender', 'JobRole', 'JobSatisfaction',\n             'MaritalStatus', 'RelationshipSatisfaction', 'WorkLifeBalance']\ndata = pd.get_dummies(data, columns=dumm_cols, drop_first=True)\n\n\n# target 확인\ntarget = 'Attrition'\n\n# 데이터 분리\nx = data.drop(target, axis=1)\ny = data.loc[:, target]\n\n\n# 모듈 불러오기\nfrom sklearn.model_selection import train_test_split\n\n# 7:3으로 분리\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n\n\n\n3. 모델링\n\n# 학습 데이터 분포 확인\nsns.scatterplot(x='Age', y='MonthlyIncome', hue=y_train, data=x_train)\nplt.show()\n\n\n\n\n\n\nCode\n# 불러오기\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import *\n\n# 선언하기\nmodel = RandomForestClassifier(max_depth=5, random_state=1)\n\n# 학습하기\nmodel.fit(x_train, y_train)\n\n# 예측하기\ny_pred = model.predict(x_test)\n\n# 평가하기\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n\n[[317   1]\n [ 26   1]]\n              precision    recall  f1-score   support\n\n           0       0.92      1.00      0.96       318\n           1       0.50      0.04      0.07        27\n\n    accuracy                           0.92       345\n   macro avg       0.71      0.52      0.51       345\nweighted avg       0.89      0.92      0.89       345\n\n\n\n\nAccuracy(정확도)는 높지만 Target 값 1에 대한 Recall(재현율, 민감도)이 매우 낮다.\n전체 데이터 중에서 Target 값이 1인 데이터가 매우 적기 때문!\n이러한 현상을 클래스 불균형 이라고 한다.\n이를 위해 Under Sampling 또는 Over Sampling 을 사용!\n\n\n\n4. Under Sampling\n\n클래스가 많은 비중의 데이터에서 표본을 적게 추출해 비율을 맞춤\n\n\n# pip install imbalanced-learn\n\n\n# 불러오기\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Under Sampling\nunder_sample = RandomUnderSampler()\nu_x_train, u_y_train = under_sample.fit_resample(x_train, y_train)\n\n# 확인\nprint('전:', np.bincount(y_train))\nprint('후:', np.bincount(u_y_train))\n\n전: [732  73]\n후: [73 73]\n\n\n- 학습 데이터 분포확인\n\n# 학습 데이터 분포 확인\nsns.scatterplot(x='Age', y='MonthlyIncome', hue=u_y_train, data=u_x_train)\nplt.show()\n\n\n\n\n- 모델 성능 다시 확인\n\n# 선언하기\nmodel = RandomForestClassifier(max_depth=5, random_state=1)\n\n# 학습하기\nmodel.fit(u_x_train, u_y_train)\n\n# 예측하기\ny_pred = model.predict(x_test)\n\n# 평가하기\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n[[237  81]\n [ 13  14]]\n              precision    recall  f1-score   support\n\n           0       0.95      0.75      0.83       318\n           1       0.15      0.52      0.23        27\n\n    accuracy                           0.73       345\n   macro avg       0.55      0.63      0.53       345\nweighted avg       0.89      0.73      0.79       345\n\n\n\n\n\n5. Over sampling\n\n# 불러오기\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Over Sampling\nover_sample = RandomOverSampler()\no_x_train, o_y_train = over_sample.fit_resample(x_train, y_train)\n\n# 확인\nprint('전:', np.bincount(y_train))\nprint('후:', np.bincount(o_y_train))\n\n전: [732  73]\n후: [732 732]\n\n\n\n# 학습 데이터 분포 확인\nsns.scatterplot(x='Age', y='MonthlyIncome', hue=o_y_train, data=o_x_train, alpha=0.3)\nplt.show()\n\n\n\n\n- 모델 성능 다시 확인\n\n# 선언하기\nmodel = RandomForestClassifier(max_depth=5, random_state=1)\n\n# 학습하기\nmodel.fit(o_x_train, o_y_train)\n\n# 예측하기\ny_pred = model.predict(x_test)\n\n# 평가하기\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n[[292  26]\n [ 16  11]]\n              precision    recall  f1-score   support\n\n           0       0.95      0.92      0.93       318\n           1       0.30      0.41      0.34        27\n\n    accuracy                           0.88       345\n   macro avg       0.62      0.66      0.64       345\nweighted avg       0.90      0.88      0.89       345\n\n\n\n\n\n6. Over Sampling #2\n- 클래스가 부족한 표본 사이에 해당 표본을 몇 개 더 추가\n\n# 불러오기\nfrom imblearn.over_sampling import SMOTE\n\n# Over Sampling\nsmote = SMOTE()\ns_x_train, s_y_train = smote.fit_resample(x_train, y_train)\n\n# 확인\nprint('전:', np.bincount(y_train))\nprint('후:', np.bincount(s_y_train))\n\n전: [732  73]\n후: [732 732]\n\n\n- 학습 데이터 분포 확인\n\n# 학습 데이터 분포 확인\nsns.scatterplot(x='Age', y='MonthlyIncome', hue=s_y_train, data=s_x_train)\nplt.show()\n\n\n\n\n- 모델 성능 다시 확인\n\n# 선언하기\nmodel = RandomForestClassifier(max_depth=5, random_state=1)\n\n# 학습하기\nmodel.fit(s_x_train, s_y_train)\n\n# 예측하기\ny_pred = model.predict(x_test)\n\n# 평가하기\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n[[301  17]\n [ 20   7]]\n              precision    recall  f1-score   support\n\n           0       0.94      0.95      0.94       318\n           1       0.29      0.26      0.27        27\n\n    accuracy                           0.89       345\n   macro avg       0.61      0.60      0.61       345\nweighted avg       0.89      0.89      0.89       345\n\n\n\n\n\n7. Class weight\n- class_weight 하이퍼파라미터를 설정해 모델링한 후 성능을 확인\n\nclass_weight는 자체적으로 불균형 데이터에 대한 균형을 맞춰줌\n\n\n# 선언하기\nmodel = RandomForestClassifier(max_depth=5, random_state=1, class_weight='balanced')\n\n# 학습하기\nmodel.fit(x_train, y_train)\n\n# 예측하기\ny_pred = model.predict(x_test)\n\n# 평가하기\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n[[302  16]\n [ 17  10]]\n              precision    recall  f1-score   support\n\n           0       0.95      0.95      0.95       318\n           1       0.38      0.37      0.38        27\n\n    accuracy                           0.90       345\n   macro avg       0.67      0.66      0.66       345\nweighted avg       0.90      0.90      0.90       345"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-15-04. 머신러닝 (4).html#voting",
    "href": "posts/DX/04. 머신러닝/2023-09-15-04. 머신러닝 (4).html#voting",
    "title": "04. 머신러닝 (4)",
    "section": "1. Voting",
    "text": "1. Voting\n- 여러 모델들의 예측 결과를 투표를 통해 최종 예측 결과를 예측하는 방법 (서로 다른 알고리즘 사용가능)\n\n하드 보팅 : 다수 모델이 예측한 값이 최종 결과값\n소프트 보팅 : 각 모델에서 구한 클래스에 속할 확률의 평균을 구해서 예측 결과 산출"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-15-04. 머신러닝 (4).html#배깅bootstrap-aggregation",
    "href": "posts/DX/04. 머신러닝/2023-09-15-04. 머신러닝 (4).html#배깅bootstrap-aggregation",
    "title": "04. 머신러닝 (4)",
    "section": "2. 배깅(Bootstrap Aggregation)",
    "text": "2. 배깅(Bootstrap Aggregation)\n- 전체 데이터에서 각각 샘플링을 하여 개별 모델을 생성 (단, 샘플링 방식은 복원추출)\n\n단, 모델은 같은 유형의 알고리즘 기반 모델들을 사용\n범주형 데이터는 Voting 방식으로 결과를 집계\n연속형 데이터는 평균으로 결과를 집계\n대표적인 알고리즘 : Random Forest\n\n\n(1) Random Forest\n\n서로 다른 데이터로 만든 Decision Tree가 개별적으로 학습하여 모든 결과를 집계한 뒤 최종 결과를 결정하는 알고리즘\n\n- Random\n\nRandom하게 데이터를 샘플링 했다. (일부는 중복이 되었지만… 그래도 뭐 인정!)\n개별 모델이 트리를 구성할 때 분할 기준이 되는 변수를 랜덤하게 선정 \\(\\to\\) 무작위로 뽑은 \\(n\\)개의 변수 중에서 가장 Gain이 큰 변수를 기준으로 트리를 분할\n2번의 이유로 개별 모델마다 다른 구조의 트리를 형성할 것임\n\n\n- 파라미터\n\nn_estimatorsr : 트리의 개수\nmax_depth : 트리의 촤대 깊이\nmin_samples_split : 트리와 동일\nmin_samples_leaf : 트리와 동일\nmax_feature : 최선의 분할을 위해 고려할 Feature 수(기본값: auto)\n\n기본값으로 설정하면 모든 Feature를 사용해서 분할 수행\n정수형으로 선언하면 Feature 수, 실수형으로 선언하면 Feature 비율\n’sqrt’로 선언하면 전체 Feature 수의 루트 값\n’auto’로 설정하면 ’sqrt’와 같은 의미\n’log’로 선언하면 log2(전체 Feature 수)\n\n\n\n\n(2) 부스팅(Boosting)\n\n같은 유형의 알고리즘 기반 모델 여러 개에 대해 순차적으로 학습을 수행\n이전 모델이 제대로 예측하지 못한 데이터에 대해서 가중치를 부여하여 다음 모델이 학습과 예측을 진행하는 방법\n계속하여 모델에게 가중치를 부스팅하며 학습을 진행해 부스팅 방식이라 함\n예측 성능이 뛰어나 앙상블 학습을 주도함\n배깅에 비해 성능이 좋지만, 속도가 느리고 과적합 발생 가능성이 있음 → 상황에 맞게 적절히 사용해야 함\n대표적인 부스팅 알고리즘: XGBoost, LightGBM`\n\n- 배깅은 투표 or 평균, 부스팅은 잔차와 가중치를 이용하여 순차적 학습을 진행!!\n\n부스팅은 잔차를 추정한다.\n\n\nGradient Boost (이 부분 다시정리)\n- \\(f_i(x)\\) : \\(i\\) 번째 예측 모델이라고하자\n- \\(e_i\\) : \\(i\\) 번째 예측 모델의 오차\n\n\nXGBoost extreme Gradient Boosting\n- 부스팅을 구현한 대표적인 알고리즘 중 하나가 GBM(Gradient Boost Machine)\n\nGBM 알고리즘을 병렬 학습이 가능하도록 구현한 것이 XGBoost\n회귀, 분류 문제를 모두 지원하며, 성능과 자원 효율이 좋아 많이 사용됨\n\n- 장점\n\n높은 예측 성능\nGBM 대비 빠른 수행 시간 : 병렬 수행 및 다양한 기능으로 GBM에 비해 빠르게 수행됨\n규제 (Regularzaiton) : GBM에 없었던 과적합 규제 기능을 가지고 있\n가지치기(Tree Pruning) : max_depth 등의 하이퍼파라미터로 가지치기를 할 수 있음\n\nTree Pruning 기능으로 성능에 이점이 없는 분할은 가지치기 할 수 있음\n\n내장된 교차 검증 : 반복 수행시마다 내부적으로 학습 데이터와 검증 데이터에 대한 교차 검증을 수행\n\n지정된 반복 횟수가 아닌 교차 검증을 통해 성능을 확인하여 필요시 조기 중단 가능 (검증 데이터에 대한 성능이 떨어질 경우!)\n\n결측치 자체 처리 : 알아서 결측치를 고려해서 학습함(결측치 여부를 노드 분시를 위한 질문에 포함시킴)\n\n하지만 명시적으로 결측치에 대한 처리를 진행하기를 권고한다…..\n\n\n\n\n\n(3) stacking\n- 여러 모델의 예측 값을 최종 모델의 학습 데이터로 사용하여 예측하는 방법 * KNN, Logistic Regression, XGBoost 모델을 사용해 4종류 예측값을 구한 후, 이 예측 값을 최종 모델인 Randomforest 학습 데이터로 사용 * 현실 모델에서 많이 사용되지 않으며, 캐글(Kaggle) 같은 미세한 성능 차이로 승부를 결정하는 대회에서 사용됨 * 기본 모델로 4개 이상 선택해야 좋은 결과를 기대할 수 있음"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-15-04. 머신러닝 (4).html#실습-1",
    "href": "posts/DX/04. 머신러닝/2023-09-15-04. 머신러닝 (4).html#실습-1",
    "title": "04. 머신러닝 (4)",
    "section": "실습",
    "text": "실습\n\n(1) 데이터 이해\n\n# 데이터 불러오기\npath = 'https://raw.githubusercontent.com/jangrae/csv/master/admission_simple.csv'\ndata = pd.read_csv(path)\n\n\n# 데이터 살펴보기\ndata.head()\n\n\n\n\n\n\n\n\nGRE\nTOEFL\nRANK\nSOP\nLOR\nGPA\nRESEARCH\nADMIT\n\n\n\n\n0\n337\n118\n4\n4.5\n4.5\n9.65\n1\n1\n\n\n1\n324\n107\n4\n4.0\n4.5\n8.87\n1\n1\n\n\n2\n316\n104\n3\n3.0\n3.5\n8.00\n1\n0\n\n\n3\n322\n110\n3\n3.5\n2.5\n8.67\n1\n1\n\n\n4\n314\n103\n2\n2.0\n3.0\n8.21\n0\n0\n\n\n\n\n\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 500 entries, 0 to 499\nData columns (total 8 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   GRE       500 non-null    int64  \n 1   TOEFL     500 non-null    int64  \n 2   RANK      500 non-null    int64  \n 3   SOP       500 non-null    float64\n 4   LOR       500 non-null    float64\n 5   GPA       500 non-null    float64\n 6   RESEARCH  500 non-null    int64  \n 7   ADMIT     500 non-null    int64  \ndtypes: float64(3), int64(5)\nmemory usage: 31.4 KB\n\n\n\ndata.isna().sum()\n\nGRE         0\nTOEFL       0\nRANK        0\nSOP         0\nLOR         0\nGPA         0\nRESEARCH    0\nADMIT       0\ndtype: int64\n\n\n\n\n(2) 데이터 준비\n\n# target 확인\ntarget = 'ADMIT'\n\n# 데이터 분리\nx = data.drop(target, axis=1)\ny = data[target]\n\n\n# 모듈 불러오기\nfrom sklearn.model_selection import train_test_split\n\n# 7:3으로 분리\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n\n\n# 모듈 불러오기\nfrom sklearn.preprocessing import MinMaxScaler\n\n# 정규화\nscaler = MinMaxScaler()\nscaler.fit(x_train)\nx_train_s = scaler.transform(x_train)\nx_test_s = scaler.transform(x_test)\n\n\n\n(3) 모델링\n\n# 라이브러리 불러오기\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.metrics import *\n\n\n# xgboost 설치\n#!pip install xgboost\n\n\n# lightgbm 설치\n#!pip install lightgbm\n\n1 Knn\n\n# 선언하기\nmodel = KNeighborsClassifier(n_neighbors=5)\n\n# 학습하기\nmodel.fit(x_train_s, y_train)\n\n# 예측하기\ny_pred = model.predict(x_test_s)\n\n# 평가하기\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n[[79  5]\n [15 51]]\n              precision    recall  f1-score   support\n\n           0       0.84      0.94      0.89        84\n           1       0.91      0.77      0.84        66\n\n    accuracy                           0.87       150\n   macro avg       0.88      0.86      0.86       150\nweighted avg       0.87      0.87      0.86       150\n\n\n\n2 tree\n\n# 선언하기\nmodel = DecisionTreeClassifier(max_depth=5, random_state=1)\n\n# 학습하기\nmodel.fit(x_train, y_train)\n\n# 예측하기\ny_pred = model.predict(x_test)\n\n# 5단계: 평가하기\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n[[77  7]\n [13 53]]\n              precision    recall  f1-score   support\n\n           0       0.86      0.92      0.89        84\n           1       0.88      0.80      0.84        66\n\n    accuracy                           0.87       150\n   macro avg       0.87      0.86      0.86       150\nweighted avg       0.87      0.87      0.87       150\n\n\n\n3 logistic\n\n# 선언하기\nmodel = LogisticRegression()\n\n# 학습하기\nmodel.fit(x_train, y_train)\n\n# 예측하기\ny_pred = model.predict(x_test)\n\n# 5단계: 평가하기\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n[[75  9]\n [14 52]]\n              precision    recall  f1-score   support\n\n           0       0.84      0.89      0.87        84\n           1       0.85      0.79      0.82        66\n\n    accuracy                           0.85       150\n   macro avg       0.85      0.84      0.84       150\nweighted avg       0.85      0.85      0.85       150\n\n\n\n4 Random Forest\n\nmodel = RandomForestClassifier(max_depth = 5, random_state = 1) ## default tree = 100\n\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n[[78  6]\n [13 53]]\n              precision    recall  f1-score   support\n\n           0       0.86      0.93      0.89        84\n           1       0.90      0.80      0.85        66\n\n    accuracy                           0.87       150\n   macro avg       0.88      0.87      0.87       150\nweighted avg       0.88      0.87      0.87       150\n\n\n\n\nprint(list(x))\nprint(model.feature_importances_)\n\n['GRE', 'TOEFL', 'RANK', 'SOP', 'LOR', 'GPA', 'RESEARCH']\n[0.22081595 0.20240024 0.09976925 0.08278875 0.04189168 0.32011405\n 0.0322201 ]\n\n\n5 XGBoost\n\nmodel = XGBClassifier(max_depth = 5, random_state = 1)\n\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n[[77  7]\n [15 51]]\n              precision    recall  f1-score   support\n\n           0       0.84      0.92      0.88        84\n           1       0.88      0.77      0.82        66\n\n    accuracy                           0.85       150\n   macro avg       0.86      0.84      0.85       150\nweighted avg       0.86      0.85      0.85       150\n\n\n\n\nprint(list(x))\nprint(model.feature_importances_)\n\n['GRE', 'TOEFL', 'RANK', 'SOP', 'LOR', 'GPA', 'RESEARCH']\n[0.10025358 0.10419714 0.07541527 0.09831444 0.07010549 0.43016294\n 0.1215511 ]\n\n\n6 LightGBM\n\n#model = LGBMClassifier(max_depth = 5, random_state = 1) ## 이렇게 하면 먼가 주르륵, 주르륵 뜬다. 그래서  verbose 인자를 넣어줌\n\nmodel = LGBMClassifier(max_depth = 5, random_state = 1,verbose = -100) \nmodel.fit(x_train, y_train) \n\ny_pred = model.predict(x_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\nprint(list(x))\nprint(model.feature_importances_)\n\n[[77  7]\n [14 52]]\n              precision    recall  f1-score   support\n\n           0       0.85      0.92      0.88        84\n           1       0.88      0.79      0.83        66\n\n    accuracy                           0.86       150\n   macro avg       0.86      0.85      0.86       150\nweighted avg       0.86      0.86      0.86       150\n\n['GRE', 'TOEFL', 'RANK', 'SOP', 'LOR', 'GPA', 'RESEARCH']\n[221 190  50 115  63 383  34]\n\n\n- 음 변수 중요도가 뭔가 다른 모델들과 비교해 굉장히 크게 나온다. 척도가 다르구나 하고 넘어가자.\n\n\n(4) 참고 : Early Stopping\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state=1)\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2, random_state=1)\n\n\nmodel = XGBClassifier(max_depth= 5, random_state = 1)\n\nmodel.fit(x_train, y_train,\n                eval_set=[(x_train, y_train), (x_val, y_val)],\n                early_stopping_rounds=10) \n\n[0] validation_0-logloss:0.50189    validation_1-logloss:0.50718\n[1] validation_0-logloss:0.39727    validation_1-logloss:0.41745\n[2] validation_0-logloss:0.32813    validation_1-logloss:0.35767\n[3] validation_0-logloss:0.28005    validation_1-logloss:0.32211\n[4] validation_0-logloss:0.24248    validation_1-logloss:0.30427\n[5] validation_0-logloss:0.21255    validation_1-logloss:0.29216\n[6] validation_0-logloss:0.19052    validation_1-logloss:0.28142\n[7] validation_0-logloss:0.17478    validation_1-logloss:0.26955\n[8] validation_0-logloss:0.16192    validation_1-logloss:0.26525\n[9] validation_0-logloss:0.15309    validation_1-logloss:0.25668\n[10]    validation_0-logloss:0.14571    validation_1-logloss:0.25679\n[11]    validation_0-logloss:0.14029    validation_1-logloss:0.25512\n[12]    validation_0-logloss:0.13597    validation_1-logloss:0.25464\n[13]    validation_0-logloss:0.12819    validation_1-logloss:0.25188\n[14]    validation_0-logloss:0.12105    validation_1-logloss:0.25316\n[15]    validation_0-logloss:0.11541    validation_1-logloss:0.25592\n[16]    validation_0-logloss:0.11220    validation_1-logloss:0.25947\n[17]    validation_0-logloss:0.10689    validation_1-logloss:0.26523\n[18]    validation_0-logloss:0.10236    validation_1-logloss:0.27018\n[19]    validation_0-logloss:0.09768    validation_1-logloss:0.27086\n[20]    validation_0-logloss:0.09465    validation_1-logloss:0.27693\n[21]    validation_0-logloss:0.09172    validation_1-logloss:0.27836\n[22]    validation_0-logloss:0.08943    validation_1-logloss:0.27502\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=1, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=1, ...)\n\n\n- 그 후에 내용은 나중에 코드 참고…\n\nresults = model.evals_result()\n#results\n\nAttributeError: 'LGBMClassifier' object has no attribute 'evals_result'\n\n\n\nplt.plot(results['validation_0'][\"logloss\"],label= \"val0\")\nplt.plot(results['validation_1'][\"logloss\"],label= \"val1\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x2b7b1243850&gt;"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-18-06. 머신러닝 (5).html",
    "href": "posts/DX/04. 머신러닝/2023-09-18-06. 머신러닝 (5).html",
    "title": "06. 머신러닝 (5)",
    "section": "",
    "text": "# 기본 라이브러리 가져오기\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import *\n\nfrom sklearn.datasets import load_breast_cancer, load_digits\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n\n\n\n\n\n\n# breast_cancer 데이터 로딩\ncancer=load_breast_cancer()\nx = cancer.data\ny = cancer.target\n\nx = pd.DataFrame(x, columns=cancer.feature_names)\n\n\nx.head()\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 30 columns\n\n\n\n\n#x.info()\n\n\n#x.describe().T\n\n\n\n\n- 거리계산 기반 차원축소이므로 스케일링이 필요\n\n\n\n\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size = .3, random_state = 20)\n\n\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA\n\n\n# feature 수\nx_train.shape[1]\n\n30\n\n\n\n# 주성분을 몇개로 할지 결정(최대값 : 전체 feature 수)\nn = x_train.shape[1] ## 일단 원래 feature의 수만 지정\n\n# 주성분 분석 선언\npca = PCA(n_components=n)\n\n# 만들고, 적용하기\nx_train_pc = pca.fit_transform(x_train)\nx_val_pc = pca.transform(x_val)\n\n- 편리한 사용을 위해 데이터프레임으로 변환\n\n# 칼럼이름 생성\ncolumn_names = [ 'PC'+str(i+1) for i in range(n) ]\ncolumn_names[:5]\n\n['PC1', 'PC2', 'PC3', 'PC4', 'PC5']\n\n\n\n# 데이터프레임으로 변환하기\nx_train_pc = pd.DataFrame(x_train_pc, columns = column_names)\nx_val_pc = pd.DataFrame(x_val_pc, columns = column_names)\nx_train_pc.head()\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\nPC9\nPC10\n...\nPC21\nPC22\nPC23\nPC24\nPC25\nPC26\nPC27\nPC28\nPC29\nPC30\n\n\n\n\n0\n-34.932206\n47.067779\n-17.756611\n-7.596006\n1.342721\n-1.956710\n1.710276\n0.641097\n0.120917\n0.139938\n...\n0.000962\n0.010435\n-0.003107\n-0.002029\n0.003974\n-0.000625\n-0.000576\n-0.000446\n0.000679\n0.001145\n\n\n1\n830.092826\n201.450565\n-6.166167\n-2.578986\n-1.794260\n2.619614\n0.284543\n0.366729\n-0.027226\n0.018051\n...\n-0.019489\n0.001261\n0.003305\n-0.011163\n0.005675\n0.001333\n0.001847\n-0.003122\n0.001580\n0.000655\n\n\n2\n-546.384045\n22.139574\n17.780018\n0.887465\n-0.308381\n-0.126655\n-1.450590\n-0.632612\n-0.009378\n-0.606726\n...\n0.002701\n-0.014928\n0.001773\n0.001490\n0.007249\n-0.000604\n-0.001318\n-0.001299\n-0.000982\n-0.000077\n\n\n3\n653.345270\n25.739603\n30.832277\n-7.437820\n2.139092\n3.686376\n-0.111756\n0.191535\n-0.181315\n0.111042\n...\n0.002200\n0.021932\n-0.012297\n0.002943\n0.002366\n-0.005320\n0.000484\n0.000967\n-0.002898\n-0.000765\n\n\n4\n-422.567968\n19.784995\n6.432610\n-10.383670\n-14.675624\n0.098355\n-0.537767\n-0.113981\n0.062963\n0.442519\n...\n0.003790\n-0.012366\n0.004250\n0.005447\n-0.002826\n-0.000611\n-0.000136\n0.000270\n-0.000225\n0.000407\n\n\n\n\n5 rows × 30 columns\n\n\n\n\n\n\n- 주성분을 1,2,3개로 선언하고, x_train을 이용해서 주성분을 추출\n\n# 주성분을 몇개로 할지 결정(최대값 : 전체 feature 수)\nn = [1,2,3]## 일단 원래 feature의 수만 지정\n\n# 주성분 분석 선언\n\nfor i in n : \n     exec(f\"pca{i} = PCA(n_components={i})\")\n     exec(f\"x_train_pc{i} = pca{i}.fit_transform(x_train)\")\n\n\n\n\n\n그래프를 보고 적절한 주성분의 개수를 지정(elbow method!)\nx축 : PC 수\ny축 : 전체 분산크기 - 누적 분산크기\n\n\npca\n\nPCA(n_components=30)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA(n_components=30)\n\n\n\nplt.plot(pca.explained_variance_ratio_, marker = '.')\nplt.xlabel('No. of PC')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n- 주성분 중 상위 2개를 뽑아 시각화\n\nsns.scatterplot(x = 'PC1', y = 'PC2', data = x_train_pc, hue = y_train)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nmodel0 = KNeighborsClassifier()\nmodel0.fit(x_train, y_train)\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier()\n\n\n\n# 원본데이터 모델의 성능\npred0 = model0.predict(x_val)\n\nprint(confusion_matrix(y_val, pred0))\nprint(accuracy_score(y_val, pred0))\nprint(classification_report(y_val, pred0))\n\n[[ 57   7]\n [  5 102]]\n0.9298245614035088\n              precision    recall  f1-score   support\n\n           0       0.92      0.89      0.90        64\n           1       0.94      0.95      0.94       107\n\n    accuracy                           0.93       171\n   macro avg       0.93      0.92      0.92       171\nweighted avg       0.93      0.93      0.93       171\n\n\n\n\n\n\n\nn = 1 \nx_train_pc_n = x_train_pc[column_names[:n]] \nx_val_pc_n = x_val_pc[column_names[:n]] \n# 뽑은 데이터로 모델링 \nmodel1 = KNeighborsClassifier() \nmodel1.fit(x_train_pc_n, y_train) # 예측 평가 \npred1 = model1.predict(x_val_pc_n) \nprint(accuracy_score(y_val, pred1))\n\n0.8830409356725146\n\n\n\n\n\n\nn = 2 \nx_train_pc_n = x_train_pc[column_names[:n]] \nx_val_pc_n = x_val_pc[column_names[:n]] \n# 뽑은 데이터로 모델링 \nmodel1 = KNeighborsClassifier() \nmodel1.fit(x_train_pc_n, y_train) # 예측 평가 \npred1 = model1.predict(x_val_pc_n) \nprint(accuracy_score(y_val, pred1))\n\n0.9181286549707602\n\n\n\n\n\n\nn = 3\nx_train_pc_n = x_train_pc[column_names[:n]] \nx_val_pc_n = x_val_pc[column_names[:n]] \n# 뽑은 데이터로 모델링 \nmodel1 = KNeighborsClassifier() \nmodel1.fit(x_train_pc_n, y_train) # 예측 평가 \npred1 = model1.predict(x_val_pc_n) \nprint(accuracy_score(y_val, pred1))\n\n0.9181286549707602\n\n\n\n\n\n\n\n1. 원본데이터의 유사도(거리) 맵을 만듬\n2 원본에서의 유사도가 축소한 차원에서도 유지 되었으면 좋겠음\n3 주성분 분석은 선형 축소, t-SNE는 비선형 축소이다.\n\n\n\nfrom sklearn.manifold import TSNE\n\n\n# 2차원으로 축소하기\ntsne = TSNE(n_components = 2, random_state=20)\nx_tsne = tsne.fit_transform(x)\n\n# 사용의 편리함을 위해 DataFrame으로` 변환\nx_tsne = pd.DataFrame(x_tsne, columns = ['T1','T2'])\n\n\nx_tsne.shape\n\n(569, 2)\n\n\n- 시각화\n\nplt.figure(figsize=(6,6))\nsns.scatterplot(x = 'T1', y = 'T2', data = x_tsne, hue = y)\nplt.grid()\n\n\n\n\n\n\n\n\n\n\ndigits = load_digits()\nx = digits.data\ny = digits.target\n\ny = pd.Categorical(y)\n\n\nx.shape\n\n(1797, 64)\n\n\n- 데이터 살펴보기\n\nprint(x[0].reshape(8,8))\n\n[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n [ 0.  0. 13. 15. 10. 15.  5.  0.]\n [ 0.  3. 15.  2.  0. 11.  8.  0.]\n [ 0.  4. 12.  0.  0.  8.  8.  0.]\n [ 0.  5.  8.  0.  0.  9.  8.  0.]\n [ 0.  4. 11.  0.  1. 12.  7.  0.]\n [ 0.  2. 14.  5. 10. 12.  0.  0.]\n [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n\n\n\n# f, axes = plt.subplots(5, 2, sharey=True, figsize=(16,6))\nplt.figure(figsize=(10, 4))\nfor i in range(10):\n    plt.subplot(2, 5, i + 1)\n    plt.imshow(x[i,:].reshape([8,8]), cmap='gray');\n\n\n\n\n\n스케일링\n\n\n# 최대, 최소값\nnp.min(x), np.max(x)\n\n(0.0, 16.0)\n\n\n\n# 최대값으로 나누면 Min Max 스케일링이 됩니다.\nx = x / 16\n\n\n\n\n- 주성분 2개로 차원을 축소하고 시각화\n\nn = 2\npca = PCA(n_components = n)\n\nx_pca = pca.fit_transform(x)\nx_pca = pd.DataFrame(x_pca, columns = ['PC1', 'PC2'])\n\n\n# 시각화\nplt.figure(figsize=(12, 4))\nsns.scatterplot(x = 'PC1', y = 'PC2', data = x_pca, hue = y)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n# 2차원으로 축소하기\ntsne = TSNE(n_components = 2, random_state=20)\nx_tsne = tsne.fit_transform(x)\n\n# 사용의 편리함을 위해 DataFrame으로` 변환\nx_tsne = pd.DataFrame(x_tsne, columns = ['T1','T2'])\n\n\n# 시각화\nplt.figure(figsize=(12, 4))\nsns.scatterplot(x = 'T1', y = 'T2', data = x_tsne, hue = y)\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-18-06. 머신러닝 (5).html#import",
    "href": "posts/DX/04. 머신러닝/2023-09-18-06. 머신러닝 (5).html#import",
    "title": "06. 머신러닝 (5)",
    "section": "",
    "text": "# 기본 라이브러리 가져오기\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import *\n\nfrom sklearn.datasets import load_breast_cancer, load_digits\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-18-06. 머신러닝 (5).html#데이터-준비",
    "href": "posts/DX/04. 머신러닝/2023-09-18-06. 머신러닝 (5).html#데이터-준비",
    "title": "06. 머신러닝 (5)",
    "section": "",
    "text": "# breast_cancer 데이터 로딩\ncancer=load_breast_cancer()\nx = cancer.data\ny = cancer.target\n\nx = pd.DataFrame(x, columns=cancer.feature_names)\n\n\nx.head()\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 30 columns\n\n\n\n\n#x.info()\n\n\n#x.describe().T\n\n\n\n\n- 거리계산 기반 차원축소이므로 스케일링이 필요\n\n\n\n\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size = .3, random_state = 20)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-18-06. 머신러닝 (5).html#차원-축소-주성분-분석-pca",
    "href": "posts/DX/04. 머신러닝/2023-09-18-06. 머신러닝 (5).html#차원-축소-주성분-분석-pca",
    "title": "06. 머신러닝 (5)",
    "section": "",
    "text": "from sklearn.decomposition import PCA\n\n\n# feature 수\nx_train.shape[1]\n\n30\n\n\n\n# 주성분을 몇개로 할지 결정(최대값 : 전체 feature 수)\nn = x_train.shape[1] ## 일단 원래 feature의 수만 지정\n\n# 주성분 분석 선언\npca = PCA(n_components=n)\n\n# 만들고, 적용하기\nx_train_pc = pca.fit_transform(x_train)\nx_val_pc = pca.transform(x_val)\n\n- 편리한 사용을 위해 데이터프레임으로 변환\n\n# 칼럼이름 생성\ncolumn_names = [ 'PC'+str(i+1) for i in range(n) ]\ncolumn_names[:5]\n\n['PC1', 'PC2', 'PC3', 'PC4', 'PC5']\n\n\n\n# 데이터프레임으로 변환하기\nx_train_pc = pd.DataFrame(x_train_pc, columns = column_names)\nx_val_pc = pd.DataFrame(x_val_pc, columns = column_names)\nx_train_pc.head()\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\nPC9\nPC10\n...\nPC21\nPC22\nPC23\nPC24\nPC25\nPC26\nPC27\nPC28\nPC29\nPC30\n\n\n\n\n0\n-34.932206\n47.067779\n-17.756611\n-7.596006\n1.342721\n-1.956710\n1.710276\n0.641097\n0.120917\n0.139938\n...\n0.000962\n0.010435\n-0.003107\n-0.002029\n0.003974\n-0.000625\n-0.000576\n-0.000446\n0.000679\n0.001145\n\n\n1\n830.092826\n201.450565\n-6.166167\n-2.578986\n-1.794260\n2.619614\n0.284543\n0.366729\n-0.027226\n0.018051\n...\n-0.019489\n0.001261\n0.003305\n-0.011163\n0.005675\n0.001333\n0.001847\n-0.003122\n0.001580\n0.000655\n\n\n2\n-546.384045\n22.139574\n17.780018\n0.887465\n-0.308381\n-0.126655\n-1.450590\n-0.632612\n-0.009378\n-0.606726\n...\n0.002701\n-0.014928\n0.001773\n0.001490\n0.007249\n-0.000604\n-0.001318\n-0.001299\n-0.000982\n-0.000077\n\n\n3\n653.345270\n25.739603\n30.832277\n-7.437820\n2.139092\n3.686376\n-0.111756\n0.191535\n-0.181315\n0.111042\n...\n0.002200\n0.021932\n-0.012297\n0.002943\n0.002366\n-0.005320\n0.000484\n0.000967\n-0.002898\n-0.000765\n\n\n4\n-422.567968\n19.784995\n6.432610\n-10.383670\n-14.675624\n0.098355\n-0.537767\n-0.113981\n0.062963\n0.442519\n...\n0.003790\n-0.012366\n0.004250\n0.005447\n-0.002826\n-0.000611\n-0.000136\n0.000270\n-0.000225\n0.000407\n\n\n\n\n5 rows × 30 columns\n\n\n\n\n\n\n- 주성분을 1,2,3개로 선언하고, x_train을 이용해서 주성분을 추출\n\n# 주성분을 몇개로 할지 결정(최대값 : 전체 feature 수)\nn = [1,2,3]## 일단 원래 feature의 수만 지정\n\n# 주성분 분석 선언\n\nfor i in n : \n     exec(f\"pca{i} = PCA(n_components={i})\")\n     exec(f\"x_train_pc{i} = pca{i}.fit_transform(x_train)\")\n\n\n\n\n\n그래프를 보고 적절한 주성분의 개수를 지정(elbow method!)\nx축 : PC 수\ny축 : 전체 분산크기 - 누적 분산크기\n\n\npca\n\nPCA(n_components=30)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA(n_components=30)\n\n\n\nplt.plot(pca.explained_variance_ratio_, marker = '.')\nplt.xlabel('No. of PC')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n- 주성분 중 상위 2개를 뽑아 시각화\n\nsns.scatterplot(x = 'PC1', y = 'PC2', data = x_train_pc, hue = y_train)\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-18-06. 머신러닝 (5).html#지도학습으로-연계",
    "href": "posts/DX/04. 머신러닝/2023-09-18-06. 머신러닝 (5).html#지도학습으로-연계",
    "title": "06. 머신러닝 (5)",
    "section": "",
    "text": "model0 = KNeighborsClassifier()\nmodel0.fit(x_train, y_train)\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier()\n\n\n\n# 원본데이터 모델의 성능\npred0 = model0.predict(x_val)\n\nprint(confusion_matrix(y_val, pred0))\nprint(accuracy_score(y_val, pred0))\nprint(classification_report(y_val, pred0))\n\n[[ 57   7]\n [  5 102]]\n0.9298245614035088\n              precision    recall  f1-score   support\n\n           0       0.92      0.89      0.90        64\n           1       0.94      0.95      0.94       107\n\n    accuracy                           0.93       171\n   macro avg       0.93      0.92      0.92       171\nweighted avg       0.93      0.93      0.93       171\n\n\n\n\n\n\n\nn = 1 \nx_train_pc_n = x_train_pc[column_names[:n]] \nx_val_pc_n = x_val_pc[column_names[:n]] \n# 뽑은 데이터로 모델링 \nmodel1 = KNeighborsClassifier() \nmodel1.fit(x_train_pc_n, y_train) # 예측 평가 \npred1 = model1.predict(x_val_pc_n) \nprint(accuracy_score(y_val, pred1))\n\n0.8830409356725146\n\n\n\n\n\n\nn = 2 \nx_train_pc_n = x_train_pc[column_names[:n]] \nx_val_pc_n = x_val_pc[column_names[:n]] \n# 뽑은 데이터로 모델링 \nmodel1 = KNeighborsClassifier() \nmodel1.fit(x_train_pc_n, y_train) # 예측 평가 \npred1 = model1.predict(x_val_pc_n) \nprint(accuracy_score(y_val, pred1))\n\n0.9181286549707602\n\n\n\n\n\n\nn = 3\nx_train_pc_n = x_train_pc[column_names[:n]] \nx_val_pc_n = x_val_pc[column_names[:n]] \n# 뽑은 데이터로 모델링 \nmodel1 = KNeighborsClassifier() \nmodel1.fit(x_train_pc_n, y_train) # 예측 평가 \npred1 = model1.predict(x_val_pc_n) \nprint(accuracy_score(y_val, pred1))\n\n0.9181286549707602"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-18-06. 머신러닝 (5).html#차원축소-t-sne",
    "href": "posts/DX/04. 머신러닝/2023-09-18-06. 머신러닝 (5).html#차원축소-t-sne",
    "title": "06. 머신러닝 (5)",
    "section": "",
    "text": "1. 원본데이터의 유사도(거리) 맵을 만듬\n2 원본에서의 유사도가 축소한 차원에서도 유지 되었으면 좋겠음\n3 주성분 분석은 선형 축소, t-SNE는 비선형 축소이다.\n\n\n\nfrom sklearn.manifold import TSNE\n\n\n# 2차원으로 축소하기\ntsne = TSNE(n_components = 2, random_state=20)\nx_tsne = tsne.fit_transform(x)\n\n# 사용의 편리함을 위해 DataFrame으로` 변환\nx_tsne = pd.DataFrame(x_tsne, columns = ['T1','T2'])\n\n\nx_tsne.shape\n\n(569, 2)\n\n\n- 시각화\n\nplt.figure(figsize=(6,6))\nsns.scatterplot(x = 'T1', y = 'T2', data = x_tsne, hue = y)\nplt.grid()\n\n\n\n\n\n\n\n\n\n\ndigits = load_digits()\nx = digits.data\ny = digits.target\n\ny = pd.Categorical(y)\n\n\nx.shape\n\n(1797, 64)\n\n\n- 데이터 살펴보기\n\nprint(x[0].reshape(8,8))\n\n[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n [ 0.  0. 13. 15. 10. 15.  5.  0.]\n [ 0.  3. 15.  2.  0. 11.  8.  0.]\n [ 0.  4. 12.  0.  0.  8.  8.  0.]\n [ 0.  5.  8.  0.  0.  9.  8.  0.]\n [ 0.  4. 11.  0.  1. 12.  7.  0.]\n [ 0.  2. 14.  5. 10. 12.  0.  0.]\n [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n\n\n\n# f, axes = plt.subplots(5, 2, sharey=True, figsize=(16,6))\nplt.figure(figsize=(10, 4))\nfor i in range(10):\n    plt.subplot(2, 5, i + 1)\n    plt.imshow(x[i,:].reshape([8,8]), cmap='gray');\n\n\n\n\n\n스케일링\n\n\n# 최대, 최소값\nnp.min(x), np.max(x)\n\n(0.0, 16.0)\n\n\n\n# 최대값으로 나누면 Min Max 스케일링이 됩니다.\nx = x / 16\n\n\n\n\n- 주성분 2개로 차원을 축소하고 시각화\n\nn = 2\npca = PCA(n_components = n)\n\nx_pca = pca.fit_transform(x)\nx_pca = pd.DataFrame(x_pca, columns = ['PC1', 'PC2'])\n\n\n# 시각화\nplt.figure(figsize=(12, 4))\nsns.scatterplot(x = 'PC1', y = 'PC2', data = x_pca, hue = y)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n# 2차원으로 축소하기\ntsne = TSNE(n_components = 2, random_state=20)\nx_tsne = tsne.fit_transform(x)\n\n# 사용의 편리함을 위해 DataFrame으로` 변환\nx_tsne = pd.DataFrame(x_tsne, columns = ['T1','T2'])\n\n\n# 시각화\nplt.figure(figsize=(12, 4))\nsns.scatterplot(x = 'T1', y = 'T2', data = x_tsne, hue = y)\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-18-06. 머신러닝 (5).html#k-means",
    "href": "posts/DX/04. 머신러닝/2023-09-18-06. 머신러닝 (5).html#k-means",
    "title": "06. 머신러닝 (5)",
    "section": "1. k-means",
    "text": "1. k-means\n\n주어진 데이터를 k개의 클러스터로 묶는 알고리즘, 각 클러스터와 거리 차이의 분산을 최소화 하는 방식으로 동작\n허나 군집의 수, 가중치와 거리 정의가 어려우며, 사전에 주어진 목적이 없으므로 결과 해석이 어려움.\n또한 잡음이나 이상값의 영향을 받으며 초기 군집 수를 결정해야 한다는 단점이 있다.\n\n\nimport\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# 샘플데이터 로딩 함수\nfrom sklearn.datasets import make_blobs, make_moons\n\n# 클러스터링을 위한 함수\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.cluster import KMeans, DBSCAN\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n\n\ndata\n\nx, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\nx = pd.DataFrame(x, columns = ['x1', 'x2'])\ny = pd.Series(y, name = 'shape')\n\nplt.figure(figsize = (12,4))\nplt.scatter(x['x1'], x['x2'])\nplt.show()\n\n\n\n\n\n\n모델링\n\n# k means 학습\nmodel = KMeans(n_clusters= 2, n_init = 'auto')\nmodel.fit(x)\n\n# 예측\npred = model.predict(x)\nprint(pred)\n\n[0 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1\n 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0\n 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0\n 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0\n 1 1 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0\n 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0\n 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0\n 0 0 1 0]\n\n\n\n# feature + pred + y 붙여 놓고 비교해 봅시다.\npred = pd.DataFrame(pred, columns = ['predicted'])\nresult = pd.concat([x, pred, y], axis = 1)\nresult.head()\n\n\n\n\n\n\n\n\nx1\nx2\npredicted\nshape\n\n\n\n\n0\n0.836857\n2.136359\n0\n1\n\n\n1\n-1.413658\n7.409623\n1\n3\n\n\n2\n1.155213\n5.099619\n0\n0\n\n\n3\n-1.018616\n7.814915\n1\n3\n\n\n4\n1.271351\n1.892542\n0\n1\n\n\n\n\n\n\n\n\n\n결과 시각화\n\n# k means 모델로 부터 클러스터의 평균 값들을 가져올 수 있습니다.\ncenters = pd.DataFrame(model.cluster_centers_, columns=['x1','x2'])\ncenters\n\n\n\n\n\n\n\n\nx1\nx2\n\n\n\n\n0\n0.452332\n2.681056\n\n\n1\n-1.334654\n7.694427\n\n\n\n\n\n\n\n\nplt.figure(figsize = (8,6))\nplt.scatter(result['x1'], result['x2'], c = result['predicted'], alpha=0.5)\nplt.scatter(centers['x1'], centers['x2'], s=50, marker='D', c='r')\nplt.show()\n\n\n\n\n\n\nk 값에 따라 모델을 생성하고 스래프 그리기\n\ndef k_means_plot(x, y, k) :\n    # 모델 생성\n    model = KMeans(n_clusters= k, n_init = 'auto')\n    model.fit(x)\n    pred = model.predict(x)\n\n    # 군집 결과와 원본 데이터 합치기(concat)\n    pred = pd.DataFrame(pred, columns = ['predicted'])\n    result = pd.concat([x, pred, y], axis = 1)\n\n    # 중앙(평균) 값 뽑기\n    centers = pd.DataFrame(model.cluster_centers_, columns=['x1','x2'])\n\n    # 그래프 그리기\n    plt.figure(figsize = (12,4))\n    plt.scatter(result['x1'],result['x2'],c=result['predicted'],alpha=0.5)\n    plt.scatter(centers['x1'], centers['x2'], s=50,marker='D',c='r')\n    plt.grid()\n    plt.show()\n\n\n\n적절한 k찾기\n- Inertia value : 군집화가 된 후에, 각 중심점에서 군집의 데이터 간의 거리를 합산한 값\n\n# k의 갯수에 따라 각 점과의 거리를 계산하여 적정한 k를 찾아 봅시다.\nkvalues = range(1, 10)\ninertias = []\n\nfor k in kvalues:\n    model = KMeans(n_clusters=k, n_init = 'auto')\n    model.fit(x)\n    inertias.append(model.inertia_)\n\n\n# Plot k vs inertias\nplt.figure(figsize = (12, 4))\nplt.plot(kvalues, inertias, '-o')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.grid()\nplt.show()\n\n\n\n\n- 난 4개의 군집이 좀 적절한 것 같음.\n\n\nkmeans의 한계\n\nx, y = make_moons(n_samples = 300, noise = .08, random_state=2)\nx = pd.DataFrame(x, columns = ['x1', 'x2'])\ny = pd.Series(y, name = 'shape')\n\nplt.figure(figsize = (12,4))\nplt.scatter(x['x1'], x['x2'])\nplt.show()\n\n\n\n\n\n# k의 갯수에 따라 각 점과의 거리를 계산하여 적정한 k를 찾아 봅시다.\nkvalues = range(1, 15)\ninertias = []\n\nfor k in kvalues:\n    model = KMeans(n_clusters=k, n_init = 'auto')\n    model.fit(x)\n    inertias.append(model.inertia_)\n\n\n# Plot k vs inertias\nplt.figure(figsize = (12,4))\nplt.plot(kvalues, inertias, '-o')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.grid()\nplt.show()\n\n\n\n\n\n# 적절한 k값으로 모델을 생성해 봅시다.\nplt.figure(figsize = (12,4))\nk_means_plot(x, y, k = 6)\n\n&lt;Figure size 1200x400 with 0 Axes&gt;\n\n\n\n\n\n\n우리가 기대하는 모델이 생성되지는 앟는다.\nKmeans는 블록한(convex) 덩어리 cluster 구분에서는 괜찮지만, 뭐야 근데 위에 잘 분류된거 아녀? 여튼 저런식에서는 잘 분류가 안된다는 것을 말하고 싶었음"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-18-06. 머신러닝 (5).html#dbscan",
    "href": "posts/DX/04. 머신러닝/2023-09-18-06. 머신러닝 (5).html#dbscan",
    "title": "06. 머신러닝 (5)",
    "section": "2. DBSCAN",
    "text": "2. DBSCAN\n- DBSCAN모델은 위 kmeans의 한계점을 보완한 모델이다.\n\n임의의 한 점에서 시작\n반경 범위내에 최소 포인트 수가 존재하는지 확인\n존재한다면 각 포인트들을 중심으로 다시 원을 그어 최소 포인트 수를 확인\n2~3을 반복 수행\n존재하지 않으면, 군집에 포함되지 않은 점으로 이동하여 1~4 반복 수행\n어느 군집에도 포함되지 않은점을 이상치로 간주\n\n\nDBSCAN의 주 매개변수\n\nmin_samples : 핵심 포인트를 중심점으로 간주하는 주변 지역의 표본 수\neps : 핵심 포인트를 중심으로 측정되는 유클리디언 거리값\n\n\n\nx, y = make_blobs(n_samples=300, centers=5, cluster_std=1.8, random_state=20)\nx = pd.DataFrame(x, columns = ['x1', 'x2'])\ny = pd.Series(y, name = 'shape')\n\nplt.figure(figsize = (8,6))\nplt.scatter(x['x1'], x['x2'])\nplt.show()\n\n\n\n\n\n# DBSCAN 모델을 만들어 봅시다.\nmodel = DBSCAN(eps=0.1, min_samples=3)\nmodel.fit(x)\n\nDBSCAN(eps=0.1, min_samples=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DBSCANDBSCAN(eps=0.1, min_samples=3)\n\n\n\n# fitting한 후에 모델의 labels_ 값이 찾아낸 군집 종류입니다.\nclusters = model.labels_\n\n\n# 군집 번호 중 -1은 이상치를 의미합니다.(어느 군집에도 포함 안되는 값들!)\nclusters\n\narray([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=int64)\n\n\n\nplt.figure(figsize = (12, 4))\nplt.scatter(x['x1'], x['x2'], c=clusters, alpha=0.5)\nplt.show()\n\n\n\n\n- eps를 적용하며 모델을 생성하고 그래프 그리기\n\ndef dbscan_plot(x, y, eps) :\n    model = DBSCAN(eps=eps, min_samples=3)\n    model.fit(x)\n    clusters = model.labels_\n    plt.figure(figsize = (12,4))\n    plt.scatter(x['x1'], x['x2'], c=clusters, alpha=0.5)\n    plt.grid()\n    plt.show()\n\n\ndbscan_plot(x,y,1.)\n\n\n\n\n\n적절한 eps 찾기\n\n역시 눈으로 보면서 찾는 것은 실전에서는 거의 불가능\nDBSCAN에도 적절한 값을 찾는 방법\n\n모든 점과 가까운 n개와 평균 거리 계산\n평균 거리순으로 정렬해서, 그래프 그리기\n평균 거리가 급격히 커지기 시작하는 지점 찾기(elbow point)\n\nDBSCAN의 주 매개변수\nmin_samples : 핵심 포인트를 중심점으로 간주하는 주변 지역의 표본 수\neps : 핵심 포인트를 중심으로 측정되는 유클리디언 거리값\n\n\n# 각점과 근처 3개 점과의 평균 거리\n# NearestNeighbors은 거리계산할 때, 자기 자신을 포함하므로 n+1\nn = 3\nknnDist = NearestNeighbors(n_neighbors = n+1).fit(x)\ndistances, _ = knnDist.kneighbors(x)\n\n- 열의 수가 4이다. \\(\\to\\) 0은 자기 자신을 의미하고 나머지 3개의 열은 가장 가까운 3개의 점과의 거리를 의미한다.\n\ndistances.shape\n\n(300, 4)\n\n\n\ndistances[:5]\n\narray([[0.        , 0.49231025, 0.55853535, 0.63526856],\n       [0.        , 0.47383177, 1.47765296, 1.67802678],\n       [0.        , 0.40879685, 0.43381656, 0.6280564 ],\n       [0.        , 0.63087629, 0.72695518, 0.73171997],\n       [0.        , 0.35638603, 0.3886451 , 0.53734417]])\n\n\n- 평균 거리 계산\n\ndist = np.mean(distances[:,1:],axis=1)\ndist = np.sqrt(dist)\n\n- 급격ㅎ; 갈;기 증가하기 시작하는 구간을 찾아 eps값 적용\n\nplt.plot(np.sort(dist))\n\n\n\n\n- 여기서 보니 1.0때가 거리가 급격히 증가하는 것 같다.\n\ndbscan_plot(x, y, eps = 1.0 )"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-18-06. 머신러닝 (5).html#실습-1",
    "href": "posts/DX/04. 머신러닝/2023-09-18-06. 머신러닝 (5).html#실습-1",
    "title": "06. 머신러닝 (5)",
    "section": "3. 실습",
    "text": "3. 실습\n\n(1) 데이터 로드\n\npath = 'https://raw.githubusercontent.com/DA4BAM/dataset/master/customer_segmentation.csv'\ndata = pd.read_csv(path)\ndata.head()\n\n\n\n\n\n\n\n\nCustID\nGender\nAge\nIncome\nScore\n\n\n\n\n0\n1\nMale\n19\n15\n39\n\n\n1\n2\nMale\n21\n15\n81\n\n\n2\n3\nFemale\n20\n16\n6\n\n\n3\n4\nFemale\n23\n16\n77\n\n\n4\n5\nFemale\n31\n17\n40\n\n\n\n\n\n\n\n\n\n(2) 데이터 전처리\n\n# 군집화는 아래 변수들만 사용합니다.\nx = data.loc[:, ['Age', 'Income', 'Score']]\n\n\nx.head()\n\n\n\n\n\n\n\n\nAge\nIncome\nScore\n\n\n\n\n0\n19\n15\n39\n\n\n1\n21\n15\n81\n\n\n2\n20\n16\n6\n\n\n3\n23\n16\n77\n\n\n4\n31\n17\n40\n\n\n\n\n\n\n\n\n\n(3) 스케일링\n\nscaler = MinMaxScaler()\nx_s = scaler.fit_transform(x)\n\n\n\n(4) 클러스터링\n\n# k의 갯수에 따라 각 점과의 거리를 계산하여 적정한 k를 찾아 봅시다.\nkvalues = range(1, 10)\ninertias = []\n\nfor k in kvalues:\n    model = KMeans(n_clusters=k, n_init = 'auto')\n    model.fit(x)\n    inertias.append(model.inertia_)\n\n- 적절한 군집의 개수는 5개로 추정된다.\n\nplt.plot(kvalues, inertias)\n\n\n\n\n\n# k means 학습\nmodel = KMeans(n_clusters= 5, n_init = 'auto')\nmodel.fit(x)\n\n# 예측\npred = model.predict(x)\nprint(pred)\n\n[1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1\n 4 1 4 1 4 1 4 1 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 3 3 4 3 3 3 3 3\n 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n 3 3 3 3 3 3 3 3 3 3 3 3 0 2 0 3 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 3 0 2 0 2 0\n 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2\n 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0]\n\n\n\n\n(5) 데이터 군집 결과 정리\n\ndata[\"pred\"] = pred\n\n\ndata.head()\n\n\n\n\n\n\n\n\nCustID\nGender\nAge\nIncome\nScore\npred\n\n\n\n\n0\n1\nMale\n19\n15\n39\n1\n\n\n1\n2\nMale\n21\n15\n81\n4\n\n\n2\n3\nFemale\n20\n16\n6\n1\n\n\n3\n4\nFemale\n23\n16\n77\n4\n\n\n4\n5\nFemale\n31\n17\n40\n1\n\n\n\n\n\n\n\n\n\n(6) 군집별 변수 비교\n\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n1 Age\n\n\nCode\nfig = data.plot(x=\"Age\", kind =\"hist\",\n                 backend = \"plotly\",color=\"pred\",nbins=50,\n                facet_col = \"pred\",\n                facet_col_wrap = 3,title = \"Age\")\n\nfig.update_layout(\n    title={\n        'text': \"distribution of Age\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nfig.show()\n\n\n\n                                                \n\n\n2 Income\n\n\nCode\nfig = data.plot(x=\"Income\", kind =\"hist\",\n                 backend = \"plotly\",color=\"pred\",nbins=50,\n                facet_col = \"pred\",\n                facet_col_wrap = 3,title = \"Age\")\n\nfig.update_layout(\n    title={\n        'text': \"distribution of Income\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nfig = data.plot(x=\"Score\", kind =\"hist\",\n                 backend = \"plotly\",color=\"pred\",nbins=50,\n                facet_col = \"pred\",\n                facet_col_wrap = 3,title = \"Age\")\n\nfig.update_layout(\n    title={\n        'text': \"distribution of Score\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nfig.show()"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-28-08. summary (1).html",
    "href": "posts/DX/04. 머신러닝/2023-09-28-08. summary (1).html",
    "title": "08. summary (1)",
    "section": "",
    "text": "- knn의 경우 필요하다면 스케일링 단계가 필요\n- 이산형 변수, 즉 범주형 변수를 모델의 예측변수로 사용할 경우 더미변수로 변환해주어야한다.\npd.get_dummies(data, columns = 더미화할컬럼리스트, dtype = (int or float))\n- 결측치 처리 : 히스토그램, boxplot, 시계열 데이터인 경우 등등을 고려하여 각 case에 맞게 적절히 결측치를 처리해준다.\n\nmisforest, EM 알고리즘을 통한 결측치 처리를 한다지만, 개인적인 생각으로는 좀 과한 결측치 처리가 아닌지 싶음\n이유는 즉슨, 결측치를 처리하기위해 결측치 처리 단계에서 모델링을 한번 더 수행하는데 이 때 시간이 생각보다 오래 걸림\n\n\n\n\n- 아래와 같이 여러개의 모델을 생성한다음 cross-validation 통해 최적의 모델을 선택하였다.\n\nexample\n\n\n# 1. knn\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nknn = KNeighborsClassifier(n_neighbors=5)\n\nknn.cv = cross_val_score(knn, x_train_s, y_train, cv = 5)\n\nknn.cv_m = knn.cv.mean()\n\n# 2. tree\nfrom sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(max_depth = 5, random_state = 1)\n\ntree.cv = cross_val_score(tree, x_train, y_train, cv = 5)\n\ntree.cv_m = tree.cv.mean()\n\n# 3. logistic\nfrom sklearn.linear_model import LogisticRegression\nlogit = LogisticRegression()\nlogit.cv = cross_val_score(logit, x_train, y_train, cv = 5)\n\nlogit.cv_m = logit.cv.mean()\n\n# 4. RF\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(max_depth = 5,  random_state = 1)\n\nrf.cv = cross_val_score(rf, x_train,y_train)\n\nrf.cv_m = rf.cv.mean()\n\n# 5. XGBoost\nfrom xgboost import XGBClassifier\nxgb =  XGBClassifier(max_depth = 5, random_state = 1)\n\nxgb.cv = cross_val_score(xgb, x_train, y_train, cv = 5)\n\nxgb.cv_m  = xgb.cv.mean()\n\nfrom lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(max_depth = 5, random_state = 1,verbose = -100) \n\nlgbm.cv = cross_val_score(lgbm, x_train, y_train, cv = 5)\n\nlgbm.cv_m = lgbm.cv.mean()\n\n\n\n- 그 후 선택한 최종 모델을 튜닝해 최종 모델을 select\nfrom sklearn.model_selection import GridSearchCV\nmodel = XGBClassifier(max_depth= 5, random_state = 1)\n\nparams=  {\"max_depth\" : range(1,21)}\n\nmodel = GridSearchCV(model,\n                     params,\n                     cv=5,\n                     scoring='r2')\n\n\n\n\n\n\n\n- 아래 링크를 참조해서 까먹을때 마다 보자~~\n- ISLP2023-00.Linear Regression\n\n\n\n- 이것두 아래 링크를 참조하자.\n- ISLP2023-01. Classification\n\n\n\n- 학습을 안함, 그냥 그 근처 K개의 녀석들을 보고 값을 할당\n\\[P(y= j | X = x_0) = \\frac {1}{K}  \\sum_{i\\in N_0} I(y_i = j)\\]\n\\[N_0  : x_0\\text {와 가장 가까운  K개의 자료의 집합}\\]\n- \\(k\\)가 작을수록 모델은 복잡해지고, 클수록 단순해짐\n\n솔직히 와닿지 않지만, 내 방식대로 이해해보자.\n이전에 선형회귀분석에서 모델 복잡도를 생각해보자, 모델을 일직선으로 예측한 경우 단순선형회귀분석이다.\n즉, 모델 하나하나의 포인트를 고려하지 않고 전체 평균적인 선형회귀식을 하나 구한 것이다.\n이를 다시 KNN예제로 생각해 \\(K\\)가 클경우 생각해보변, 주변 녀석들의 하나하나 개인적은 특성을 고려하기보단 전체적인 특성에 기반하여 주어진\\(x_0\\)에 대한 \\(y\\)를 예측하는 것이다.\n따라서, \\(k\\)가 작을수록 주변 녀석들의 특징을 하나하나 잘 고려해서 모델이 복잡한 것이고, \\(k\\)가 크면 전체적인 평균을 고려한 것이기 때문에 모델이 단순해진다… \\(\\to\\) 사실 이것도 그렇게 와닿지 않음 나중에 더 찾아보자…\n\n\n\n\n\n나무모형은 간단하고 해석상에 장점이 있으나 다른 방법들에 비해 좋은 성능을 보이지 못하는 경우가 있음\n\n\n\n1 설명변수들의 가능한 조합을 이용하여 예측공간을 \\(J\\)개의 겹치지 않는(non-overlaping)구역으로 분할\n2 각 관측값은 \\(R_j\\) 구역에 포함되며, \\(R_j\\) 구역에 포함된 training data의 반응변수 (\\(y\\))의 평균 (분류문제에선 voting방식)을 이용하여 예측\n\\[\\hat {y}_{R_j} = \\frac {1}{n_j} \\sum_{k\\in R_J} y_k \\]\n3 목표 : 다음의 RSS를 최소화 하는 구역 \\(R_1,R_2, R_J\\)를 찾는 것\n\\[RSS = \\sum_{j=1}^{J} \\sum_{i \\in R_j} (y_i - \\hat {y}_{R_{j}})^2\\]\n4 모든 조합을 확인하는 것은 불가능…(사실 가능하다. tree를 무한정 쪼개면)\n\n근데 tree를 무한정 쪼갤경우 과적합문제가 무조건 발생\n\n5 정지규칙 (stopping rule)\n\n모든 자료가 한 범주에 속할 때\n노드에 속하는 자료가 일정 수 이하일 떄\nMSE의 감소량이 아주 작을 떄\n뿌리마디로부터의 깊이가 일정 수 이상일 떄 등 (max_depth)\n\n6 가지치기 : 과적합을 막기위한 방법\n\n사실 정지규칙도 이에 포함됨, 따라서 위에거 + 빠진 내용을 적겠음\nmin_samples_leaf(default = 1) : leaf노드가 되기 위한 최소한의 샘플 수\nmin_samples_split(default = 20 : 노드를 분할하기 위한 최소한의 샘플 수 (값을 적게 설정할수록 계순 분할되어, 과적합 발생 위험 증가)\nmax_feature : 최선의 분할을 위해 고려할 변수(feature) 개수\n\nsqrt : 전체 변수 개수의 루트\nauto : sqrt와 같은 의미\nlog : \\(\\log_{2}\\) (전체 변수의 수)\n\nmax_leaf_node : 리프 노드의 최대 개수 \\(\\to\\) \\(\\text{Cost complexity Pruning}\\)\n\n\\(|T|\\)는 터미널도드로 리프노드의 개수를 뜻한다.\n\\(R_{\\alpha}(T)\\)는 변하지 않는 비용함수로 \\(R(T)\\)는 우리가 알고 있는 \\(RSS\\)와 같다.\n아래식이 뜻하는 바는 리프노드의 개수가 클수록 \\(R(T)\\) 훈련 데이터 셋에대한 \\(RSS\\)가 작아져 과적합 문제가 발생할 수 있기 때문에 적절한 리프노드의 개수를 설정해야한다는 의미이다.\n\\(\\alpha\\)는 \\(\\text {tuning parameter}\\)로 복잡도를 조절한다. 만약 \\(\\alpha\\)가 0이라면 기존의 비용함수와 같고 1에 가까워질수록 \\(R(T)\\)값이 작아진다.\n따라서 우리는 적절한 \\(\\alpha\\)값과 \\(|T|\\)값을 교차검증 기법을 통해 찾아내어 가지치기를 수행하여야한다.\n\n\n\\[\\begin {align}R_{\\alpha}(T) &= \\sum R(T) + \\alpha |T|  \\\\ \\\\\n                                                          &= \\sum_{m=1}^{|T|} \\sum_{x_i \\in R_m} (y_i - \\hat {y}_{R_m})^2 +  \\alpha |T|  \\end {align}\\]\n7 비용함수\n\n지니 지수 (Gini Index)\n\n\\[Gini (D) = 1- \\sum_{k=1}^{K}p_{k}^2 = \\sum_{k=1}^{K} p_k(1-p_k)\\]\n\\[p_k : \\text{Node D에서 k번째 범주에 속하는 관측 비율}\\]\n\n\n\n\n순수하게 분류되면 값은 0이다.\n만약 분리규칙 \\(A\\)에 의해서 Node D가 \\(D_1, D_2\\)로 분리된다면, 분리규칙 \\(A\\)에서 Ginin지수는 다음과 같다.\n\n\\[Gini_A(D) =\\frac {|D_1|}{|D|}Gini(D_1) +\\frac {|D_2|}{|D|}Gini(D_2) \\]\n\n위에 근거하여 분리규칙 A에서 발생한 불순도 감소량은 다음과 같이 정의할 수 있다.\n\n\\[\\Delta Gini(A) = Gini(D) - Gini_{A}(D)\\]\n\n따라서, \\(Gini_{A}(D)\\)를 가장 작게 하거나 \\(\\Delta Gini(A)\\)를 가장 크게 하는 분리 규칙을 선택!\n\n\n\n엔트로피(Entropy)\n\n\\[\\text {Entropy}  = -\\sum_{i=1}^m p_i\\log_{2} p_i\\]\n\n순수하게 분류되면 0\n\n\n정보 이득\n\n\n엔트로피와 지니지수는 단지 속성의 불순도를 표현한다.\n우리가 알고 싶은 것은 “어떠한 속성이 얼마나 많은 정보를 제공하는가!” 이다.\n\n\\[\\text {Gain}(T,X)= \\text{Entropy}(T)-\\text{Entropy}(T,X)\\]\n\n위 식을 살펴보니 지니지수에서 했던 불순도 감소량과 비슷하지 않은가?\n\n\\[\\Delta Gini(A) = Gini(D) - Gini_{A}(D)\\]\n\n\n\n\n\n앞서 언급한 tree는 과대적합의 위험이 큰 모형임 \\(\\to\\) max_depth를 무작정 깊게 하면 과대적합이 발생하므로\n앙상블의 아이디어 : 이러한 test데이터 셋에 예측력이 약한 모델을 결합해서 성능이 좋은 모델을 만들자!\n\n\n\n- 여러 모델들의 예측결과를 투표를 통해 최종 예측결과를 결정\n\n하드 보팅 : 다수 모델이 예측한 값이 최종 결과값\n소프트 보팅 : 모든 모델이 예측한 레이블 값의 결정 확률 평균을 구한 뒤 가장 확률이 높은 값을 최종 선택\n\n\n\n\n- Boostrap Aggregating\n- 아이디어 : 모형의 분산을 줄여 과적합을 방지하자.\n\n만약, 모집단으로부터 여러개의 훈련자료를 얻을 수 있고 이로부터 여러개의 모형 \\(\\hat{f}_1(x)\\dots \\hat{f}_b(x)\\)를 얻을 수 있다면, 다음과 같이 분산을 줄일 수 있다.\n\n\\[\\hat{f}_{avg}(x) = \\frac {1}{B} \\sum_{i=1}^{B} \\hat{f}_b(x)\\]\n\n보통은 한 set의 자료만이 주어지게 되므로 위 방식은 직접 적용이 불가능\n그래서 우리는 복원추출을 기반으로 같은 size의 표본을 추출해 각각의 모델링을 수행한다. (Bootstrap sample)\n\n\\[X_1^{*}\\dots X_B^{*}\\]\n\\[\\hat{f}_{\\text{bag}}(x) = \\frac {1}{B} \\sum_{i=1}^{B} \\hat{f}^{*}_b(x)\\]\n\n보팅과 다른점은 보팅은 여러개의 예측모델, 배깅은 동일한 예측모델 여러개를 앙상블하는 것임!\n대표적인 모델 : Randoms Forest\n\n\n\n- 여러 tree모델이 전체 데이터에서 배깅 방식으로 각자의 데이터를 샘플링\n\n모델들이 개별적으로 학습을 수행한 뒤 모든 결과를 집계하여 최종 결과를 결정\n\n# 불러오기\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import *\n\n# 선언하기\nmodel = RandomForestClassifier(max_depth=5, random_state=1)\n\n# 학습하기\nmodel.fit(x_train, y_train)\n\n# 예측하기\ny_pred = model.predict(x_test)\n- Out-of-Bag은 생략 (ISLP 교재참고)\n- RF 모델의 변수 선택\n\n하나의 트리를 형성하는 과정에서, 각 노드에서 전체 \\(p\\)개의 설명변수 중 \\(m\\)개만을 임의로 추출하여 분리 규칙을 생성한다.\n\n일반적으로 \\(m \\approx \\sqrt {p}\\)\n\n\nRandomForestClassifier( max_features='sqrt') ## default\n- 변수 중요도 (Variable Importance measur)\n\n사실 여러 형태의 나무를 결합하여 산출된 모델은…. 해석이 거의 불가능해진다.\n대안적으로, 나무들을 생성할 떄 어떠한 변수들이 RSS 혹은 Gini index 등에 큰 감소를 가져왔는지를 요약한 값으로 변수의 중요도를 파악할 수 있음.\n\\(B\\)개의 모형에 대한 평균적인 기여 정도로 변수의 중요도를 평가하게 된다!\nscikit-learn 참고링크\n\n\n\n\n\n\n\n\n\n\n같은 유형의 약한 예측 모형을 결합하여 매우 정확한 예측 모형을 만드는 방법\n예측 모형을 순차적으로(Sequentially) 학습하여 먼저 학습된 모형의 결과가 다음 모형의 학습 시 정보를 제공\n즉, 이전 모형의 약점(잔차)를 학습하여 보완한다.\n\n\n\n\n\n배깅에 비해 성능이 좋지만, 속도가 느리고 과적합 발생 가능성이 있음.\n대표적인 부스팅 알고리즘 : XGBoost, LightGBM\n\n- Boosting의 원리 (ISLP 기준)\n\n초기값 셋팅 \\(\\hat{f}(x) = 0, r_1 = y_1\\)\nFor \\(b = 1, 2\\dots B , repaet\\) :\n\n\\[\\hat {f}(x)_{i+1} = \\hat {f}(x)_{i} + \\lambda \\hat {f}^{b}(x)\\]\n\nupdate the residual,\n\n\\[r_{i+1} = r_{i}- \\lambda \\hat{f}^{b} (x) \\]\n\n초기 셋팅된 \\(\\hat {f}_1 = 0\\) 이므로\n\n\\[\\hat {f}(x)_{\\text{final}} = \\sum_{i=1}^{B} \\lambda \\hat {f}^{b}(x)\\]\n- 위 같은 방식의 문제점 \\(\\to\\) 과적합발생… 당연하다. 예측 모형을 순차적으로 학습한다는 것은 모형간 자기 상관성이 존재하고 모형의 분산이 증가하기 때문에 과적합이 발생할 수 밖에 없다…\n- 이를 막기위해 나온 모델이 XGBoost!\n\n\n- Extreme Gradient Boost\n\nreview : 방금 정리했던 Boosting기법과 같이 기본 학습기를 의사결정나무로 하며, 잔차를 이용해 이전 모형의 약점을 보완하는 방식으로 학습한다.\n+\\(\\alpha\\) : 기존의 Graident Tree Boosting에 과적합 방지를 위한 파라미터\\((\\lambda, \\gamma)\\)가 추가된 알고리즘이다.\n\n\n\n0 parameter\n\n\\(L\\) : 손실함수\n\\(M\\) : 개별 모형의 최대 개수\n\\(l\\) : 학습률\n\\(\\gamma, \\lambda\\) : 과적합 방지를 위한 파라미터\n\n1. 초기 모형은 상수로 설정하며 다음과 같이 손실함수를 최소화 하는 모형으로 설정한다.\n\n초기 모형(상수값)을 아무렇게나 설정해도 된다고 하지만… 최적화 관점에서 아래처럼 잡아주는게 적절한 듯 하다.\n\n\\[F_{0}(x) = \\underset {c}{\\text{arg min}}  \\sum_{i=1}^{n} L(y_i,c)  \\]\n2 \\(m = 1,\\dots M\\)에 대하여 다음을 반복\n\n\nGradient \\(g_i\\)와 Hessian \\(h_i\\)를 계산\n\n\n\\[g_i = \\left[ \\frac {\\partial L(y_i, F(x_i))}{\\partial  F(x_i)}\\right],\\quad F(x_i) = \\hat {F}_{m-1}(x)\\]\n\\[h_i = \\left[ \\frac {\\partial^2 L(y_i, F(x_i))}{\\partial  F(x_i)^2}\\right],\\quad F(x_i) = \\hat {F}_{m-1}(x)\\]\n\n\n회귀나무 \\(\\phi_m\\)을 다음과 같이 적합\n\n\n\\[l = \\sum_{i=1}^{n} \\frac {1}{2}h_i \\left [ - \\frac {g_i}{h_i} - \\phi(x_i) \\right ]  + \\gamma T + \\frac {1}{2} \\lambda ||\\phi||^2\\]\n\\[\\phi_m = \\underset {\\phi} {\\text{arg min}} \\sum_{i=1}^{n} \\frac {1}{2}h_i \\left [ - \\frac {g_i}{h_i} - \\phi(x_i) \\right ]  + \\gamma T + \\frac {1}{2} \\lambda ||\\phi||^2\\]\n\n여기서 \\(T\\)는 \\(\\phi\\)의 끝마디 개수, \\(||\\phi||^2 = \\sum_{j=1}^{T} w_j^2\\) 이며 \\(w_j\\)는 \\(j\\)번째 끝마디에서의 출력값이다.\n\n잘 살펴보면 릿지회귀분석에 L2 penalty와 비슷한데, L1 penalty 방식도 지원하는 것 같다.\n\n\n다음과 같이 업데이트 한다.\n\n\n\\[ F_{m}(x) = F_{m-1}(x) + l\\cdot \\phi_m(x)\\]\n3 최종모형은?\n\\[F_M(x) =  \\sum_{m=0}^{M} F_m(x)\\]\n4 summary\n\nXGBoost는 기존 Gradient Boosting기법에 문제인 과적합문제를 해결하기 위해 \\(\\gamma, \\lambda\\) 파라미터를 사용한다.(규제)\n손실함수를 살펴보면 터미널 노드(끝마디 노드)의 수와 끝마디에서의 출력값에 대한 패널티 파라미터가 들어가있다,\n의사결정나무에 가지치기 과정에서 터미널 노드의 개수에 따라 panelty를 부여하는 방식을 생각해보면 비슷한 방식이다.\n또한, 내장된 교차 검증? (이거는 이론적으로 구현되어있다기보단 사이킷런에서 내부적으로 동작하게 만든것 같음)\n\n여튼 여기서 조기 중단을 가능하게끔 지원해준다.\n\n결측치 자체 처리 : 알아서 결측치를 고려해서 학습을 한다.(결측치 여부를 노드 분기를 위한 질문에 포함시킴)\n\n이것도 사이킷런에서 내부적으로 구현한듯\n그래도 명시적으로 결측치에 대한 처리를 진행하기를 권고…\n\n\n5 실습 코드\n# 불러오기\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error, r2_score\n\n# 선언하기\nmodel = XGBRegressor(max_depth=5, n_estimators=100, random_state=1)\n\n# 학습하기\nmodel.fit(x_train, y_train)\n\n# 예측하기\ny_pred = model.predict(x_test)\n\n# 평가하기\nprint(mean_absolute_error(y_test, y_pred))\nprint(r2_score(y_test, y_pred))\n6 주요 파라미터\n\nlearning_rate : 학습률(default = 0.1)\nn_estimators : 나무의 개수 (default = 100)\nmid_child_weight : 트리에서 추가적으로 분할할 지를 결정하기 위해 필요한 데이터들의 weight(\\(w_i\\))들의 총함 (default = 1)\ngamma : 트리에서 추가적으로 분할할지를 결정하기 위한 값 \\(\\gamma T\\) (default = 0)\nmax_depth : 나무의 깊이 (default = 6)\nsub_sample : weak learner가 학습에 사용되는 데이터 샘플링 비율\n\n과적합이 염려되는 경우 1보다 작은 값으로 설정, (default = 1)\n\ncolsample_bytree : 트리 생성에 필요한 변수 선택 비율\n\n변수가 많은 경우 과적합을 조절하기 위해서 사용, 기본값 = 1\n\nreg_lambda : L2규제 적용값 (\\(\\lambda \\sum_{j=1}^{T} w_j^2\\), 기본값 \\(\\lambda = 1\\))\nreg_alpha : L1규제 적용값 (\\(\\alpha \\sum_{j=1}^{T} |w|\\), 기본값 \\(\\alpha = 0\\))\nearly_stopping_rounds : n_estimators만큼 학습을 반복하지 않더라도 조기 종료 가능(default = 10, 10번 동안 성능 향상이 없으면 학습 중단.)\n\n\n\n\n\n\n\n- 기본 아이디어 : 두 클래스 사이에 가장 넓은 도로를 내는 것\n- 용어 정리\n\n결정 경계 (Decision Boundary) or 초평면\n\n클래스를 구분하는 경계선\n결정 경계가 바로 모델 (Hyper plane)이라고 부름\n\n벡터 : 모든 데이터 포인트\n서포트 벡터 : 결정경계와 가끼운 데이터 포인트\n\n마진의 크기와 결정경계에 영향을 미침\n\n마진(margin) : 서포트 벡터와 결정경계 사이의 거리\n\n폭이 가장 넓은 도로를 찾는 것이 SVM의 목표\n마진이 클수록 새로운 데이터에 대해 안정적인 분류가 가능해지는 것임\n\n잘 생각해보면 마진의 크기가 좁을수록 정확한 분류가 일어나나 과적합 문제가 발생하므로\n마진의 크기와 오류에 대한 허용 정도는 Trade-off 관계에 있는 것을 알 수 있다.\n이것을 조절하는 파라미터 \\(\\to\\) 비용(C)\n\n- 이를 이해하기 위해서!\n\nSupport vector classifier의 decision boundary는 다음과 같은 최적화 문제의 해로 정의된다.\n\n\n만약, label = {1,-1}이라 하면 초평면은 다음과 같은 성질을 가진다.\n\n\\[\\text {Hyper plane}=  \\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip} = 0\\]\n\\[\\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip} &gt; 0, \\quad \\text{if }  y_i=1\\]\n\\[\\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip} &lt; 0, \\quad \\text{if }  y_i=-1\\]\n\n이는 다음과 같이 간단히 표현할 수 있다.\n\n\\[y_i(\\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip}) &gt; 0\\]\n\n아래의 두 식은 관측치가 초평면을 중심으로 두 class를 정확히 구분되고, 관측치와 초평면사이의 직교거리가 최소 \\(M\\)이상이 되도록 보장해 주는 조건이다.\n\n\\[\\underset{\\beta_0,\\beta_1 \\dots \\beta_p, M}{\\text {maximize} M} \\]\n\\[ \\sum_{i=1}^{p} \\beta_j^2 =  1\\]\n\\[y_i(\\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip}) &gt; M\\]\n\n그러나 실제로 관측치는 두 개의 class로 정확히 구분되지 않은 경우가 더 많으며 한 두개의 관측치에 큰 영향을 받을 수 있다,(Not robust)\n\n\n관측치에 영향을 받아 초평면이 영향을 받은 예시\n\n\n\n\n\n이를 해결하기 위해 \\(C&gt;0\\), tuning parameter와 \\(\\epsilon_i\\)(slack bariable)를 사용\n\n\\[\\underset{\\beta_0,\\beta_1 \\dots \\beta_p, M}{\\text {maximize} M}, \\sum_{i=1}^{p} \\beta_j^2 =  1 \\]\n\\[y_i(\\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip}) &gt; M(1-\\epsilon_i) \\quad  (\\epsilon_i &gt; 0,  \\sum_{i=1}^n \\epsilon_i \\leq C)\\]\n\n\\(C\\)가 커질수록 margin이 넓어짐\n\\(C\\)를 작게하면 현 데이터의 정확한 분류에 더 집중하게 되므로 자료 적합성이 좋아짐, 즉 bias는 감소하고 variance는 증가\nmargin 위 혹은 안쪽에 위치한 관측치들을 일컬어 support vector라고 한다.\n\n\nC값에 따른 margin 변화\n\n\n\n\n\n여기에서 \\(C\\)값이 가장 큰 것은 왼쪽 맨위 그림이다. \\(\\to\\) 직관적으로 \\(C\\)는 허용한계이므로 \\(M\\)즉, 마진의 넓이를 넓힌다고 생각하자.\n잠깐 생각해야될 문제\n\n\n\n\n\n우리는 여태껏 직선의 경우인 Support vector classifier를 생각했다. 근데 위에 처럼 생긴다면….\n\n- 그래서 고안된 방법이 본격적인 SVM(support vector machine)이다.\n\n고차원의 decision boundary를 고려함. \\(\\to\\) 예를 들어, 2차항까지 고려한 최적화 문제의 해로써 decision boundary를 정의할 수 있음…\n\n\\[\\underset{\\beta_0,\\beta_{11},\\dots,\\beta_{p1},\\beta_{12},\\dots,\\beta_{p2},\\varepsilon_1,\\dots ,\\varepsilon_n,M} {\\text {maximize}}M\\]\n\\[\\sum_{i=1}^{p}\\sum_{k=1}^2 \\beta_{jk}^2 = 1\\]\n\\[y_i \\left( \\beta_0 + \\sum_{j=1}^{p} \\beta_{j1}x_{ij} + \\sum_{j=1}^{p}\\beta_{i2}x_{ij}^2\\right) &gt; M(1-\\varepsilon_i)\\]\n\\[\\epsilon_i &gt; 0,  \\sum_{i=1}^n \\epsilon_i \\leq C\\]\n- 단순히 확장한 것에 불과하지 않은가???\n\nsupport vector classifier을 찾기 위해서는 관측치들간의 내적(inner product)을 계산하는 것으로 충분함이 알려져 있음\n이러한 내적을 여러 방면으로 일반화하여 표현할 수 있는데 이를 규정해 주는 함수를 kernel 이라 한다. (이 부분은 설명 생략!)\n\n- kernel 종류\n\npoly(다항), rbf(Radial Basis Function), sigmoid, linear\n\n\n\n- 걍 넘어가려고 했는데 안되겠음 \\(\\to\\) ㅅㅂ… 결과를 납득할 수 없다. 진짜 다시보자\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import svm\n\n# we create 40 separable points\nnp.random.seed(0)\nX = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\nY = [0] * 20 + [1] * 20\n\n# figure number\nfignum = 1\n\n# fit the model\nfor name, penalty in ((\"unreg\", 1), (\"reg\", 0.05)):\n    clf = svm.SVC(kernel=\"linear\", C=penalty)\n    clf.fit(X, Y)\n\n    # get the separating hyperplane\n    w = clf.coef_[0]\n    a = -w[0] / w[1]\n    xx = np.linspace(-5, 5)\n    yy = a * xx - (clf.intercept_[0]) / w[1]\n\n    # plot the parallels to the separating hyperplane that pass through the\n    # support vectors (margin away from hyperplane in direction\n    # perpendicular to hyperplane). This is sqrt(1+a^2) away vertically in\n    # 2-d.\n    margin = 1 / np.sqrt(np.sum(clf.coef_**2))\n    yy_down = yy - np.sqrt(1 + a**2) * margin\n    yy_up = yy + np.sqrt(1 + a**2) * margin\n\n    # plot the line, the points, and the nearest vectors to the plane\n    plt.figure(fignum, figsize=(4, 3))\n    plt.clf()\n    plt.plot(xx, yy, \"k-\")\n    plt.plot(xx, yy_down, \"k--\")\n    plt.plot(xx, yy_up, \"k--\")\n    plt.title(f\"C={penalty}\")\n    plt.scatter(\n        clf.support_vectors_[:, 0],\n        clf.support_vectors_[:, 1],\n        s=80,\n        facecolors=\"none\",\n        zorder=10,\n        edgecolors=\"k\",\n    )\n    plt.scatter(\n        X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.get_cmap(\"RdBu\"), edgecolors=\"k\"\n    )\n\n    plt.axis(\"tight\")\n    x_min = -4.8\n    x_max = 4.2\n    y_min = -6\n    y_max = 6\n\n    YY, XX = np.meshgrid(yy, xx)\n    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n    Z = clf.decision_function(xy).reshape(XX.shape)\n\n    # Put the result into a contour plot\n    plt.contourf(XX, YY, Z, cmap=plt.get_cmap(\"RdBu\"), alpha=0.5, linestyles=[\"-\"])\n\n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)\n    fignum = fignum + 1\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n- 우리가 기존의 하던 방식\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 2023)\n- 단점\n\n랜덤하게 자료를 분할하기 때문에 분할결과에 따라 추정의 변동성이 크다….\n특히 자료위 크기가 작거나 이상/영향치들이 포함되어 있는 경우에 더더욱 그러함.\n또한, 원 자료의 크기 보다 작은 집합의 훈련자료가 모형적합에 사용되기 때문에 test error가 과대추정될 수 있음.\n\n\n이러한 방법을 \\(\\text {Validation set Approach}\\)라고 한다….\n\n- K-fold Cross Validation 방식\n\n\n\n\n전체 자료를 \\(k\\)개의 집합으로 분할한 후그 중 하나의 집합 (\\(i\\))번째 집합을 평가자료롤 설정(위 그림의 경우 \\(i=1,2 \\dots 5\\))\n그 후 각 정확도를 평균냄 (교제에서는 \\(MSE\\)를 평균 냈는데, 이번 강의에서는 정확도를 평균내더라…)\n\n\\[\\text {CV}_{5} = \\frac {1}{5} \\sum_{i=1}^{5} \\text{Accuracy}_{i}\\]\n# 1단계: 불러오기\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# 2단계: 선언하기\nmodel = DecisionTreeClassifier(max_depth=3)\n\n# 3단계: 검증하기\ncv_score = cross_val_score(model, x_train, y_train, cv=10)\n\n# 확인\nprint(cv_score)\nprint(cv_score.mean())\n= 장점\n\n모든 데이터가 학습과 평가에 사용됨\n데이터가 부족해서 발생하는 과소적합 문제을 방지할 수 있음\n 좀 더 일반화된 모델 을 만들 수 있음\ntest error의 과대추정을 방지\n\n- 단점\n\n반복 횟수가 많아서 모델 학습과 평가에 많은 시간이 소요\n\n\n\n\n\n\n\n- 일단, 튜닝 시 모델들의 각 파라미터들에 값을 어떻게 하느냐에 따라 성능이 달라지는 것을 확인할 수 있었음\n- grid search의 아이디어는 가능한 파라미터 값 범위를 지정해 해당 범위에서의 값을 모두 사용하는 것이다.\n\n당연히 정확도는 높으나….시간이 오래 걸리겠지라는 생각을 해볼 수 있다.\n\n\n요런느낌\n\n# 함수 불러오기\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# 파라미터 선언\nparam = {'n_neighbors': range(1, 500, 10), 'metric': ['euclidean', 'manhattan']}\n\n# 기본모델 선언\nknn_model = KNeighborsClassifier()\n\n# Grid Search 선언\nmodel = GridSearchCV(knn_model, param, cv=3)\n\n\n\n- 그리드 서치처럼 파라미터 범위를 지정하는 것은 동일\n- 설정한 파라미터 값 범위에서 몇 개를 선택할지를 정하여 Random Search 모델 선언 후 학습\n- 학습 데이터에 대해 가장 좋은 성능을 보인 파라미터 값으로 자동 학습함.\n\n참고로 Grid Search, Random Search를 사용할 때 내부적으로 K-Fold Cross Validation을 위해 cv값을 지정하므로!\n실제 수행되는 횟수는  파라미터 조합 수 x CV값이 된다.\n\n\ncode\n\n# 함수 불러오기\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\n# 파라미터 선언\nparam = {'n_neighbors': range(1, 500, 10),\n'metric': ['euclidean', 'manhattan']}\n\n# 기본모델 선언\nknn_model = KNeighborsClassifier()\n# Random Search 선언\nmodel = RandomizedSearchCV(knn_model, \n                                                       param, cv=3, # (default = 5)\n                                                           n_iter=20) ### 전체 파라미터 범위에서 몇 개를 뽑을 것인지 (default = 10)\n\n또한, 두 가지 기법을 섞어서 사용할 수 있음! (강의자료 참고)\n\n\n\n\n\n\n- 머신러닝 알고리즘은 데이터가 클래스 간에 균형 있게 분포되어 있다고 가정함\n\nexample : 생존자와 사망자 수가 거의 같을 것이다~~\n\n- 클래스 불균형으로 인한 재현율이 형편없어지는 경우는 아래 링크를 참고!\n\n재현율\n\n\n\n- under sampling\n\n다수 클래스 데이터를 소수 클래스 수 만큼 랜덤 샘플링\n\n- over sampling\n\n소수의 클래스 데이터를 다수 클래스 수 만큼 랜덤 샘플링\n\n# pip install imbalanced-learn\n# 불러오기\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Under Sampling\nunder_sample = RandomUnderSampler()\nu_x_train, u_y_train = under_sample.fit_resample(x_train, y_train)\n\n# 확인\nprint('전:', np.bincount(y_train))\nprint('후:', np.bincount(u_y_train))\n\n# 불러오기\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Over Sampling\nover_sample = RandomOverSampler()\no_x_train, o_y_train = over_sample.fit_resample(x_train, y_train)\n\n# 확인\nprint('전:', np.bincount(y_train))\nprint('후:', np.bincount(o_y_train))\n- over sampling (2)\n# 불러오기\nfrom imblearn.over_sampling import SMOTE\n\n# Over Sampling\nsmote = SMOTE()\ns_x_train, s_y_train = smote.fit_resample(x_train, y_train)\n\n# 확인\nprint('전:', np.bincount(y_train))\nprint('후:', np.bincount(s_y_train))\n\n\n\n- resampling 없이 클래스에 가중치를 부여하여 클래스 불균형 문제를 해결해줌\n\n학습하는 동안 알고리즘의 비용함수에서 소수 클래스에 더 많은 가중치를 부여하여 소수 클래스에 더 높은 패널티를 제공함으로써, 소수 클래스에 대한 오류를 줄이게 됨\nsklearn에서 제공하는 알고리즘 대부분 class_weight라는 하이퍼 파라미터를 제공한다.\n\nclass_weight = ‘None’ : 기본값\nclass_weight = ‘balanced’: y_train의 class 비율을 역으로 적용\nclass_weight={0:0.2, 1:0.8}: 비율 지정, 단 비율의 합은 1\n\n주의! \\(\\to\\) 전반적인 성능을 높이기 위한 작업이 아니라, 소수 클래스 성능을 높이기 위한 작업임!\n\n\n\n\n\n\n\n\n\n# 라이브러리 불러오기\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\nwarnings.filterwarnings(action='ignore')\n%config InlineBackend.figure_format = 'retina'\n\n\n\n\n\n# 데이터 불러오기\npath = 'https://raw.githubusercontent.com/jangrae/csv/master/airline_satisfaction_small.csv'\ndata = pd.read_csv(path)\n\n\ndata.head()\n\n\n\n\n\n\n\n\nid\ngender\ncustomer_type\nage\ntype_of_travel\nclass\nflight_distance\ninflight_wifi_service\ndeparture/arrival_time_convenient\nease_of_online_booking\n...\ninflight_entertainment\non-board_service\nleg_room_service\nbaggage_handling\ncheckin_service\ninflight_service\ncleanliness\ndeparture_delay_in_minutes\narrival_delay_in_minutes\nsatisfaction\n\n\n\n\n0\n70172\nMale\nLoyal Customer\n13\nPersonal Travel\nEco Plus\n460\n3\n4\n3\n...\n5\n4\n3\n4\n4\n5\n5\n25\n18.0\n0\n\n\n1\n5047\nMale\ndisloyal Customer\n25\nBusiness travel\nBusiness\n235\n3\n2\n3\n...\n1\n1\n5\n3\n1\n4\n1\n1\n6.0\n0\n\n\n2\n110028\nFemale\nLoyal Customer\n26\nBusiness travel\nBusiness\n1142\n2\n2\n2\n...\n5\n4\n3\n4\n4\n4\n5\n0\n0.0\n1\n\n\n3\n24026\nFemale\nLoyal Customer\n25\nBusiness travel\nBusiness\n562\n2\n5\n5\n...\n2\n2\n5\n3\n1\n4\n2\n11\n9.0\n0\n\n\n4\n119299\nMale\nLoyal Customer\n61\nBusiness travel\nBusiness\n214\n3\n3\n3\n...\n3\n3\n4\n4\n3\n3\n3\n0\n0.0\n1\n\n\n\n\n5 rows × 24 columns\n\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2580 entries, 0 to 2579\nData columns (total 24 columns):\n #   Column                             Non-Null Count  Dtype  \n---  ------                             --------------  -----  \n 0   id                                 2580 non-null   int64  \n 1   gender                             2580 non-null   object \n 2   customer_type                      2580 non-null   object \n 3   age                                2580 non-null   int64  \n 4   type_of_travel                     2580 non-null   object \n 5   class                              2580 non-null   object \n 6   flight_distance                    2580 non-null   int64  \n 7   inflight_wifi_service              2580 non-null   int64  \n 8   departure/arrival_time_convenient  2580 non-null   int64  \n 9   ease_of_online_booking             2580 non-null   int64  \n 10  gate_location                      2580 non-null   int64  \n 11  food_and_drink                     2580 non-null   int64  \n 12  online_boarding                    2580 non-null   int64  \n 13  seat_comfort                       2580 non-null   int64  \n 14  inflight_entertainment             2580 non-null   int64  \n 15  on-board_service                   2580 non-null   int64  \n 16  leg_room_service                   2580 non-null   int64  \n 17  baggage_handling                   2580 non-null   int64  \n 18  checkin_service                    2580 non-null   int64  \n 19  inflight_service                   2580 non-null   int64  \n 20  cleanliness                        2580 non-null   int64  \n 21  departure_delay_in_minutes         2580 non-null   int64  \n 22  arrival_delay_in_minutes           2574 non-null   float64\n 23  satisfaction                       2580 non-null   int64  \ndtypes: float64(1), int64(19), object(4)\nmemory usage: 483.9+ KB\n\n\n- 쓸모없는 변수 제거\n\n# 변수 제거\nd_cols = [\"id\", \"departure/arrival_time_convenient\", \"gate_location\", \"departure_delay_in_minutes\"]\n\n\ndata.drop(d_cols,axis=1, inplace = True)\ndata.head()\n\n\n\n\n\n\n\n\ngender\ncustomer_type\nage\ntype_of_travel\nclass\nflight_distance\ninflight_wifi_service\nease_of_online_booking\nfood_and_drink\nonline_boarding\nseat_comfort\ninflight_entertainment\non-board_service\nleg_room_service\nbaggage_handling\ncheckin_service\ninflight_service\ncleanliness\narrival_delay_in_minutes\nsatisfaction\n\n\n\n\n0\nMale\nLoyal Customer\n13\nPersonal Travel\nEco Plus\n460\n3\n3\n5\n3\n5\n5\n4\n3\n4\n4\n5\n5\n18.0\n0\n\n\n1\nMale\ndisloyal Customer\n25\nBusiness travel\nBusiness\n235\n3\n3\n1\n3\n1\n1\n1\n5\n3\n1\n4\n1\n6.0\n0\n\n\n2\nFemale\nLoyal Customer\n26\nBusiness travel\nBusiness\n1142\n2\n2\n5\n5\n5\n5\n4\n3\n4\n4\n4\n5\n0.0\n1\n\n\n3\nFemale\nLoyal Customer\n25\nBusiness travel\nBusiness\n562\n2\n5\n2\n2\n2\n2\n2\n5\n3\n1\n4\n2\n9.0\n0\n\n\n4\nMale\nLoyal Customer\n61\nBusiness travel\nBusiness\n214\n3\n3\n4\n5\n5\n3\n3\n4\n4\n3\n3\n3\n0.0\n1\n\n\n\n\n\n\n\n- 결측치 확인\n\nprint(data.columns[data.isna().sum() !=0])\n\nIndex(['arrival_delay_in_minutes'], dtype='object')\n\n\n\ndata.arrival_delay_in_minutes.isna().sum()\n\n6\n\n\n- 현재 arrival_delay_in_minutes의 6개 결측치가 확인된다.\n\n결측치 제거를 위해 해당 변수의 분포를 확인하자.\n\n\ndata.plot(x=\"arrival_delay_in_minutes\",  kind = \"hist\", backend  = \"plotly\",\n                  width = 1000, height = 400,nbins=100)\n\n\n                                                \n\n\n- 대부분의 값들이 0값 근처에 몰려있으니 결측값을 0으로 대체하자.\n\ndata.arrival_delay_in_minutes.fillna(0, inplace = True)\nprint(data.columns[data.isna().sum() !=0])\n\nIndex([], dtype='object')\n\n\n- x, y 분리\n\ntarget = \"satisfaction\"\n\nx = data.drop(target, axis = 1)\ny = data[target]\n\n- 가변수화\n\nd = [\"gender\", \"customer_type\", \"type_of_travel\", \"class\"]\n\nx = pd.get_dummies(x, columns = d, drop_first = True, dtype = float)\nx.head()\n\n\n\n\n\n\n\n\nage\nflight_distance\ninflight_wifi_service\nease_of_online_booking\nfood_and_drink\nonline_boarding\nseat_comfort\ninflight_entertainment\non-board_service\nleg_room_service\nbaggage_handling\ncheckin_service\ninflight_service\ncleanliness\narrival_delay_in_minutes\ngender_Male\ncustomer_type_disloyal Customer\ntype_of_travel_Personal Travel\nclass_Eco\nclass_Eco Plus\n\n\n\n\n0\n13\n460\n3\n3\n5\n3\n5\n5\n4\n3\n4\n4\n5\n5\n18.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n1\n25\n235\n3\n3\n1\n3\n1\n1\n1\n5\n3\n1\n4\n1\n6.0\n1.0\n1.0\n0.0\n0.0\n0.0\n\n\n2\n26\n1142\n2\n2\n5\n5\n5\n5\n4\n3\n4\n4\n4\n5\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n25\n562\n2\n5\n2\n2\n2\n2\n2\n5\n3\n1\n4\n2\n9.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n61\n214\n3\n3\n4\n5\n5\n3\n3\n4\n4\n3\n3\n3\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n- 학습, 평가용 데이터 분리\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 1, test_size = 0.3)\n\n\n\n\n\n\nCode\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n# 1. tree\ntree = DecisionTreeClassifier(max_depth = 5, random_state = 1)\ntree.cv = cross_val_score(tree, x_train, y_train, cv = 5)\ntree.cv_m = tree.cv.mean()\n\n# 2. logistic\nfrom sklearn.linear_model import LogisticRegression\nlogit = LogisticRegression()\nlogit.cv = cross_val_score(logit, x_train, y_train, cv = 5)\nlogit.cv_m = logit.cv.mean()\n\n# 3. RF\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(max_depth = 5,  random_state = 1)\nrf.cv = cross_val_score(rf, x_train,y_train)\nrf.cv_m = rf.cv.mean()\n\n# 4. XGBoost\n\nfrom xgboost import XGBClassifier\nxgb =  XGBClassifier(max_depth = 5, random_state = 1)\nxgb.cv = cross_val_score(xgb, x_train, y_train, cv = 5)\nxgb.cv_m  = xgb.cv.mean()\n\n\n\nresult = [tree.cv_m, logit.cv_m, rf.cv_m,  xgb.cv_m]\nmodel = [\"tree\",\"logit\", \"rf\", \"xgb\"]\n\nfig = pd.DataFrame({\"model\" : model, \"result\" : result}).\\\n            sort_values(\"result\", ascending = False).plot(x = \"result\", y = \"model\", kind = \"bar\", backend = \"plotly\",color = \"model\")\n\nfig.update_xaxes(range = [0.7, 1.0])\n\n\n                                                \n\n\n- cv를 기준으로 XGB 모델이 최적의 모델인 것 같다.\n\ngrid search기법을 이용하여 모델 튜닝\n\n\nfrom sklearn.model_selection import GridSearchCV\nmodel = XGBClassifier(max_depth= 5, random_state = 1)\n\nparams=  {\"max_depth\" : range(1,21)}\n\nmodel = GridSearchCV(model,\n                     params,\n                     cv=5,\n                     scoring=\"accuracy\")\n\n\nmodel.fit(x_train, y_train)\n\nGridSearchCV(cv=5,\n             estimator=XGBClassifier(base_score=None, booster=None,\n                                     callbacks=None, colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None, device=None,\n                                     early_stopping_rounds=None,\n                                     enable_categorical=False, eval_metric=None,\n                                     feature_types=None, gamma=None,\n                                     grow_policy=None, importance_type=None,\n                                     interaction_constraints=None,\n                                     learning_rate=None, max_bin=None,\n                                     max_cat_threshold=None,\n                                     max_cat_to_onehot=None,\n                                     max_delta_step=None, max_depth=5,\n                                     max_leaves=None, min_child_weight=None,\n                                     missing=nan, monotone_constraints=None,\n                                     multi_strategy=None, n_estimators=None,\n                                     n_jobs=None, num_parallel_tree=None,\n                                     random_state=1, ...),\n             param_grid={'max_depth': range(1, 21)}, scoring='accuracy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=XGBClassifier(base_score=None, booster=None,\n                                     callbacks=None, colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None, device=None,\n                                     early_stopping_rounds=None,\n                                     enable_categorical=False, eval_metric=None,\n                                     feature_types=None, gamma=None,\n                                     grow_policy=None, importance_type=None,\n                                     interaction_constraints=None,\n                                     learning_rate=None, max_bin=None,\n                                     max_cat_threshold=None,\n                                     max_cat_to_onehot=None,\n                                     max_delta_step=None, max_depth=5,\n                                     max_leaves=None, min_child_weight=None,\n                                     missing=nan, monotone_constraints=None,\n                                     multi_strategy=None, n_estimators=None,\n                                     n_jobs=None, num_parallel_tree=None,\n                                     random_state=1, ...),\n             param_grid={'max_depth': range(1, 21)}, scoring='accuracy')estimator: XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=1, ...)XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=1, ...)\n\n\n\ny_pred = model.predict(x_test)\n\n\n# 예측 결과 확인\nprint(model.best_params_)\nprint(model.best_score_)\n\n{'max_depth': 3}\n0.9363202277283789\n\n\n\nfig = pd.DataFrame([model.best_estimator_.feature_names_in_,model.best_estimator_.feature_importances_]).T.\\\n        rename(columns = {0 : \"feature\", 1 : \"importance\"}).sort_values(\"importance\", ascending = False).\\\n            plot(y = \"feature\", x=  \"importance\", kind = \"barh\",\n                    backend = \"plotly\",color = \"feature\")\n\nfig.update_layout(showlegend = False)\n\n\n                                                \n\n\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nmeasure = [\"accuracy\", \"precision\", \"reacll\", \"f1-score\"]\n\nacc = accuracy_score(y_test, y_pred)\npre = precision_score(y_test, y_pred)\nre = recall_score(y_test, y_pred)\nf1_score = f1_score(y_test, y_pred)\n\n\npd.DataFrame({\"measure\" : measure, \"score\" : [acc, pre, re, f1_score]})\n\n\n\n\n\n\n\n\nmeasure\nscore\n\n\n\n\n0\naccuracy\n0.931525\n\n\n1\nprecision\n0.949102\n\n\n2\nreacll\n0.898017\n\n\n3\nf1-score\n0.922853"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-28-08. summary (1).html#모델링-단계",
    "href": "posts/DX/04. 머신러닝/2023-09-28-08. summary (1).html#모델링-단계",
    "title": "08. summary (1)",
    "section": "",
    "text": "- knn의 경우 필요하다면 스케일링 단계가 필요\n- 이산형 변수, 즉 범주형 변수를 모델의 예측변수로 사용할 경우 더미변수로 변환해주어야한다.\npd.get_dummies(data, columns = 더미화할컬럼리스트, dtype = (int or float))\n- 결측치 처리 : 히스토그램, boxplot, 시계열 데이터인 경우 등등을 고려하여 각 case에 맞게 적절히 결측치를 처리해준다.\n\nmisforest, EM 알고리즘을 통한 결측치 처리를 한다지만, 개인적인 생각으로는 좀 과한 결측치 처리가 아닌지 싶음\n이유는 즉슨, 결측치를 처리하기위해 결측치 처리 단계에서 모델링을 한번 더 수행하는데 이 때 시간이 생각보다 오래 걸림\n\n\n\n\n- 아래와 같이 여러개의 모델을 생성한다음 cross-validation 통해 최적의 모델을 선택하였다.\n\nexample\n\n\n# 1. knn\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nknn = KNeighborsClassifier(n_neighbors=5)\n\nknn.cv = cross_val_score(knn, x_train_s, y_train, cv = 5)\n\nknn.cv_m = knn.cv.mean()\n\n# 2. tree\nfrom sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(max_depth = 5, random_state = 1)\n\ntree.cv = cross_val_score(tree, x_train, y_train, cv = 5)\n\ntree.cv_m = tree.cv.mean()\n\n# 3. logistic\nfrom sklearn.linear_model import LogisticRegression\nlogit = LogisticRegression()\nlogit.cv = cross_val_score(logit, x_train, y_train, cv = 5)\n\nlogit.cv_m = logit.cv.mean()\n\n# 4. RF\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(max_depth = 5,  random_state = 1)\n\nrf.cv = cross_val_score(rf, x_train,y_train)\n\nrf.cv_m = rf.cv.mean()\n\n# 5. XGBoost\nfrom xgboost import XGBClassifier\nxgb =  XGBClassifier(max_depth = 5, random_state = 1)\n\nxgb.cv = cross_val_score(xgb, x_train, y_train, cv = 5)\n\nxgb.cv_m  = xgb.cv.mean()\n\nfrom lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(max_depth = 5, random_state = 1,verbose = -100) \n\nlgbm.cv = cross_val_score(lgbm, x_train, y_train, cv = 5)\n\nlgbm.cv_m = lgbm.cv.mean()\n\n\n\n- 그 후 선택한 최종 모델을 튜닝해 최종 모델을 select\nfrom sklearn.model_selection import GridSearchCV\nmodel = XGBClassifier(max_depth= 5, random_state = 1)\n\nparams=  {\"max_depth\" : range(1,21)}\n\nmodel = GridSearchCV(model,\n                     params,\n                     cv=5,\n                     scoring='r2')"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-28-08. summary (1).html#각-모델-소개",
    "href": "posts/DX/04. 머신러닝/2023-09-28-08. summary (1).html#각-모델-소개",
    "title": "08. summary (1)",
    "section": "",
    "text": "- 아래 링크를 참조해서 까먹을때 마다 보자~~\n- ISLP2023-00.Linear Regression\n\n\n\n- 이것두 아래 링크를 참조하자.\n- ISLP2023-01. Classification\n\n\n\n- 학습을 안함, 그냥 그 근처 K개의 녀석들을 보고 값을 할당\n\\[P(y= j | X = x_0) = \\frac {1}{K}  \\sum_{i\\in N_0} I(y_i = j)\\]\n\\[N_0  : x_0\\text {와 가장 가까운  K개의 자료의 집합}\\]\n- \\(k\\)가 작을수록 모델은 복잡해지고, 클수록 단순해짐\n\n솔직히 와닿지 않지만, 내 방식대로 이해해보자.\n이전에 선형회귀분석에서 모델 복잡도를 생각해보자, 모델을 일직선으로 예측한 경우 단순선형회귀분석이다.\n즉, 모델 하나하나의 포인트를 고려하지 않고 전체 평균적인 선형회귀식을 하나 구한 것이다.\n이를 다시 KNN예제로 생각해 \\(K\\)가 클경우 생각해보변, 주변 녀석들의 하나하나 개인적은 특성을 고려하기보단 전체적인 특성에 기반하여 주어진\\(x_0\\)에 대한 \\(y\\)를 예측하는 것이다.\n따라서, \\(k\\)가 작을수록 주변 녀석들의 특징을 하나하나 잘 고려해서 모델이 복잡한 것이고, \\(k\\)가 크면 전체적인 평균을 고려한 것이기 때문에 모델이 단순해진다… \\(\\to\\) 사실 이것도 그렇게 와닿지 않음 나중에 더 찾아보자…\n\n\n\n\n\n나무모형은 간단하고 해석상에 장점이 있으나 다른 방법들에 비해 좋은 성능을 보이지 못하는 경우가 있음\n\n\n\n1 설명변수들의 가능한 조합을 이용하여 예측공간을 \\(J\\)개의 겹치지 않는(non-overlaping)구역으로 분할\n2 각 관측값은 \\(R_j\\) 구역에 포함되며, \\(R_j\\) 구역에 포함된 training data의 반응변수 (\\(y\\))의 평균 (분류문제에선 voting방식)을 이용하여 예측\n\\[\\hat {y}_{R_j} = \\frac {1}{n_j} \\sum_{k\\in R_J} y_k \\]\n3 목표 : 다음의 RSS를 최소화 하는 구역 \\(R_1,R_2, R_J\\)를 찾는 것\n\\[RSS = \\sum_{j=1}^{J} \\sum_{i \\in R_j} (y_i - \\hat {y}_{R_{j}})^2\\]\n4 모든 조합을 확인하는 것은 불가능…(사실 가능하다. tree를 무한정 쪼개면)\n\n근데 tree를 무한정 쪼갤경우 과적합문제가 무조건 발생\n\n5 정지규칙 (stopping rule)\n\n모든 자료가 한 범주에 속할 때\n노드에 속하는 자료가 일정 수 이하일 떄\nMSE의 감소량이 아주 작을 떄\n뿌리마디로부터의 깊이가 일정 수 이상일 떄 등 (max_depth)\n\n6 가지치기 : 과적합을 막기위한 방법\n\n사실 정지규칙도 이에 포함됨, 따라서 위에거 + 빠진 내용을 적겠음\nmin_samples_leaf(default = 1) : leaf노드가 되기 위한 최소한의 샘플 수\nmin_samples_split(default = 20 : 노드를 분할하기 위한 최소한의 샘플 수 (값을 적게 설정할수록 계순 분할되어, 과적합 발생 위험 증가)\nmax_feature : 최선의 분할을 위해 고려할 변수(feature) 개수\n\nsqrt : 전체 변수 개수의 루트\nauto : sqrt와 같은 의미\nlog : \\(\\log_{2}\\) (전체 변수의 수)\n\nmax_leaf_node : 리프 노드의 최대 개수 \\(\\to\\) \\(\\text{Cost complexity Pruning}\\)\n\n\\(|T|\\)는 터미널도드로 리프노드의 개수를 뜻한다.\n\\(R_{\\alpha}(T)\\)는 변하지 않는 비용함수로 \\(R(T)\\)는 우리가 알고 있는 \\(RSS\\)와 같다.\n아래식이 뜻하는 바는 리프노드의 개수가 클수록 \\(R(T)\\) 훈련 데이터 셋에대한 \\(RSS\\)가 작아져 과적합 문제가 발생할 수 있기 때문에 적절한 리프노드의 개수를 설정해야한다는 의미이다.\n\\(\\alpha\\)는 \\(\\text {tuning parameter}\\)로 복잡도를 조절한다. 만약 \\(\\alpha\\)가 0이라면 기존의 비용함수와 같고 1에 가까워질수록 \\(R(T)\\)값이 작아진다.\n따라서 우리는 적절한 \\(\\alpha\\)값과 \\(|T|\\)값을 교차검증 기법을 통해 찾아내어 가지치기를 수행하여야한다.\n\n\n\\[\\begin {align}R_{\\alpha}(T) &= \\sum R(T) + \\alpha |T|  \\\\ \\\\\n                                                          &= \\sum_{m=1}^{|T|} \\sum_{x_i \\in R_m} (y_i - \\hat {y}_{R_m})^2 +  \\alpha |T|  \\end {align}\\]\n7 비용함수\n\n지니 지수 (Gini Index)\n\n\\[Gini (D) = 1- \\sum_{k=1}^{K}p_{k}^2 = \\sum_{k=1}^{K} p_k(1-p_k)\\]\n\\[p_k : \\text{Node D에서 k번째 범주에 속하는 관측 비율}\\]\n\n\n\n\n순수하게 분류되면 값은 0이다.\n만약 분리규칙 \\(A\\)에 의해서 Node D가 \\(D_1, D_2\\)로 분리된다면, 분리규칙 \\(A\\)에서 Ginin지수는 다음과 같다.\n\n\\[Gini_A(D) =\\frac {|D_1|}{|D|}Gini(D_1) +\\frac {|D_2|}{|D|}Gini(D_2) \\]\n\n위에 근거하여 분리규칙 A에서 발생한 불순도 감소량은 다음과 같이 정의할 수 있다.\n\n\\[\\Delta Gini(A) = Gini(D) - Gini_{A}(D)\\]\n\n따라서, \\(Gini_{A}(D)\\)를 가장 작게 하거나 \\(\\Delta Gini(A)\\)를 가장 크게 하는 분리 규칙을 선택!\n\n\n\n엔트로피(Entropy)\n\n\\[\\text {Entropy}  = -\\sum_{i=1}^m p_i\\log_{2} p_i\\]\n\n순수하게 분류되면 0\n\n\n정보 이득\n\n\n엔트로피와 지니지수는 단지 속성의 불순도를 표현한다.\n우리가 알고 싶은 것은 “어떠한 속성이 얼마나 많은 정보를 제공하는가!” 이다.\n\n\\[\\text {Gain}(T,X)= \\text{Entropy}(T)-\\text{Entropy}(T,X)\\]\n\n위 식을 살펴보니 지니지수에서 했던 불순도 감소량과 비슷하지 않은가?\n\n\\[\\Delta Gini(A) = Gini(D) - Gini_{A}(D)\\]\n\n\n\n\n\n앞서 언급한 tree는 과대적합의 위험이 큰 모형임 \\(\\to\\) max_depth를 무작정 깊게 하면 과대적합이 발생하므로\n앙상블의 아이디어 : 이러한 test데이터 셋에 예측력이 약한 모델을 결합해서 성능이 좋은 모델을 만들자!\n\n\n\n- 여러 모델들의 예측결과를 투표를 통해 최종 예측결과를 결정\n\n하드 보팅 : 다수 모델이 예측한 값이 최종 결과값\n소프트 보팅 : 모든 모델이 예측한 레이블 값의 결정 확률 평균을 구한 뒤 가장 확률이 높은 값을 최종 선택\n\n\n\n\n- Boostrap Aggregating\n- 아이디어 : 모형의 분산을 줄여 과적합을 방지하자.\n\n만약, 모집단으로부터 여러개의 훈련자료를 얻을 수 있고 이로부터 여러개의 모형 \\(\\hat{f}_1(x)\\dots \\hat{f}_b(x)\\)를 얻을 수 있다면, 다음과 같이 분산을 줄일 수 있다.\n\n\\[\\hat{f}_{avg}(x) = \\frac {1}{B} \\sum_{i=1}^{B} \\hat{f}_b(x)\\]\n\n보통은 한 set의 자료만이 주어지게 되므로 위 방식은 직접 적용이 불가능\n그래서 우리는 복원추출을 기반으로 같은 size의 표본을 추출해 각각의 모델링을 수행한다. (Bootstrap sample)\n\n\\[X_1^{*}\\dots X_B^{*}\\]\n\\[\\hat{f}_{\\text{bag}}(x) = \\frac {1}{B} \\sum_{i=1}^{B} \\hat{f}^{*}_b(x)\\]\n\n보팅과 다른점은 보팅은 여러개의 예측모델, 배깅은 동일한 예측모델 여러개를 앙상블하는 것임!\n대표적인 모델 : Randoms Forest\n\n\n\n- 여러 tree모델이 전체 데이터에서 배깅 방식으로 각자의 데이터를 샘플링\n\n모델들이 개별적으로 학습을 수행한 뒤 모든 결과를 집계하여 최종 결과를 결정\n\n# 불러오기\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import *\n\n# 선언하기\nmodel = RandomForestClassifier(max_depth=5, random_state=1)\n\n# 학습하기\nmodel.fit(x_train, y_train)\n\n# 예측하기\ny_pred = model.predict(x_test)\n- Out-of-Bag은 생략 (ISLP 교재참고)\n- RF 모델의 변수 선택\n\n하나의 트리를 형성하는 과정에서, 각 노드에서 전체 \\(p\\)개의 설명변수 중 \\(m\\)개만을 임의로 추출하여 분리 규칙을 생성한다.\n\n일반적으로 \\(m \\approx \\sqrt {p}\\)\n\n\nRandomForestClassifier( max_features='sqrt') ## default\n- 변수 중요도 (Variable Importance measur)\n\n사실 여러 형태의 나무를 결합하여 산출된 모델은…. 해석이 거의 불가능해진다.\n대안적으로, 나무들을 생성할 떄 어떠한 변수들이 RSS 혹은 Gini index 등에 큰 감소를 가져왔는지를 요약한 값으로 변수의 중요도를 파악할 수 있음.\n\\(B\\)개의 모형에 대한 평균적인 기여 정도로 변수의 중요도를 평가하게 된다!\nscikit-learn 참고링크\n\n\n\n\n\n\n\n\n\n\n같은 유형의 약한 예측 모형을 결합하여 매우 정확한 예측 모형을 만드는 방법\n예측 모형을 순차적으로(Sequentially) 학습하여 먼저 학습된 모형의 결과가 다음 모형의 학습 시 정보를 제공\n즉, 이전 모형의 약점(잔차)를 학습하여 보완한다.\n\n\n\n\n\n배깅에 비해 성능이 좋지만, 속도가 느리고 과적합 발생 가능성이 있음.\n대표적인 부스팅 알고리즘 : XGBoost, LightGBM\n\n- Boosting의 원리 (ISLP 기준)\n\n초기값 셋팅 \\(\\hat{f}(x) = 0, r_1 = y_1\\)\nFor \\(b = 1, 2\\dots B , repaet\\) :\n\n\\[\\hat {f}(x)_{i+1} = \\hat {f}(x)_{i} + \\lambda \\hat {f}^{b}(x)\\]\n\nupdate the residual,\n\n\\[r_{i+1} = r_{i}- \\lambda \\hat{f}^{b} (x) \\]\n\n초기 셋팅된 \\(\\hat {f}_1 = 0\\) 이므로\n\n\\[\\hat {f}(x)_{\\text{final}} = \\sum_{i=1}^{B} \\lambda \\hat {f}^{b}(x)\\]\n- 위 같은 방식의 문제점 \\(\\to\\) 과적합발생… 당연하다. 예측 모형을 순차적으로 학습한다는 것은 모형간 자기 상관성이 존재하고 모형의 분산이 증가하기 때문에 과적합이 발생할 수 밖에 없다…\n- 이를 막기위해 나온 모델이 XGBoost!\n\n\n- Extreme Gradient Boost\n\nreview : 방금 정리했던 Boosting기법과 같이 기본 학습기를 의사결정나무로 하며, 잔차를 이용해 이전 모형의 약점을 보완하는 방식으로 학습한다.\n+\\(\\alpha\\) : 기존의 Graident Tree Boosting에 과적합 방지를 위한 파라미터\\((\\lambda, \\gamma)\\)가 추가된 알고리즘이다.\n\n\n\n0 parameter\n\n\\(L\\) : 손실함수\n\\(M\\) : 개별 모형의 최대 개수\n\\(l\\) : 학습률\n\\(\\gamma, \\lambda\\) : 과적합 방지를 위한 파라미터\n\n1. 초기 모형은 상수로 설정하며 다음과 같이 손실함수를 최소화 하는 모형으로 설정한다.\n\n초기 모형(상수값)을 아무렇게나 설정해도 된다고 하지만… 최적화 관점에서 아래처럼 잡아주는게 적절한 듯 하다.\n\n\\[F_{0}(x) = \\underset {c}{\\text{arg min}}  \\sum_{i=1}^{n} L(y_i,c)  \\]\n2 \\(m = 1,\\dots M\\)에 대하여 다음을 반복\n\n\nGradient \\(g_i\\)와 Hessian \\(h_i\\)를 계산\n\n\n\\[g_i = \\left[ \\frac {\\partial L(y_i, F(x_i))}{\\partial  F(x_i)}\\right],\\quad F(x_i) = \\hat {F}_{m-1}(x)\\]\n\\[h_i = \\left[ \\frac {\\partial^2 L(y_i, F(x_i))}{\\partial  F(x_i)^2}\\right],\\quad F(x_i) = \\hat {F}_{m-1}(x)\\]\n\n\n회귀나무 \\(\\phi_m\\)을 다음과 같이 적합\n\n\n\\[l = \\sum_{i=1}^{n} \\frac {1}{2}h_i \\left [ - \\frac {g_i}{h_i} - \\phi(x_i) \\right ]  + \\gamma T + \\frac {1}{2} \\lambda ||\\phi||^2\\]\n\\[\\phi_m = \\underset {\\phi} {\\text{arg min}} \\sum_{i=1}^{n} \\frac {1}{2}h_i \\left [ - \\frac {g_i}{h_i} - \\phi(x_i) \\right ]  + \\gamma T + \\frac {1}{2} \\lambda ||\\phi||^2\\]\n\n여기서 \\(T\\)는 \\(\\phi\\)의 끝마디 개수, \\(||\\phi||^2 = \\sum_{j=1}^{T} w_j^2\\) 이며 \\(w_j\\)는 \\(j\\)번째 끝마디에서의 출력값이다.\n\n잘 살펴보면 릿지회귀분석에 L2 penalty와 비슷한데, L1 penalty 방식도 지원하는 것 같다.\n\n\n다음과 같이 업데이트 한다.\n\n\n\\[ F_{m}(x) = F_{m-1}(x) + l\\cdot \\phi_m(x)\\]\n3 최종모형은?\n\\[F_M(x) =  \\sum_{m=0}^{M} F_m(x)\\]\n4 summary\n\nXGBoost는 기존 Gradient Boosting기법에 문제인 과적합문제를 해결하기 위해 \\(\\gamma, \\lambda\\) 파라미터를 사용한다.(규제)\n손실함수를 살펴보면 터미널 노드(끝마디 노드)의 수와 끝마디에서의 출력값에 대한 패널티 파라미터가 들어가있다,\n의사결정나무에 가지치기 과정에서 터미널 노드의 개수에 따라 panelty를 부여하는 방식을 생각해보면 비슷한 방식이다.\n또한, 내장된 교차 검증? (이거는 이론적으로 구현되어있다기보단 사이킷런에서 내부적으로 동작하게 만든것 같음)\n\n여튼 여기서 조기 중단을 가능하게끔 지원해준다.\n\n결측치 자체 처리 : 알아서 결측치를 고려해서 학습을 한다.(결측치 여부를 노드 분기를 위한 질문에 포함시킴)\n\n이것도 사이킷런에서 내부적으로 구현한듯\n그래도 명시적으로 결측치에 대한 처리를 진행하기를 권고…\n\n\n5 실습 코드\n# 불러오기\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error, r2_score\n\n# 선언하기\nmodel = XGBRegressor(max_depth=5, n_estimators=100, random_state=1)\n\n# 학습하기\nmodel.fit(x_train, y_train)\n\n# 예측하기\ny_pred = model.predict(x_test)\n\n# 평가하기\nprint(mean_absolute_error(y_test, y_pred))\nprint(r2_score(y_test, y_pred))\n6 주요 파라미터\n\nlearning_rate : 학습률(default = 0.1)\nn_estimators : 나무의 개수 (default = 100)\nmid_child_weight : 트리에서 추가적으로 분할할 지를 결정하기 위해 필요한 데이터들의 weight(\\(w_i\\))들의 총함 (default = 1)\ngamma : 트리에서 추가적으로 분할할지를 결정하기 위한 값 \\(\\gamma T\\) (default = 0)\nmax_depth : 나무의 깊이 (default = 6)\nsub_sample : weak learner가 학습에 사용되는 데이터 샘플링 비율\n\n과적합이 염려되는 경우 1보다 작은 값으로 설정, (default = 1)\n\ncolsample_bytree : 트리 생성에 필요한 변수 선택 비율\n\n변수가 많은 경우 과적합을 조절하기 위해서 사용, 기본값 = 1\n\nreg_lambda : L2규제 적용값 (\\(\\lambda \\sum_{j=1}^{T} w_j^2\\), 기본값 \\(\\lambda = 1\\))\nreg_alpha : L1규제 적용값 (\\(\\alpha \\sum_{j=1}^{T} |w|\\), 기본값 \\(\\alpha = 0\\))\nearly_stopping_rounds : n_estimators만큼 학습을 반복하지 않더라도 조기 종료 가능(default = 10, 10번 동안 성능 향상이 없으면 학습 중단.)\n\n\n\n\n\n\n\n- 기본 아이디어 : 두 클래스 사이에 가장 넓은 도로를 내는 것\n- 용어 정리\n\n결정 경계 (Decision Boundary) or 초평면\n\n클래스를 구분하는 경계선\n결정 경계가 바로 모델 (Hyper plane)이라고 부름\n\n벡터 : 모든 데이터 포인트\n서포트 벡터 : 결정경계와 가끼운 데이터 포인트\n\n마진의 크기와 결정경계에 영향을 미침\n\n마진(margin) : 서포트 벡터와 결정경계 사이의 거리\n\n폭이 가장 넓은 도로를 찾는 것이 SVM의 목표\n마진이 클수록 새로운 데이터에 대해 안정적인 분류가 가능해지는 것임\n\n잘 생각해보면 마진의 크기가 좁을수록 정확한 분류가 일어나나 과적합 문제가 발생하므로\n마진의 크기와 오류에 대한 허용 정도는 Trade-off 관계에 있는 것을 알 수 있다.\n이것을 조절하는 파라미터 \\(\\to\\) 비용(C)\n\n- 이를 이해하기 위해서!\n\nSupport vector classifier의 decision boundary는 다음과 같은 최적화 문제의 해로 정의된다.\n\n\n만약, label = {1,-1}이라 하면 초평면은 다음과 같은 성질을 가진다.\n\n\\[\\text {Hyper plane}=  \\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip} = 0\\]\n\\[\\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip} &gt; 0, \\quad \\text{if }  y_i=1\\]\n\\[\\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip} &lt; 0, \\quad \\text{if }  y_i=-1\\]\n\n이는 다음과 같이 간단히 표현할 수 있다.\n\n\\[y_i(\\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip}) &gt; 0\\]\n\n아래의 두 식은 관측치가 초평면을 중심으로 두 class를 정확히 구분되고, 관측치와 초평면사이의 직교거리가 최소 \\(M\\)이상이 되도록 보장해 주는 조건이다.\n\n\\[\\underset{\\beta_0,\\beta_1 \\dots \\beta_p, M}{\\text {maximize} M} \\]\n\\[ \\sum_{i=1}^{p} \\beta_j^2 =  1\\]\n\\[y_i(\\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip}) &gt; M\\]\n\n그러나 실제로 관측치는 두 개의 class로 정확히 구분되지 않은 경우가 더 많으며 한 두개의 관측치에 큰 영향을 받을 수 있다,(Not robust)\n\n\n관측치에 영향을 받아 초평면이 영향을 받은 예시\n\n\n\n\n\n이를 해결하기 위해 \\(C&gt;0\\), tuning parameter와 \\(\\epsilon_i\\)(slack bariable)를 사용\n\n\\[\\underset{\\beta_0,\\beta_1 \\dots \\beta_p, M}{\\text {maximize} M}, \\sum_{i=1}^{p} \\beta_j^2 =  1 \\]\n\\[y_i(\\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip}) &gt; M(1-\\epsilon_i) \\quad  (\\epsilon_i &gt; 0,  \\sum_{i=1}^n \\epsilon_i \\leq C)\\]\n\n\\(C\\)가 커질수록 margin이 넓어짐\n\\(C\\)를 작게하면 현 데이터의 정확한 분류에 더 집중하게 되므로 자료 적합성이 좋아짐, 즉 bias는 감소하고 variance는 증가\nmargin 위 혹은 안쪽에 위치한 관측치들을 일컬어 support vector라고 한다.\n\n\nC값에 따른 margin 변화\n\n\n\n\n\n여기에서 \\(C\\)값이 가장 큰 것은 왼쪽 맨위 그림이다. \\(\\to\\) 직관적으로 \\(C\\)는 허용한계이므로 \\(M\\)즉, 마진의 넓이를 넓힌다고 생각하자.\n잠깐 생각해야될 문제\n\n\n\n\n\n우리는 여태껏 직선의 경우인 Support vector classifier를 생각했다. 근데 위에 처럼 생긴다면….\n\n- 그래서 고안된 방법이 본격적인 SVM(support vector machine)이다.\n\n고차원의 decision boundary를 고려함. \\(\\to\\) 예를 들어, 2차항까지 고려한 최적화 문제의 해로써 decision boundary를 정의할 수 있음…\n\n\\[\\underset{\\beta_0,\\beta_{11},\\dots,\\beta_{p1},\\beta_{12},\\dots,\\beta_{p2},\\varepsilon_1,\\dots ,\\varepsilon_n,M} {\\text {maximize}}M\\]\n\\[\\sum_{i=1}^{p}\\sum_{k=1}^2 \\beta_{jk}^2 = 1\\]\n\\[y_i \\left( \\beta_0 + \\sum_{j=1}^{p} \\beta_{j1}x_{ij} + \\sum_{j=1}^{p}\\beta_{i2}x_{ij}^2\\right) &gt; M(1-\\varepsilon_i)\\]\n\\[\\epsilon_i &gt; 0,  \\sum_{i=1}^n \\epsilon_i \\leq C\\]\n- 단순히 확장한 것에 불과하지 않은가???\n\nsupport vector classifier을 찾기 위해서는 관측치들간의 내적(inner product)을 계산하는 것으로 충분함이 알려져 있음\n이러한 내적을 여러 방면으로 일반화하여 표현할 수 있는데 이를 규정해 주는 함수를 kernel 이라 한다. (이 부분은 설명 생략!)\n\n- kernel 종류\n\npoly(다항), rbf(Radial Basis Function), sigmoid, linear\n\n\n\n- 걍 넘어가려고 했는데 안되겠음 \\(\\to\\) ㅅㅂ… 결과를 납득할 수 없다. 진짜 다시보자\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import svm\n\n# we create 40 separable points\nnp.random.seed(0)\nX = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\nY = [0] * 20 + [1] * 20\n\n# figure number\nfignum = 1\n\n# fit the model\nfor name, penalty in ((\"unreg\", 1), (\"reg\", 0.05)):\n    clf = svm.SVC(kernel=\"linear\", C=penalty)\n    clf.fit(X, Y)\n\n    # get the separating hyperplane\n    w = clf.coef_[0]\n    a = -w[0] / w[1]\n    xx = np.linspace(-5, 5)\n    yy = a * xx - (clf.intercept_[0]) / w[1]\n\n    # plot the parallels to the separating hyperplane that pass through the\n    # support vectors (margin away from hyperplane in direction\n    # perpendicular to hyperplane). This is sqrt(1+a^2) away vertically in\n    # 2-d.\n    margin = 1 / np.sqrt(np.sum(clf.coef_**2))\n    yy_down = yy - np.sqrt(1 + a**2) * margin\n    yy_up = yy + np.sqrt(1 + a**2) * margin\n\n    # plot the line, the points, and the nearest vectors to the plane\n    plt.figure(fignum, figsize=(4, 3))\n    plt.clf()\n    plt.plot(xx, yy, \"k-\")\n    plt.plot(xx, yy_down, \"k--\")\n    plt.plot(xx, yy_up, \"k--\")\n    plt.title(f\"C={penalty}\")\n    plt.scatter(\n        clf.support_vectors_[:, 0],\n        clf.support_vectors_[:, 1],\n        s=80,\n        facecolors=\"none\",\n        zorder=10,\n        edgecolors=\"k\",\n    )\n    plt.scatter(\n        X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.get_cmap(\"RdBu\"), edgecolors=\"k\"\n    )\n\n    plt.axis(\"tight\")\n    x_min = -4.8\n    x_max = 4.2\n    y_min = -6\n    y_max = 6\n\n    YY, XX = np.meshgrid(yy, xx)\n    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n    Z = clf.decision_function(xy).reshape(XX.shape)\n\n    # Put the result into a contour plot\n    plt.contourf(XX, YY, Z, cmap=plt.get_cmap(\"RdBu\"), alpha=0.5, linestyles=[\"-\"])\n\n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)\n    fignum = fignum + 1\n\nplt.show()"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-28-08. summary (1).html#k-fold-cross-validation",
    "href": "posts/DX/04. 머신러닝/2023-09-28-08. summary (1).html#k-fold-cross-validation",
    "title": "08. summary (1)",
    "section": "",
    "text": "- 우리가 기존의 하던 방식\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 2023)\n- 단점\n\n랜덤하게 자료를 분할하기 때문에 분할결과에 따라 추정의 변동성이 크다….\n특히 자료위 크기가 작거나 이상/영향치들이 포함되어 있는 경우에 더더욱 그러함.\n또한, 원 자료의 크기 보다 작은 집합의 훈련자료가 모형적합에 사용되기 때문에 test error가 과대추정될 수 있음.\n\n\n이러한 방법을 \\(\\text {Validation set Approach}\\)라고 한다….\n\n- K-fold Cross Validation 방식\n\n\n\n\n전체 자료를 \\(k\\)개의 집합으로 분할한 후그 중 하나의 집합 (\\(i\\))번째 집합을 평가자료롤 설정(위 그림의 경우 \\(i=1,2 \\dots 5\\))\n그 후 각 정확도를 평균냄 (교제에서는 \\(MSE\\)를 평균 냈는데, 이번 강의에서는 정확도를 평균내더라…)\n\n\\[\\text {CV}_{5} = \\frac {1}{5} \\sum_{i=1}^{5} \\text{Accuracy}_{i}\\]\n# 1단계: 불러오기\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# 2단계: 선언하기\nmodel = DecisionTreeClassifier(max_depth=3)\n\n# 3단계: 검증하기\ncv_score = cross_val_score(model, x_train, y_train, cv=10)\n\n# 확인\nprint(cv_score)\nprint(cv_score.mean())\n= 장점\n\n모든 데이터가 학습과 평가에 사용됨\n데이터가 부족해서 발생하는 과소적합 문제을 방지할 수 있음\n 좀 더 일반화된 모델 을 만들 수 있음\ntest error의 과대추정을 방지\n\n- 단점\n\n반복 횟수가 많아서 모델 학습과 평가에 많은 시간이 소요"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-28-08. summary (1).html#search",
    "href": "posts/DX/04. 머신러닝/2023-09-28-08. summary (1).html#search",
    "title": "08. summary (1)",
    "section": "",
    "text": "- 일단, 튜닝 시 모델들의 각 파라미터들에 값을 어떻게 하느냐에 따라 성능이 달라지는 것을 확인할 수 있었음\n- grid search의 아이디어는 가능한 파라미터 값 범위를 지정해 해당 범위에서의 값을 모두 사용하는 것이다.\n\n당연히 정확도는 높으나….시간이 오래 걸리겠지라는 생각을 해볼 수 있다.\n\n\n요런느낌\n\n# 함수 불러오기\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# 파라미터 선언\nparam = {'n_neighbors': range(1, 500, 10), 'metric': ['euclidean', 'manhattan']}\n\n# 기본모델 선언\nknn_model = KNeighborsClassifier()\n\n# Grid Search 선언\nmodel = GridSearchCV(knn_model, param, cv=3)\n\n\n\n- 그리드 서치처럼 파라미터 범위를 지정하는 것은 동일\n- 설정한 파라미터 값 범위에서 몇 개를 선택할지를 정하여 Random Search 모델 선언 후 학습\n- 학습 데이터에 대해 가장 좋은 성능을 보인 파라미터 값으로 자동 학습함.\n\n참고로 Grid Search, Random Search를 사용할 때 내부적으로 K-Fold Cross Validation을 위해 cv값을 지정하므로!\n실제 수행되는 횟수는  파라미터 조합 수 x CV값이 된다.\n\n\ncode\n\n# 함수 불러오기\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\n# 파라미터 선언\nparam = {'n_neighbors': range(1, 500, 10),\n'metric': ['euclidean', 'manhattan']}\n\n# 기본모델 선언\nknn_model = KNeighborsClassifier()\n# Random Search 선언\nmodel = RandomizedSearchCV(knn_model, \n                                                       param, cv=3, # (default = 5)\n                                                           n_iter=20) ### 전체 파라미터 범위에서 몇 개를 뽑을 것인지 (default = 10)\n\n또한, 두 가지 기법을 섞어서 사용할 수 있음! (강의자료 참고)"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-28-08. summary (1).html#클래스-불균형",
    "href": "posts/DX/04. 머신러닝/2023-09-28-08. summary (1).html#클래스-불균형",
    "title": "08. summary (1)",
    "section": "",
    "text": "- 머신러닝 알고리즘은 데이터가 클래스 간에 균형 있게 분포되어 있다고 가정함\n\nexample : 생존자와 사망자 수가 거의 같을 것이다~~\n\n- 클래스 불균형으로 인한 재현율이 형편없어지는 경우는 아래 링크를 참고!\n\n재현율\n\n\n\n- under sampling\n\n다수 클래스 데이터를 소수 클래스 수 만큼 랜덤 샘플링\n\n- over sampling\n\n소수의 클래스 데이터를 다수 클래스 수 만큼 랜덤 샘플링\n\n# pip install imbalanced-learn\n# 불러오기\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Under Sampling\nunder_sample = RandomUnderSampler()\nu_x_train, u_y_train = under_sample.fit_resample(x_train, y_train)\n\n# 확인\nprint('전:', np.bincount(y_train))\nprint('후:', np.bincount(u_y_train))\n\n# 불러오기\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Over Sampling\nover_sample = RandomOverSampler()\no_x_train, o_y_train = over_sample.fit_resample(x_train, y_train)\n\n# 확인\nprint('전:', np.bincount(y_train))\nprint('후:', np.bincount(o_y_train))\n- over sampling (2)\n# 불러오기\nfrom imblearn.over_sampling import SMOTE\n\n# Over Sampling\nsmote = SMOTE()\ns_x_train, s_y_train = smote.fit_resample(x_train, y_train)\n\n# 확인\nprint('전:', np.bincount(y_train))\nprint('후:', np.bincount(s_y_train))\n\n\n\n- resampling 없이 클래스에 가중치를 부여하여 클래스 불균형 문제를 해결해줌\n\n학습하는 동안 알고리즘의 비용함수에서 소수 클래스에 더 많은 가중치를 부여하여 소수 클래스에 더 높은 패널티를 제공함으로써, 소수 클래스에 대한 오류를 줄이게 됨\nsklearn에서 제공하는 알고리즘 대부분 class_weight라는 하이퍼 파라미터를 제공한다.\n\nclass_weight = ‘None’ : 기본값\nclass_weight = ‘balanced’: y_train의 class 비율을 역으로 적용\nclass_weight={0:0.2, 1:0.8}: 비율 지정, 단 비율의 합은 1\n\n주의! \\(\\to\\) 전반적인 성능을 높이기 위한 작업이 아니라, 소수 클래스 성능을 높이기 위한 작업임!"
  },
  {
    "objectID": "posts/DX/04. 머신러닝/2023-09-28-08. summary (1).html#excersise",
    "href": "posts/DX/04. 머신러닝/2023-09-28-08. summary (1).html#excersise",
    "title": "08. summary (1)",
    "section": "",
    "text": "# 라이브러리 불러오기\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\nwarnings.filterwarnings(action='ignore')\n%config InlineBackend.figure_format = 'retina'\n\n\n\n\n\n# 데이터 불러오기\npath = 'https://raw.githubusercontent.com/jangrae/csv/master/airline_satisfaction_small.csv'\ndata = pd.read_csv(path)\n\n\ndata.head()\n\n\n\n\n\n\n\n\nid\ngender\ncustomer_type\nage\ntype_of_travel\nclass\nflight_distance\ninflight_wifi_service\ndeparture/arrival_time_convenient\nease_of_online_booking\n...\ninflight_entertainment\non-board_service\nleg_room_service\nbaggage_handling\ncheckin_service\ninflight_service\ncleanliness\ndeparture_delay_in_minutes\narrival_delay_in_minutes\nsatisfaction\n\n\n\n\n0\n70172\nMale\nLoyal Customer\n13\nPersonal Travel\nEco Plus\n460\n3\n4\n3\n...\n5\n4\n3\n4\n4\n5\n5\n25\n18.0\n0\n\n\n1\n5047\nMale\ndisloyal Customer\n25\nBusiness travel\nBusiness\n235\n3\n2\n3\n...\n1\n1\n5\n3\n1\n4\n1\n1\n6.0\n0\n\n\n2\n110028\nFemale\nLoyal Customer\n26\nBusiness travel\nBusiness\n1142\n2\n2\n2\n...\n5\n4\n3\n4\n4\n4\n5\n0\n0.0\n1\n\n\n3\n24026\nFemale\nLoyal Customer\n25\nBusiness travel\nBusiness\n562\n2\n5\n5\n...\n2\n2\n5\n3\n1\n4\n2\n11\n9.0\n0\n\n\n4\n119299\nMale\nLoyal Customer\n61\nBusiness travel\nBusiness\n214\n3\n3\n3\n...\n3\n3\n4\n4\n3\n3\n3\n0\n0.0\n1\n\n\n\n\n5 rows × 24 columns\n\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2580 entries, 0 to 2579\nData columns (total 24 columns):\n #   Column                             Non-Null Count  Dtype  \n---  ------                             --------------  -----  \n 0   id                                 2580 non-null   int64  \n 1   gender                             2580 non-null   object \n 2   customer_type                      2580 non-null   object \n 3   age                                2580 non-null   int64  \n 4   type_of_travel                     2580 non-null   object \n 5   class                              2580 non-null   object \n 6   flight_distance                    2580 non-null   int64  \n 7   inflight_wifi_service              2580 non-null   int64  \n 8   departure/arrival_time_convenient  2580 non-null   int64  \n 9   ease_of_online_booking             2580 non-null   int64  \n 10  gate_location                      2580 non-null   int64  \n 11  food_and_drink                     2580 non-null   int64  \n 12  online_boarding                    2580 non-null   int64  \n 13  seat_comfort                       2580 non-null   int64  \n 14  inflight_entertainment             2580 non-null   int64  \n 15  on-board_service                   2580 non-null   int64  \n 16  leg_room_service                   2580 non-null   int64  \n 17  baggage_handling                   2580 non-null   int64  \n 18  checkin_service                    2580 non-null   int64  \n 19  inflight_service                   2580 non-null   int64  \n 20  cleanliness                        2580 non-null   int64  \n 21  departure_delay_in_minutes         2580 non-null   int64  \n 22  arrival_delay_in_minutes           2574 non-null   float64\n 23  satisfaction                       2580 non-null   int64  \ndtypes: float64(1), int64(19), object(4)\nmemory usage: 483.9+ KB\n\n\n- 쓸모없는 변수 제거\n\n# 변수 제거\nd_cols = [\"id\", \"departure/arrival_time_convenient\", \"gate_location\", \"departure_delay_in_minutes\"]\n\n\ndata.drop(d_cols,axis=1, inplace = True)\ndata.head()\n\n\n\n\n\n\n\n\ngender\ncustomer_type\nage\ntype_of_travel\nclass\nflight_distance\ninflight_wifi_service\nease_of_online_booking\nfood_and_drink\nonline_boarding\nseat_comfort\ninflight_entertainment\non-board_service\nleg_room_service\nbaggage_handling\ncheckin_service\ninflight_service\ncleanliness\narrival_delay_in_minutes\nsatisfaction\n\n\n\n\n0\nMale\nLoyal Customer\n13\nPersonal Travel\nEco Plus\n460\n3\n3\n5\n3\n5\n5\n4\n3\n4\n4\n5\n5\n18.0\n0\n\n\n1\nMale\ndisloyal Customer\n25\nBusiness travel\nBusiness\n235\n3\n3\n1\n3\n1\n1\n1\n5\n3\n1\n4\n1\n6.0\n0\n\n\n2\nFemale\nLoyal Customer\n26\nBusiness travel\nBusiness\n1142\n2\n2\n5\n5\n5\n5\n4\n3\n4\n4\n4\n5\n0.0\n1\n\n\n3\nFemale\nLoyal Customer\n25\nBusiness travel\nBusiness\n562\n2\n5\n2\n2\n2\n2\n2\n5\n3\n1\n4\n2\n9.0\n0\n\n\n4\nMale\nLoyal Customer\n61\nBusiness travel\nBusiness\n214\n3\n3\n4\n5\n5\n3\n3\n4\n4\n3\n3\n3\n0.0\n1\n\n\n\n\n\n\n\n- 결측치 확인\n\nprint(data.columns[data.isna().sum() !=0])\n\nIndex(['arrival_delay_in_minutes'], dtype='object')\n\n\n\ndata.arrival_delay_in_minutes.isna().sum()\n\n6\n\n\n- 현재 arrival_delay_in_minutes의 6개 결측치가 확인된다.\n\n결측치 제거를 위해 해당 변수의 분포를 확인하자.\n\n\ndata.plot(x=\"arrival_delay_in_minutes\",  kind = \"hist\", backend  = \"plotly\",\n                  width = 1000, height = 400,nbins=100)\n\n\n                                                \n\n\n- 대부분의 값들이 0값 근처에 몰려있으니 결측값을 0으로 대체하자.\n\ndata.arrival_delay_in_minutes.fillna(0, inplace = True)\nprint(data.columns[data.isna().sum() !=0])\n\nIndex([], dtype='object')\n\n\n- x, y 분리\n\ntarget = \"satisfaction\"\n\nx = data.drop(target, axis = 1)\ny = data[target]\n\n- 가변수화\n\nd = [\"gender\", \"customer_type\", \"type_of_travel\", \"class\"]\n\nx = pd.get_dummies(x, columns = d, drop_first = True, dtype = float)\nx.head()\n\n\n\n\n\n\n\n\nage\nflight_distance\ninflight_wifi_service\nease_of_online_booking\nfood_and_drink\nonline_boarding\nseat_comfort\ninflight_entertainment\non-board_service\nleg_room_service\nbaggage_handling\ncheckin_service\ninflight_service\ncleanliness\narrival_delay_in_minutes\ngender_Male\ncustomer_type_disloyal Customer\ntype_of_travel_Personal Travel\nclass_Eco\nclass_Eco Plus\n\n\n\n\n0\n13\n460\n3\n3\n5\n3\n5\n5\n4\n3\n4\n4\n5\n5\n18.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n1\n25\n235\n3\n3\n1\n3\n1\n1\n1\n5\n3\n1\n4\n1\n6.0\n1.0\n1.0\n0.0\n0.0\n0.0\n\n\n2\n26\n1142\n2\n2\n5\n5\n5\n5\n4\n3\n4\n4\n4\n5\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n25\n562\n2\n5\n2\n2\n2\n2\n2\n5\n3\n1\n4\n2\n9.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n61\n214\n3\n3\n4\n5\n5\n3\n3\n4\n4\n3\n3\n3\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n- 학습, 평가용 데이터 분리\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 1, test_size = 0.3)\n\n\n\n\n\n\nCode\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n# 1. tree\ntree = DecisionTreeClassifier(max_depth = 5, random_state = 1)\ntree.cv = cross_val_score(tree, x_train, y_train, cv = 5)\ntree.cv_m = tree.cv.mean()\n\n# 2. logistic\nfrom sklearn.linear_model import LogisticRegression\nlogit = LogisticRegression()\nlogit.cv = cross_val_score(logit, x_train, y_train, cv = 5)\nlogit.cv_m = logit.cv.mean()\n\n# 3. RF\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(max_depth = 5,  random_state = 1)\nrf.cv = cross_val_score(rf, x_train,y_train)\nrf.cv_m = rf.cv.mean()\n\n# 4. XGBoost\n\nfrom xgboost import XGBClassifier\nxgb =  XGBClassifier(max_depth = 5, random_state = 1)\nxgb.cv = cross_val_score(xgb, x_train, y_train, cv = 5)\nxgb.cv_m  = xgb.cv.mean()\n\n\n\nresult = [tree.cv_m, logit.cv_m, rf.cv_m,  xgb.cv_m]\nmodel = [\"tree\",\"logit\", \"rf\", \"xgb\"]\n\nfig = pd.DataFrame({\"model\" : model, \"result\" : result}).\\\n            sort_values(\"result\", ascending = False).plot(x = \"result\", y = \"model\", kind = \"bar\", backend = \"plotly\",color = \"model\")\n\nfig.update_xaxes(range = [0.7, 1.0])\n\n\n                                                \n\n\n- cv를 기준으로 XGB 모델이 최적의 모델인 것 같다.\n\ngrid search기법을 이용하여 모델 튜닝\n\n\nfrom sklearn.model_selection import GridSearchCV\nmodel = XGBClassifier(max_depth= 5, random_state = 1)\n\nparams=  {\"max_depth\" : range(1,21)}\n\nmodel = GridSearchCV(model,\n                     params,\n                     cv=5,\n                     scoring=\"accuracy\")\n\n\nmodel.fit(x_train, y_train)\n\nGridSearchCV(cv=5,\n             estimator=XGBClassifier(base_score=None, booster=None,\n                                     callbacks=None, colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None, device=None,\n                                     early_stopping_rounds=None,\n                                     enable_categorical=False, eval_metric=None,\n                                     feature_types=None, gamma=None,\n                                     grow_policy=None, importance_type=None,\n                                     interaction_constraints=None,\n                                     learning_rate=None, max_bin=None,\n                                     max_cat_threshold=None,\n                                     max_cat_to_onehot=None,\n                                     max_delta_step=None, max_depth=5,\n                                     max_leaves=None, min_child_weight=None,\n                                     missing=nan, monotone_constraints=None,\n                                     multi_strategy=None, n_estimators=None,\n                                     n_jobs=None, num_parallel_tree=None,\n                                     random_state=1, ...),\n             param_grid={'max_depth': range(1, 21)}, scoring='accuracy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=XGBClassifier(base_score=None, booster=None,\n                                     callbacks=None, colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None, device=None,\n                                     early_stopping_rounds=None,\n                                     enable_categorical=False, eval_metric=None,\n                                     feature_types=None, gamma=None,\n                                     grow_policy=None, importance_type=None,\n                                     interaction_constraints=None,\n                                     learning_rate=None, max_bin=None,\n                                     max_cat_threshold=None,\n                                     max_cat_to_onehot=None,\n                                     max_delta_step=None, max_depth=5,\n                                     max_leaves=None, min_child_weight=None,\n                                     missing=nan, monotone_constraints=None,\n                                     multi_strategy=None, n_estimators=None,\n                                     n_jobs=None, num_parallel_tree=None,\n                                     random_state=1, ...),\n             param_grid={'max_depth': range(1, 21)}, scoring='accuracy')estimator: XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=1, ...)XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=1, ...)\n\n\n\ny_pred = model.predict(x_test)\n\n\n# 예측 결과 확인\nprint(model.best_params_)\nprint(model.best_score_)\n\n{'max_depth': 3}\n0.9363202277283789\n\n\n\nfig = pd.DataFrame([model.best_estimator_.feature_names_in_,model.best_estimator_.feature_importances_]).T.\\\n        rename(columns = {0 : \"feature\", 1 : \"importance\"}).sort_values(\"importance\", ascending = False).\\\n            plot(y = \"feature\", x=  \"importance\", kind = \"barh\",\n                    backend = \"plotly\",color = \"feature\")\n\nfig.update_layout(showlegend = False)\n\n\n                                                \n\n\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nmeasure = [\"accuracy\", \"precision\", \"reacll\", \"f1-score\"]\n\nacc = accuracy_score(y_test, y_pred)\npre = precision_score(y_test, y_pred)\nre = recall_score(y_test, y_pred)\nf1_score = f1_score(y_test, y_pred)\n\n\npd.DataFrame({\"measure\" : measure, \"score\" : [acc, pre, re, f1_score]})\n\n\n\n\n\n\n\n\nmeasure\nscore\n\n\n\n\n0\naccuracy\n0.931525\n\n\n1\nprecision\n0.949102\n\n\n2\nreacll\n0.898017\n\n\n3\nf1-score\n0.922853"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-04-01. 딥러닝 (2).html",
    "href": "posts/DX/05. 딥러닝/2023-10-04-01. 딥러닝 (2).html",
    "title": "01. 딥러닝 (2)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\n\nimport plotly.express as px\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\ntnp.experimental_enable_numpy_behavior()"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-04-01. 딥러닝 (2).html#데이터-이해-및-정리",
    "href": "posts/DX/05. 딥러닝/2023-10-04-01. 딥러닝 (2).html#데이터-이해-및-정리",
    "title": "01. 딥러닝 (2)",
    "section": "(1) 데이터 이해 및 정리",
    "text": "(1) 데이터 이해 및 정리\n\npath = \"https://raw.githubusercontent.com/DA4BAM/dataset/master/Graduate_apply.csv\"\ndata = pd.read_csv(path)\ndata.head()\n\n\n  \n    \n\n\n\n\n\n\nadmit\ngre\ngpa\nrank\n\n\n\n\n0\n0\n380\n3.61\n3\n\n\n1\n1\n660\n3.67\n3\n\n\n2\n1\n800\n4.00\n1\n\n\n3\n1\n640\n3.19\n4\n\n\n4\n0\n520\n2.93\n4\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ntarget = 'admit'\nx = data.drop(target, axis=1)\ny = data.loc[:, target]\n\n\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=.2, random_state = 20)"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-04-01. 딥러닝 (2).html#ml-model",
    "href": "posts/DX/05. 딥러닝/2023-10-04-01. 딥러닝 (2).html#ml-model",
    "title": "01. 딥러닝 (2)",
    "section": "(2) ML model",
    "text": "(2) ML model\n\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(x_train, y_train)\nml_pred = model.predict(x_val)\n\n\nprint(confusion_matrix(y_val, ml_pred))\nprint('-'*50)\nprint(classification_report(y_val, ml_pred))\n\n[[48  7]\n [18  7]]\n--------------------------------------------------\n              precision    recall  f1-score   support\n\n           0       0.73      0.87      0.79        55\n           1       0.50      0.28      0.36        25\n\n    accuracy                           0.69        80\n   macro avg       0.61      0.58      0.58        80\nweighted avg       0.66      0.69      0.66        80"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-04-01. 딥러닝 (2).html#dl-model",
    "href": "posts/DX/05. 딥러닝/2023-10-04-01. 딥러닝 (2).html#dl-model",
    "title": "01. 딥러닝 (2)",
    "section": "(3) DL model",
    "text": "(3) DL model\n\n1) 전처리\n\nscaler = MinMaxScaler()\nx_train = scaler.fit_transform(x_train)\nx_val = scaler.fit_transform(x_val)\n\n\n\n2) 모델링\n\nnf = x_train.shape[1]\n\n- 현재 이거는 ouput layer 하나만 만든 것임\n\n또한, 분류 모델링이므로 ouput layer에 활성화 함수로 sigmoid 함수를 사용\n\n\nd_model = Sequential(Dense(1,input_shape = (nf,), activation = \"sigmoid\"))\n\nd_model.summary()\n\nModel: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_5 (Dense)             (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 4 (16.00 Byte)\nTrainable params: 4 (16.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nd_model.compile( optimizer = Adam(learning_rate = 0.01), loss = 'binary_crossentropy')\n\n- fit & predict\n\nimport tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\ndevice_name\n\n'/device:GPU:0'\n\n\n\nwith tf.device(\"/device:GPU:0\") :\n         d_model.fit(x_train, y_train, epochs = 50, validation_split = 0.2, verbose = 0)\n\n\ndl_pred = d_model.predict (x_val)\n\ndl_pred = np.where(dl_pred&gt;= 0.5, 1, 0)\n\n3/3 [==============================] - 0s 3ms/step\n\n\n\nprint(classification_report(dl_pred.reshape(-1), y_val))\n\n              precision    recall  f1-score   support\n\n           0       0.96      0.71      0.82        75\n           1       0.12      0.60      0.20         5\n\n    accuracy                           0.70        80\n   macro avg       0.54      0.65      0.51        80\nweighted avg       0.91      0.70      0.78        80"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-04-01. 딥러닝 (2).html#데이터-이해-및-정리-1",
    "href": "posts/DX/05. 딥러닝/2023-10-04-01. 딥러닝 (2).html#데이터-이해-및-정리-1",
    "title": "01. 딥러닝 (2)",
    "section": "(1) 데이터 이해 및 정리",
    "text": "(1) 데이터 이해 및 정리\n\npath = \"https://raw.githubusercontent.com/DA4BAM/dataset/master/titanic.3.csv\"\ndata = pd.read_csv(path)\ndata.drop(['Age_scale1', 'AgeGroup', 'SibSp','Parch' ], axis = 1, inplace = True)\ndata.head()\n\n\n  \n    \n\n\n\n\n\n\nSurvived\nPclass\nSex\nAge\nFare\nEmbarked\nFamily\n\n\n\n\n0\n0\n3\nmale\n22.0\n7.2500\nS\n2\n\n\n1\n1\n1\nfemale\n38.0\n71.2833\nC\n2\n\n\n2\n1\n3\nfemale\n26.0\n7.9250\nS\n1\n\n\n3\n1\n1\nfemale\n35.0\n53.1000\nS\n2\n\n\n4\n0\n3\nmale\n35.0\n8.0500\nS\n1\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- x,y 분리\n\ntarget = 'Survived'\nx = data.drop(target, axis = 1)\ny = data.loc[:, target]\n\n- 가변수화\n\ncat_cols = ['Pclass','Sex', 'Embarked']\nx = pd.get_dummies(x, columns = cat_cols, drop_first = True)\n\n- 데이터셋 분할\n\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=.3, random_state = 20)\n\n\nscaler = MinMaxScaler()\nx_train = scaler.fit_transform(x_train)\nx_val = scaler.transform(x_val)"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-04-01. 딥러닝 (2).html#모델링-1",
    "href": "posts/DX/05. 딥러닝/2023-10-04-01. 딥러닝 (2).html#모델링-1",
    "title": "01. 딥러닝 (2)",
    "section": "(2) 모델링",
    "text": "(2) 모델링\n\n1) 전체 feature + 은닉층 x\n\nnf = x_train.shape[1]\n\n\nmodel1 = Sequential(Dense(1, input_shape = (nf,), activation = \"sigmoid\"))\nmodel1.compile(optimizer = Adam(0.01), loss= tf.keras.losses.BinaryCrossentropy())\n\nmodel1.fit(x_train, y_train,\n                    epochs = 50, validation_split=0.2, verbose = 0)\n\n&lt;keras.src.callbacks.History at 0x7d3f65573eb0&gt;\n\n\n\nmodel1_pred = np.where(model1.predict(x_val)&gt;=0.5, 1,  0).reshape(-1)\n\n9/9 [==============================] - 0s 3ms/step\n\n\n\n\n2) 전체 feature + 은닉층 1개 추가\n\nmodel2 = Sequential(Dense(16, input_shape = (nf,), activation = \"relu\"))\n\nmodel2.add(Dense(1, activation = \"sigmoid\"))\nmodel2.compile(optimizer = Adam(0.01), loss= tf.keras.losses.BinaryCrossentropy())\nmodel2.fit(x_train, y_train,\n                    epochs = 50, validation_split=0.2, verbose = 0)\n\n&lt;keras.src.callbacks.History at 0x7d3f645c0ee0&gt;\n\n\n\nmodel2_pred = np.where(model2.predict(x_val)&gt;=0.5, 1,  0).reshape(-1)\n\n9/9 [==============================] - 0s 2ms/step\n\n\n\n\n3) 전체 feature + 은닉층 2개\n\nmodel3 = Sequential(Dense(16, input_shape = (nf,), activation = \"relu\"))\nmodel3.add(Dense(8, activation  = \"relu\"))\nmodel3.add(Dense(1, activation = \"sigmoid\"))\n\nmodel3.compile(optimizer = Adam(0.01), loss= tf.keras.losses.BinaryCrossentropy())\nmodel3.fit(x_train, y_train,\n                    epochs = 50, validation_split=0.2, verbose = 0)\n\n&lt;keras.src.callbacks.History at 0x7d3f64384dc0&gt;\n\n\n\nmodel3_pred = np.where(model3.predict(x_val)&gt;=0.5, 1,  0).reshape(-1)\n\n9/9 [==============================] - 0s 2ms/step\n\n\n\n\n4) summary (모델 비교)\n\nmodel3.summary()\n\nModel: \"sequential_20\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_28 (Dense)            (None, 16)                144       \n                                                                 \n dense_29 (Dense)            (None, 8)                 136       \n                                                                 \n dense_30 (Dense)            (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 289 (1.13 KB)\nTrainable params: 289 (1.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n\n\n\n\n\n\n\n\n\n변수 개수\n은닉층 수\n층별 output\n파라미터 수\n\n\n\n\nmodel1\n8\n0\n1\n(8 +1) = 9\n\n\nmodel2\n8\n1\n(16, 1)\n(8 x 16 +16) + (16 x 1 + 1) = (144 + 17) = 163\n\n\nmodel3\n8\n2\n(16, 8 ,1)\n(8 x 16 +16) + (16 x 8 + 8) + (8 x 1 + 1) = (144 + 136 + 9) = 289\n\n\n\n\nh1 = pd.DataFrame(model1.fit(x_train, y_train,\n                    epochs = 50, validation_split=0.2, verbose = 0).history)\nh2 = pd.DataFrame(model2.fit(x_train, y_train,\n                    epochs = 50, validation_split=0.2, verbose = 0).history)\nh3 = pd.DataFrame(model3.fit(x_train, y_train,\n                    epochs = 50, validation_split=0.2, verbose = 0).history)\n\n- 비교를 위해 결과를 합침\n\n[\"a\"*3,\"b\",\"c\"]\n\n['aaa', 'b', 'c']\n\n\n\ntemp = [\"model1\", \"model2\", \"model3\"]*50\ntemp.sort()\n\n\n\n\n\n\n\n\n\n\n\n\n\n변수 개수\n은닉층 수\n층별 output\nactivation\n파라미터 수\n\n\n\n\nmodel1\n8\n0\n1\nsigmoid\n(8 +1) = 9\n\n\nmodel2\n8\n1\n(16, 1)\nrelu, sigmoid\n(8 x 16 +16) + (16 x 1 + 1) = (144 + 17) = 163\n\n\nmodel3\n8\n2\n(16, 8 ,1)\nrelu, relu, sigmoid\n(8 x 16 +16) + (16 x 8 + 8) + (8 x 1 + 1) = (144 + 136 + 9) = 289\n\n\n\n\n\nCode\nfig,axes = plt.subplots(1,3, figsize = (12, 4))\n\nax1, ax2, ax3 = axes\n\nax1.plot(h1[\"loss\"],label = \"train_loss\")\nax1.plot(h1[\"val_loss\"],label = \"val_loss\")\nax1.set_title(\"model1 train and val error\")\nax1.legend()\nax1.set_ylim(0.3, 0.5)\n\nax2.plot(h2[\"loss\"],label = \"train_loss\")\nax2.plot(h2[\"val_loss\"],label = \"val_loss\")\nax2.set_title(\"model2 train and val error\")\nax2.legend()\nax2.set_ylim(0.3, 0.5)\n\n\nax3.plot(h3[\"loss\"],label = \"train_loss\")\nax3.plot(h3[\"val_loss\"],label = \"val_loss\")\nax3.set_title(\"model3 train and val error\")\nax3.legend()\nax3.set_ylim(0.3, 0.5)\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n- 루프로 구현\n\n\nCode\nfig,axes = plt.subplots(1,3, figsize = (12, 4))\n\nax1, ax2, ax3 = axes\n\nfor i in range(1,4) :\n       exec(f'ax{i}.plot(h{i}[\"loss\"],label = \"train_loss\")')\n       exec(f'ax{i}.plot(h{i}[\"val_loss\"],label = \"val_loss\")')\n       exec(f'ax{i}.set_title(\"model{i} train & val error\")')\n       exec(f\"ax{i}.legend()\")\n       exec(f\"ax{i}.set_ylim(0.3, 0.5)\")\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n- 예측 성능 비교\n\nprint(np.mean(y_val == model1_pred))\nprint(np.mean(y_val == model2_pred))\nprint(np.mean(y_val == model3_pred))\n\n0.7723880597014925\n0.7835820895522388\n0.7873134328358209"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-04-01. 딥러닝 (2).html#데이터-이해-및-준비",
    "href": "posts/DX/05. 딥러닝/2023-10-04-01. 딥러닝 (2).html#데이터-이해-및-준비",
    "title": "01. 딥러닝 (2)",
    "section": "(1) 데이터 이해 및 준비",
    "text": "(1) 데이터 이해 및 준비\n\n# data data\npath = \"https://raw.githubusercontent.com/DA4BAM/dataset/master/Attrition_train_validation.CSV\"\ndata = pd.read_csv(path)\ndata['Attrition'] = np.where(data['Attrition']=='Yes', 1, 0)\ndata.head(5)\n\n\n  \n    \n\n\n\n\n\n\nAttrition\nAge\nBusinessTravel\nDepartment\nDistanceFromHome\nEducation\nEducationField\nEmployeeNumber\nEnvironmentSatisfaction\nGender\n...\nOverTime\nPercentSalaryHike\nRelationshipSatisfaction\nStockOptionLevel\nTotalWorkingYears\nTrainingTimesLastYear\nWorkLifeBalance\nYearsAtCompany\nYearsInCurrentRole\nYearsWithCurrManager\n\n\n\n\n0\n0\n33\nTravel_Rarely\nResearch & Development\n7\n3\nMedical\n817\n3\nMale\n...\nNo\n11\n4\n0\n14\n3\n4\n13\n9\n7\n\n\n1\n0\n35\nTravel_Frequently\nResearch & Development\n18\n2\nLife Sciences\n1412\n3\nMale\n...\nNo\n11\n3\n0\n10\n2\n3\n2\n2\n2\n\n\n2\n0\n42\nTravel_Rarely\nResearch & Development\n6\n3\nMedical\n1911\n3\nMale\n...\nNo\n13\n2\n1\n18\n3\n4\n13\n7\n7\n\n\n3\n0\n46\nTravel_Rarely\nSales\n2\n3\nMarketing\n1204\n3\nFemale\n...\nNo\n23\n1\n0\n28\n2\n3\n26\n15\n9\n\n\n4\n0\n39\nTravel_Frequently\nSales\n20\n3\nLife Sciences\n1812\n3\nMale\n...\nNo\n18\n4\n1\n7\n6\n3\n2\n1\n2\n\n\n\n\n\n5 rows × 26 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1250 entries, 0 to 1249\nData columns (total 26 columns):\n #   Column                    Non-Null Count  Dtype \n---  ------                    --------------  ----- \n 0   Attrition                 1250 non-null   int64 \n 1   Age                       1250 non-null   int64 \n 2   BusinessTravel            1250 non-null   object\n 3   Department                1250 non-null   object\n 4   DistanceFromHome          1250 non-null   int64 \n 5   Education                 1250 non-null   int64 \n 6   EducationField            1250 non-null   object\n 7   EmployeeNumber            1250 non-null   int64 \n 8   EnvironmentSatisfaction   1250 non-null   int64 \n 9   Gender                    1250 non-null   object\n 10  JobInvolvement            1250 non-null   int64 \n 11  JobRole                   1250 non-null   object\n 12  JobSatisfaction           1250 non-null   int64 \n 13  MaritalStatus             1250 non-null   object\n 14  MonthlyIncome             1250 non-null   int64 \n 15  NumCompaniesWorked        1250 non-null   int64 \n 16  OverTime                  1250 non-null   object\n 17  PercentSalaryHike         1250 non-null   int64 \n 18  RelationshipSatisfaction  1250 non-null   int64 \n 19  StockOptionLevel          1250 non-null   int64 \n 20  TotalWorkingYears         1250 non-null   int64 \n 21  TrainingTimesLastYear     1250 non-null   int64 \n 22  WorkLifeBalance           1250 non-null   int64 \n 23  YearsAtCompany            1250 non-null   int64 \n 24  YearsInCurrentRole        1250 non-null   int64 \n 25  YearsWithCurrManager      1250 non-null   int64 \ndtypes: int64(19), object(7)\nmemory usage: 254.0+ KB\n\n\n- x,y 분리\n\ntarget = \"Attrition\"\n\ndata.drop(\"EmployeeNumber\", axis = 1, inplace =True)\n\n\nx = data.drop(target, axis = 1)\ny = data[target]\n\n- 가변수화\n\ndum_cols = ['BusinessTravel','Department','Education','EducationField','EnvironmentSatisfaction','Gender',\n            'JobRole', 'JobInvolvement', 'JobSatisfaction', 'MaritalStatus', 'OverTime', 'RelationshipSatisfaction',\n            'StockOptionLevel','WorkLifeBalance' ]\nx = pd.get_dummies(x, columns = dum_cols ,drop_first = True)\nx.head()\n\n\n  \n    \n\n\n\n\n\n\nAge\nDistanceFromHome\nMonthlyIncome\nNumCompaniesWorked\nPercentSalaryHike\nTotalWorkingYears\nTrainingTimesLastYear\nYearsAtCompany\nYearsInCurrentRole\nYearsWithCurrManager\n...\nOverTime_Yes\nRelationshipSatisfaction_2\nRelationshipSatisfaction_3\nRelationshipSatisfaction_4\nStockOptionLevel_1\nStockOptionLevel_2\nStockOptionLevel_3\nWorkLifeBalance_2\nWorkLifeBalance_3\nWorkLifeBalance_4\n\n\n\n\n0\n33\n7\n11691\n0\n11\n14\n3\n13\n9\n7\n...\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n1\n35\n18\n9362\n2\n11\n10\n2\n2\n2\n2\n...\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n\n\n2\n42\n6\n13348\n9\n13\n18\n3\n13\n7\n7\n...\n0\n1\n0\n0\n1\n0\n0\n0\n0\n1\n\n\n3\n46\n2\n17048\n8\n23\n28\n2\n26\n15\n9\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n4\n39\n20\n4127\n2\n18\n7\n6\n2\n1\n2\n...\n0\n0\n0\n1\n1\n0\n0\n0\n1\n0\n\n\n\n\n\n5 rows × 53 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- 데이터셋 분할\n\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 200, random_state = 2022)\n\n- 스케일링\n\nscaler = MinMaxScaler()\nx_train = scaler.fit_transform(x_train)\nx_val = scaler.transform(x_val)"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-04-01. 딥러닝 (2).html#모델링-2",
    "href": "posts/DX/05. 딥러닝/2023-10-04-01. 딥러닝 (2).html#모델링-2",
    "title": "01. 딥러닝 (2)",
    "section": "(2) 모델링",
    "text": "(2) 모델링\n- 아래와 같이 3개의 모델을 만들자.\n\n\n\n\n변수 개수\n은닉층 수\n층별 output\nactivation\n\n\n\n\nmodel1\n53\n0\n1\nsigmoid\n\n\nmodel2\n53\n1\n(16, 1)\nrelu, sigmoid\n\n\nmodel3\n53\n2\n(16, 8 ,1)\nrelu, relu, sigmoid\n\n\n\n\nnf = x_train.shape[1]\nnf\n\n53\n\n\n= 모델 셋팅\n\n# model1\nmodel1 = Sequential()\nmodel1.add(Dense(1, input_shape = (nf,), activation = \"sigmoid\"))\nmodel1.compile(optimizer = Adam(0.01), loss = tf.keras.losses.BinaryCrossentropy())\n\n# model2\nmodel2 = Sequential()\nmodel2.add(Dense(16, input_shape = (nf,), activation = \"relu\"))\nmodel2.add(Dense(1, input_shape = (nf,), activation = \"sigmoid\"))\nmodel2.compile(optimizer = Adam(0.01), loss = tf.keras.losses.BinaryCrossentropy())\n\n# model2\nmodel3 = Sequential()\nmodel3.add(Dense(16, input_shape = (nf,), activation = \"relu\"))\nmodel3.add(Dense(8, input_shape = (nf,), activation = \"relu\"))\nmodel3.add(Dense(1, input_shape = (nf,), activation = \"sigmoid\"))\nmodel3.compile(optimizer = Adam(0.01), loss = tf.keras.losses.BinaryCrossentropy())\n\n- 모댈 fit\n\nfor i in range(1,4) :\n       exec(f\"h{i} = pd.DataFrame(model{i}.fit(x_train, y_train, epochs = 50, validation_split = 0.2,  verbose = 0).history)\")\n       exec(f\"h{i}['epochs'] = list(range(1,51))\")\n\n\nh1[\"model\"] = \"model1\"\nh2[\"model\"] = \"model2\"\nh3[\"model\"] = \"model3\"\n\n- train, val error 시각화\n\n\n\n\n변수 개수\n은닉층 수\n층별 output\nactivation\n\n\n\n\nmodel1\n53\n0\n1\nsigmoid\n\n\nmodel2\n53\n1\n(16, 1)\nrelu, sigmoid\n\n\nmodel3\n53\n2\n(16, 8 ,1)\nrelu, relu, sigmoid\n\n\n\n\n\nCode\nfig, axes = plt.subplots(1,3, figsize = (12, 4))\n\nax1, ax2, ax3 = axes\n\nfor i in range(1,4) :\n      exec(f\"ax{i}.plot(h{i}['loss'],label = 'train_loss')\")\n      exec(f\"ax{i}.plot(h{i}['val_loss'],label = 'val_loss')\")\n      exec(f\"ax{i}.legend(loc = 'upper left')\")\n      exec(f\"ax{i}.set_title('model{i} train & val error')\")\n      exec(f\"ax{i}.set_ylim(0,1.5)\")\nfig.tight_layout()\nfig.show()\n\n\n\n\n\n- test error 시각화\n\nfor i in range(1,4) :\n    exec(f\"p{i} = np.where(model{i}.predict(x_val)&gt;= 0.5 , 1, 0).reshape(-1)\")\n\n7/7 [==============================] - 0s 3ms/step\n7/7 [==============================] - 0s 3ms/step\n7/7 [==============================] - 0s 3ms/step\n\n\n\nmodel = [\"model1\", \"model2\", \"model3\"]\nacc1 = [np.mean(y_val == p1),np.mean(y_val == p2),np.mean(y_val == p3)]\npre = [precision_score(y_val, p1), precision_score(y_val, p2),precision_score(y_val, p3)]\nre = [recall_score(y_val, p1), recall_score(y_val, p2),recall_score(y_val, p3)]\nf1= [f1_score(y_val, p1), f1_score(y_val, p2),f1_score(y_val, p3)]\n\n\n\nCode\npd.DataFrame({\"model\" : model,\n                            \"acc\" : acc1,\n                            \"precision\" : pre,\n                            \"recall\" : re,\n                            \"f1_score\" : f1}).melt(id_vars = \"model\",\n                                                                  var_name = \"measure\").plot(x  = \"measure\", y = \"value\", facet_col = \"model\",\n                                                                                                                    kind = \"bar\", backend = \"plotly\", color = \"measure\", text_auto = True)"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-06-03. 딥러닝 (4).html",
    "href": "posts/DX/05. 딥러닝/2023-10-06-03. 딥러닝 (4).html",
    "title": "03. 딥러닝 (4)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random as rd\nimport cv2, os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\nfrom keras.backend import clear_session\nfrom keras.optimizers import Adam\nfrom keras.datasets import mnist, fashion_mnist\n\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\ntnp.experimental_enable_numpy_behavior()\n\n\n# 학습곡선 함수\ndef dl_history_plot(history):\n    plt.figure(figsize=(12,4))\n    plt.plot(history['loss'], label='train_err', marker = ',')\n    plt.plot(history['val_loss'], label='val_err', marker = ',')\n\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend()\n    plt.grid()\n    plt.show()\n\n\ntf.config.experimental.list_physical_devices(\"GPU\")\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-06-03. 딥러닝 (4).html#가벼운-실습",
    "href": "posts/DX/05. 딥러닝/2023-10-06-03. 딥러닝 (4).html#가벼운-실습",
    "title": "03. 딥러닝 (4)",
    "section": "가벼운 실습",
    "text": "가벼운 실습\n- 이미지의 각 영역에 대한 특징들을 추출하는 것임\n- 아래 그림에서 4가지만 기억하자\n\ninput shape : [가로 x 세로 x 채널(흑백이면 1, 컬러면 3)]\nConvolution layer : 이미지의 지역적인 특성들을 추출\nMax pooling layer : 뽑는 특징을 요약 (최대값, 필터)\nFlatten : Dense layer에 연결하기 위해 펼침\n\n\n\n(x_train, y_train), (x_val, y_val) = mnist.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11490434/11490434 [==============================] - 0s 0us/step\n\n\n\nx_train = x_train/255\nx_val = x_val/255\n\n\nx_train.shape\n\n(60000, 28, 28)\n\n\n\n\n\n- 위 모델을 설계해보자\n\nclear_session()\nmodel3 = Sequential([\n                    Conv2D(32, kernel_size = (3, 3), input_shape =(28, 28, 1), padding='same', activation =  \"relu\"), ## 지역적인 특징 32개 도출\n                     MaxPooling2D(pool_size = (2, 2), strides=2), ## 특징이 너무 많으니 좀 줄여\n                     Conv2D (64, kernel_size = (3, 3), padding='same', activation = \"relu\" ), ## 다시, 특징 64개 추출\n                     MaxPooling2D (pool_size = (2, 2), strides=2), ## 다시 반으로 줄임\n                     Flatten (), ## Dense에 전달하기 위해 펼침\n                     Dense(128, activation =  \"relu\"), ## 128개의 node를 가진 Dense\n                     Dense(10, activation = \"softmax\") ## output =10, target class가 총 10개 이므로!\n])\n\nmodel3.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 28, 28, 32)        320       \n                                                                 \n max_pooling2d (MaxPooling2  (None, 14, 14, 32)        0         \n D)                                                              \n                                                                 \n conv2d_1 (Conv2D)           (None, 14, 14, 64)        18496     \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 7, 7, 64)          0         \n g2D)                                                            \n                                                                 \n flatten (Flatten)           (None, 3136)              0         \n                                                                 \n dense (Dense)               (None, 128)               401536    \n                                                                 \n dense_1 (Dense)             (None, 10)                1290      \n                                                                 \n=================================================================\nTotal params: 421642 (1.61 MB)\nTrainable params: 421642 (1.61 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n- 학습\n\nmodel3.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy')\n\n\nhistory = model3.fit(x_train, y_train, epochs = 10, validation_split=0.2).history\n\nEpoch 1/10\n1500/1500 [==============================] - 25s 6ms/step - loss: 0.1369 - val_loss: 0.0552\nEpoch 2/10\n1500/1500 [==============================] - 6s 4ms/step - loss: 0.0448 - val_loss: 0.0453\nEpoch 3/10\n1500/1500 [==============================] - 6s 4ms/step - loss: 0.0305 - val_loss: 0.0381\nEpoch 4/10\n1500/1500 [==============================] - 6s 4ms/step - loss: 0.0216 - val_loss: 0.0352\nEpoch 5/10\n1500/1500 [==============================] - 5s 4ms/step - loss: 0.0169 - val_loss: 0.0358\nEpoch 6/10\n1500/1500 [==============================] - 7s 5ms/step - loss: 0.0123 - val_loss: 0.0419\nEpoch 7/10\n1500/1500 [==============================] - 6s 4ms/step - loss: 0.0105 - val_loss: 0.0417\nEpoch 8/10\n1500/1500 [==============================] - 6s 4ms/step - loss: 0.0088 - val_loss: 0.0401\nEpoch 9/10\n1500/1500 [==============================] - 5s 4ms/step - loss: 0.0061 - val_loss: 0.0587\nEpoch 10/10\n1500/1500 [==============================] - 6s 4ms/step - loss: 0.0066 - val_loss: 0.0465\n\n\n- train-loss 시각화\n\nplt.figure(figsize = (12, 4))\nplt.plot(history[\"loss\"], \"-.r\", label = \"train_loss\", alpha = 0.3)\nplt.plot(history[\"val_loss\"], \"-.b\", label = \"val_loss\", alpha = 0.3)\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-06-03. 딥러닝 (4).html#모델링-1",
    "href": "posts/DX/05. 딥러닝/2023-10-06-03. 딥러닝 (4).html#모델링-1",
    "title": "03. 딥러닝 (4)",
    "section": "모델링 1",
    "text": "모델링 1\n\n\n\nLayer (type)\nOutput Shape\nParam #\n\n\n\n\nconv2d (Conv2D)\n(None, 28, 28, 16)\n160\n\n\nmax_pooling2d\n(None, 14, 14, 16)\n0\n\n\nflatten (Flatten)\n(None, 3136)\n0\n\n\ndense (Dense)\n(None, 128)\n401536\n\n\ndense_1 (Dense)\n(None, 10)\n1290\n\n\n\n\nclear_session()\n\nmodel = Sequential([Conv2D(16, kernel_size=(3, 3), input_shape=(28, 28, 1), padding='same', strides = 1, activation=\"relu\"  ),\n                    MaxPooling2D(pool_size=(2, 2), strides=2),\n                    Flatten(),\n                    Dense( 128, activation = \"relu\"),\n                    Dense(10, activation = \"softmax\" )\n])\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 28, 28, 16)        160       \n                                                                 \n max_pooling2d (MaxPooling2  (None, 14, 14, 16)        0         \n D)                                                              \n                                                                 \n flatten (Flatten)           (None, 3136)              0         \n                                                                 \n dense (Dense)               (None, 128)               401536    \n                                                                 \n dense_1 (Dense)             (None, 10)                1290      \n                                                                 \n=================================================================\nTotal params: 402986 (1.54 MB)\nTrainable params: 402986 (1.54 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy')\n\nhistory = model.fit(x_train, y_train, epochs = 10,\n                    validation_split=0.2).history\n\nEpoch 1/10\n1500/1500 [==============================] - 6s 3ms/step - loss: 0.2019 - val_loss: 0.0923\nEpoch 2/10\n1500/1500 [==============================] - 6s 4ms/step - loss: 0.0684 - val_loss: 0.0688\nEpoch 3/10\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0427 - val_loss: 0.0589\nEpoch 4/10\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0296 - val_loss: 0.0525\nEpoch 5/10\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0206 - val_loss: 0.0571\nEpoch 6/10\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0149 - val_loss: 0.0538\nEpoch 7/10\n1500/1500 [==============================] - 5s 4ms/step - loss: 0.0107 - val_loss: 0.0614\nEpoch 8/10\n1500/1500 [==============================] - 6s 4ms/step - loss: 0.0074 - val_loss: 0.0704\nEpoch 9/10\n1500/1500 [==============================] - 5s 4ms/step - loss: 0.0070 - val_loss: 0.0637\nEpoch 10/10\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0058 - val_loss: 0.0641\n\n\n\ndl_history_plot(history)\n\n\n\n\n\npred = model.predict(x_val)\npred_1 = pred.argmax(axis=1)\n\n313/313 [==============================] - 1s 2ms/step\n\n\n\nprint(accuracy_score(y_val,pred_1))\nprint('-'*60)\nprint(classification_report(y_val, pred_1))\n\n0.9866\n------------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99       980\n           1       0.99      0.99      0.99      1135\n           2       0.98      0.98      0.98      1032\n           3       0.98      0.99      0.99      1010\n           4       0.98      0.99      0.99       982\n           5       0.99      0.98      0.99       892\n           6       0.99      0.99      0.99       958\n           7       0.99      0.98      0.98      1028\n           8       0.99      0.98      0.99       974\n           9       0.99      0.98      0.98      1009\n\n    accuracy                           0.99     10000\n   macro avg       0.99      0.99      0.99     10000\nweighted avg       0.99      0.99      0.99     10000"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-06-03. 딥러닝 (4).html#모델링-2",
    "href": "posts/DX/05. 딥러닝/2023-10-06-03. 딥러닝 (4).html#모델링-2",
    "title": "03. 딥러닝 (4)",
    "section": "모델링 2",
    "text": "모델링 2\n\n\n\nLayer (type)\nOutput Shape\nParam #\n\n\n\n\nconv2d (Conv2D)\n(None, 28, 28, 32)\n320\n\n\nmax_pooling2d\n(None, 14, 14, 32)\n0\n\n\nflatten (Flatten)\n(None, 6272)\n0\n\n\ndense (Dense)\n(None, 128)\n802944\n\n\ndense_1 (Dense)\n(None, 10)\n1290\n\n\n\n\nclear_session()\nmodel = Sequential([ Conv2D(32, kernel_size  =(3, 3), input_shape = (28, 28, 1), padding='same', strides = 1, activation = \"relu\"  ),\n                     MaxPooling2D(pool_size =(2, 2), strides=2),\n                     Flatten (),\n                     Dense(128, activation = \"relu\" ),\n                     Dense(10, activation= \"softmax\")\n])\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 28, 28, 32)        320       \n                                                                 \n max_pooling2d (MaxPooling2  (None, 14, 14, 32)        0         \n D)                                                              \n                                                                 \n flatten (Flatten)           (None, 6272)              0         \n                                                                 \n dense (Dense)               (None, 128)               802944    \n                                                                 \n dense_1 (Dense)             (None, 10)                1290      \n                                                                 \n=================================================================\nTotal params: 804554 (3.07 MB)\nTrainable params: 804554 (3.07 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy')\n\nhistory = model.fit(x_train, y_train, epochs = 10,\n                    validation_split=0.2).history\n\nEpoch 1/10\n1500/1500 [==============================] - 6s 3ms/step - loss: 0.1732 - val_loss: 0.0848\nEpoch 2/10\n1500/1500 [==============================] - 5s 4ms/step - loss: 0.0571 - val_loss: 0.0609\nEpoch 3/10\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0370 - val_loss: 0.0557\nEpoch 4/10\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0234 - val_loss: 0.0528\nEpoch 5/10\n1500/1500 [==============================] - 5s 4ms/step - loss: 0.0167 - val_loss: 0.0600\nEpoch 6/10\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0119 - val_loss: 0.0634\nEpoch 7/10\n1500/1500 [==============================] - 5s 4ms/step - loss: 0.0089 - val_loss: 0.0663\nEpoch 8/10\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0063 - val_loss: 0.0588\nEpoch 9/10\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0059 - val_loss: 0.0903\nEpoch 10/10\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0047 - val_loss: 0.0744\n\n\n\ndl_history_plot(history)\n\n\n\n\n\npred = model.predict(x_val)\npred_1 = pred.argmax(axis=1)\n\n313/313 [==============================] - 1s 2ms/step\n\n\n\nprint(accuracy_score(y_val,pred_1))\nprint('-'*60)\nprint(confusion_matrix(y_val, pred_1))\nprint('-'*60)\nprint(classification_report(y_val, pred_1))\n\n0.9844\n------------------------------------------------------------\n[[ 976    0    0    1    0    0    3    0    0    0]\n [   0 1133    2    0    0    0    0    0    0    0]\n [   2    7 1019    1    1    0    1    1    0    0]\n [   1    0    3 1001    0    2    0    1    1    1]\n [   0    3    0    0  976    0    2    0    0    1]\n [   3    0    1   14    0  870    3    0    1    0]\n [   5    4    1    1    4    2  939    0    2    0]\n [   1   10   11    1    0    0    0 1000    5    0]\n [   5    1    2    2    1    0    1    1  958    3]\n [   4    4    0    3   17    1    0    5    3  972]]\n------------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99       980\n           1       0.98      1.00      0.99      1135\n           2       0.98      0.99      0.98      1032\n           3       0.98      0.99      0.98      1010\n           4       0.98      0.99      0.99       982\n           5       0.99      0.98      0.98       892\n           6       0.99      0.98      0.98       958\n           7       0.99      0.97      0.98      1028\n           8       0.99      0.98      0.99       974\n           9       0.99      0.96      0.98      1009\n\n    accuracy                           0.98     10000\n   macro avg       0.98      0.98      0.98     10000\nweighted avg       0.98      0.98      0.98     10000"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-06-03. 딥러닝 (4).html#틀린그림-찾아보기",
    "href": "posts/DX/05. 딥러닝/2023-10-06-03. 딥러닝 (4).html#틀린그림-찾아보기",
    "title": "03. 딥러닝 (4)",
    "section": "틀린그림 찾아보기",
    "text": "틀린그림 찾아보기\n\n\n\n- 위와 같이 설계한 모델을 가지고 틀린그림을 찾아보자.\n- 코드를 이해하기보단 그냥 사용하자..\n\npred_1 = model3.predict(x_val).argmax(axis=1)\n\n313/313 [==============================] - 1s 2ms/step\n\n\n\nidx = (y_val != pred_1)\nx_val_wr = x_val[idx]\ny_val_wr = y_val[idx]\npred_wr = pred_1[idx]\n\nx_val_wr = x_val_wr.reshape(-1,28,28)\nprint(x_val_wr.shape)\n\n(94, 28, 28)\n\n\n\nidx = rd.sample(range(x_val_wr.shape[0]),25)\nx_temp = x_val_wr[idx]\ny_temp = y_val_wr[idx]\np_temp = pred_wr[idx]\n\nplt.figure(figsize=(14,4))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(x_temp[i], cmap=plt.cm.binary)\n    plt.xlabel(f'actual : {y_temp[i]},  predict : {p_temp[i]}')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-06-03. 딥러닝 (4).html#모델-저장하기",
    "href": "posts/DX/05. 딥러닝/2023-10-06-03. 딥러닝 (4).html#모델-저장하기",
    "title": "03. 딥러닝 (4)",
    "section": "모델 저장하기",
    "text": "모델 저장하기\n- model save\n\nmodel3.save('mnist_model.h5')\n\n/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n\n\n- 저장된 경로 : /content/mnist_model.h5\n\n드라이브 마운트를 안해서… 드라이브에선 저장된 모델을 찾을 수 없다.. 다음부턴 드라이브 마운트를 꼭하자!\n\n- 다시 불러와서 저장\n\nfrom keras.models import load_model\nm3 = load_model('mnist_model.h5')\n\n- 예측\n\npred = m3.predict(x_val)\npred_1 = pred.argmax(axis=1)\n\n313/313 [==============================] - 1s 2ms/step\n\n\n- 결과 report\n\nprint(accuracy_score(y_val,pred_1))\nprint('-'*60)\nprint(classification_report(y_val, pred_1))\n\n0.9906\n------------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99       980\n           1       0.99      1.00      0.99      1135\n           2       1.00      0.99      0.99      1032\n           3       0.99      1.00      0.99      1010\n           4       0.99      0.99      0.99       982\n           5       0.98      0.99      0.99       892\n           6       1.00      0.98      0.99       958\n           7       0.98      0.99      0.99      1028\n           8       1.00      0.99      0.99       974\n           9       0.99      0.98      0.98      1009\n\n    accuracy                           0.99     10000\n   macro avg       0.99      0.99      0.99     10000\nweighted avg       0.99      0.99      0.99     10000"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-06-03. 딥러닝 (4).html#데이터-이해-및-정리",
    "href": "posts/DX/05. 딥러닝/2023-10-06-03. 딥러닝 (4).html#데이터-이해-및-정리",
    "title": "03. 딥러닝 (4)",
    "section": "(1) 데이터 이해 및 정리",
    "text": "(1) 데이터 이해 및 정리\n\ndata = pd.read_csv('https://raw.githubusercontent.com/DA4BAM/dataset/master/temperature.csv')\ndata.head(10)\n\n\n  \n    \n\n\n\n\n\n\nyear\nweek\nAvgTemp\n\n\n\n\n0\n2010\n1\n-3.000000\n\n\n1\n2010\n2\n-7.500000\n\n\n2\n2010\n3\n-7.900000\n\n\n3\n2010\n4\n-2.357143\n\n\n4\n2010\n5\n-3.342857\n\n\n5\n2010\n6\n-1.800000\n\n\n6\n2010\n7\n-0.314286\n\n\n7\n2010\n8\n-2.142857\n\n\n8\n2010\n9\n4.400000\n\n\n9\n2010\n10\n7.057143\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- 온도의 흐름을 살펴보자!\n\nplt.figure(figsize = (12,4))\nplt.plot(data.AvgTemp)\nplt.grid()\nplt.show()\n\n\n\n\n- y만들기\n\n우리가 하려는거 저번주 온도를 가지고 그 다음주 온도를 예측하는것!!! \\(\\to\\) shift 함수를 이용!\n\n\ndata['y'] = data['AvgTemp'].shift(-1) ##\ndata.dropna(axis = 0, inplace = True)\ndata.head()\n\n\n  \n    \n\n\n\n\n\n\nyear\nweek\nAvgTemp\ny\n\n\n\n\n0\n2010\n1\n-3.000000\n-7.500000\n\n\n1\n2010\n2\n-7.500000\n-7.900000\n\n\n2\n2010\n3\n-7.900000\n-2.357143\n\n\n3\n2010\n4\n-2.357143\n-3.342857\n\n\n4\n2010\n5\n-3.342857\n-1.800000\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- x, y 분리\n\nx = data.loc[:, ['AvgTemp']]\ny = data.loc[:,'y']\n\n- 스케일링\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nx = scaler.fit_transform(x)\n\n- 3차원 구조 만들기\n\nx2, y2 = temporalize(x, y, 4)\nx2.shape, y2.shape\n\n((260, 4, 1), (260,))\n\n\n\nx2[:2]\n\narray([[[0.15106693],\n        [0.0364937 ],\n        [0.02630941],\n        [0.16743453]],\n\n       [[0.0364937 ],\n        [0.02630941],\n        [0.16743453],\n        [0.14233754]]])\n\n\n\ny[:7], y2[:4]\n\n(0   -7.500000\n 1   -7.900000\n 2   -2.357143\n 3   -3.342857\n 4   -1.800000\n 5   -0.314286\n 6   -2.142857\n Name: y, dtype: float64,\n array([-3.34285714, -1.8       , -0.31428571, -2.14285714]))\n\n\n- 데이터셋 분할\n\nx_train, x_val, y_train, y_val = train_test_split(x2, y2, test_size= 53, shuffle = False) ## test_size를 53개로 지정\n\n\nx_train.shape, y_train.shape\n\n((207, 4, 1), (207,))\n\n\n\nx_val.shape, y_val.shape\n\n((53, 4, 1), (53,))"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-06-03. 딥러닝 (4).html#모델링",
    "href": "posts/DX/05. 딥러닝/2023-10-06-03. 딥러닝 (4).html#모델링",
    "title": "03. 딥러닝 (4)",
    "section": "(2) 모델링",
    "text": "(2) 모델링\n\n1) 모델 설계\n\ntimestep = x_train.shape[1]\nnfeatures = x_train.shape[2]\n\n\nclear_session()\n\nmodel = Sequential([SimpleRNN(8, input_shape = (timestep, nfeatures)),\n                    Dense(1)])\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn (SimpleRNN)      (None, 8)                 80        \n                                                                 \n dense (Dense)               (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 89 (356.00 Byte)\nTrainable params: 89 (356.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\n2) 모델 fit\n\nmodel.compile(optimizer = Adam(learning_rate = 0.01), loss = 'mse' )\nhistory = model.fit(x_train, y_train, epochs = 100, validation_split = .2, verbose= 0).history\n\n\n\n3) train, val loss 확인\n\n# 학습 곡선을 그려봅시다.\ndl_history_plot(history)\n\n\n\n\n\n\n4) 예측 및 평가\n\n# 예측\npred = model.predict(x_val)\n\n2/2 [==============================] - 0s 5ms/step\n\n\n\n# 평가\nprint(mean_absolute_error(y_val, pred))\nprint(mean_absolute_percentage_error(y_val, pred))\n\n2.580778564868836\n0.5399836745573298\n\n\n\nplt.figure(figsize = (12,4))\nplt.plot(y_val, label = 'actual')\nplt.plot(pred, label = 'predicted')\nplt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-06-03. 딥러닝 (4).html#실습-1",
    "href": "posts/DX/05. 딥러닝/2023-10-06-03. 딥러닝 (4).html#실습-1",
    "title": "03. 딥러닝 (4)",
    "section": "(3) 실습 1",
    "text": "(3) 실습 1\n- RNN 레이어를 하나 더 추가…\n\nclear_session()\n\nmodel = Sequential([SimpleRNN(8, input_shape = (timestep, nfeatures), return_sequences=True),\n                                      SimpleRNN(4),\n                                          Dense(1)])\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn (SimpleRNN)      (None, 4, 8)              80        \n                                                                 \n simple_rnn_1 (SimpleRNN)    (None, 4)                 52        \n                                                                 \n dense (Dense)               (None, 1)                 5         \n                                                                 \n=================================================================\nTotal params: 137 (548.00 Byte)\nTrainable params: 137 (548.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nmodel.compile(optimizer = Adam(lr = 0.01), loss = 'mse' )\nhistory = model.fit(x_train, y_train, epochs = 100, validation_split = .2).history\n\nWARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n\n\nEpoch 1/100\n6/6 [==============================] - 4s 116ms/step - loss: 293.2144 - val_loss: 387.0168\nEpoch 2/100\n6/6 [==============================] - 0s 36ms/step - loss: 283.8497 - val_loss: 372.7898\nEpoch 3/100\n6/6 [==============================] - 0s 24ms/step - loss: 273.1573 - val_loss: 356.3178\nEpoch 4/100\n6/6 [==============================] - 0s 28ms/step - loss: 261.0764 - val_loss: 339.0669\nEpoch 5/100\n6/6 [==============================] - 0s 31ms/step - loss: 248.5183 - val_loss: 322.3875\nEpoch 6/100\n6/6 [==============================] - 0s 29ms/step - loss: 237.2658 - val_loss: 308.3640\nEpoch 7/100\n6/6 [==============================] - 0s 25ms/step - loss: 227.8352 - val_loss: 297.9115\nEpoch 8/100\n6/6 [==============================] - 0s 33ms/step - loss: 221.3156 - val_loss: 289.8501\nEpoch 9/100\n6/6 [==============================] - 0s 33ms/step - loss: 216.1329 - val_loss: 284.1252\nEpoch 10/100\n6/6 [==============================] - 0s 33ms/step - loss: 212.7198 - val_loss: 279.6017\nEpoch 11/100\n6/6 [==============================] - 0s 27ms/step - loss: 209.7165 - val_loss: 276.0281\nEpoch 12/100\n6/6 [==============================] - 0s 32ms/step - loss: 207.5396 - val_loss: 273.0580\nEpoch 13/100\n6/6 [==============================] - 0s 25ms/step - loss: 205.6913 - val_loss: 270.4893\nEpoch 14/100\n6/6 [==============================] - 0s 32ms/step - loss: 204.0771 - val_loss: 268.3944\nEpoch 15/100\n6/6 [==============================] - 0s 28ms/step - loss: 202.7937 - val_loss: 266.5363\nEpoch 16/100\n6/6 [==============================] - 0s 20ms/step - loss: 201.6105 - val_loss: 264.8059\nEpoch 17/100\n6/6 [==============================] - 0s 14ms/step - loss: 200.5000 - val_loss: 263.1637\nEpoch 18/100\n6/6 [==============================] - 0s 19ms/step - loss: 199.4231 - val_loss: 261.6020\nEpoch 19/100\n6/6 [==============================] - 0s 16ms/step - loss: 198.4034 - val_loss: 260.1485\nEpoch 20/100\n6/6 [==============================] - 0s 18ms/step - loss: 197.5113 - val_loss: 258.7556\nEpoch 21/100\n6/6 [==============================] - 0s 16ms/step - loss: 196.6525 - val_loss: 257.5038\nEpoch 22/100\n6/6 [==============================] - 0s 15ms/step - loss: 195.8433 - val_loss: 256.3315\nEpoch 23/100\n6/6 [==============================] - 0s 17ms/step - loss: 195.0920 - val_loss: 255.1981\nEpoch 24/100\n6/6 [==============================] - 0s 18ms/step - loss: 194.3712 - val_loss: 254.0450\nEpoch 25/100\n6/6 [==============================] - 0s 14ms/step - loss: 193.6489 - val_loss: 253.0111\nEpoch 26/100\n6/6 [==============================] - 0s 18ms/step - loss: 192.9810 - val_loss: 251.9454\nEpoch 27/100\n6/6 [==============================] - 0s 18ms/step - loss: 192.2778 - val_loss: 250.9067\nEpoch 28/100\n6/6 [==============================] - 0s 14ms/step - loss: 191.6269 - val_loss: 249.8762\nEpoch 29/100\n6/6 [==============================] - 0s 16ms/step - loss: 190.9538 - val_loss: 248.8816\nEpoch 30/100\n6/6 [==============================] - 0s 14ms/step - loss: 190.3148 - val_loss: 247.8869\nEpoch 31/100\n6/6 [==============================] - 0s 15ms/step - loss: 189.6668 - val_loss: 246.8659\nEpoch 32/100\n6/6 [==============================] - 0s 18ms/step - loss: 189.0218 - val_loss: 245.8512\nEpoch 33/100\n6/6 [==============================] - 0s 19ms/step - loss: 188.3576 - val_loss: 244.8610\nEpoch 34/100\n6/6 [==============================] - 0s 16ms/step - loss: 187.7428 - val_loss: 243.9608\nEpoch 35/100\n6/6 [==============================] - 0s 19ms/step - loss: 187.1909 - val_loss: 243.1584\nEpoch 36/100\n6/6 [==============================] - 0s 16ms/step - loss: 186.6783 - val_loss: 242.3853\nEpoch 37/100\n6/6 [==============================] - 0s 18ms/step - loss: 186.1597 - val_loss: 241.6176\nEpoch 38/100\n6/6 [==============================] - 0s 14ms/step - loss: 185.6394 - val_loss: 240.8237\nEpoch 39/100\n6/6 [==============================] - 0s 18ms/step - loss: 185.1329 - val_loss: 240.0594\nEpoch 40/100\n6/6 [==============================] - 0s 16ms/step - loss: 184.6021 - val_loss: 239.2886\nEpoch 41/100\n6/6 [==============================] - 0s 17ms/step - loss: 184.0619 - val_loss: 238.4772\nEpoch 42/100\n6/6 [==============================] - 0s 19ms/step - loss: 183.5141 - val_loss: 237.6295\nEpoch 43/100\n6/6 [==============================] - 0s 21ms/step - loss: 182.9427 - val_loss: 236.8006\nEpoch 44/100\n6/6 [==============================] - 0s 17ms/step - loss: 182.4017 - val_loss: 235.9711\nEpoch 45/100\n6/6 [==============================] - 0s 16ms/step - loss: 181.8439 - val_loss: 235.2101\nEpoch 46/100\n6/6 [==============================] - 0s 18ms/step - loss: 181.3132 - val_loss: 234.4904\nEpoch 47/100\n6/6 [==============================] - 0s 17ms/step - loss: 180.7986 - val_loss: 233.7523\nEpoch 48/100\n6/6 [==============================] - 0s 14ms/step - loss: 180.2276 - val_loss: 233.0964\nEpoch 49/100\n6/6 [==============================] - 0s 18ms/step - loss: 179.6889 - val_loss: 232.3902\nEpoch 50/100\n6/6 [==============================] - 0s 14ms/step - loss: 179.0958 - val_loss: 231.7125\nEpoch 51/100\n6/6 [==============================] - 0s 18ms/step - loss: 178.4332 - val_loss: 231.0642\nEpoch 52/100\n6/6 [==============================] - 0s 17ms/step - loss: 177.7105 - val_loss: 230.5040\nEpoch 53/100\n6/6 [==============================] - 0s 16ms/step - loss: 176.9390 - val_loss: 230.0341\nEpoch 54/100\n6/6 [==============================] - 0s 16ms/step - loss: 175.9770 - val_loss: 229.5280\nEpoch 55/100\n6/6 [==============================] - 0s 15ms/step - loss: 174.9057 - val_loss: 229.0144\nEpoch 56/100\n6/6 [==============================] - 0s 18ms/step - loss: 173.8212 - val_loss: 228.5216\nEpoch 57/100\n6/6 [==============================] - 0s 17ms/step - loss: 172.6341 - val_loss: 228.0173\nEpoch 58/100\n6/6 [==============================] - 0s 16ms/step - loss: 171.4734 - val_loss: 227.5555\nEpoch 59/100\n6/6 [==============================] - 0s 19ms/step - loss: 170.3074 - val_loss: 227.3842\nEpoch 60/100\n6/6 [==============================] - 0s 16ms/step - loss: 169.3547 - val_loss: 227.1719\nEpoch 61/100\n6/6 [==============================] - 0s 14ms/step - loss: 168.5049 - val_loss: 226.4924\nEpoch 62/100\n6/6 [==============================] - 0s 17ms/step - loss: 167.6840 - val_loss: 225.5365\nEpoch 63/100\n6/6 [==============================] - 0s 18ms/step - loss: 166.8976 - val_loss: 224.3217\nEpoch 64/100\n6/6 [==============================] - 0s 19ms/step - loss: 166.0759 - val_loss: 223.1580\nEpoch 65/100\n6/6 [==============================] - 0s 16ms/step - loss: 165.2865 - val_loss: 222.1851\nEpoch 66/100\n6/6 [==============================] - 0s 17ms/step - loss: 164.5682 - val_loss: 221.3897\nEpoch 67/100\n6/6 [==============================] - 0s 16ms/step - loss: 163.8982 - val_loss: 220.6226\nEpoch 68/100\n6/6 [==============================] - 0s 28ms/step - loss: 163.2355 - val_loss: 219.7232\nEpoch 69/100\n6/6 [==============================] - 0s 27ms/step - loss: 162.5961 - val_loss: 218.9677\nEpoch 70/100\n6/6 [==============================] - 0s 27ms/step - loss: 162.0276 - val_loss: 218.0989\nEpoch 71/100\n6/6 [==============================] - 0s 31ms/step - loss: 161.4014 - val_loss: 217.4666\nEpoch 72/100\n6/6 [==============================] - 0s 28ms/step - loss: 160.8126 - val_loss: 216.8285\nEpoch 73/100\n6/6 [==============================] - 0s 30ms/step - loss: 160.1931 - val_loss: 215.9785\nEpoch 74/100\n6/6 [==============================] - 0s 39ms/step - loss: 159.5959 - val_loss: 215.1314\nEpoch 75/100\n6/6 [==============================] - 0s 51ms/step - loss: 159.0238 - val_loss: 214.7249\nEpoch 76/100\n6/6 [==============================] - 0s 49ms/step - loss: 158.4952 - val_loss: 214.3441\nEpoch 77/100\n6/6 [==============================] - 0s 37ms/step - loss: 158.0654 - val_loss: 213.8266\nEpoch 78/100\n6/6 [==============================] - 0s 40ms/step - loss: 157.4570 - val_loss: 212.9087\nEpoch 79/100\n6/6 [==============================] - 0s 45ms/step - loss: 156.9074 - val_loss: 212.0208\nEpoch 80/100\n6/6 [==============================] - 0s 29ms/step - loss: 156.4676 - val_loss: 211.0118\nEpoch 81/100\n6/6 [==============================] - 0s 29ms/step - loss: 155.8935 - val_loss: 210.2588\nEpoch 82/100\n6/6 [==============================] - 0s 25ms/step - loss: 155.3580 - val_loss: 209.6723\nEpoch 83/100\n6/6 [==============================] - 0s 30ms/step - loss: 154.8305 - val_loss: 209.1811\nEpoch 84/100\n6/6 [==============================] - 0s 26ms/step - loss: 154.3170 - val_loss: 208.6301\nEpoch 85/100\n6/6 [==============================] - 0s 21ms/step - loss: 153.8134 - val_loss: 207.9755\nEpoch 86/100\n6/6 [==============================] - 0s 23ms/step - loss: 153.3001 - val_loss: 207.2960\nEpoch 87/100\n6/6 [==============================] - 0s 30ms/step - loss: 152.8152 - val_loss: 206.6420\nEpoch 88/100\n6/6 [==============================] - 0s 28ms/step - loss: 152.2935 - val_loss: 206.0468\nEpoch 89/100\n6/6 [==============================] - 0s 22ms/step - loss: 151.8174 - val_loss: 205.4739\nEpoch 90/100\n6/6 [==============================] - 0s 27ms/step - loss: 151.3512 - val_loss: 204.9155\nEpoch 91/100\n6/6 [==============================] - 0s 27ms/step - loss: 150.9671 - val_loss: 204.4529\nEpoch 92/100\n6/6 [==============================] - 0s 26ms/step - loss: 150.4604 - val_loss: 203.5670\nEpoch 93/100\n6/6 [==============================] - 0s 32ms/step - loss: 149.9585 - val_loss: 202.8719\nEpoch 94/100\n6/6 [==============================] - 0s 31ms/step - loss: 149.5003 - val_loss: 202.2268\nEpoch 95/100\n6/6 [==============================] - 0s 21ms/step - loss: 149.0123 - val_loss: 201.3930\nEpoch 96/100\n6/6 [==============================] - 0s 16ms/step - loss: 148.5756 - val_loss: 200.6106\nEpoch 97/100\n6/6 [==============================] - 0s 14ms/step - loss: 148.0978 - val_loss: 199.9606\nEpoch 98/100\n6/6 [==============================] - 0s 26ms/step - loss: 147.6903 - val_loss: 199.3005\nEpoch 99/100\n6/6 [==============================] - 0s 26ms/step - loss: 147.1875 - val_loss: 198.8547\nEpoch 100/100\n6/6 [==============================] - 0s 32ms/step - loss: 146.7144 - val_loss: 198.4536\n\n\n\n# 학습 곡선을 그려봅시다.\ndl_history_plot(history)\n\n\n\n\n\n# 예측\npred = model.predict(x_val)\n\n2/2 [==============================] - 0s 6ms/step\n\n\n\n# 평가\nprint(mean_absolute_error(y_val, pred))\nprint(mean_absolute_percentage_error(y_val, pred))\n\n10.461073768535277\n0.9029887280232256\n\n\n\nplt.figure(figsize = (12,4))\nplt.plot(y_val, label = 'actual')\nplt.plot(pred, label = 'predicted')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n- 성능이 더 떨어짐…\n\n당연한 것임. 히든스테이트의 값을 계속 가져왔으니 train data에 오버피팅이 될 수 밖에 없음\n\n- 근데 RNN layer를 2개 쓸거면 첫 번째 layer에서는 무조건 return_sequences  = True로 줘야함\n\n왜? 시간의 흐름에 따라 순차적으로 두 번째 RNN layer가 학습해야되기 때문!!"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-06-03. 딥러닝 (4).html#실습-2",
    "href": "posts/DX/05. 딥러닝/2023-10-06-03. 딥러닝 (4).html#실습-2",
    "title": "03. 딥러닝 (4)",
    "section": "(4) 실습 2",
    "text": "(4) 실습 2\n- 위에 모델을 2개의 모델 정도 수정해보자\n\nmodel1 : RNN(8) + RNN(2) + output\nmodel2: RNN(8) + RNN(2)+ Dense(4) + output\n\n\nclear_session()\n\nmodel1 = Sequential([SimpleRNN(8, input_shape = (timestep, nfeatures), return_sequences=True),\n                                      SimpleRNN(2),\n                                          Dense(1)])\n\nmodel2 = Sequential([SimpleRNN(8, input_shape = (timestep, nfeatures), return_sequences=True),\n                                      SimpleRNN(2),\n                                          Dense(4, activation = \"tanh\"),\n                                          Dense(1)])\n\n\nmodel1.compile(optimizer = Adam(0.01), loss = 'mse' )\nmodel2.compile(optimizer = Adam(0.01), loss = 'mse' )\n\nh1 = model1.fit(x_train, y_train, epochs = 100, validation_split = .2, verbose = 0).history\nh2 = model1.fit(x_train, y_train, epochs = 100, validation_split = .2, verbose = 0).history\n\n\n\nCode\nfig, axes = plt.subplots(1,2, figsize = (12,4))\n\nax1, ax2 =axes\n\nax1.plot(h1[\"loss\"], label = \"train_loss\")\nax1.plot(h1[\"val_loss\"], label = \"val_loss\")\nax1.legend()\n\nax2.plot(h2[\"loss\"], label = \"train_loss\")\nax2.plot(h2[\"val_loss\"], label = \"val_loss\")\nax2.legend()\n\nfig.tight_layout()\nfig.show()\n\n\n\n\n\n- 안정적인 폭은 model1이지만 전반적인 loss가 model2가 훨씬 작다.\n\n# 예측\npred1 = model1.predict(x_val)\npred2 = model2.predict(x_val)\n\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 6ms/step\n\n\n\nprint(mean_absolute_error(y_val, pred1))\nprint(mean_absolute_error(y_val, pred2))\n\n3.0550256566582155\n14.10010453259718\n\n\n\nfig, axes = plt.subplots(1,2, figsize = (12, 4))\n\nax1, ax2 = axes\n\nax1.plot(y_val, \".-r\" ,label = r\"$(y_{true})$\",alpha = 0.3)\nax1.plot(pred1, \".-b\" ,label = r\"$\\hat {y}_{model1}$\", alpha = 0.5)\nax1.legend()\n\nax2.plot(y_val, \".-r\" ,label = r\"$(y_{true})$\",alpha = 0.3)\nax2.plot(pred2, \".-b\" ,label = r\"$\\hat {y}_{model2}$\", alpha = 0.5)\nax2.legend()\n\n&lt;matplotlib.legend.Legend at 0x7e3f74d93400&gt;"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html",
    "href": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html",
    "title": "Extra 02. 시각지능",
    "section": "",
    "text": "cd /content/drive/MyDrive/Colab Notebooks/DX/딥러닝\n\n/content/drive/MyDrive/Colab Notebooks/DX/딥러닝"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#라이브러리-및-데이터-불러오기",
    "href": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#라이브러리-및-데이터-불러오기",
    "title": "Extra 02. 시각지능",
    "section": "1. 라이브러리 및 데이터 불러오기",
    "text": "1. 라이브러리 및 데이터 불러오기\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n\nfrom tensorflow.keras.datasets import fashion_mnist\n\n\n(train_x, train_y), (test_x, test_y) = fashion_mnist.load_data()\n\n\ntrain_x.shape, train_y.shape, test_x.shape, test_y.shape\n\n((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))\n\n\n\nlabels = ['T-shirt',\n          'Trouser',\n          'Pullover',\n          'Dress',\n          'Coat',\n          'Sandal',\n          'Shirt',\n          'Sneaker',\n          'Bag',\n          'Ankle boot'\n          ]\n\nprint(labels)\n\n['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nplt.imshow(test_x[1], cmap='Greys')\nplt.show()"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#데이터-전처리",
    "href": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#데이터-전처리",
    "title": "Extra 02. 시각지능",
    "section": "2. 데이터 전처리",
    "text": "2. 데이터 전처리\n- Min-Max scaling (MinMax scaler 금지 : 태블러 데이터에 최적화되어있기 때문)\n\n전처리 규칙은 트레인셋을 따라야한다. (test set을 반영할 경우 이미 정답을 알고 있기 때문에 반칙임)\n\n\nmax_n, min_n = train_x.max(), train_x.min()\nmax_n, min_n\n\n(255, 0)\n\n\n\ntrain_x = (train_x - min_n) / (max_n - min_n)\ntest_x = (test_x - min_n) / (max_n - min_n)\n\n- Date reshape (흑백 채널 추가)\n\ntrain_x_re1 = train_x.reshape(train_x.shape[0], 28, 28, -1)\ntest_x_re1 = test_x.reshape(test_x.shape[0], 28, 28, -1)\n\n- Y : one-hot Encoding\n\nfrom keras.utils import to_categorical\ntrain_y_c = to_categorical(train_y, 10)\ntest_y_c = to_categorical(test_y, 10)"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#모델링",
    "href": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#모델링",
    "title": "Extra 02. 시각지능",
    "section": "3. 모델링",
    "text": "3. 모델링\n\n(1) Sequential API\n- 레이어를 순차적으로 쌓음\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom tensorflow.keras.backend import clear_session\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense, Flatten, BatchNormalization, Dropout\n\n\n# 1. 세션 클리어\nclear_session()\n\n# 2. 모델 설계\nmodel1 = Sequential()\n\nmodel1.add( Input(shape = (28, 28, 1)) )\nmodel1.add( Flatten() )\nmodel1.add( Dense(1024, activation = \"relu\") )\nmodel1.add( Dense(1024, activation = \"relu\") )\nmodel1.add( BatchNormalization() )\nmodel1.add( Dropout(0.25) )\n\nmodel1.add( Dense(512, activation = \"relu\") )\nmodel1.add( Dense(512, activation = \"relu\") )\nmodel1.add( BatchNormalization() )\nmodel1.add( Dropout(0.25) )\n\nmodel1.add( Dense(256, activation = \"relu\") )\nmodel1.add( Dense(128, activation = \"relu\") )\nmodel1.add( BatchNormalization() )\nmodel1.add( Dropout(0.25) )\n\nmodel1.add( Dense(10, activation = \"softmax\")  )\n\n# 3. 모델 컴파일\nmodel1.compile(optimizer = \"adam\", loss = tf.keras.losses.categorical_crossentropy,\n                              metrics = [\"accuracy\"])\n\n\nmodel1.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten (Flatten)           (None, 784)               0         \n                                                                 \n dense (Dense)               (None, 1024)              803840    \n                                                                 \n dense_1 (Dense)             (None, 1024)              1049600   \n                                                                 \n batch_normalization (Batch  (None, 1024)              4096      \n Normalization)                                                  \n                                                                 \n dropout (Dropout)           (None, 1024)              0         \n                                                                 \n dense_2 (Dense)             (None, 512)               524800    \n                                                                 \n dense_3 (Dense)             (None, 512)               262656    \n                                                                 \n batch_normalization_1 (Bat  (None, 512)               2048      \n chNormalization)                                                \n                                                                 \n dropout_1 (Dropout)         (None, 512)               0         \n                                                                 \n dense_4 (Dense)             (None, 256)               131328    \n                                                                 \n dense_5 (Dense)             (None, 128)               32896     \n                                                                 \n batch_normalization_2 (Bat  (None, 128)               512       \n chNormalization)                                                \n                                                                 \n dropout_2 (Dropout)         (None, 128)               0         \n                                                                 \n dense_6 (Dense)             (None, 10)                1290      \n                                                                 \n=================================================================\nTotal params: 2813066 (10.73 MB)\nTrainable params: 2809738 (10.72 MB)\nNon-trainable params: 3328 (13.00 KB)\n_________________________________________________________________\n\n\n\n\n(2) Functial API\n- 레이어를 사슬처럼 엮기!\n\n## Functional\n\n# 1. 세션 클리어\nclear_session()\n\n# 2. 레이어를 사슬처럼 엮기\nil = Input(shape = (28, 28, 1))\nhl = Flatten()(il)\n\nhl = Dense(1024, activation = \"relu\")(hl)\nhl = Dense(1024, activation = \"relu\")(hl)\nhl = BatchNormalization()(hl)\nhl = Dropout(0.25)(hl)\n\nhl = Dense(512, activation = \"relu\")(hl)\nhl = Dense(512, activation = \"relu\")(hl)\nhl = BatchNormalization()(hl)\nhl = Dropout(0.25)(hl)\n\nhl = Dense(256, activation = \"relu\")(hl)\nhl = Dense(128, activation = \"relu\")(hl)\nhl = BatchNormalization()(hl)\nhl = Dropout(0.25)(hl)\n\nol = Dense(10, activation  = \"softmax\")(hl)\n\n# 3. 모델의 시작과 끝 지정/알려줌\nmodel2 = Model(il, ol)\n\n# 4. 컴파일\nmodel2.compile(optimizer = \"adam\",\n                             loss = tf.keras.losses.categorical_crossentropy,\n                             metrics = [\"accuracy\"])\n\n\nmodel2.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n                                                                 \n flatten (Flatten)           (None, 784)               0         \n                                                                 \n dense (Dense)               (None, 1024)              803840    \n                                                                 \n dense_1 (Dense)             (None, 1024)              1049600   \n                                                                 \n batch_normalization (Batch  (None, 1024)              4096      \n Normalization)                                                  \n                                                                 \n dropout (Dropout)           (None, 1024)              0         \n                                                                 \n dense_2 (Dense)             (None, 512)               524800    \n                                                                 \n dense_3 (Dense)             (None, 512)               262656    \n                                                                 \n batch_normalization_1 (Bat  (None, 512)               2048      \n chNormalization)                                                \n                                                                 \n dropout_1 (Dropout)         (None, 512)               0         \n                                                                 \n dense_4 (Dense)             (None, 256)               131328    \n                                                                 \n dense_5 (Dense)             (None, 128)               32896     \n                                                                 \n batch_normalization_2 (Bat  (None, 128)               512       \n chNormalization)                                                \n                                                                 \n dropout_2 (Dropout)         (None, 128)               0         \n                                                                 \n dense_6 (Dense)             (None, 10)                1290      \n                                                                 \n=================================================================\nTotal params: 2813066 (10.73 MB)\nTrainable params: 2809738 (10.72 MB)\nNon-trainable params: 3328 (13.00 KB)\n_________________________________________________________________\n\n\n\n\n(3) Early Stoppin!\n- 과적합 방지\n\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n- validation_set이 존재하지 않으면 학습은 되지만 조기 종료가 적용되지는 않음\n\nes = EarlyStopping(monitor = \"val_loss\",  ## 과적합 방지 기준\n                                   min_delta = 0, ## 설정한 값으로 변화해야 모델 성능이 개선되었다고 판단하는 기준 ( Threshole)\n                                    patience = 3, ## 성능이 개선되지 않을 떄 얼마나 참을건지\n                                    verbose =1,\n                                    restore_best_weights = True  ## 조기 종료 적용 후 최적 가중치를 모델의 전달\n                                )\n\n\n\n(4) 학습\n\nmodel1.fit(train_x_re1, train_y_c, epochs = 10000, verbose = 1,\n                    validation_split = 0.2,\n                    callbacks = [es]\n                    )\n\nEpoch 1/10000\n1500/1500 [==============================] - 16s 8ms/step - loss: 0.6193 - accuracy: 0.7822 - val_loss: 0.4821 - val_accuracy: 0.8240\nEpoch 2/10000\n1500/1500 [==============================] - 12s 8ms/step - loss: 0.4524 - accuracy: 0.8396 - val_loss: 0.5486 - val_accuracy: 0.8242\nEpoch 3/10000\n1500/1500 [==============================] - 11s 7ms/step - loss: 0.4079 - accuracy: 0.8548 - val_loss: 0.4085 - val_accuracy: 0.8562\nEpoch 4/10000\n1500/1500 [==============================] - 11s 7ms/step - loss: 0.3751 - accuracy: 0.8657 - val_loss: 0.4142 - val_accuracy: 0.8517\nEpoch 5/10000\n1500/1500 [==============================] - 12s 8ms/step - loss: 0.3548 - accuracy: 0.8730 - val_loss: 0.3567 - val_accuracy: 0.8732\nEpoch 6/10000\n1500/1500 [==============================] - 12s 8ms/step - loss: 0.3312 - accuracy: 0.8806 - val_loss: 0.3416 - val_accuracy: 0.8790\nEpoch 7/10000\n1500/1500 [==============================] - 11s 8ms/step - loss: 0.3140 - accuracy: 0.8861 - val_loss: 0.3585 - val_accuracy: 0.8742\nEpoch 8/10000\n1500/1500 [==============================] - 10s 7ms/step - loss: 0.2995 - accuracy: 0.8911 - val_loss: 0.3653 - val_accuracy: 0.8683\nEpoch 9/10000\n1500/1500 [==============================] - 11s 7ms/step - loss: 0.2877 - accuracy: 0.8955 - val_loss: 0.3087 - val_accuracy: 0.8884\nEpoch 10/10000\n1500/1500 [==============================] - 11s 7ms/step - loss: 0.2783 - accuracy: 0.8988 - val_loss: 0.3358 - val_accuracy: 0.8770\nEpoch 11/10000\n1500/1500 [==============================] - 11s 7ms/step - loss: 0.2653 - accuracy: 0.9031 - val_loss: 0.3065 - val_accuracy: 0.8913\nEpoch 12/10000\n1500/1500 [==============================] - 10s 7ms/step - loss: 0.2560 - accuracy: 0.9062 - val_loss: 0.3090 - val_accuracy: 0.8924\nEpoch 13/10000\n1500/1500 [==============================] - 11s 7ms/step - loss: 0.2483 - accuracy: 0.9083 - val_loss: 0.3146 - val_accuracy: 0.8907\nEpoch 14/10000\n1496/1500 [============================&gt;.] - ETA: 0s - loss: 0.2407 - accuracy: 0.9128Restoring model weights from the end of the best epoch: 11.\n1500/1500 [==============================] - 11s 7ms/step - loss: 0.2408 - accuracy: 0.9128 - val_loss: 0.3109 - val_accuracy: 0.8942\nEpoch 14: early stopping\n\n\n&lt;keras.src.callbacks.History at 0x7cf396614a90&gt;\n\n\n\n\n(5) 예측\n\ny_pred = model1.predict(test_x_re1).argmax(axis  = 1)\n\n313/313 [==============================] - 1s 3ms/step\n\n\n\nfrom sklearn.metrics import *\n\n\nprint(classification_report(test_y, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.82      0.87      0.84      1000\n           1       0.99      0.96      0.98      1000\n           2       0.78      0.81      0.79      1000\n           3       0.89      0.89      0.89      1000\n           4       0.76      0.82      0.79      1000\n           5       0.99      0.95      0.97      1000\n           6       0.75      0.63      0.68      1000\n           7       0.93      0.97      0.95      1000\n           8       0.97      0.98      0.97      1000\n           9       0.95      0.95      0.95      1000\n\n    accuracy                           0.88     10000\n   macro avg       0.88      0.88      0.88     10000\nweighted avg       0.88      0.88      0.88     10000"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#실습",
    "href": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#실습",
    "title": "Extra 02. 시각지능",
    "section": "실습",
    "text": "실습\n\n(0) import\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport random as rd\nfrom sklearn.metrics import accuracy_score\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n\n(train_x, train_y), (test_x, test_y) = keras.datasets.cifar10.load_data()\n\n\nprint(train_x.shape, train_y.shape, test_x.shape, test_y.shape)\n\n(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n\n\n\nlabels = {0 : 'Airplane',\n          1 : 'Automobile',\n          2 : 'Bird',\n          3 : 'Cat',\n          4 : 'Deer',\n          5 : 'Dog',\n          6 : 'Frog',\n          7 : 'Horse',\n          8 : 'Ship',\n          9 : 'Truck' }\n\nprint(labels)\n\n{0: 'Airplane', 1: 'Automobile', 2: 'Bird', 3: 'Cat', 4: 'Deer', 5: 'Dog', 6: 'Frog', 7: 'Horse', 8: 'Ship', 9: 'Truck'}\n\n\n\n\n(1) 데이터 스케일링\n\\[ X_{scaled} = {{x_{original} - mean(x)}\\over{std(x)} } \\]\n1 한꺼번에 스케일링\n\nmean_n,  std_n = train_x.mean(), train_x.std()\n\n\ntrain_x = (train_x - mean_n)/std_n\ntest_x = (test_x-mean_n)/std_n\n\n\ntrain_x.mean(),  train_x.std()\n\n(-2.5247951877342226e-17, 1.0000000000000022)\n\n\n2 채널별로 스케일링\n\ntr_r_mean, tr_r_std = train_x[:,:,:,0].mean(), train_x[:,:,:,0].std() ##  첫번째 채널에 대한 평균과 표준편차\ntr_g_mean, tr_g_std = train_x[:,:,:,1].mean(), train_x[:,:,:,1].std()\ntr_b_mean, tr_b_std = train_x[:,:,:,2].mean(), train_x[:,:,:,2].std()\n\n\ntrain_x_r = (train_x[:,:,:,0]- tr_r_mean)/tr_r_std\ntrain_x_g = (train_x[:,:,:,1]- tr_g_mean)/tr_g_std\ntrain_x_b = (train_x[:,:,:,2]- tr_b_mean)/tr_b_std\n\n\ntrain_x = np.stack([train_x_r, train_x_g, train_x_b], axis = 3)\n\n\ntrain_x.shape\n\n(50000, 32, 32, 3)\n\n\n\ntest_x_r = (test_x[:,:,:,0]- tr_r_mean)/tr_r_std\ntest_x_g = (test_x[:,:,:,1]- tr_g_mean)/tr_g_std\ntest_x_b = (test_x[:,:,:,2]- tr_b_mean)/tr_b_std\n\n\ntest_x = np.stack([test_x_r, test_x_g, test_x_b], axis = 3)\n\n\ntest_x.shape\n\n(10000, 32, 32, 3)\n\n\n\n\n(2) 원핫인코딩\n\ntrain_y[:5]\n\narray([[6],\n       [9],\n       [9],\n       [4],\n       [1]], dtype=uint8)\n\n\n\ntrain_y = np.ravel(train_y)\ntest_y = np.ravel(test_y)\n\n\nclass_n = len(np.unique(train_y))\n\n\nfrom tensorflow.keras.utils import to_categorical\n\n\ntrain_y = to_categorical(train_y, class_n)\ntest_y = to_categorical(test_y, class_n)\n\n\ntrain_y.shape, test_y.shape\n\n((50000, 10), (10000, 10))\n\n\n\n\n(4) Modeling 1. Sequential\n\nEarlyStopping 의 옵션도 조절해보자.\n.fit( )\n.predict( )\n\n\n\n자유롭게 먼저 해보는 것을 추천\n\n\n\n구조를 따라서 코딩을 한다면\n\nFunctional, Sequential 중 택일\n인풋레이어\nConvolution : 필터수 32개, 사이즈(3, 3), same padding\nConvolution : 필터수 32개, 사이즈(3, 3), same padding\nBatchNormalization\nMaxPooling : 사이즈(2,2) 스트라이드(2,2)\nDropOut : 25% 비활성화\nConvolution : 필터수 64개, 사이즈(3, 3), same padding\nConvolution : 필터수 64개, 사이즈(3, 3), same padding\nBatchNormalization\nMaxPooling : 사이즈(2,2) 스트라이드(2,2)\nDropOut : 25% 비활성화\nFlatten( )\nFully Connected Layer : 노드 1024개\nBatchNormalization\nDropOut : 35% 비활성화\n아웃풋레이어\n\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n\n# 1. 세션클리어\n\nkeras.backend.clear_session()\n\n# 2. 모델 발판 생성 : 레이이 블록을 조립할 발판 생성\nmodel1 = keras.models.Sequential()\n# 3. 레이어 블록 조합 : .add()\nmodel1.add( keras.layers.Input(shape = (32, 32, 3)) )\n\n# Convolution : 필터수 32개, 사이즈(3, 3), same padding\n\nmodel1.add( keras.layers.Conv2D(filters = 32, ## 새롭게 제작하려는 feature 맵의 수\n                                                            kernel_size = (3,3),  ## 커널 사이즈\n                                                              strides = (1,1),     ## Conv2D 필터위 이동보폭\n                                                              padding = \"same\", ## 1. 이전 feature map 사이즈 유지 | 2. 외곽 정보를 더 반영\n                                                                activation = \"relu\"  ) )\n\n# Convolution : 필터수 32개, 사이즈(3, 3), same padding\nmodel1.add( keras.layers.Conv2D(filters = 32, ## 새롭게 제작하려는 feature 맵의 수\n                                                            kernel_size = (3,3),  ## 커널 사이즈\n                                                              strides = (1,1),     ## Conv2D 필터위 이동보폭\n                                                              padding = \"same\", ## 1. 이전 feature map 사이즈 유지 | 2. 외곽 정보를 더 반영\n                                                                activation = \"relu\"  ) )\n\n#BatchNormalization\nmodel1.add( keras.layers.BatchNormalization() )\n# MaxPooling : 사이즈(2,2) 스트라이드(2,2)\nmodel1.add( keras.layers.MaxPool2D(pool_size = (2, 2), ## Maxpool2D 필터의 가로세로 사이즈\n                                                                    strides = (2,2),  ##  Maxpool2D 필터의 이동 보폭! # default는 필터 사즈를 따름\n                                                              ))\n\n# DropOut : 25% 비활성화\nmodel1.add ( keras.layers.Dropout(0.25) )\n\n# Convolution : 필터수 32개, 사이즈(3, 3), same padding\n\nmodel1.add( keras.layers.Conv2D(filters = 64, ## 새롭게 제작하려는 feature 맵의 수\n                                                            kernel_size = (3,3),  ## 커널 사이즈\n                                                              strides = (1,1),     ## Conv2D 필터위 이동보폭\n                                                              padding = \"same\", ## 1. 이전 feature map 사이즈 유지 | 2. 외곽 정보를 더 반영\n                                                                activation = \"relu\"  ) )\n\n# Convolution : 필터수 32개, 사이즈(3, 3), same padding\nmodel1.add( keras.layers.Conv2D(filters = 64, ## 새롭게 제작하려는 feature 맵의 수\n                                                            kernel_size = (3,3),  ## 커널 사이즈\n                                                              strides = (1,1),     ## Conv2D 필터위 이동보폭\n                                                              padding = \"same\", ## 1. 이전 feature map 사이즈 유지 | 2. 외곽 정보를 더 반영\n                                                                activation = \"relu\"  ) )\n\n#BatchNormalization\nmodel1.add( keras.layers.BatchNormalization() )\n# MaxPooling : 사이즈(2,2) 스트라이드(2,2)\nmodel1.add( keras.layers.MaxPool2D(pool_size = (2, 2), ## Maxpool2D 필터의 가로세로 사이즈\n                                                                    strides = (2,2),  ##  Maxpool2D 필터의 이동 보폭! # default는 필터 사즈를 따름\n                                                              ))\n\n# DropOut : 25% 비활성화\nmodel1.add ( keras.layers.Dropout(0.25) )\n\n# Flatten( )\nmodel1.add( keras.layers.Flatten() )\n\n# Fully Connected Layer : 노드 1024개\nmodel1.add( keras.layers.Dense(1024, activation = \"relu\"))\n\n# BatchNormalization\n\nmodel1.add(keras.layers.BatchNormalization())\n\n# DropOut : 35% 비활성화\n\nmodel1.add( keras.layers.Dropout(0.35) )\n\n# 아웃풋레이어\nmodel1.add( keras.layers.Dense(10, activation = \"softmax\"))\n\n# 4. 컴파일\n\nmodel1.compile(optimizer = \"adam\",\n                              loss = keras.losses.categorical_crossentropy,\n                              metrics = [\"accuracy\"])\nmodel1.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 32, 32, 32)        896       \n                                                                 \n conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n                                                                 \n batch_normalization (Batch  (None, 32, 32, 32)        128       \n Normalization)                                                  \n                                                                 \n max_pooling2d (MaxPooling2  (None, 16, 16, 32)        0         \n D)                                                              \n                                                                 \n dropout (Dropout)           (None, 16, 16, 32)        0         \n                                                                 \n conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n                                                                 \n conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n                                                                 \n batch_normalization_1 (Bat  (None, 16, 16, 64)        256       \n chNormalization)                                                \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 8, 8, 64)          0         \n g2D)                                                            \n                                                                 \n dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n                                                                 \n flatten (Flatten)           (None, 4096)              0         \n                                                                 \n dense (Dense)               (None, 1024)              4195328   \n                                                                 \n batch_normalization_2 (Bat  (None, 1024)              4096      \n chNormalization)                                                \n                                                                 \n dropout_2 (Dropout)         (None, 1024)              0         \n                                                                 \n dense_1 (Dense)             (None, 10)                10250     \n                                                                 \n=================================================================\nTotal params: 4275626 (16.31 MB)\nTrainable params: 4273386 (16.30 MB)\nNon-trainable params: 2240 (8.75 KB)\n_________________________________________________________________\n\n\n\n\n(5) Modeling 2. Functional\n\nfrom tensorflow.keras.backend import clear_session\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Flatten, BatchNormalization, Dropout\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D\nfrom tensorflow.keras.losses import categorical_crossentropy\n\n\n## Functional API\n# 1. 세션 클리어 : 청소\nclear_session()\n\n# 2. 레이어 사슬처럼 엮기\n# 인풋레이어\nil = Input(shape=(32,32,3))\n# Convolution : 필터수 32개, 사이즈(3, 3), same padding\nhl = Conv2D(filters=32,         # 새롭게 제작하려는 feature map의 수! 혹은 서로 다른 필터 32개 사용!\n            kernel_size=(3,3),  # Conv2D 필터의 가로세로 사이즈!\n            strides=(1,1),      # Conv2D 필터의 이동 보폭!\n            padding='same',     # 1.이전 feature map 사이즈 보존! 2. 외곽 정보 더 반영!\n            activation='relu'\n            )(il)               # 주의!!!\n# Convolution : 필터수 32개, 사이즈(3, 3), same padding\nhl = Conv2D(filters=32,         # 새롭게 제작하려는 feature map의 수! 혹은 서로 다른 필터 32개 사용!\n            kernel_size=(3,3),  # Conv2D 필터의 가로세로 사이즈!\n            strides=(1,1),      # Conv2D 필터의 이동 보폭!\n            padding='same',     # 1.이전 feature map 사이즈 보존! 2. 외곽 정보 더 반영!\n            activation='relu'\n            )(hl)               # 주의!!!\n# BatchNormalization\nhl = BatchNormalization()(hl)\n# MaxPooling : 사이즈(2,2) 스트라이드(2,2)\nhl = MaxPool2D(pool_size=(2,2), # Maxpool2D 필터의 가로세로 사이즈!\n               strides=(2,2)    # Maxpool2D 필터의 이동 보폭! 기본적으로 필터 사이즈를 따름!\n               )(hl)\n# DropOut : 25% 비활성화\nhl = Dropout(0.25)(hl)\n\n# Convolution : 필터수 64개, 사이즈(3, 3), same padding\nhl = Conv2D(filters=64,         # 새롭게 제작하려는 feature map의 수! 혹은 서로 다른 필터 32개 사용!\n            kernel_size=(3,3),  # Conv2D 필터의 가로세로 사이즈!\n            strides=(1,1),      # Conv2D 필터의 이동 보폭!\n            padding='same',     # 1.이전 feature map 사이즈 보존! 2. 외곽 정보 더 반영!\n            activation='relu'\n            )(hl)               # 주의!!!\n# Convolution : 필터수 64개, 사이즈(3, 3), same padding\nhl = Conv2D(filters=64,         # 새롭게 제작하려는 feature map의 수! 혹은 서로 다른 필터 32개 사용!\n            kernel_size=(3,3),  # Conv2D 필터의 가로세로 사이즈!\n            strides=(1,1),      # Conv2D 필터의 이동 보폭!\n            padding='same',     # 1.이전 feature map 사이즈 보존! 2. 외곽 정보 더 반영!\n            activation='relu'\n            )(hl)               # 주의!!!\n# BatchNormalization\nhl = BatchNormalization()(hl)\n# MaxPooling : 사이즈(2,2) 스트라이드(2,2)\nhl = MaxPool2D(pool_size=(2,2), # Maxpool2D 필터의 가로세로 사이즈!\n               strides=(2,2)    # Maxpool2D 필터의 이동 보폭! 기본적으로 필터 사이즈를 따름!\n               )(hl)\n# DropOut : 25% 비활성화\nhl = Dropout(0.25)(hl)\n\n# Flatten( )\nhl = Flatten()(hl)\n# Fully Connected Layer : 노드 1024개\nhl = Dense(1024, activation='relu')(hl)\n# BatchNormalization\nhl = BatchNormalization()(hl)\n# DropOut : 35% 비활성화\nhl = Dropout(0.35)(hl)\n# 아웃풋레이어\nol = Dense(10, activation='softmax')(hl)\n\n# 3. 모델의 시작과 끝 지정\nmodel2 = Model(il, ol)\n\n# 4. 컴파일\nmodel2.compile(optimizer='adam',\n               loss=categorical_crossentropy,\n               metrics=['accuracy']\n               )\n\n\nmodel2.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n conv2d (Conv2D)             (None, 32, 32, 32)        896       \n                                                                 \n conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n                                                                 \n batch_normalization (Batch  (None, 32, 32, 32)        128       \n Normalization)                                                  \n                                                                 \n max_pooling2d (MaxPooling2  (None, 16, 16, 32)        0         \n D)                                                              \n                                                                 \n dropout (Dropout)           (None, 16, 16, 32)        0         \n                                                                 \n conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n                                                                 \n conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n                                                                 \n batch_normalization_1 (Bat  (None, 16, 16, 64)        256       \n chNormalization)                                                \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 8, 8, 64)          0         \n g2D)                                                            \n                                                                 \n dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n                                                                 \n flatten (Flatten)           (None, 4096)              0         \n                                                                 \n dense (Dense)               (None, 1024)              4195328   \n                                                                 \n batch_normalization_2 (Bat  (None, 1024)              4096      \n chNormalization)                                                \n                                                                 \n dropout_2 (Dropout)         (None, 1024)              0         \n                                                                 \n dense_1 (Dense)             (None, 10)                10250     \n                                                                 \n=================================================================\nTotal params: 4275626 (16.31 MB)\nTrainable params: 4273386 (16.30 MB)\nNon-trainable params: 2240 (8.75 KB)\n_________________________________________________________________\n\n\n\n\n(6) 조기 종료 & fit\n\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n\nes = EarlyStopping(monitor='val_loss',     # 얼리스토핑을 적용할 대상!\n                   min_delta=0,            # Threshold. 이보다 크게 변화해야 성능 개선 간주!\n                   patience=3,             # 성능 개선이 발생하지 않을 때, 몇 epochs 더 볼 것인지!\n                   verbose=1,              # 어느 epoch가 최적인지 알려줌!\n                   restore_best_weights=True # 얼리스토핑으로 학습이 멈췄을 때, 최적의 가중치를 가진 시점으로 돌려줌!\n                   )\n\n\nmodel2.fit(train_x, train_y, epochs=10000, verbose=1,\n           validation_split=0.2,   # 매 epoch마다 training set에서 20%를 validation으로 지정!\n           callbacks=[es]          # 얼리스토핑 적용!\n           )\n\nEpoch 1/10000\n1250/1250 [==============================] - 22s 6ms/step - loss: 1.5738 - accuracy: 0.4804 - val_loss: 1.1359 - val_accuracy: 0.6085\nEpoch 2/10000\n1250/1250 [==============================] - 7s 5ms/step - loss: 1.0663 - accuracy: 0.6273 - val_loss: 1.0045 - val_accuracy: 0.6558\nEpoch 3/10000\n1250/1250 [==============================] - 7s 6ms/step - loss: 0.9055 - accuracy: 0.6866 - val_loss: 0.8294 - val_accuracy: 0.7143\nEpoch 4/10000\n1250/1250 [==============================] - 7s 5ms/step - loss: 0.8033 - accuracy: 0.7213 - val_loss: 0.7584 - val_accuracy: 0.7390\nEpoch 5/10000\n1250/1250 [==============================] - 7s 5ms/step - loss: 0.7185 - accuracy: 0.7477 - val_loss: 0.7612 - val_accuracy: 0.7412\nEpoch 6/10000\n1250/1250 [==============================] - 7s 5ms/step - loss: 0.6460 - accuracy: 0.7753 - val_loss: 0.7051 - val_accuracy: 0.7644\nEpoch 7/10000\n1250/1250 [==============================] - 7s 5ms/step - loss: 0.5871 - accuracy: 0.7934 - val_loss: 0.8016 - val_accuracy: 0.7550\nEpoch 8/10000\n1250/1250 [==============================] - 7s 5ms/step - loss: 0.5219 - accuracy: 0.8176 - val_loss: 0.6479 - val_accuracy: 0.7814\nEpoch 9/10000\n1250/1250 [==============================] - 7s 5ms/step - loss: 0.4823 - accuracy: 0.8328 - val_loss: 0.6468 - val_accuracy: 0.7819\nEpoch 10/10000\n1250/1250 [==============================] - 7s 5ms/step - loss: 0.4417 - accuracy: 0.8443 - val_loss: 0.6724 - val_accuracy: 0.7899\nEpoch 11/10000\n1250/1250 [==============================] - 7s 5ms/step - loss: 0.3917 - accuracy: 0.8618 - val_loss: 0.6094 - val_accuracy: 0.8018\nEpoch 12/10000\n1250/1250 [==============================] - 7s 5ms/step - loss: 0.3662 - accuracy: 0.8711 - val_loss: 0.7233 - val_accuracy: 0.7808\nEpoch 13/10000\n1250/1250 [==============================] - 7s 5ms/step - loss: 0.3395 - accuracy: 0.8798 - val_loss: 0.6376 - val_accuracy: 0.8038\nEpoch 14/10000\n1247/1250 [============================&gt;.] - ETA: 0s - loss: 0.3188 - accuracy: 0.8862Restoring model weights from the end of the best epoch: 11.\n1250/1250 [==============================] - 7s 5ms/step - loss: 0.3188 - accuracy: 0.8861 - val_loss: 0.6312 - val_accuracy: 0.8038\nEpoch 14: early stopping\n\n\n&lt;keras.src.callbacks.History at 0x7c75dc2aa110&gt;\n\n\n\n\n(7) 예측\n\n# 원핫 인코딩 해제 : 카테고리 중 가장 높은 값\ntrain_y = train_y.argmax(axis=1)\ntest_y = test_y.argmax(axis=1)\n\n\npred_train = model2.predict(train_x)\npred_test = model2.predict(test_x)\n\nsingle_pred_train = pred_train.argmax(axis=1)\nsingle_pred_test = pred_test.argmax(axis=1)\n\nlogi_train_accuracy = accuracy_score(train_y, single_pred_train)\nlogi_test_accuracy = accuracy_score(test_y, single_pred_test)\n\nprint('CNN')\nprint(f'트레이닝 정확도 : {logi_train_accuracy*100:.2f}%')\nprint(f'테스트 정확도 : {logi_test_accuracy*100:.2f}%')\n\n1563/1563 [==============================] - 3s 2ms/step\n313/313 [==============================] - 1s 2ms/step\nCNN\n트레이닝 정확도 : 92.77%\n테스트 정확도 : 79.73%"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#import-1",
    "href": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#import-1",
    "title": "Extra 02. 시각지능",
    "section": "import",
    "text": "import\n\n!pip install ultralytics\n\nCollecting ultralytics\n  Downloading ultralytics-8.0.197-py3-none-any.whl (640 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 641.0/641.0 kB 8.7 MB/s eta 0:00:0000:0100:01\nRequirement already satisfied: matplotlib&gt;=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\nRequirement already satisfied: numpy&gt;=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\nRequirement already satisfied: opencv-python&gt;=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\nRequirement already satisfied: pillow&gt;=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\nRequirement already satisfied: pyyaml&gt;=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\nRequirement already satisfied: requests&gt;=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\nRequirement already satisfied: scipy&gt;=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.3)\nRequirement already satisfied: torch&gt;=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.1+cu118)\nRequirement already satisfied: torchvision&gt;=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.15.2+cu118)\nRequirement already satisfied: tqdm&gt;=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.1)\nRequirement already satisfied: pandas&gt;=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\nRequirement already satisfied: seaborn&gt;=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\nCollecting thop&gt;=0.1.1 (from ultralytics)\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.3.0-&gt;ultralytics) (1.1.1)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.3.0-&gt;ultralytics) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.3.0-&gt;ultralytics) (4.43.1)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.3.0-&gt;ultralytics) (1.4.5)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.3.0-&gt;ultralytics) (23.2)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.3.0-&gt;ultralytics) (3.1.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.3.0-&gt;ultralytics) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.1.4-&gt;ultralytics) (2023.3.post1)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.23.0-&gt;ultralytics) (3.3.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.23.0-&gt;ultralytics) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.23.0-&gt;ultralytics) (2.0.6)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.23.0-&gt;ultralytics) (2023.7.22)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.8.0-&gt;ultralytics) (3.12.4)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.8.0-&gt;ultralytics) (4.5.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.8.0-&gt;ultralytics) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.8.0-&gt;ultralytics) (3.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.8.0-&gt;ultralytics) (3.1.2)\nRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.8.0-&gt;ultralytics) (2.0.0)\nRequirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch&gt;=1.8.0-&gt;ultralytics) (3.27.6)\nRequirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch&gt;=1.8.0-&gt;ultralytics) (17.0.2)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&gt;=3.3.0-&gt;ultralytics) (1.16.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch&gt;=1.8.0-&gt;ultralytics) (2.1.3)\nRequirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch&gt;=1.8.0-&gt;ultralytics) (1.3.0)\nInstalling collected packages: thop, ultralytics\nSuccessfully installed thop-0.1.1.post2209072238 ultralytics-8.0.197\n\n\n\nfrom ultralytics import YOLO\nfrom ultralytics import settings"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#모델링.-연습",
    "href": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#모델링.-연습",
    "title": "Extra 02. 시각지능",
    "section": "모델링. 연습",
    "text": "모델링. 연습\n- yolov8n.t라는 파이토치 파일을 받게됨\n\nmodel = YOLO()  ## step1. 모델 로드\nmodel.train() ##  step2. 모델학습\n\nUltralytics YOLOv8.0.197 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (NVIDIA A100-SXM4-40GB, 40514MiB)\nengine/trainer: task=detect, mode=train, model=yolov8n.pt, data=coco8.yaml, epochs=100, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \nModel summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n\nTransferred 355/355 items from pretrained weights\nTensorBoard: Start with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\nAMP: checks passed ✅\ntrain: Scanning /content/datasets/coco8/labels/train.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00&lt;?, ?it/s]\nalbumentations: Blur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\nval: Scanning /content/datasets/coco8/labels/val.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00&lt;?, ?it/s]\nPlotting labels to runs/detect/train2/labels.jpg... \noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to runs/detect/train2\nStarting training for 100 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      1/100      1.34G      0.931      3.153      1.292         32        640: 100%|██████████| 1/1 [00:00&lt;00:00, 12.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00,  4.13it/s]\n                   all          4         17      0.889      0.522      0.726       0.51\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      2/100      0.86G      1.162      3.128      1.518         33        640: 100%|██████████| 1/1 [00:00&lt;00:00, 13.62it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00,  4.15it/s]\n                   all          4         17      0.904      0.527      0.731      0.496\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      3/100     0.864G      1.392      3.712      1.773         13        640: 100%|██████████| 1/1 [00:00&lt;00:00, 13.54it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00,  7.35it/s]\n                   all          4         17      0.906      0.531      0.741       0.51\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      4/100     0.862G      1.546      2.966      1.764         23        640: 100%|██████████| 1/1 [00:00&lt;00:00, 13.61it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 22.58it/s]\n                   all          4         17      0.907      0.536       0.75      0.516\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      5/100      0.86G       1.14      2.792      1.397         26        640: 100%|██████████| 1/1 [00:00&lt;00:00, 11.82it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 22.35it/s]\n                   all          4         17      0.611      0.716      0.753      0.534\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      6/100     0.883G      1.333      3.425      1.683         32        640: 100%|██████████| 1/1 [00:00&lt;00:00, 12.54it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.46it/s]\n                   all          4         17       0.62      0.735      0.749      0.548\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      7/100     0.883G      1.049       3.02      1.402         20        640: 100%|██████████| 1/1 [00:00&lt;00:00, 12.61it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 26.51it/s]\n                   all          4         17       0.91       0.55       0.75       0.55\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      8/100     0.885G      1.451      2.211      1.765         24        640: 100%|██████████| 1/1 [00:00&lt;00:00, 10.42it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 21.03it/s]\n                   all          4         17      0.912       0.55       0.75      0.548\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      9/100     0.883G     0.9272      2.569      1.365         18        640: 100%|██████████| 1/1 [00:00&lt;00:00, 12.39it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.76it/s]\n                   all          4         17      0.912       0.55      0.751      0.529\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     10/100     0.883G     0.9733      2.397      1.324         25        640: 100%|██████████| 1/1 [00:00&lt;00:00, 12.89it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.64it/s]\n                   all          4         17      0.915       0.55      0.753      0.527\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     11/100     0.883G      0.921      2.254      1.209         45        640: 100%|██████████| 1/1 [00:00&lt;00:00, 11.70it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.58it/s]\n                   all          4         17      0.928       0.55      0.754      0.519\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     12/100     0.889G     0.9443      2.423      1.324         23        640: 100%|██████████| 1/1 [00:00&lt;00:00, 12.52it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 28.09it/s]\n                   all          4         17      0.934      0.533       0.84      0.556\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     13/100     0.883G      1.001      2.195      1.302         29        640: 100%|██████████| 1/1 [00:00&lt;00:00, 12.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.62it/s]\n                   all          4         17      0.939      0.533       0.84      0.556\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     14/100     0.885G      1.099      2.787      1.426         43        640: 100%|██████████| 1/1 [00:00&lt;00:00,  8.86it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 26.80it/s]\n                   all          4         17      0.916       0.55      0.806      0.538\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     15/100     0.883G       1.22      2.534      1.571         24        640: 100%|██████████| 1/1 [00:00&lt;00:00, 11.00it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 21.05it/s]\n                   all          4         17      0.899       0.55       0.66      0.504\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     16/100     0.883G      1.015       1.64      1.355         31        640: 100%|██████████| 1/1 [00:00&lt;00:00, 11.40it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 26.57it/s]\n                   all          4         17      0.877       0.55       0.66      0.505\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     17/100     0.885G       1.01      2.443      1.385         25        640: 100%|██████████| 1/1 [00:00&lt;00:00,  9.51it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 22.72it/s]\n                   all          4         17      0.852       0.55       0.66      0.503\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     18/100     0.885G      1.123      2.954      1.512         42        640: 100%|██████████| 1/1 [00:00&lt;00:00, 16.54it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 28.23it/s]\n                   all          4         17      0.852       0.55       0.66      0.503\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     19/100     0.883G     0.9312      1.645      1.258         29        640: 100%|██████████| 1/1 [00:00&lt;00:00, 11.40it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 22.30it/s]\n                   all          4         17      0.798       0.56      0.633       0.47\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     20/100     0.883G      1.127      1.659      1.563         29        640: 100%|██████████| 1/1 [00:00&lt;00:00, 17.84it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 26.78it/s]\n                   all          4         17      0.798       0.56      0.633       0.47\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     21/100     0.885G       1.05       2.93      1.424         24        640: 100%|██████████| 1/1 [00:00&lt;00:00, 11.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 28.04it/s]\n                   all          4         17      0.768      0.583       0.61       0.46\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     22/100     0.885G      1.057      1.975      1.256         41        640: 100%|██████████| 1/1 [00:00&lt;00:00, 17.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 26.88it/s]\n                   all          4         17      0.768      0.583       0.61       0.46\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     23/100     0.885G     0.9229      1.251      1.224         27        640: 100%|██████████| 1/1 [00:00&lt;00:00, 10.34it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.88it/s]\n                   all          4         17      0.787      0.583      0.587      0.455\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     24/100     0.889G     0.9696      1.723      1.384         16        640: 100%|██████████| 1/1 [00:00&lt;00:00, 16.63it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 22.82it/s]\n                   all          4         17      0.787      0.583      0.587      0.455\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     25/100     0.885G     0.9315      2.105      1.369         30        640: 100%|██████████| 1/1 [00:00&lt;00:00, 11.05it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.45it/s]\n                   all          4         17      0.945      0.383      0.578      0.454\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     26/100     0.883G      0.849      1.423      1.321         20        640: 100%|██████████| 1/1 [00:00&lt;00:00, 17.67it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.52it/s]\n                   all          4         17      0.945      0.383      0.578      0.454\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     27/100     0.885G      1.162      1.523      1.325         50        640: 100%|██████████| 1/1 [00:00&lt;00:00, 11.65it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 28.18it/s]\n                   all          4         17      0.945      0.383      0.581      0.455\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     28/100     0.885G     0.7378      2.115      1.291         13        640: 100%|██████████| 1/1 [00:00&lt;00:00, 15.34it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.18it/s]\n                   all          4         17      0.945      0.383      0.581      0.455\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     29/100     0.883G     0.8003      1.996      1.254         19        640: 100%|██████████| 1/1 [00:00&lt;00:00, 11.39it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.74it/s]\n                   all          4         17      0.946      0.383      0.577      0.461\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     30/100     0.883G     0.6998       1.42      1.142         31        640: 100%|██████████| 1/1 [00:00&lt;00:00, 16.94it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 22.10it/s]\n                   all          4         17      0.946      0.383      0.577      0.461\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     31/100     0.885G     0.8479      1.438      1.336         25        640: 100%|██████████| 1/1 [00:00&lt;00:00, 12.58it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 28.00it/s]\n                   all          4         17      0.947      0.383      0.579      0.446\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     32/100     0.885G     0.7952     0.9185      1.123         28        640: 100%|██████████| 1/1 [00:00&lt;00:00, 13.76it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.25it/s]\n                   all          4         17      0.947      0.383      0.579      0.446\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     33/100     0.885G     0.9491      1.461      1.379         23        640: 100%|██████████| 1/1 [00:00&lt;00:00, 11.51it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.62it/s]\n                   all          4         17      0.946      0.383      0.579      0.444\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     34/100     0.883G     0.9768      1.508      1.271         34        640: 100%|██████████| 1/1 [00:00&lt;00:00, 16.93it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 22.13it/s]\n                   all          4         17      0.946      0.383      0.579      0.444\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     35/100     0.889G     0.5931      1.351      1.171         17        640: 100%|██████████| 1/1 [00:00&lt;00:00, 11.37it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 22.86it/s]\n                   all          4         17      0.946      0.383      0.579      0.469\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     36/100     0.883G     0.8994      1.591      1.127         42        640: 100%|██████████| 1/1 [00:00&lt;00:00, 14.88it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 21.89it/s]\n                   all          4         17      0.946      0.383      0.579      0.469\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     37/100     0.885G     0.8501      1.993      1.307         30        640: 100%|██████████| 1/1 [00:00&lt;00:00, 10.68it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.45it/s]\n                   all          4         17      0.946      0.383       0.58       0.47\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     38/100     0.883G      0.851       1.69      1.291         25        640: 100%|██████████| 1/1 [00:00&lt;00:00, 17.60it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 26.13it/s]\n                   all          4         17      0.946      0.383       0.58       0.47\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     39/100     0.885G     0.8789      1.113      1.296         24        640: 100%|██████████| 1/1 [00:00&lt;00:00, 12.82it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.71it/s]\n                   all          4         17      0.947      0.383      0.582      0.467\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     40/100     0.883G     0.6864     0.9524      1.192         16        640: 100%|██████████| 1/1 [00:00&lt;00:00, 17.64it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.15it/s]\n                   all          4         17      0.947      0.383      0.582      0.467\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     41/100     0.889G     0.9022      1.421      1.439         18        640: 100%|██████████| 1/1 [00:00&lt;00:00, 12.43it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 28.87it/s]\n                   all          4         17      0.948      0.383      0.502      0.378\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     42/100     0.885G     0.6732     0.7753      1.052         28        640: 100%|██████████| 1/1 [00:00&lt;00:00, 17.64it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 26.45it/s]\n                   all          4         17      0.948      0.383      0.502      0.378\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     43/100     0.885G     0.8295      1.088      1.222         31        640: 100%|██████████| 1/1 [00:00&lt;00:00, 12.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.20it/s]\n                   all          4         17      0.949      0.385      0.503       0.38\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     44/100     0.885G     0.8264      1.095      1.161         30        640: 100%|██████████| 1/1 [00:00&lt;00:00, 15.43it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 26.98it/s]\n                   all          4         17      0.949      0.385      0.503       0.38\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     45/100     0.885G     0.7451      0.901      1.115         34        640: 100%|██████████| 1/1 [00:00&lt;00:00, 12.67it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 26.20it/s]\n                   all          4         17      0.949      0.385      0.505      0.372\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     46/100     0.885G     0.8442      1.273      1.216         43        640: 100%|██████████| 1/1 [00:00&lt;00:00, 17.76it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.85it/s]\n                   all          4         17      0.949      0.385      0.505      0.372\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     47/100     0.885G     0.8889      1.193      1.286         32        640: 100%|██████████| 1/1 [00:00&lt;00:00, 12.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 26.88it/s]\n                   all          4         17      0.783      0.385      0.502      0.386\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     48/100     0.885G     0.9941      1.577      1.356         49        640: 100%|██████████| 1/1 [00:00&lt;00:00, 15.60it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 26.09it/s]\n                   all          4         17      0.783      0.385      0.502      0.386\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     49/100     0.885G     0.6939      1.069       1.18         26        640: 100%|██████████| 1/1 [00:00&lt;00:00, 12.67it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 26.46it/s]\n                   all          4         17       0.78      0.386      0.502      0.385\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     50/100     0.885G      0.759     0.9462      1.106         22        640: 100%|██████████| 1/1 [00:00&lt;00:00, 17.90it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.81it/s]\n                   all          4         17       0.78      0.386      0.502      0.385\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     51/100     0.885G     0.6943     0.6863      1.154         27        640: 100%|██████████| 1/1 [00:00&lt;00:00, 12.69it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.41it/s]\n                   all          4         17      0.946      0.386      0.503      0.386\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     52/100     0.883G      1.136      1.289      1.488         21        640: 100%|██████████| 1/1 [00:00&lt;00:00, 15.60it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 26.54it/s]\n                   all          4         17      0.946      0.386      0.503      0.386\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     53/100     0.885G     0.6884     0.9578      1.217         31        640: 100%|██████████| 1/1 [00:00&lt;00:00, 16.77it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 25.43it/s]\n                   all          4         17      0.946      0.386      0.503      0.386\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     54/100     0.885G     0.7019     0.8484      1.155         28        640: 100%|██████████| 1/1 [00:00&lt;00:00, 12.43it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.89it/s]\n                   all          4         17      0.947      0.386      0.478      0.361\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     55/100     0.885G     0.8294      1.017      1.235         31        640: 100%|██████████| 1/1 [00:00&lt;00:00, 16.68it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 26.64it/s]\n                   all          4         17      0.947      0.386      0.478      0.361\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     56/100     0.885G     0.6298      1.129      1.139         39        640: 100%|██████████| 1/1 [00:00&lt;00:00, 16.32it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.09it/s]\n                   all          4         17      0.947      0.386      0.478      0.361\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     57/100     0.887G     0.8228     0.9488      1.187         36        640: 100%|██████████| 1/1 [00:00&lt;00:00, 11.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 26.67it/s]\n                   all          4         17      0.948      0.387      0.423      0.295\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     58/100     0.885G     0.7947     0.9784      1.207         24        640: 100%|██████████| 1/1 [00:00&lt;00:00, 17.87it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 28.70it/s]\n                   all          4         17      0.948      0.387      0.423      0.295\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     59/100     0.885G     0.5336     0.7548      1.044         26        640: 100%|██████████| 1/1 [00:00&lt;00:00, 17.60it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.07it/s]\n                   all          4         17      0.948      0.387      0.423      0.295\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     60/100     0.885G      0.576     0.7445      1.147         22        640: 100%|██████████| 1/1 [00:00&lt;00:00, 12.89it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.06it/s]\n                   all          4         17      0.949      0.387      0.423      0.297\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     61/100     0.885G     0.9361     0.9783      1.355         32        640: 100%|██████████| 1/1 [00:00&lt;00:00, 15.31it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 26.71it/s]\n                   all          4         17      0.949      0.387      0.423      0.297\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     62/100     0.885G     0.6982     0.7596      1.043         35        640: 100%|██████████| 1/1 [00:00&lt;00:00, 16.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 26.78it/s]\n                   all          4         17      0.949      0.387      0.423      0.297\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     63/100     0.885G     0.7105      1.095      1.192         26        640: 100%|██████████| 1/1 [00:00&lt;00:00, 12.32it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 27.39it/s]\n                   all          4         17      0.947      0.388      0.423      0.297\nStopping training early as no improvement observed in last 50 epochs. Best results observed at epoch 13, best model saved as best.pt.\nTo update EarlyStopping(patience=50) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n\n63 epochs completed in 0.027 hours.\nOptimizer stripped from runs/detect/train2/weights/last.pt, 6.5MB\nOptimizer stripped from runs/detect/train2/weights/best.pt, 6.5MB\n\nValidating runs/detect/train2/weights/best.pt...\nUltralytics YOLOv8.0.197 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (NVIDIA A100-SXM4-40GB, 40514MiB)\nModel summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 32.24it/s]\n                   all          4         17      0.938      0.533       0.84      0.553\n                person          4         10          1      0.198      0.482      0.228\n                   dog          4          1          1          0      0.995      0.443\n                 horse          4          2      0.945          1      0.995      0.699\n              elephant          4          2          1          0       0.58      0.058\n              umbrella          4          1      0.763          1      0.995      0.995\n          potted plant          4          1      0.922          1      0.995      0.895\nSpeed: 0.2ms preprocess, 1.9ms inference, 0.0ms loss, 0.8ms postprocess per image\nResults saved to runs/detect/train2\n\n\nultralytics.utils.metrics.DetMetrics object with attributes:\n\nap_class_index: array([ 0, 16, 17, 20, 25, 58])\nbox: ultralytics.utils.metrics.Metric object\nconfusion_matrix: &lt;ultralytics.utils.metrics.ConfusionMatrix object at 0x78a0206fdc00&gt;\ncurves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\ncurves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,   0.0010854,  0.00054271,           0],\n       [          1,           1,           1, ...,           1,           1,           0],\n       [          1,           1,           1, ...,           1,           1,           0],\n       [          1,           1,           1, ...,   0.0013347,  0.00066733,           0],\n       [          1,           1,           1, ...,           1,           1,           0],\n       [          1,           1,           1, ...,           1,           1,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.10227,     0.10227,     0.12498, ...,           0,           0,           0],\n       [    0.11111,     0.11111,     0.12499, ...,           0,           0,           0],\n       [    0.11111,     0.11111,     0.17025, ...,           0,           0,           0],\n       [        0.4,         0.4,     0.46741, ...,           0,           0,           0],\n       [        0.1,         0.1,      0.1431, ...,           0,           0,           0],\n       [    0.18182,     0.18182,     0.22654, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.054217,    0.054217,    0.067783, ...,           1,           1,           1],\n       [   0.058824,    0.058824,    0.066663, ...,           1,           1,           1],\n       [   0.058824,    0.058824,    0.093047, ...,           1,           1,           1],\n       [    0.33333,     0.33333,      0.4388, ...,           1,           1,           1],\n       [   0.052632,    0.052632,    0.077062, ...,           1,           1,           1],\n       [        0.1,         0.1,     0.12774, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[        0.9,         0.9,         0.8, ...,           0,           0,           0],\n       [          1,           1,           1, ...,           0,           0,           0],\n       [          1,           1,           1, ...,           0,           0,           0],\n       [        0.5,         0.5,         0.5, ...,           0,           0,           0],\n       [          1,           1,           1, ...,           0,           0,           0],\n       [          1,           1,           1, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\nfitness: 0.5817263856635563\nkeys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\nmaps: array([    0.22776,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,     0.44323,      0.6985,       0.553,       0.553,       0.058,       0.553,       0.553,       0.553,\n             0.553,       0.995,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,\n             0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,      0.8955,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,\n             0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553,       0.553])\nnames: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\nplot: True\nresults_dict: {'metrics/precision(B)': 0.9384332295736534, 'metrics/recall(B)': 0.533083083083083, 'metrics/mAP50(B)': 0.8402904053187541, 'metrics/mAP50-95(B)': 0.5529970501463121, 'fitness': 0.5817263856635563}\nsave_dir: PosixPath('runs/detect/train2')\nspeed: {'preprocess': 0.17499923706054688, 'inference': 1.9223690032958984, 'loss': 0.0029802322387695312, 'postprocess': 0.8054971694946289}\ntask: 'detect'\n\n\n\nmodel.val()  ## step3. 모델 평가\n\nUltralytics YOLOv8.0.197 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (NVIDIA A100-SXM4-40GB, 40514MiB)\nModel summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\nval: Scanning /content/datasets/coco8/labels/val.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00&lt;?, ?it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00&lt;00:00, 11.59it/s]\n                   all          4         17      0.939      0.533       0.84      0.554\n                person          4         10          1      0.198      0.481      0.233\n                   dog          4          1          1          0      0.995      0.443\n                 horse          4          2      0.946          1      0.995      0.699\n              elephant          4          2          1          0       0.58      0.058\n              umbrella          4          1      0.763          1      0.995      0.995\n          potted plant          4          1      0.924          1      0.995      0.895\nSpeed: 0.2ms preprocess, 4.6ms inference, 0.0ms loss, 1.9ms postprocess per image\nResults saved to runs/detect/val2\nWARNING ⚠️ 'source' is missing. Using 'source=/usr/local/lib/python3.10/dist-packages/ultralytics/assets'.\n\nimage 1/2 /usr/local/lib/python3.10/dist-packages/ultralytics/assets/bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 9.6ms\nimage 2/2 /usr/local/lib/python3.10/dist-packages/ultralytics/assets/zidane.jpg: 384x640 3 persons, 1 tie, 9.8ms\nSpeed: 2.0ms preprocess, 9.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n\n[ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n orig_img: array([[[119, 146, 172],\n         [121, 148, 174],\n         [122, 152, 177],\n         ...,\n         [161, 171, 188],\n         [160, 170, 187],\n         [160, 170, 187]],\n \n        [[120, 147, 173],\n         [122, 149, 175],\n         [123, 153, 178],\n         ...,\n         [161, 171, 188],\n         [160, 170, 187],\n         [160, 170, 187]],\n \n        [[123, 150, 176],\n         [124, 151, 177],\n         [125, 155, 180],\n         ...,\n         [161, 171, 188],\n         [160, 170, 187],\n         [160, 170, 187]],\n \n        ...,\n \n        [[183, 182, 186],\n         [179, 178, 182],\n         [180, 179, 183],\n         ...,\n         [121, 111, 117],\n         [113, 103, 109],\n         [115, 105, 111]],\n \n        [[165, 164, 168],\n         [173, 172, 176],\n         [187, 186, 190],\n         ...,\n         [102,  92,  98],\n         [101,  91,  97],\n         [103,  93,  99]],\n \n        [[123, 122, 126],\n         [145, 144, 148],\n         [176, 175, 179],\n         ...,\n         [ 95,  85,  91],\n         [ 96,  86,  92],\n         [ 98,  88,  94]]], dtype=uint8)\n orig_shape: (1080, 810)\n path: '/usr/local/lib/python3.10/dist-packages/ultralytics/assets/bus.jpg'\n probs: None\n save_dir: None\n speed: {'preprocess': 2.4955272674560547, 'inference': 9.618997573852539, 'postprocess': 2.129793167114258},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n orig_img: array([[[44, 51, 76],\n         [43, 50, 75],\n         [41, 48, 73],\n         ...,\n         [20, 18, 54],\n         [18, 16, 52],\n         [17, 15, 51]],\n \n        [[44, 51, 76],\n         [43, 50, 75],\n         [41, 48, 73],\n         ...,\n         [20, 18, 54],\n         [18, 16, 52],\n         [18, 16, 52]],\n \n        [[44, 51, 76],\n         [43, 50, 75],\n         [41, 48, 73],\n         ...,\n         [21, 18, 57],\n         [19, 16, 55],\n         [18, 15, 54]],\n \n        ...,\n \n        [[53, 44, 40],\n         [52, 43, 39],\n         [51, 42, 38],\n         ...,\n         [50, 50, 38],\n         [51, 51, 39],\n         [52, 52, 40]],\n \n        [[53, 44, 40],\n         [52, 43, 39],\n         [51, 42, 38],\n         ...,\n         [50, 50, 38],\n         [51, 51, 39],\n         [52, 52, 40]],\n \n        [[53, 44, 40],\n         [52, 43, 39],\n         [51, 42, 38],\n         ...,\n         [49, 49, 37],\n         [51, 51, 39],\n         [52, 52, 40]]], dtype=uint8)\n orig_shape: (720, 1280)\n path: '/usr/local/lib/python3.10/dist-packages/ultralytics/assets/zidane.jpg'\n probs: None\n save_dir: None\n speed: {'preprocess': 1.5532970428466797, 'inference': 9.807109832763672, 'postprocess': 1.6529560089111328}]\n\n\n\nmodel.predict(save = True)  ## step4. 예측\n\nWARNING ⚠️ 'source' is missing. Using 'source=/usr/local/lib/python3.10/dist-packages/ultralytics/assets'.\n\nimage 1/2 /usr/local/lib/python3.10/dist-packages/ultralytics/assets/bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 11.3ms\nimage 2/2 /usr/local/lib/python3.10/dist-packages/ultralytics/assets/zidane.jpg: 384x640 3 persons, 1 tie, 12.4ms\nSpeed: 2.6ms preprocess, 11.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\nResults saved to runs/detect/predict\n\n\n[ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n orig_img: array([[[119, 146, 172],\n         [121, 148, 174],\n         [122, 152, 177],\n         ...,\n         [161, 171, 188],\n         [160, 170, 187],\n         [160, 170, 187]],\n \n        [[120, 147, 173],\n         [122, 149, 175],\n         [123, 153, 178],\n         ...,\n         [161, 171, 188],\n         [160, 170, 187],\n         [160, 170, 187]],\n \n        [[123, 150, 176],\n         [124, 151, 177],\n         [125, 155, 180],\n         ...,\n         [161, 171, 188],\n         [160, 170, 187],\n         [160, 170, 187]],\n \n        ...,\n \n        [[183, 182, 186],\n         [179, 178, 182],\n         [180, 179, 183],\n         ...,\n         [121, 111, 117],\n         [113, 103, 109],\n         [115, 105, 111]],\n \n        [[165, 164, 168],\n         [173, 172, 176],\n         [187, 186, 190],\n         ...,\n         [102,  92,  98],\n         [101,  91,  97],\n         [103,  93,  99]],\n \n        [[123, 122, 126],\n         [145, 144, 148],\n         [176, 175, 179],\n         ...,\n         [ 95,  85,  91],\n         [ 96,  86,  92],\n         [ 98,  88,  94]]], dtype=uint8)\n orig_shape: (1080, 810)\n path: '/usr/local/lib/python3.10/dist-packages/ultralytics/assets/bus.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 3.505229949951172, 'inference': 11.263132095336914, 'postprocess': 2.3272037506103516},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n orig_img: array([[[44, 51, 76],\n         [43, 50, 75],\n         [41, 48, 73],\n         ...,\n         [20, 18, 54],\n         [18, 16, 52],\n         [17, 15, 51]],\n \n        [[44, 51, 76],\n         [43, 50, 75],\n         [41, 48, 73],\n         ...,\n         [20, 18, 54],\n         [18, 16, 52],\n         [18, 16, 52]],\n \n        [[44, 51, 76],\n         [43, 50, 75],\n         [41, 48, 73],\n         ...,\n         [21, 18, 57],\n         [19, 16, 55],\n         [18, 15, 54]],\n \n        ...,\n \n        [[53, 44, 40],\n         [52, 43, 39],\n         [51, 42, 38],\n         ...,\n         [50, 50, 38],\n         [51, 51, 39],\n         [52, 52, 40]],\n \n        [[53, 44, 40],\n         [52, 43, 39],\n         [51, 42, 38],\n         ...,\n         [50, 50, 38],\n         [51, 51, 39],\n         [52, 52, 40]],\n \n        [[53, 44, 40],\n         [52, 43, 39],\n         [51, 42, 38],\n         ...,\n         [49, 49, 37],\n         [51, 51, 39],\n         [52, 52, 40]]], dtype=uint8)\n orig_shape: (720, 1280)\n path: '/usr/local/lib/python3.10/dist-packages/ultralytics/assets/zidane.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.77764892578125, 'inference': 12.398242950439453, 'postprocess': 2.211332321166992}]"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#모델링-1",
    "href": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#모델링-1",
    "title": "Extra 02. 시각지능",
    "section": "(1) 모델링",
    "text": "(1) 모델링\n\n모델 선언\n\nn, s, m, l, x 순으로 모델의 크기가 크다\n모델의 구조와 해당 구조에 맞게 사전 학습된 가중치를 불러온다.\nParameters\n\nmodel : 모델 구조 또는 모델 구조 + 가중치 설정. task와 맞는 모델을 선택해야 한다.\ntask : detect, segment, classify, pose 중 택일\n\n\n\nmodel = YOLO(model='yolov8n.pt', task='detect')\n\n- 모델 학습\n\nParameters\n\ndata : 학습시킬 데이터셋의 경로. default ‘coco128.yaml’\nepochs : 학습 데이터 전체를 총 몇 번씩 학습시킬 것인지 설정. default 100\npatience : 학습 과정에서 성능 개선이 발생하지 않을 때 몇 epoch 더 지켜볼 것인지 설정. default 50\nbatch : 미니 배치의 사이즈 설정. default 16. -1일 경우 자동 설정.\nimgsz : 입력 이미지의 크기. default 640\nsave : 학습 과정을 저장할 것인지 설정. default True\nproject : 학습 과정이 저장되는 폴더의 이름.\nname : project 내부에 생성되는 폴더의 이름.\nexist_ok : 동일한 이름의 폴더가 있을 때 덮어씌울 것인지 설정. default False\npretrained : 사전 학습된 모델을 사용할 것인지 설정. default False\noptimizer : 경사 하강법의 세부 방법 설정. default ‘auto’\nverbose : 학습 과정을 상세하게 출력할 것인지 설정. default False\nseed : 재현성을 위한 난수 설정\nresume : 마지막 학습부터 다시 학습할 것인지 설정. default False\nfreeze : 첫 레이어부터 몇 레이어까지 기존 가중치를 유지할 것인지 설정. default None"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#모델-학습",
    "href": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#모델-학습",
    "title": "Extra 02. 시각지능",
    "section": "(2) 모델 학습",
    "text": "(2) 모델 학습\n\nmodel.train(data='coco128.yaml',  ## 학습시킬 데이터셋의 경로. default 'coco128.yaml'\n            epochs=10, ##  학습 데이터 전체를 총 몇 번씩 학습시킬 것인지 설정. default 100\n            patience=5, ## 학습 과정에서 성능 개선이 발생하지 않을 때 몇 epoch 더 지켜볼 것인지 설정. default 50\n            save=True, ## 학습 과정을 저장할 것인지 설정. default True\n            project='trained', ## 학습 과정이 저장되는 폴더의 이름.\n            name='trained_model', ## project 내부에 생성되는 폴더의 이름.\n            exist_ok=False, ## 동일한 이름의 폴더가 있을 때 덮어씌울 것인지 설정. default False\n            pretrained= True, ##  사전 학습된 모델을 사용할 것인지 설정. default False\n            optimizer='auto', ## 경사 하강법의 세부 방법 설정. default 'auto'\n            verbose=True, ##  학습 과정을 상세하게 출력할 것인지 설정. default False\n            seed=2023,\n            resume=False, ## 마지막 학습부터 다시 학습할 것인지 설정. default False\n            freeze=None ## 첫 레이어부터 몇 레이어까지 기존 가중치를 유지할 것인지 설정. default None\n            )\n\nUltralytics YOLOv8.0.197 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (NVIDIA A100-SXM4-40GB, 40514MiB)\nengine/trainer: task=detect, mode=train, model=yolov8n.pt, data=coco128.yaml, epochs=10, patience=5, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=trained, name=trained_model, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=2023, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=trained/trained_model2\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \nModel summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n\nTransferred 355/355 items from pretrained weights\nTensorBoard: Start with 'tensorboard --logdir trained/trained_model2', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\nAMP: checks passed ✅\ntrain: Scanning /content/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00&lt;?, ?it/s]\nalbumentations: Blur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\nval: Scanning /content/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00&lt;?, ?it/s]\nPlotting labels to trained/trained_model2/labels.jpg... \noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to trained/trained_model2\nStarting training for 10 epochs...\nClosing dataloader mosaic\nalbumentations: Blur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/10      2.63G      1.069      1.101      1.127        129        640: 100%|██████████| 8/8 [00:03&lt;00:00,  2.58it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:00&lt;00:00,  6.62it/s]\n                   all        128        929      0.707      0.601      0.675      0.502\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/10      2.58G     0.9988     0.9911      1.108        113        640: 100%|██████████| 8/8 [00:00&lt;00:00, 10.90it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:00&lt;00:00,  6.90it/s]\n                   all        128        929      0.678      0.648      0.698      0.513\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/10      2.58G      1.013     0.9847      1.109        118        640: 100%|██████████| 8/8 [00:00&lt;00:00, 11.81it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:00&lt;00:00,  6.97it/s]\n                   all        128        929      0.719      0.639      0.708      0.524\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/10      2.58G      1.027      1.012      1.113         68        640: 100%|██████████| 8/8 [00:00&lt;00:00, 11.91it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:00&lt;00:00,  7.07it/s]\n                   all        128        929      0.745      0.644      0.721      0.535\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/10      2.59G      1.041     0.9985      1.136         97        640: 100%|██████████| 8/8 [00:00&lt;00:00, 12.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:00&lt;00:00,  6.81it/s]\n                   all        128        929       0.78      0.652      0.732      0.543\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/10      2.58G      1.036     0.9964      1.122        125        640: 100%|██████████| 8/8 [00:00&lt;00:00, 12.40it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:00&lt;00:00,  7.17it/s]\n                   all        128        929      0.796      0.658      0.734      0.549\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/10      2.58G      1.029     0.9944       1.12         75        640: 100%|██████████| 8/8 [00:00&lt;00:00, 12.53it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:00&lt;00:00,  6.80it/s]\n                   all        128        929      0.805      0.664      0.739       0.55\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/10      2.58G      1.009     0.9899      1.089        143        640: 100%|██████████| 8/8 [00:00&lt;00:00, 13.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:00&lt;00:00,  6.67it/s]\n                   all        128        929      0.813      0.661      0.744      0.555\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/10      2.58G      1.046      1.018      1.138        112        640: 100%|██████████| 8/8 [00:00&lt;00:00, 13.34it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:00&lt;00:00,  7.18it/s]\n                   all        128        929       0.79      0.674      0.748      0.559\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/10      2.58G      1.072       1.07       1.15        171        640: 100%|██████████| 8/8 [00:00&lt;00:00, 11.83it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:00&lt;00:00,  7.29it/s]\n                   all        128        929      0.793      0.674      0.749      0.561\n\n10 epochs completed in 0.006 hours.\nOptimizer stripped from trained/trained_model2/weights/last.pt, 6.5MB\nOptimizer stripped from trained/trained_model2/weights/best.pt, 6.5MB\n\nValidating trained/trained_model2/weights/best.pt...\nUltralytics YOLOv8.0.197 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (NVIDIA A100-SXM4-40GB, 40514MiB)\nModel summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:06&lt;00:00,  1.70s/it]\n                   all        128        929      0.794      0.675       0.75      0.561\n                person        128        254      0.929      0.616      0.797      0.582\n               bicycle        128          6      0.996      0.333       0.39      0.325\n                   car        128         46      0.729      0.234      0.371      0.222\n            motorcycle        128          5       0.83        0.8      0.962      0.745\n              airplane        128          6      0.803          1      0.995      0.862\n                   bus        128          7      0.907      0.714       0.72      0.646\n                 train        128          3      0.811          1      0.995      0.852\n                 truck        128         12      0.835        0.5      0.561      0.367\n                  boat        128          6      0.707      0.333      0.676      0.394\n         traffic light        128         14          1       0.21      0.223      0.149\n             stop sign        128          2      0.774          1      0.995      0.746\n                 bench        128          9          1      0.519       0.79      0.589\n                  bird        128         16      0.932      0.863      0.957      0.714\n                   cat        128          4      0.899          1      0.995      0.799\n                   dog        128          9      0.834      0.889      0.961      0.798\n                 horse        128          2      0.864          1      0.995      0.597\n              elephant        128         17      0.876      0.941      0.947      0.742\n                  bear        128          1      0.647          1      0.995      0.895\n                 zebra        128          4      0.885          1      0.995      0.972\n               giraffe        128          9      0.952          1      0.995      0.752\n              backpack        128          6          1      0.332      0.523      0.371\n              umbrella        128         18      0.747      0.611      0.796      0.528\n               handbag        128         19      0.709      0.132      0.391      0.174\n                   tie        128          7      0.733      0.714      0.822      0.606\n              suitcase        128          4      0.753          1      0.895      0.612\n               frisbee        128          5      0.919        0.8      0.799      0.714\n                  skis        128          1      0.752          1      0.995      0.547\n             snowboard        128          7      0.707      0.571      0.698       0.55\n           sports ball        128          6          1      0.396      0.613      0.357\n                  kite        128         10      0.793      0.386      0.637      0.215\n          baseball bat        128          4      0.825        0.5      0.505      0.267\n        baseball glove        128          7      0.978      0.429      0.432      0.304\n            skateboard        128          5      0.561        0.6      0.697      0.531\n         tennis racket        128          7      0.512      0.286      0.594       0.39\n                bottle        128         18      0.616      0.268      0.473      0.284\n            wine glass        128         16      0.847        0.5      0.755      0.445\n                   cup        128         36      0.887      0.333      0.553      0.406\n                  fork        128          6          1      0.276      0.399      0.285\n                 knife        128         16      0.757      0.585      0.678        0.4\n                 spoon        128         22      0.559      0.364      0.474      0.255\n                  bowl        128         28      0.728      0.766      0.766      0.658\n                banana        128          1       0.92          1      0.995      0.117\n              sandwich        128          2      0.781          1      0.995      0.995\n                orange        128          4      0.771          1      0.895      0.676\n              broccoli        128         11      0.709      0.364      0.438      0.328\n                carrot        128         24      0.823      0.708      0.829       0.51\n               hot dog        128          2      0.684          1      0.995      0.995\n                 pizza        128          5      0.896          1      0.995      0.929\n                 donut        128         14      0.619          1      0.943      0.866\n                  cake        128          4      0.853          1      0.995      0.921\n                 chair        128         35      0.591      0.496      0.566      0.375\n                 couch        128          6      0.679      0.833      0.931      0.751\n          potted plant        128         14      0.824      0.786       0.83      0.566\n                   bed        128          3          1      0.948      0.995      0.847\n          dining table        128         13      0.799      0.611      0.615      0.516\n                toilet        128          2       0.88          1      0.995      0.946\n                    tv        128          2          1      0.954      0.995      0.846\n                laptop        128          3      0.641      0.333       0.72      0.644\n                 mouse        128          2          1          0          0          0\n                remote        128          8      0.898      0.625      0.688      0.611\n            cell phone        128          8          0          0      0.149     0.0358\n             microwave        128          3      0.806          1      0.995      0.865\n                  oven        128          5      0.742      0.581      0.551      0.406\n                  sink        128          6      0.647      0.313      0.427      0.307\n          refrigerator        128          5      0.768        0.8      0.848      0.688\n                  book        128         29      0.684      0.299       0.47      0.251\n                 clock        128          9      0.874      0.889      0.913      0.792\n                  vase        128          2       0.56          1      0.828      0.795\n              scissors        128          1      0.641          1      0.995      0.398\n            teddy bear        128         21      0.723       0.62      0.819      0.552\n            toothbrush        128          5          1       0.93      0.995      0.652\nSpeed: 1.2ms preprocess, 0.4ms inference, 0.0ms loss, 0.7ms postprocess per image\nResults saved to trained/trained_model2\n\n\nultralytics.utils.metrics.DetMetrics object with attributes:\n\nap_class_index: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 11, 13, 14, 15, 16, 17, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 79])\nbox: ultralytics.utils.metrics.Metric object\nconfusion_matrix: &lt;ultralytics.utils.metrics.ConfusionMatrix object at 0x78a1df66ecb0&gt;\ncurves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\ncurves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,   0.0016689,  0.00083444,           0],\n       [          1,           1,           1, ...,  0.00023279,   0.0001164,           0],\n       [          1,           1,           1, ...,  0.00014963,  7.4815e-05,           0],\n       ...,\n       [          1,           1,           1, ...,           1,           1,           0],\n       [          1,           1,           1, ...,        0.28,        0.28,           0],\n       [          1,           1,           1, ...,           1,           1,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.12255,     0.12255,     0.14931, ...,           0,           0,           0],\n       [   0.037879,    0.037879,    0.060096, ...,           0,           0,           0],\n       [       0.05,        0.05,    0.064582, ...,           0,           0,           0],\n       ...,\n       [   0.039216,    0.039216,    0.054993, ...,           0,           0,           0],\n       [    0.12174,     0.12174,      0.1391, ...,           0,           0,           0],\n       [   0.080645,    0.080645,     0.11764, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.065638,    0.065638,    0.081267, ...,           1,           1,           1],\n       [    0.01938,     0.01938,    0.031466, ...,           1,           1,           1],\n       [   0.025997,    0.025997,    0.033973, ...,           1,           1,           1],\n       ...,\n       [       0.02,        0.02,    0.028274, ...,           1,           1,           1],\n       [   0.064815,    0.064815,    0.074751, ...,           1,           1,           1],\n       [   0.042017,    0.042017,    0.062493, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.92126,     0.92126,     0.91732, ...,           0,           0,           0],\n       [    0.83333,     0.83333,     0.66667, ...,           0,           0,           0],\n       [    0.65217,     0.65217,     0.65217, ...,           0,           0,           0],\n       ...,\n       [          1,           1,           1, ...,           0,           0,           0],\n       [          1,           1,           1, ...,           0,           0,           0],\n       [          1,           1,           1, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\nfitness: 0.5798387975060392\nkeys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\nmaps: array([    0.58161,     0.32497,      0.2223,     0.74509,     0.86212,     0.64631,     0.85231,       0.367,     0.39376,     0.14908,     0.56099,     0.74642,     0.56099,     0.58939,     0.71419,     0.79927,     0.79763,       0.597,     0.56099,     0.56099,     0.74179,      0.8955,     0.97195,     0.75161,\n           0.37102,     0.52847,     0.17428,      0.6055,     0.61189,     0.71413,     0.54725,     0.55013,     0.35706,     0.21519,     0.26706,     0.30428,     0.53079,     0.56099,     0.38993,     0.28448,     0.44538,     0.40643,     0.28485,     0.39966,     0.25522,     0.65847,     0.11666,     0.56099,\n             0.995,     0.67642,     0.32773,     0.50958,       0.995,      0.9294,     0.86582,     0.92111,     0.37457,     0.75071,     0.56613,     0.84729,     0.51573,     0.94565,       0.846,     0.64364,           0,     0.61124,     0.56099,    0.035772,     0.86514,      0.4056,     0.56099,     0.30748,\n           0.68774,     0.25075,     0.79201,     0.79517,       0.398,     0.55206,     0.56099,     0.65175])\nnames: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\nplot: True\nresults_dict: {'metrics/precision(B)': 0.7944368225022634, 'metrics/recall(B)': 0.6745558873077249, 'metrics/mAP50(B)': 0.7495226621010267, 'metrics/mAP50-95(B)': 0.5609850347732628, 'fitness': 0.5798387975060392}\nsave_dir: PosixPath('trained/trained_model2')\nspeed: {'preprocess': 1.1724922806024551, 'inference': 0.4428420215845108, 'loss': 0.00037066638469696045, 'postprocess': 0.72428397834301}\ntask: 'detect'\n\n\n\n예측값 생성\n\nParameters\n\nsource : 예측 대상 이미지/동영상의 경로\nconf : confidence score threshold. default 0.25\niou : NMS에 적용되는 IoU threshold. default 0.7. threshold를 넘기면 같은 object를 가리키는 거라고 판단.\nsave : 예측된 이미지/동영상을 저장할 것인지 설정. default False\nsave_txt : Annotation 정보도 함께 저장할 것인지 설정. default False\nsave_conf : Annotation 정보 맨 끝에 Confidence Score도 추가할 것인지 설정. default False\nline_width : 그려지는 박스의 두께 설정. default None\n\n\n\nmodel.predict(source='https://images.pexels.com/photos/139303/pexels-photo-139303.jpeg',\n              save=True, save_txt=True)\n\n\nDownloading https://images.pexels.com/photos/139303/pexels-photo-139303.jpeg to 'pexels-photo-139303.jpeg'...\n100%|██████████| 6.94M/6.94M [00:00&lt;00:00, 85.1MB/s]\nimage 1/1 /content/pexels-photo-139303.jpeg: 448x640 21 persons, 7 cars, 1 traffic light, 125.6ms\nSpeed: 3.2ms preprocess, 125.6ms inference, 2.3ms postprocess per image at shape (1, 3, 448, 640)\nResults saved to trained/trained_model3\n1 label saved to trained/trained_model3/labels\n\n\n[ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n orig_img: array([[[ 71,  64,  47],\n         [ 69,  62,  45],\n         [ 71,  64,  47],\n         ...,\n         [182, 191, 205],\n         [181, 190, 204],\n         [183, 192, 206]],\n \n        [[ 72,  65,  48],\n         [ 72,  65,  48],\n         [ 76,  69,  52],\n         ...,\n         [183, 192, 206],\n         [184, 193, 207],\n         [187, 196, 210]],\n \n        [[ 75,  68,  51],\n         [ 78,  71,  54],\n         [ 81,  74,  57],\n         ...,\n         [185, 194, 208],\n         [185, 194, 208],\n         [186, 195, 209]],\n \n        ...,\n \n        [[170, 169, 171],\n         [170, 169, 171],\n         [169, 168, 170],\n         ...,\n         [208, 216, 229],\n         [205, 213, 226],\n         [200, 208, 221]],\n \n        [[169, 168, 170],\n         [168, 167, 169],\n         [168, 167, 169],\n         ...,\n         [206, 214, 227],\n         [203, 211, 224],\n         [199, 207, 220]],\n \n        [[173, 172, 174],\n         [170, 169, 171],\n         [167, 166, 168],\n         ...,\n         [205, 213, 226],\n         [202, 210, 223],\n         [198, 206, 219]]], dtype=uint8)\n orig_shape: (3592, 5388)\n path: '/content/pexels-photo-139303.jpeg'\n probs: None\n save_dir: 'trained/trained_model3'\n speed: {'preprocess': 3.210306167602539, 'inference': 125.57768821716309, 'postprocess': 2.302885055541992}]\n\n\n- 간판을 보고 신호등이라고 예측함…\n\n오브젝트 디텍션의 문제점 1. 작은 개체들을 잘 인식하지 못함.."
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#locale-설정",
    "href": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#locale-설정",
    "title": "Extra 02. 시각지능",
    "section": "locale 설정",
    "text": "locale 설정\n\nimport locale\ndef getpreferredencoding(do_setlocale = True):\n    return \"UTF-8\"\nlocale.getpreferredencoding = getpreferredencoding"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#데이터-로드",
    "href": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#데이터-로드",
    "title": "Extra 02. 시각지능",
    "section": "데이터 로드",
    "text": "데이터 로드\n- 데이터 링크\n\n!pip install roboflow\n\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"CFN8PqXoTmjvuzqg4YVV\")\nproject = rf.workspace(\"joaoissamu\").project(\"objectdetect-fs9hx\")\ndataset = project.version(1).download(\"yolov8\")\n\nRequirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.7)\nRequirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2022.12.7)\nRequirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.0.0)\nRequirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.10.0)\nRequirement already satisfied: idna==2.10 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.10)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\nRequirement already satisfied: numpy&gt;=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.23.5)\nRequirement already satisfied: opencv-python-headless==4.8.0.74 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.8.0.74)\nRequirement already satisfied: Pillow&gt;=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\nRequirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.4.7)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\nRequirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.31.0)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\nRequirement already satisfied: supervision in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.15.0)\nRequirement already satisfied: urllib3&gt;=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.6)\nRequirement already satisfied: tqdm&gt;=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.1)\nRequirement already satisfied: PyYAML&gt;=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\nRequirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;roboflow) (1.1.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;roboflow) (4.43.1)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;roboflow) (23.2)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;roboflow) (3.3.0)\nRequirement already satisfied: scipy&lt;2.0.0,&gt;=1.9.0 in /usr/local/lib/python3.10/dist-packages (from supervision-&gt;roboflow) (1.11.3)\nloading Roboflow workspace...\nloading Roboflow project...\nDependency ultralytics==8.0.134 is required but found version=8.0.197, to fix: `pip install ultralytics==8.0.134`\n\n\n\nDownloading Dataset Version Zip in ObjectDetect-1 to yolov8:: 100%|██████████| 18167/18167 [00:00&lt;00:00, 62265.11it/s]\nExtracting Dataset Version Zip to ObjectDetect-1 in yolov8:: 100%|██████████| 952/952 [00:00&lt;00:00, 6824.04it/s]\n\n\nObjectDetect-1 폴더가 생김\n\n데이터 셋의 형식만 yolov8에 맞게 생성되서 온 것임"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#모델링-기존-모델로-예측",
    "href": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#모델링-기존-모델로-예측",
    "title": "Extra 02. 시각지능",
    "section": "모델링 : 기존 모델로 예측",
    "text": "모델링 : 기존 모델로 예측\n\nfrom ultralytics import YOLO\n\n1 모델 로드\n\nmodel = YOLO( model = \"yolov8s.pt\", task = \"detect\" ) ## 두번째로 작은 모델을 불러옴\n\nDownloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt to 'yolov8s.pt'...\n100%|██████████| 21.5M/21.5M [00:00&lt;00:00, 177MB/s]\n\n\n2 모델학습\n\nmodel.train(\n                         data = \"/content/ObjectDetect-1/data.yaml\",  ## 이미지 경로 data.yaml 파일에서 확인\n                         epochs = 1,\n                          patience = 1,\n                          save = True,\n                         pretrained = True,    ## 사전학습된 가중치까지 가져옴\n                         verbose = False,\n                         seed = 2023\n                    )\n\nUltralytics YOLOv8.0.197 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (NVIDIA A100-SXM4-40GB, 40514MiB)\nengine/trainer: task=detect, mode=train, model=yolov8s.pt, data=/content/ObjectDetect-1/data.yaml, epochs=1, patience=1, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=2023, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train9\nOverriding model.yaml nc=80 with nc=4\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n 22        [15, 18, 21]  1   2117596  ultralytics.nn.modules.head.Detect           [4, [128, 256, 512]]          \nModel summary: 225 layers, 11137148 parameters, 11137132 gradients, 28.7 GFLOPs\n\nTransferred 349/355 items from pretrained weights\nTensorBoard: Start with 'tensorboard --logdir runs/detect/train9', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\nAMP: checks passed ✅\ntrain: Scanning /content/ObjectDetect-1/train/labels... 248 images, 0 backgrounds, 0 corrupt: 100%|██████████| 248/248 [00:00&lt;00:00, 1297.09it/s]\ntrain: New cache created: /content/ObjectDetect-1/train/labels.cache\nalbumentations: Blur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\nval: Scanning /content/ObjectDetect-1/valid/labels... 198 images, 0 backgrounds, 0 corrupt: 100%|██████████| 198/198 [00:00&lt;00:00, 1195.04it/s]\nval: New cache created: /content/ObjectDetect-1/valid/labels.cache\nPlotting labels to runs/detect/train9/labels.jpg... \noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \noptimizer: AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/train9\nStarting training for 1 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n        1/1      3.97G      1.175      3.516      1.653         19        640: 100%|██████████| 16/16 [00:03&lt;00:00,  4.51it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:04&lt;00:00,  1.55it/s]\n                   all        198        224      0.442      0.478      0.318       0.21\n\n1 epochs completed in 0.004 hours.\nOptimizer stripped from runs/detect/train9/weights/last.pt, 22.5MB\nOptimizer stripped from runs/detect/train9/weights/best.pt, 22.5MB\n\nValidating runs/detect/train9/weights/best.pt...\nUltralytics YOLOv8.0.197 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (NVIDIA A100-SXM4-40GB, 40514MiB)\nModel summary (fused): 168 layers, 11127132 parameters, 0 gradients, 28.4 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:04&lt;00:00,  1.69it/s]\n                   all        198        224      0.442      0.477      0.318      0.211\nSpeed: 1.0ms preprocess, 0.9ms inference, 0.0ms loss, 1.7ms postprocess per image\nResults saved to runs/detect/train9\n\n\nultralytics.utils.metrics.DetMetrics object with attributes:\n\nap_class_index: array([0, 1, 2, 3])\nbox: ultralytics.utils.metrics.Metric object\nconfusion_matrix: &lt;ultralytics.utils.metrics.ConfusionMatrix object at 0x7d421013aa10&gt;\ncurves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\ncurves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.54545,     0.54545,     0.54545, ...,     0.12338,     0.12338,           0],\n       [          1,           1,           1, ...,   0.0003213,  0.00016065,           0],\n       [          1,           1,           1, ...,  5.4153e-05,  2.7076e-05,           0],\n       [        0.6,         0.6,         0.6, ...,  0.00027141,   0.0001357,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[  0.0020534,   0.0020534,   0.0020534, ...,     0.31526,     0.28644,    0.093023],\n       [   0.025078,    0.025078,    0.025078, ...,           0,           0,           0],\n       [   0.006533,    0.006533,    0.006533, ...,           0,           0,           0],\n       [   0.015514,    0.015514,    0.015514, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[  0.0010277,   0.0010277,   0.0010277, ...,     0.47131,      0.5175,         0.4],\n       [   0.012712,    0.012712,    0.012712, ...,           1,           1,           1],\n       [  0.0032787,   0.0032787,   0.0032787, ...,           1,           1,           1],\n       [  0.0078212,   0.0078212,   0.0078212, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,     0.23684,     0.19803,    0.052632],\n       [    0.92079,     0.92079,     0.92079, ...,           0,           0,           0],\n       [    0.87879,     0.87879,     0.87879, ...,           0,           0,           0],\n       [    0.94231,     0.94231,     0.94231, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\nfitness: 0.22144024745415936\nkeys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\nmaps: array([    0.28454,     0.10796,     0.33447,     0.11585])\nnames: {0: 'device', 1: 'live', 2: 'mask', 3: 'photo'}\nplot: True\nresults_dict: {'metrics/precision(B)': 0.4423546022223553, 'metrics/recall(B)': 0.47715051480153536, 'metrics/mAP50(B)': 0.31803908723835606, 'metrics/mAP50-95(B)': 0.21070704303369303, 'fitness': 0.22144024745415936}\nsave_dir: PosixPath('runs/detect/train9')\nspeed: {'preprocess': 1.0087634577895657, 'inference': 0.9465494541206745, 'loss': 0.0005779844341856061, 'postprocess': 1.7342784188010476}\ntask: 'detect'"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#예측-2",
    "href": "posts/DX/05. 딥러닝/2023-10-11-Extra 02. 시각지능.html#예측-2",
    "title": "Extra 02. 시각지능",
    "section": "예측",
    "text": "예측\n- 경로 : runs -&gt; detect&gt; -&gt; predict\n\nresults = model.predict(source='https://images.pexels.com/photos/139303/pexels-photo-139303.jpeg',\n               save=True, save_txt=True, line_width=2)\n\nFound https://images.pexels.com/photos/139303/pexels-photo-139303.jpeg locally at pexels-photo-139303.jpeg\nResults saved to runs/detect/predict5\n1 label saved to runs/detect/predict5/labels\n\n\n- 별로 예측을 잘 하지는 않음\n\nepochs = 1 이라서 그런 것 같음,,\n\n\nfor r in results :\n          boxes = r.boxes\n\n\nboxes\n\nultralytics.engine.results.Boxes object with attributes:\n\ncls: tensor([0., 0., 0., 0., 0.], device='cuda:0')\nconf: tensor([0.6365, 0.4913, 0.4734, 0.4352, 0.2725], device='cuda:0')\ndata: tensor([[2.2125e+03, 0.0000e+00, 3.2761e+03, 3.2103e+03, 6.3646e-01, 0.0000e+00],\n        [2.8615e+03, 0.0000e+00, 3.8910e+03, 3.1475e+03, 4.9131e-01, 0.0000e+00],\n        [3.6554e+03, 0.0000e+00, 5.3880e+03, 3.2764e+03, 4.7340e-01, 0.0000e+00],\n        [2.2317e+03, 0.0000e+00, 2.8250e+03, 2.9325e+03, 4.3523e-01, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 1.8446e+03, 3.4361e+03, 2.7247e-01, 0.0000e+00]], device='cuda:0')\nid: None\nis_track: False\norig_shape: (3592, 5388)\nshape: torch.Size([5, 6])\nxywh: tensor([[2744.3359, 1605.1661, 1063.5923, 3210.3323],\n        [3376.2246, 1573.7708, 1029.5210, 3147.5415],\n        [4521.7246, 1638.2061, 1732.5503, 3276.4121],\n        [2528.3904, 1466.2367,  593.2974, 2932.4734],\n        [ 922.3008, 1718.0380, 1844.6017, 3436.0759]], device='cuda:0')\nxywhn: tensor([[0.5093, 0.4469, 0.1974, 0.8937],\n        [0.6266, 0.4381, 0.1911, 0.8763],\n        [0.8392, 0.4561, 0.3216, 0.9121],\n        [0.4693, 0.4082, 0.1101, 0.8164],\n        [0.1712, 0.4783, 0.3424, 0.9566]], device='cuda:0')\nxyxy: tensor([[2212.5398,    0.0000, 3276.1321, 3210.3323],\n        [2861.4641,    0.0000, 3890.9851, 3147.5415],\n        [3655.4497,    0.0000, 5388.0000, 3276.4121],\n        [2231.7417,    0.0000, 2825.0391, 2932.4734],\n        [   0.0000,    0.0000, 1844.6017, 3436.0759]], device='cuda:0')\nxyxyn: tensor([[0.4106, 0.0000, 0.6080, 0.8937],\n        [0.5311, 0.0000, 0.7222, 0.8763],\n        [0.6784, 0.0000, 1.0000, 0.9121],\n        [0.4142, 0.0000, 0.5243, 0.8164],\n        [0.0000, 0.0000, 0.3424, 0.9566]], device='cuda:0')"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 02. LSTM.html",
    "href": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 02. LSTM.html",
    "title": "summary 02. LSTM",
    "section": "",
    "text": "import tensorflow as tf\nimport os\n\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n\ntf.config.experimental_connect_to_cluster(resolver)\ntf.tpu.experimental.initialize_tpu_system(resolver)\n\n&lt;tensorflow.python.tpu.topology.Topology at 0x79268336a2c0&gt;\n\n\n\nstrategy = tf.distribute.TPUStrategy(resolver)"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 02. LSTM.html#실습-.-imdb-데이터",
    "href": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 02. LSTM.html#실습-.-imdb-데이터",
    "title": "summary 02. LSTM",
    "section": "실습 . IMDB 데이터",
    "text": "실습 . IMDB 데이터\n\nimport tensorflow as tf\n\n\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n\n(1) 데이터 로드 및 전처리\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()\n\n- 주어진 데이터는 정수인코딩이 되어 있으므로 일단 maxlen을 세어보자\n\nmax([len(i) for i in x_train]), max([len(i) for i in x_test])\n\n(2494, 2315)\n\n\n- 이전에 RNN 모형에서 maxlen을 20으로 설정했는데 위에 수치를 보니 그러면 안될 것 같다. 적당히 500정도로 잡자..\n\nx_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen = 500)\nx_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen = 500)\n\n- shape 확인\n\nx_train.shape, x_test.shape\n\n((25000, 500), (25000, 500))\n\n\n- 고유단어개수 확인\n\nlen(set([j for i in x_train for j in i]))\n\n85524\n\n\n\n\n(2) LSTM 모델링\n\ntf.keras.backend.clear_session()\n\nwith strategy.scope() :\n        X = Input(shape = [500])\n\n        h1 = Embedding(input_dim = 85524, output_dim = 120)(X)\n        h2 = LSTM(120)(h1)\n\n        Y = Dense(1, activation = \"sigmoid\")(h2)\n\n        model = Model(X, Y)\n        model.compile(loss = tf.keras.losses.binary_crossentropy,\n                                    optimizer = Adam(0.01))\n\n\n\n(3) 모델 학습\n\nh=model.fit(x_train, y_train, epochs= 10, validation_split = 0.2,\n                  verbose = 0).history\n\n\n\n(4) train, val loss 확인\n\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize = (10,4))\nplt.plot(h[\"loss\"], label = \"train\")\nplt.plot(h[\"val_loss\"], label = \"val\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n(5) 예측\n\npred = model.predict(x_test)\n\n782/782 [==============================] - 15s 17ms/step\n\n\n\nimport numpy as np\n\n\ny_pred = np.where(pred&gt;=0.5, 1, 0)\n\n\nfrom sklearn.metrics import *\n\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.82      0.83      0.82     12500\n           1       0.83      0.82      0.82     12500\n\n    accuracy                           0.82     25000\n   macro avg       0.82      0.82      0.82     25000\nweighted avg       0.82      0.82      0.82     25000\n\n\n\n- 오 아까 RNN보다 성능이 좋게 나왔다"
  },
  {
    "objectID": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 02. LSTM.html#ref",
    "href": "posts/DX/05. 딥러닝/summary/2023-10-18-summary 02. LSTM.html#ref",
    "title": "summary 02. LSTM",
    "section": "ref",
    "text": "ref\nratsgo’s NLPBOOK : Copyright © 2020 Gichang LEE."
  },
  {
    "objectID": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html",
    "href": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html",
    "title": "00. streamlit",
    "section": "",
    "text": "-해당 코드 부분은 *.py에서 작성 후 터미널에서 streamit run *.py를 입력해야 stream io가 만들어 진다."
  },
  {
    "objectID": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#unit-1.-text-elements",
    "href": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#unit-1.-text-elements",
    "title": "00. streamlit",
    "section": "unit 1. Text elements",
    "text": "unit 1. Text elements\n# header, subheader, text, caption 연습하기\n# 공간을 1:1 으로 분할하여 col1과 col2라는 이름을 가진 컬럼을 생성합니다.  \nst.title('Text elements')\nst.caption('text 참고사이트: https://docs.streamlit.io/library/api-reference/text')\n\nst.header(\"Header : 데이터 분석 표현\")\nst.subheader(\"Subheader : 스트림릿\")\n\nst.text(\"Text : this is the Streamlit\")\n\nst.caption(\"Caption : Streamlit은 2019년 하반기에 등장한 파이썬 기반의 웹어플리케이션 툴이다.\")\n\n# markdown 연습하기\n \nst.markdown(\"# **Markdown 1st**\")\n\nst.markdown(\"## Markdown 2nd\")\n\nst.markdown(\"### Markdown 3rd\")\n\nst.markdown(\"***Markdown 진하고 기울임***\")\n\nst.markdown(\"* Markdown 글머리 기호\\n\"\n                \"   * Markdown 글머리 기호\")\n\n    \n\n# Latex & Code 연습하기\nst.markdown('## Code & Latex')\nst.code(\"x = 1234\")\n\nst.latex(r'''\\begin{align*} a + ar + ar^2 + ar^3 + \\cdots + ar^{n-1} \n                                        &= \\sum_{k=0}^{n-1} ar^{k} \\\\\n                                        &= a\\left (\\frac {1-r^{n}}{1-r}\\right)\n                    \\end{align*}''')\n\n# write 연습하기\n\nst.title('write')\nst.caption('참고사이트: https://docs.streamlit.io/library/api-reference/write-magic/st.write')\nst.text('아래 딕셔너리를 판다스 데이터프레임으로 변경')\nst.caption(\"{'이름': ['홍길동', '김사랑', '일지매', '이루리'],'수준': ['금', '동', '은', '은']}\")\ndf = pd.DataFrame({'이름': ['홍길동', '김사랑', '일지매', '이루리'],'수준': ['금', '동', '은', '은']})\nst.write('Below is a DataFrame:', df, 'Above is a dataframe.')\n\n# 파일실행: File &gt; New &gt; Terminal(anaconda prompt) - streamlit run streamlit\\1.text.py"
  },
  {
    "objectID": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#unit-2.-media",
    "href": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#unit-2.-media",
    "title": "00. streamlit",
    "section": "unit 2. media",
    "text": "unit 2. media\n# streamlit 라이브러리 불러오기 \nimport streamlit as st\n\nst.title('Unit 2. Media elements')\nst.caption('참조사이트: https://docs.streamlit.io/library/api-reference/media')\n\nst.header('1. Image')\n# 이미지 주소: https://images.unsplash.com/photo-1548407260-da850faa41e3?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1487&q=80\nst.image('https://images.unsplash.com/photo-1548407260-da850faa41e3?ixlib=rb1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1487&q=80',\ncaption='Sunrise by the mountains')\n\n\nst.header('2. Audio')\n# 오디오 주소: streamlit\\MusicSample.mp3\nst.audio('MusicSample.mp3')\n\n\nst.header('3. Video')\n# 비디오 주소: streamlit\\VideoSample.mp4\nst.video(\"VideoSample.mp4\")\n\n# 파일실행: File &gt; New &gt; Terminal(anaconda prompt) - streamlit run streamlit\\2.media.py"
  },
  {
    "objectID": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#unit-3.-data",
    "href": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#unit-3.-data",
    "title": "00. streamlit",
    "section": "unit 3. data",
    "text": "unit 3. data\n# streamlit 라이브러리 불러오기 \nimport streamlit as st\n\nst.title('Unit 2. Media elements')\nst.caption('참조사이트: https://docs.streamlit.io/library/api-reference/media')\n\nst.header('1. Image')\n# 이미지 주소: https://images.unsplash.com/photo-1548407260-da850faa41e3?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1487&q=80\nst.image('https://images.unsplash.com/photo-1548407260-da850faa41e3?ixlib=rb1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1487&q=80',\ncaption='Sunrise by the mountains')\n\n\nst.header('2. Audio')\n# 오디오 주소: streamlit\\MusicSample.mp3\nst.audio('MusicSample.mp3')\n\n\nst.header('3. Video')\n# 비디오 주소: streamlit\\VideoSample.mp4\nst.video(\"VideoSample.mp4\")\n\n# 파일실행: File &gt; New &gt; Terminal(anaconda prompt) - streamlit run streamlit\\2.media.py"
  },
  {
    "objectID": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#unit-4.-input-1",
    "href": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#unit-4.-input-1",
    "title": "00. streamlit",
    "section": "unit 4. input-1",
    "text": "unit 4. input-1\n# streamlit 라이브러리와 datetime 모듈 불러오기\nimport streamlit as st\nimport pandas as pd\nfrom datetime import datetime  \n\nst.title('Unit 4. Input Widgets')\nst.caption('참조사이트: https://docs.streamlit.io/library/api-reference/widgets')\n\n## 버튼\nst.header('1. Button')\nif st.button(\"Say hello\") :\n    st.write(\"hello\")\nelse :\n    st.write(\"Good bye\")\n\nst.header('2. Radio button')\n\nst.write(\"***\")\n\n## 라디오 버튼\ngenre = st.radio(\"좋아하는 영호 장르를 선택하세요\",\n                 (\"코미디\", \"SF\", \"액션\"))\n\nif genre ==  \"코미디\" :\n    st.write(\"유쾌하신 분이시군요\")\nif genre == \"SF\" :\n    st.write(\"저도 SF 좋아합니다.\")\nelse :\n    st.write(\"멋지십니다.\")\n\nst.write(\"***\")\n\n\n## 체크박스\nst.header('3. Checkbox')    # 😄\n\nagree = st.checkbox(\"I agree\")\n\nif agree : \n    st.write(\"😄\"*10)\n\ne = st.checkbox(\"데이터를 조작하시겠습니까?\")\ntitanic = pd.read_csv(\"https://raw.githubusercontent.com/huhshin/streamlit/master/data_titanic.csv\")\n\nif e :  \n    st.write(titanic)\n\n    \nst.write(\"***\")\n\n\nst.header(\"4. select box\")\n## 셀렉트 박스\noption = st.selectbox(\"어떻게 연락드릴까요?\", (\"Email\", \"Mobile phone\",\"Office phone\"))\n\nst.write(\"네\", option, \"잘 알겟습니다.\")\n\n## 멀티 셀렉트\noptions = st.multiselect(\"좋아하는 색을 모두 선택하세요\",\n                         [\"Greeen\",\"Yellow\", \"Red\", \"Blue\"],\n                         [\"Yellow\",\"Red\"])\n\nst.write(\"선호 색상 : \")\nfor i in options :\n    st.write(i)\n\n\nst.markdown(\"***\")  \n\n\n## 텍스트, 문자, 숫자 입력 받기\n\nst.header(\"5. 텍스트, 문자, 숫자 입력 받기\")\n\ntitle = st.text_input(\"최애 영화를 입력하세요\", \"Sound of Music\")\nst.write(\"당신이 가장 좋아하는 영화는 :\", title)\n\nnumber = st.number_input(\"Insert a number(1-10)\", min_value = 1, max_value = 10, value = 5, step = 1)\nst.write(\"The current number is\", number)\n\nymd = st.date_input(\"When is your birthday\",\n                    datetime(2000, 9, 6))\nst.write(\"Your birthday is : \", ymd)\n\nst.markdown(\"***\")\n\n### slider\n\nst.header(\"6. slider\")\n\nage = st.slider(\"나이가 어떻게 되세요?\", 0,130,25)\nst.write(\"I am\", age,\"years old\")\n\nvalues = st.slider(\"값 구간을 입력하세요\",0.0, 100.0, (25.0, 75.0))\nst.write(\"Values : \", values)\n\n\nst.subheader(\"7. date input\")\n\nslider_date = st.slider(\"날자 구간을 선택하세요\",\n                         min_value = datetime(2022, 1, 1),\n                         max_value = datetime(2022, 12, 31),                              \n                         value = (datetime(2022,6,1),datetime(2022,7,31)))\n\nst.write(\"slider date :\", slider_date)\nst.write(\"slider date[0] :\", slider_date[0], \"slider date[1] :\", slider_date[1])\n\n\n\ndf = pd.read_csv(\"data_subway_in_seoul.csv\", encoding = \"cp949\")\ndf[\"날짜\"] = pd.to_datetime(df[\"날짜\"],format = \"%Y-%m-%d\")\nst.write(\"날짜필드형식 :\", df[\"날짜\"].dtypes)\nst.write(df)\n\n# 년 월 일 시 사이 구간\nslider_time = st.slider(\n     'Select a range of datetime?',\n     datetime(2022, 1, 1, 0, 30), datetime(2022, 12, 31, 0, 30),\n     value=(datetime(2022, 7, 1, 0, 30), datetime(2022, 10, 31, 9, 30)),\n     format='MM/DD/YY - hh:mm')\nst.write('Slider time: ', slider_time)\n\n\n# slider를 사용하여 날짜 구간 설정하기\nst.subheader(\"날짜구간 설정 후, 설정한 구간으로 새로운 df 저장하고 확인\")\n\nslider_date = st.slider(\"날자 구간을 선택하세요.\",\n                         datetime(2022,1,1),datetime(2021,12,31),\n                         value = (datetime(2021,7,1),datetime(2021,7,31)),\n                         format = \"YY/MM/DD\")\n\nst.write(\"시작 날짜 :\", slider_date[0], \"끝날짜 : \", slider_date[1])\nstart = slider_date[0]\nend = slider_date[1]\n\n## df읽기\n\nsel_df = df.loc[df[\"날짜\"].between(start,end)]\nst.dataframe(sel_df)\n# 파일실행: File &gt; New &gt; Terminal(anaconda prompt) - streamlit run streamlit\\4-1.input.py"
  },
  {
    "objectID": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#unit-5-1.-layouts",
    "href": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#unit-5-1.-layouts",
    "title": "00. streamlit",
    "section": "unit 5-1. layouts",
    "text": "unit 5-1. layouts\n\nimport streamlit as st\n\nst.title('Unit 5. Layouts & Containers')\nst.caption('참조사이트: https://docs.streamlit.io/library/api-reference/layout')\n\n# sidebar- with 사용하기 📧  📱  ☎︎\nwith st.sidebar :\n    st.header(\"1. Sidebar\")\n\nadd_selectbox = st.sidebar.selectbox(\"어떻게 연락 드릴까요?\",\n                                    (\"Email\",\"mobile phone\",\"Office phone\"))\n\nif add_selectbox == \"Email\" :\n    st.sidebar.title(\"📧\")\n\nelif add_selectbox == \"Mobiile phone\" :\n    st.sidebar.title(\"📱\")\nelse :\n    st.sidebar.title(\"☎︎\")\n    \n\n# columns  \n# 고양이 https://images.pexels.com/photos/2071873/pexels-photo-2071873.jpeg?auto=compress&cs=tinysrgb\n# 개     https://images.pexels.com/photos/3361739/pexels-photo-3361739.jpeg?auto=compress&cs=tinysrgb\n# 부엉이 https://images.pexels.com/photos/3737300/pexels-photo-3737300.jpeg?auto=compress&cs=tinysrgb\n\nst.header('2. Columns')\n\ncol1, col2, col3 = st.columns(3)\n\nwith col1 :\n    st.text(\"A cat\")\n    st.image(\"https://images.pexels.com/photos/2071873/pexels-photo-2071873.jpeg?auto=compress&cs=tinysrgb\")\n\nwith col2 : \n    st.text(\"A dog\")\n    st.image(\"https://images.pexels.com/photos/3361739/pexels-photo-3361739.jpeg?auto=compress&cs=tinysrgb\")\n    \nwith col3 :\n    st.text(\"An owl\")\n    st.image(\"https://images.pexels.com/photos/3737300/pexels-photo-3737300.jpeg?auto=compress&cs=tinysrgb\")\n    \n \n    \n# tabs - width=200\n\nst.header('3. Tabs')\n\ntab1, tab2, tab3 = st.tabs([\"고양이\", \"개\", \"부엉이\"])\n\nwith tab1 :\n    st.caption(\"A cat\")\n    st.image(\"https://images.pexels.com/photos/2071873/pexels-photo-2071873.jpeg?auto=compress&cs=tinysrgb\", width=200)\n\nwith tab2 : \n    st.caption(\"A dog\")\n    st.image(\"https://images.pexels.com/photos/3361739/pexels-photo-3361739.jpeg?auto=compress&cs=tinysrgb\", width=200)\n    \nwith tab3 :\n    st.caption(\"An owl\")\n    st.image(\"https://images.pexels.com/photos/3737300/pexels-photo-3737300.jpeg?auto=compress&cs=tinysrgb\", width=200)\n\n    \n# 파일실행: File &gt; New &gt; Terminal(anaconda prompt) - streamlit run streamlit\\5-1.layouts.py"
  },
  {
    "objectID": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#unit-5-2.-layoutsstarsstarsstars",
    "href": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#unit-5-2.-layoutsstarsstarsstars",
    "title": "00. streamlit",
    "section": "unit 5-2. layouts(\\(\\stars\\stars\\stars\\))",
    "text": "unit 5-2. layouts(\\(\\stars\\stars\\stars\\))\n### 가장 중요\nimport streamlit as st\nst.caption('참조사이트: https://docs.streamlit.io/library/get-started/multipage-apps/create-a-multipage-app')\n\n# 페이지 선언 (🎈 ❄️ 🎉)\ndef main_page():\n    st.title(\"Main page 🎈\")\n    st.sidebar.title(\"side Main 🎈\")\n    \ndef page2():\n    st.title(\"Page2 ❄️\")\n    st.sidebar.title(\"Side2 ❄️\")\n\ndef page3():\n    st.title(\"Page3 🎉\")\n    st.sidebar.title(\"Side3 🎉\")\n    \n# 딕셔너리 선언 {  ‘selectbox항목’ : ‘페이지명’ …  }\npage_names_to_funcs = {'Main Page': main_page, 'Page 2': page2, 'Page 3': page3}\n\n# 사이드 바에서 selectbox 선언 & 선택 결과 저장\nselected_page = st.sidebar.selectbox(\"Select a page\", page_names_to_funcs.keys())\n\n# 해당 페이지 부르기\npage_names_to_funcs[selected_page]()\n\n# 파일실행: File &gt; New &gt; Terminal(anaconda prompt) - streamlit run streamlit\\5-2.layouts.py"
  },
  {
    "objectID": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#unit-5-3.-layouts",
    "href": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#unit-5-3.-layouts",
    "title": "00. streamlit",
    "section": "unit 5-3. layouts",
    "text": "unit 5-3. layouts\nimport streamlit as st\nimport pandas as pd\n\n# 메인페이지 \n# Iris 사진 나타하기 - https://images.pexels.com/photos/5677011/pexels-photo-5677011.jpeg?auto=compress&cs=tinysrgb&w=200\n# https://raw.githubusercontent.com/huhshin/streamlit/master/data_iris.csv 읽고 나타내기 \ndef main_page():\n    st.header('Main Page 🎈')\n    st.sidebar.title(\"side Main 🎈\")\n    st.image(\"https://images.pexels.com/photos/5677011/pexels-photo-5677011.jpeg?auto=compress&cs=tinysrgb&w=200\")\n    df = pd.read_csv(\"https://raw.githubusercontent.com/huhshin/streamlit/master/data_iris.csv\")\n    st.write(df.head(1))\n\n    # 2페이지: 세 개의 columns으로 나누어 꽃 이름과 사진 나타내기\n\ndef page2():\n    st.header(\"Page 2\")\n    st.sidebar.title(\"Page 2\")\n    col1, col2, col3 = st.columns(3)\n\n    with col1 :\n        st.text('Setosa')\n        st.image('https://m.media-amazon.com/images/I/61pLvdbjC7L._AC_.jpg')\n    \n    with col2 :\n        st.text('Versicolor')\n        st.image('https://upload.wikimedia.org/wikipedia/commons/2/27/Blue_Flag%2C_Ottawa.jpg')\n    \n    with col3 :\n        st.text('Virginica')\n        st.image('https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Iris_virginica_2.jpg/1920px-Iris_virginica_2.jpg')\n\n# 3페이지: 세 개의 tab을 사용하여 iris 3가지 꽃 나타내기 (width=500)\n\ndef page3() :\n    st.header(\"Page 3\")\n    st.sidebar.title(\"Page 3\")\n\n    tab1, tab2, tab3 = st.tabs(['Setosa', 'Versicolor', 'Virginica']) \n\n    with tab1 :\n        st.caption(\"Setosa\")\n        st.image('https://m.media-amazon.com/images/I/61pLvdbjC7L._AC_.jpg', width = 200)\n\n    with tab2 : \n        st.caption(\"Versicolor\")\n        st.image('https://upload.wikimedia.org/wikipedia/commons/2/27/Blue_Flag%2C_Ottawa.jpg', width = 200)\n\n    with tab3 :\n        st.caption(\"Virginica\")\n        st.image('https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Iris_virginica_2.jpg/1920px-Iris_virginica_2.jpg', width = 200)\n\n### 딕셔너리 선언 {  ‘selectbox항목’ : 페이지명 …  }\npage_names_to_funcs = {'Main Page': main_page, 'Page 2': page2, \"Page3\":page3}\n\n# 사이드 바에서 selectbox 선언 & 선택 결과 저장\nselected_page = st.sidebar.selectbox(\"Select a page\", page_names_to_funcs.keys())\n\n# 해당 페이지 부르기\npage_names_to_funcs[selected_page]()\n\n# 파일실행: File &gt; New &gt; Terminal(anaconda prompt) - streamlit run streamlit\\5-3.layouts.py"
  },
  {
    "objectID": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#데이터-핸들링",
    "href": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#데이터-핸들링",
    "title": "00. streamlit",
    "section": "1. 데이터 핸들링",
    "text": "1. 데이터 핸들링\nimport streamlit as st\nimport pandas as pd\n\nst.text('1. 지하철 데이터 읽고 확인- data_subway_in_seoul.csv')   \n# streamlit\\data_subway_in_seoul.csv\n# encoding='cp949'  읽어오고 확인하기 \ndf = pd.read_csv(\"data_subway_in_seoul.csv\", encoding = \"cp949\")\nst.dataframe(df)\n\nst.markdown(\"---\")\nst.text(\"2. 구분이 '하차'인 행만 새로운 데이터프레임으로 저장 & 확인\") \ndf_off = df.loc[df[\"구분\"] == \"하차\"]\nst.dataframe(df_off)\n\nst.markdown(\"---\")\nst.text(\"3. '날짜','연번','역번호','역명','구분','합계' 제외하고 저장 & 확인\")\ndf_line = df_off.drop(['날짜','연번','역번호','역명','구분','합계'], axis = 1)\nst.dataframe(df_line)\n\nst.markdown(\"---\")\nst.text(\"4. 아래 방법으로 데이터프레임 변환하여 저장 & 확인\")\nst.caption(\"melt 함수 사용 unpivot: identifier-'호선', unpivot column-'시간', value column-'인원수'\")\ndf_line_melted = df_line.melt(id_vars = \"호선\", var_name = \"시간\", value_name = \"인원수\")\nst.dataframe(df_line_melted)\n\nst.markdown(\"---\")\nst.text(\"5. '호선','시간' 별 '인원수' 합,  as_index=False 저장 & 확인\")\ndf_line_groupby = df_line_melted.groupby([\"호선\", \"시간\"], as_index = False)[[\"인원수\"]].sum()\nst.dataframe(df_line_groupby)\n\n# 파일실행: File &gt; New &gt; Terminal(anaconda prompt) - streamlit run streamlit\\6-1.datahandling.py"
  },
  {
    "objectID": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#simple-chart",
    "href": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#simple-chart",
    "title": "00. streamlit",
    "section": "2. simple chart",
    "text": "2. simple chart\n#### 1.line chart\nst.markdown(\"---\")\nst.markdown(\"## line chart\")\n\nchart_data = pd.read_csv(\"https://raw.githubusercontent.com/huhshin/streamlit/master/data_sales.csv\")\nst.dataframe(chart_data)\nst.line_chart(chart_data)\n\n#### 2.bar chart\nst.markdown(\"---\")\n\nst.markdown(\"## bar chart\")\nst.bar_chart(chart_data)\n\nst.markdown(\"---\")\n\n## 3.  Simple area chart\nst.markdown(\"## area chart\")\nst.area_chart(chart_data)\n\nst.markdown(\"---\")"
  },
  {
    "objectID": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#altair-chart",
    "href": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#altair-chart",
    "title": "00. streamlit",
    "section": "3. altair chart",
    "text": "3. altair chart\nimport altair as alt\nst.markdown(\"# Altair chart\")\ndf = pd.read_csv('https://raw.githubusercontent.com/huhshin/streamlit/master/data_retail.csv')\ndf_melted = pd.melt(df, id_vars=['date'], var_name='teams', value_name='sales')\n\ncol1, col2 = st.columns(2) \n\nwith col1 :\n    st.text('원본 데이터')\n    st.dataframe(df)\nwith col2 :\n    st.text(\"변환 데이터\")\n    st.dataframe(df_melted)\n\nst.markdown(\"---\")\n\n### line chart\nst.markdown(\"## line chart\")\nchart = alt.Chart(df_melted, title='일별 팀 매출 비교').mark_line().encode(\nx='date', y='sales', color='teams', strokeDash='teams').properties(width=650, height=350)\nst.altair_chart(chart, use_container_width=True)\n\n### bar chart\nst.markdown(\"---\")\nst.markdown(\"## bar chart\")\nchart = alt.Chart(df_melted, title='일별 매출').\\\n                    mark_bar().\\\n                        encode(x='date', y='sales', color='teams')\n\ntext = alt.Chart(df_melted).\\\n                        mark_text(dx=0, dy=0, color='black',size = 8).\\\n                               encode(x='date', y='sales', \n                                      detail='teams', text=alt.Text('sales:Q')) # 소수점 이하 1자리: 'sales:Q',format='.1f'\nst.altair_chart(chart+text, use_container_width=True)\n\n### scatter chart\nst.markdown(\"---\")\nst.markdown(\"## scatter chart\")\n\niris = pd.read_csv(\"https://raw.githubusercontent.com/huhshin/streamlit/master/data_iris.csv\")\n\ncol1, col2 = st.columns([1,2]) ## 컬럼 비율 조정\n\nwith col1 :\n    st.text(\"iris dataframe\")\n    st.dataframe(iris)\nwith col2 : \n    st.text(\"iris scatter chart\")\n    chart = alt.Chart(iris).mark_circle().\\\n                    encode(x = 'petal_length', y='petal_width', \n                           color = 'species') \n    st.altair_chart(chart, use_container_width=True)"
  },
  {
    "objectID": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#plotly-chart",
    "href": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#plotly-chart",
    "title": "00. streamlit",
    "section": "4. plotly chart",
    "text": "4. plotly chart\n# plotly chart\nimport plotly.express as px\n\nst.header('Unit 5. Plotly chart')\n\nst.subheader(\"data check\")\nmedal = pd.read_csv(\"https://raw.githubusercontent.com/huhshin/streamlit/master/data_medal.csv\")\nst.dataframe(medal)\n## pie chart\nst.markdown(\"---\")\nst.subheader(\"pie chart & bar chart\")\n\nco1, col2 = st.columns(2)\n\nfig = px.pie(medal, names = \"nation\", values = \"gold\",\n            title = \"올림픽 양궁 금메달 현황\", hole = .3)\nfig.update_traces(textposition = \"inside\", textinfo=\"percent + label + value\")\nfig.update_layout(font = dict(size = 14)) \n\n# # 범례 표시 제거 : fig.update(layout_showlegend=False) \nst.plotly_chart(fig)\n\n### bar chart\nst.markdown(\"---\")\nst.subheader(\"bar chart\")\n\nfig = px.bar(medal, x=\"nation\", y=[\"gold\", \"silver\", \"bronze\"],\n                            text_auto=True, title=\"올림픽 양궁 메달 현황\") # text_auto=True 값 표시 여부\nfig.update_layout(width=800, height=600 ) # 그래프 크기 조절\n# fig.update_traces(textangle=0) # 그래프 안의 텍스트가 바로 씌여지게 설정\nst.plotly_chart(fig)"
  },
  {
    "objectID": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#map",
    "href": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#map",
    "title": "00. streamlit",
    "section": "5. map",
    "text": "5. map\nst.markdown(\"---\")\nst.header(\"map\")\nmap_data = pd.DataFrame({ 'lat': [-34, 49, -38, 59.93, 5.33, 45.52, -1.29, -12.97],\n                'lon': [-58, 2, 145, 30.32, -4.03, -73.57, 36.82, -38.5],\n                'name': ['Buenos Aires', 'Paris', 'melbourne', 'St Petersbourg', 'Abidjan', 'Montreal', 'Nairobi', 'Salvador'],\n                'value': [10, 12, 40, 70, 23, 43, 100, 43]\n                    })\nst.subheader(\"Aivle Map\")\ncol1, col2 =  st.columns([1.2,2.8])\nwith col1 : \n    if st.checkbox(\"Display Data?\") :\n        st.write(map_data)\nwith col2 : \n    st.map(map_data,\n       latitude='lat',\n       longitude='lon')\n\nst.markdown(\"---\")"
  },
  {
    "objectID": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#folium.map",
    "href": "posts/DX/06. 데이터분석표현/2023-10-23-00. streamlit.html#folium.map",
    "title": "00. streamlit",
    "section": "6. folium.Map",
    "text": "6. folium.Map\nst.subheader(\"folium.map\")\nimport folium\nmy_map = folium.Map( \n                     location = [map_data[\"lat\"].mean(),map_data[\"lon\"].mean()], ## 중심점 잡기\n                     zoom_start = 2 ## 얼마나 가깝게 볼건지\n                )\n\nfor index, row in map_data.iterrows(): # 데이터프레임 한 행 씩 index, row에 담아서 처리\n    folium.CircleMarker( # 원 표시 선언\n                         location=[row['lat'], row['lon']], # 원 중심- 위도, 경도\n                         radius=row['value'] / 5, # 원의 반지름\n                         color='pink', # 원의 테두리 색상\n                         fill=True, # 원을 채움\n                         fill_opacity=1.0 # 원의 내부를 채울 때의 투명도\n                         ).add_to(my_map) # my_map에 원형 마커 추가\n    folium.Marker( # 값 표시 선언\n                        location=[row['lat'], row['lon']], # 값 표시 위치- 위도, 경도\n                         icon=folium.DivIcon(html=f\"&lt;div&gt;{row['name']} {row['value']}&lt;/div&gt;\"), # row['name'], row['value'] 표시\n                         ).add_to(my_map) # my_map에 값 추가\n\n# 타이틀과 캡션 표시하기\nst.title('Map with Location Data')\nst.caption('Displaying geographical data on a map using Streamlit and Folium')\n\n# 지도 그리기\n# st.components.v1.html : Streamlit 라이브러리의 components 모듈에서 html 함수 사용\n# ._repr_html_() : 지도를 HTML 형식으로 표시\nst.components.v1.html(my_map._repr_html_(), width=800, height=600)\n\n\n### covid 시각화\nst.subheader(\"covid 시각화\")\nmap_data = pd.read_csv(\"covid_map.csv\")\nmy_map = folium.Map( \n                     location = [map_data[\"lat\"].mean(),map_data[\"lon\"].mean()], ## 중심점 잡기\n                     zoom_start = 2 ## 얼마나 가깝게 볼건지\n                )\n\nfor index, row in map_data.iterrows(): # 데이터프레임 한 행 씩 index, row에 담아서 처리\n    folium.CircleMarker( # 원 표시 선언\n                         location=[row['lat'], row['lon']], # 원 중심- 위도, 경도\n                         radius=row['value'] / 10000, # 원의 반지름\n                         color='pink', # 원의 테두리 색상\n                         fill=True, # 원을 채움\n                         fill_opacity=1.0 # 원의 내부를 채울 때의 투명도\n                         ).add_to(my_map) # my_map에 원형 마커 추가\n    folium.Marker( # 값 표시 선언\n                        location=[row['lat'], row['lon']], # 값 표시 위치- 위도, 경도\n                         icon=folium.DivIcon(html=f\"&lt;div&gt;{row['name']} {row['value']}&lt;/div&gt;\"), # row['name'], row['value'] 표시\n                         ).add_to(my_map) # my_map에 값 추가\n\n# 타이틀과 캡션 표시하기\nst.title('covid 시각화')\nst.caption('Displaying geographical data on a map using Streamlit and Folium')\nst.components.v1.html(my_map._repr_html_(), width=800, height=600)"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-23-00. 제안전략수립.html",
    "href": "posts/DX/09. 제안전략수립/2023-11-23-00. 제안전략수립.html",
    "title": "00. 제안전략수립",
    "section": "",
    "text": "- 사업의 개념과 도메인 지식이 필요\n- 문제정의\n\n\n\nB2B 산업의 환경구조 \\(\\to\\) 환경분석 프레임\n\n\n\n- 고객사에 대한 충분한 이해가 필요\n\n고객정의(구매센터) \\(\\to\\) 니즈 정의\n\n\n\n\n\n세분화 타겟팅 \\(\\to\\) 포지셔닝\n아이디어 도출\n\n\n\n\n\n가치제안서/실행전략 \\(\\to\\) 비즈니스 모델"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-23-00. 제안전략수립.html#제안컨설팅을-위한-사업화-프로세스",
    "href": "posts/DX/09. 제안전략수립/2023-11-23-00. 제안전략수립.html#제안컨설팅을-위한-사업화-프로세스",
    "title": "00. 제안전략수립",
    "section": "",
    "text": "- 사업의 개념과 도메인 지식이 필요\n- 문제정의\n\n\n\nB2B 산업의 환경구조 \\(\\to\\) 환경분석 프레임\n\n\n\n- 고객사에 대한 충분한 이해가 필요\n\n고객정의(구매센터) \\(\\to\\) 니즈 정의\n\n\n\n\n\n세분화 타겟팅 \\(\\to\\) 포지셔닝\n아이디어 도출\n\n\n\n\n\n가치제안서/실행전략 \\(\\to\\) 비즈니스 모델"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-23-00. 제안전략수립.html#biz-체계의-이해",
    "href": "posts/DX/09. 제안전략수립/2023-11-23-00. 제안전략수립.html#biz-체계의-이해",
    "title": "00. 제안전략수립",
    "section": "Biz 체계의 이해",
    "text": "Biz 체계의 이해\n- biz는 Business의 약자로 상품이나 서비스를 생산하고 판매하며, 수익을 얻는 활동을 의미함\n- biz 체계\n\n비즈니스의 전반적인 구조와 기능을 나타냄\n기업이 제품이나 서비스를 제공하고, 조직 구조, 프로세스, 기술, 인력 등을 포함한 다양한 요소를 통합하여 운영하는 방식을 의미한다\n\n- biz체게는 일반적으로 처음에는 고객중심으로 시작했다가 관리 중심으로 변환함\n\n관리 중심으로 시작한 기업은… 결과가 좋지않음 (관리가 고객의 이익을 침해하면 안됨…)\n\n기업의 존재 이유는 고객이고 기업의 목적은 시장을 창조하는 것이다 -피터 드리커-"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-23-00. 제안전략수립.html#b2b-사업의-본질",
    "href": "posts/DX/09. 제안전략수립/2023-11-23-00. 제안전략수립.html#b2b-사업의-본질",
    "title": "00. 제안전략수립",
    "section": "B2B 사업의 본질",
    "text": "B2B 사업의 본질\n- B2B : Bussiness-to-Bussiness의 약어로, 기업 간에 이루어지는 거래를 나타냄\n\nB2B 비즈니스는 B2B 고객의 성공적인 전략실행을 위한 솔루션을 제시하는 것이다."
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-23-00. 제안전략수립.html#문제의-종류",
    "href": "posts/DX/09. 제안전략수립/2023-11-23-00. 제안전략수립.html#문제의-종류",
    "title": "00. 제안전략수립",
    "section": "문제의 종류",
    "text": "문제의 종류\n\n(1) 탐색형\n- 시간축 : 과거\n- 발생원인 : 기준이탈 \\(\\cdot\\) 미달\n- 성격 : 이미 일어나버린 문제\n\n\n(2) 탐색형\n- 시간축 : 현재\n- 발생원인 : 개선 \\(\\cdot\\) 개량 \\(\\cdot\\) 강화\n- 성격 : 더 잘해보고 싶은 문제\n\n\n(3) 설정형\n- 시간축 : 미래\n- 발생원인 : 개발 \\(\\cdot\\) 기획 \\(\\cdot\\) 리스크 회피\n- 성격 : 앞으로 어떻게 할 것인가의 문제\n\\(\\divideontimes\\) 우리는 일반적으로 설정형으로 문제를 정의하고 해결할 줄 알아야 한다.\n\n\n(4) 요약"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-23-00. 제안전략수립.html#example.-문제정의",
    "href": "posts/DX/09. 제안전략수립/2023-11-23-00. 제안전략수립.html#example.-문제정의",
    "title": "00. 제안전략수립",
    "section": "example. 문제정의",
    "text": "example. 문제정의"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-23-00. 제안전략수립.html#forecasting",
    "href": "posts/DX/09. 제안전략수립/2023-11-23-00. 제안전략수립.html#forecasting",
    "title": "00. 제안전략수립",
    "section": "Forecasting",
    "text": "Forecasting\n- 현재 시점에서 미래를 보는 사고법\n\n활용 가능 자료 수집 \\(\\to\\) 자료분석 \\(\\to\\) Ouput 도출\n\n- 자료 분석에 많은 시간을 들이고 정작 전략 수집에는 소홀하게 됨, 특히 기존 유사 프로젝트와 비슷한 결론이 나올 가능성이 높음"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-23-00. 제안전략수립.html#backcasting",
    "href": "posts/DX/09. 제안전략수립/2023-11-23-00. 제안전략수립.html#backcasting",
    "title": "00. 제안전략수립",
    "section": "Backcasting",
    "text": "Backcasting\n- 미래 시점에서 현재를 보는 사고법\n\nOuput 추론/상상 \\(\\to\\) Output 실현을 위한 조건과 가정 정의 \\(\\to\\) 조건과 가정을 구현하기 위한 방법 정의\n\n- 역가치사슬분석, 보도자료기반 가치정의\n\nProcess\n1 Awarensss : 이상상황 정의\n\n주요 참여자 정의\n이상적 가치 정의 \\(\\to\\) 전체 최적화를 고려 (풍선효과 등을 막기 위한 방안)\n\n\\(\\divideontimes\\) 풍선효과 : 풍선의 한쪽을 누르면 다른 쪽이 불룩 튀어나오는 모습을 빗댄 표현으로, 어떤 현상이나 문제를 억제하면 다른 현상이나 문제가 새로이 불거져 나오는 상황\n2 interests : 이해관계자 정의\n\n이상적 가치 제공 및 운영 시 참여/고려 되는 이해관계자 도출\n이해관계자 니즈 정의\n\n3 Down to Action : 목표 도달을 위한 활동/조건 정의\n\n이상목표 달성을 위한 세부활동 정의 : 참여자/조건 별\n\n4 Baseline : 현 상황 정의\n5 Gap Analysis : Gap 및 장애요소 도출\n6 핵심 성공요소 정의 및 세부 실행계획 정의"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-23-00. 제안전략수립.html#sic",
    "href": "posts/DX/09. 제안전략수립/2023-11-23-00. 제안전략수립.html#sic",
    "title": "00. 제안전략수립",
    "section": "SIC",
    "text": "SIC\n- success image canvas\n- 고려사항 : 사람, 기술, 재료, 방법 등\n- 주요요소\n\n경쟁사/대체재의 미래 제시가치\n필요 인프라 (H/W 및 S/W)\n이해관계자 : 보안/협력자, 관련기관 등\n필요자원"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-27-02. 고객가치정의 및 아이디어 도출.html",
    "href": "posts/DX/09. 제안전략수립/2023-11-27-02. 고객가치정의 및 아이디어 도출.html",
    "title": "02. 고객가치정의 및 아이디어 도출",
    "section": "",
    "text": "- 이번장에서는 STP전략에 대해 세밀하게 배움\n- 3c : customer, company, competitor\n- STP : segmentation, targeting, positioning\n- 4p : product, price, place, promotion"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-27-02. 고객가치정의 및 아이디어 도출.html#시장-세분화의-목적",
    "href": "posts/DX/09. 제안전략수립/2023-11-27-02. 고객가치정의 및 아이디어 도출.html#시장-세분화의-목적",
    "title": "02. 고객가치정의 및 아이디어 도출",
    "section": "시장 세분화의 목적",
    "text": "시장 세분화의 목적\n- 시장 세분화를 통한 마케팅 효과\n\n경쟁우위 확보\n마케팅 기회 발견\n차별화를 통한 가격 경쟁 완화\n\n- B2B 고객 세분화를 통한 부가적 효과\n\n공략 대상 기업에 대한 심도 있는 분석\n공략 우선순위 선정 & 자원의 효율적 분배\n실행전략 구체화\n\n- 세분화를 위한 핵심 요소 : Activity + interests + opinion = life style\n\n즉, 비슷한 life style을 가진 사람은 동일한 need를 가질 것이다!\n\n- 성공적 시장세분화의 5가지 조건\n\n측정 가능성 : 세분화에 사용된 벼수들은 측정 가능하고 Data를 구할 수 있는가?\n접근 가능성 : 최소한의 비용과 투자로 접근이 가능한가?\n시장의 중요성 : 이윤을 보장할 만큼 충분히 큰가?\n차별성 : 세분화된 시장들은 마케팅활동에 각각 다르게 반응하는가?\n실행가능성 : 세분 시장을 공랴하기 위한 효과적이고 실행 가능한 마케팅 프로그램이 개발될 수 있는가?"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-27-02. 고객가치정의 및 아이디어 도출.html#b2b-세분화",
    "href": "posts/DX/09. 제안전략수립/2023-11-27-02. 고객가치정의 및 아이디어 도출.html#b2b-세분화",
    "title": "02. 고객가치정의 및 아이디어 도출",
    "section": "B2B 세분화",
    "text": "B2B 세분화\n- B2B 세분화의 핵심은 공통의 니즈를 중심으로 정의하는 것임\n- 따라서, 고객을 다양한 집단으로 분류하기 보다는 고객 조지사의 하나의 니즈를 더 깊고 자세하게 분석하여 파악할 필요가 있음\n\n(1) 유형\n\n프로그램형 (Programmatic): 자동화된 시스템을 사용하여 특정 조건을 충족하는 비즈니스 거래를 식별하고 처리하는 방식. 프로그래밍된 소프트웨어나 알고리즘을 활용하여 효율적인 거래를 이룸.\n\n\n예시: 프로그램매틱 광고, 자동화된 주문 및 공급망 관리.\n\n\n관계지향형 (Relationship-Based): 장기적인 협력과 관계 구축을 강조. 고객과의 깊은 관계를 중시하며, 상호 신뢰와 협력이 중요\n\n\n예시: 장기 계약, 고객 충성도 프로그램, 맞춤형 서비스 제공.\n\n\n거래중시형 (Transaction-Based): 거래중시형 B2B 세분화에서는 단기적이며 거래 중심의 관계가 강조. 주로 일회성 거래나 특정 거래 이벤트에 중점.\n\n\n예시: 온라인 거래 플랫폼을 통한 짧은 기간의 거래, 경매.\n\n\n교섭 사냥꾼형 (Hunters vs. Farmers): “사냥꾼”은 새로운 고객을 발굴하고 획득, “농부”는 기존 고객과의 관계를 유지하고 키움.\n\n\n예시: 사냥꾼은 신규 고객 확보, 농부는 기존 고객 관리 및 유지.\n\n\\(\\divideontimes\\) 그래프 참고 (ppt 116)"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-27-02. 고객가치정의 및 아이디어 도출.html#nested-approach",
    "href": "posts/DX/09. 제안전략수립/2023-11-27-02. 고객가치정의 및 아이디어 도출.html#nested-approach",
    "title": "02. 고객가치정의 및 아이디어 도출",
    "section": "(2) Nested Approach",
    "text": "(2) Nested Approach\nNested Approach는 조직의 인구통계적인 Macro 변수부터 개인 변수인 Micro변수까지 구체화하여 고객을 세분화하는 방법임\n\n조직의 인구 통계 : 조직의 크기, 산업 부문, 지리적 위치, 기업 문화 등과 같은 특성\n운영변수들(구매정책 등) : ex) 삼성과 LG는 서로의 부품을 구매하지 않음\n\n\n구매 정책, 협력 관계, 생산 및 공급망 전략 등을 포함\n\n\n구매 접근\n\n\n조직이 상품이나 서비스를 구매하는 방식(중앙 집중형, 분산형, 전략적 관리 등)\n\n\n상황 변수들 : 특정한 시점이나 환경에서 조직이 직면하는 변수들. 이는 특별한 프로젝트, 시장 변동성, 경기 변동 등을 포함\n\n\n1개 또는 여러개의 공급 업체 선택\n\n\n개인변수들 : 조직 내 구매 의사 결정에 영향을 미치는 개인들의 특성\n\n\n업무단순화, 성과금 등"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-27-02. 고객가치정의 및 아이디어 도출.html#c",
    "href": "posts/DX/09. 제안전략수립/2023-11-27-02. 고객가치정의 및 아이디어 도출.html#c",
    "title": "02. 고객가치정의 및 아이디어 도출",
    "section": "3C",
    "text": "3C\n1 고객 : 주요 타겟 고객 KBF 규명\n2 경쟁 : 경쟁사와의 차별성 확보\n3 자사 : 자사의 강점 반영"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-27-02. 고객가치정의 및 아이디어 도출.html#비교-우위-파악",
    "href": "posts/DX/09. 제안전략수립/2023-11-27-02. 고객가치정의 및 아이디어 도출.html#비교-우위-파악",
    "title": "02. 고객가치정의 및 아이디어 도출",
    "section": "비교 우위 파악",
    "text": "비교 우위 파악\n1 상품 차별화 : 기능, 성능, 디자인 또는 속성\n2 서비스 차별회 : 배달, 정치, 수리, 고객 교육\n3 이미지 차별화 : 상징, 분위기\n4 인적 차별화 : 고용, 훈련, 우수한 인적 자원\n\\(\\divideontimes\\) 포지셔닝을 통해 기업은 시장 경쟁구조, 자사제품의 경쟁사대비 위치 파악, 신제품 기회 발견 및 기존 제품의 포지셔닝 변경을 수행한다."
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-27-02. 고객가치정의 및 아이디어 도출.html#가치제안-구성요소-1.-솔루션",
    "href": "posts/DX/09. 제안전략수립/2023-11-27-02. 고객가치정의 및 아이디어 도출.html#가치제안-구성요소-1.-솔루션",
    "title": "02. 고객가치정의 및 아이디어 도출",
    "section": "가치제안 구성요소 1. 솔루션",
    "text": "가치제안 구성요소 1. 솔루션\n- 고객과 판매자가 협력하여 개발해야함\n\n그렇지 않으면. 고객들은 판매자가 일반적으로 개발한 가치 제안에서 결점을 찾아내거나 가치제안 전체를 거부"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-27-02. 고객가치정의 및 아이디어 도출.html#가치제안-구성요소-2.-차별화",
    "href": "posts/DX/09. 제안전략수립/2023-11-27-02. 고객가치정의 및 아이디어 도출.html#가치제안-구성요소-2.-차별화",
    "title": "02. 고객가치정의 및 아이디어 도출",
    "section": "가치제안 구성요소 2. 차별화",
    "text": "가치제안 구성요소 2. 차별화\n\n(1) 차별화를 위한 기본구조\n\n독점시장에서 가치 제안 : Value &gt; Cost\n경쟁시장에서 가치 제안 : 자사(Value-Cost) &gt; 경쟁사(Value-Cost)\n\n\n\n(2) 차별화 추구를 위한 고려요소\n1 정서가치 : 사용 가치를 통해 개인의 가치관과 생활에 변화를 주는 심리적 가치(Persona)\n2 기능가치 : 제품/서비스가 고객에게 제공하는 물리적 속성(Attribute)\n3 사용가치 : 물리적 속성으로 부터 고객이 얻는 이익(Benefit)"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-27-02. 고객가치정의 및 아이디어 도출.html#효과적인-가치-제안의-3요소",
    "href": "posts/DX/09. 제안전략수립/2023-11-27-02. 고객가치정의 및 아이디어 도출.html#효과적인-가치-제안의-3요소",
    "title": "02. 고객가치정의 및 아이디어 도출",
    "section": "효과적인 가치 제안의 3요소",
    "text": "효과적인 가치 제안의 3요소\n\nQuantify : 예측되는 성과 향상의 정량화\nTiming : 효율의 발생, 비용 부가, 투자금액 회수 시기\nMeasurement & Monitoring : 성과의 측정과 추적 방법 명시"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-29-04. 사업정의.html",
    "href": "posts/DX/09. 제안전략수립/2023-11-29-04. 사업정의.html",
    "title": "04. 사업정의",
    "section": "",
    "text": "- 사업의 본질\n\n식당에서는 음식을 파는 것이 본질이 아닌 가족(사람)과 행복한 시간을 보내는 것이 본질이다.\n즉, 위 예시처럼 사업의 본질을 파악하는 것이 비즈니스 모델을 이해하는데 중요하다.\n\n\n\n- 고객가치 = 필요성 \\(\\times\\) 차별성\n\n고객가치를 잘 찾으려면 3c분석이 중요(후에 따로 설명)\n즉, 3c 분석에서 고객과 자사 분석을 통해 필요성을 찾고, 경쟁사 분석을 통해 차별성을 찾아 고객이 인지할 수 있도록 솔루션을 만들어야 한다\n필요성 : 표현된 니즈 중심\n차별성 : 표현하지 않은 니즈 중심\n\n조사결과 내가 몰랐던 사실을 발견하면 조사를 잘한 것임\n\n\n- 현재 가치는 단순한 가치에서 복잡한 가치로 변했음. \\(\\to\\) ex) 맥심에서 캡슐커피, 무색커피 등등\n\n그러나 이 또한 어떻게 바뀔지 모른다… 가치는 항상 변하기 때문!\n\n\n\n\n\n- biz 모델 : 수익 모델을 설게해서 고객이 추구하는 가치모델을 창출\n\n일반적으로 수익 모델 설계는 비용은 적게 매출은 크게 설계함\n\n\n\n\n\n목표 고객\n가치 제안 : 어떤 가치, 솔루션을?\n가치사슬/조직 : 어떻게 창출할 것인가?\n전달방식 : 어떻게 유통할 것인지\n수익 흐름 : 어떻게 기업의 수익으로 연결할 것인가?\n\n\n\n\n1 혁신자(Rule breaker) : 특정 사업의 모델이나 게임 룰을 파괴하여 자신 나름대로 독특한 모델을 만드는 기업 (ex : amazon)\n2 선도자(Rule maker) : 처음 사적을 개척한 기업 (Airbnb)\n3 모방자(Rule taker) : 선도 기업이 만든 비즈니스 모델을 따라하는 기업 (디즈니+)\n\n\n\n\n\n1 명확한 고객가치 제안\n2 수익 메커니즘\n\n일반적으로 프렌차이즈 매장은 수익 메커니즘(cash flow pipline)이 여러개임 \\(\\to\\) 가맹비, 원재료 공급, 부자재, 일부 마케팅\n그래서 동네 매장에 비해 유지가 잘됨\n\n\n\n\n3 선순환 구조\n4 모방 불가능"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-29-04. 사업정의.html#고객가치",
    "href": "posts/DX/09. 제안전략수립/2023-11-29-04. 사업정의.html#고객가치",
    "title": "04. 사업정의",
    "section": "",
    "text": "- 고객가치 = 필요성 \\(\\times\\) 차별성\n\n고객가치를 잘 찾으려면 3c분석이 중요(후에 따로 설명)\n즉, 3c 분석에서 고객과 자사 분석을 통해 필요성을 찾고, 경쟁사 분석을 통해 차별성을 찾아 고객이 인지할 수 있도록 솔루션을 만들어야 한다\n필요성 : 표현된 니즈 중심\n차별성 : 표현하지 않은 니즈 중심\n\n조사결과 내가 몰랐던 사실을 발견하면 조사를 잘한 것임\n\n\n- 현재 가치는 단순한 가치에서 복잡한 가치로 변했음. \\(\\to\\) ex) 맥심에서 캡슐커피, 무색커피 등등\n\n그러나 이 또한 어떻게 바뀔지 모른다… 가치는 항상 변하기 때문!"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-29-04. 사업정의.html#biz-model의-구조",
    "href": "posts/DX/09. 제안전략수립/2023-11-29-04. 사업정의.html#biz-model의-구조",
    "title": "04. 사업정의",
    "section": "",
    "text": "- biz 모델 : 수익 모델을 설게해서 고객이 추구하는 가치모델을 창출\n\n일반적으로 수익 모델 설계는 비용은 적게 매출은 크게 설계함"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-29-04. 사업정의.html#biz-model의-구성요소",
    "href": "posts/DX/09. 제안전략수립/2023-11-29-04. 사업정의.html#biz-model의-구성요소",
    "title": "04. 사업정의",
    "section": "",
    "text": "목표 고객\n가치 제안 : 어떤 가치, 솔루션을?\n가치사슬/조직 : 어떻게 창출할 것인가?\n전달방식 : 어떻게 유통할 것인지\n수익 흐름 : 어떻게 기업의 수익으로 연결할 것인가?"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-29-04. 사업정의.html#biz-model별-to-be-요소",
    "href": "posts/DX/09. 제안전략수립/2023-11-29-04. 사업정의.html#biz-model별-to-be-요소",
    "title": "04. 사업정의",
    "section": "",
    "text": "1 혁신자(Rule breaker) : 특정 사업의 모델이나 게임 룰을 파괴하여 자신 나름대로 독특한 모델을 만드는 기업 (ex : amazon)\n2 선도자(Rule maker) : 처음 사적을 개척한 기업 (Airbnb)\n3 모방자(Rule taker) : 선도 기업이 만든 비즈니스 모델을 따라하는 기업 (디즈니+)"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-29-04. 사업정의.html#성공적인-biz-model-조건",
    "href": "posts/DX/09. 제안전략수립/2023-11-29-04. 사업정의.html#성공적인-biz-model-조건",
    "title": "04. 사업정의",
    "section": "",
    "text": "1 명확한 고객가치 제안\n2 수익 메커니즘\n\n일반적으로 프렌차이즈 매장은 수익 메커니즘(cash flow pipline)이 여러개임 \\(\\to\\) 가맹비, 원재료 공급, 부자재, 일부 마케팅\n그래서 동네 매장에 비해 유지가 잘됨\n\n\n\n\n3 선순환 구조\n4 모방 불가능"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-29-04. 사업정의.html#partenership-breadth",
    "href": "posts/DX/09. 제안전략수립/2023-11-29-04. 사업정의.html#partenership-breadth",
    "title": "04. 사업정의",
    "section": "Partenership breadth",
    "text": "Partenership breadth\n1 Employees (직원들): 기업이나 조직의 일부로서 일하는 개인들을 나타냄.\n2 Volunteering(자원봉사) : 개인이나 그룹이 비영리적인 목적으로 시간, 노력, 또는 기술을 기부하는 활동\n\n업은 종종 직원들을 독려하여 지역사회나 사회적인 문제에 대한 자원봉사 활동에 참여하게 하고, 이는 기업의 사회적 책임 활동 중 하나임\n\n3 Programmatic : 광고 소재의 구매와 판매를 자동화하고 최적화하는 기술적인 접근\n\n대개 실시간으로 이루아지며, 데이타와 알고리즘을 활용하여 효율적인 디지털광고 캠페인 실행을 목표로함.\n\n4 Marketing : 제품이나 서비스를 소비자에게 홍보하고 판매하기 위한 전략적인 활동\n5 Commercial : 이윤을 창출하거나 사업 목적을 달성하기 위한 활동\n\n제품이나 서비스의 판매, 이익 창출 등을 목표로 함"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-29-04. 사업정의.html#가치창출-유형",
    "href": "posts/DX/09. 제안전략수립/2023-11-29-04. 사업정의.html#가치창출-유형",
    "title": "04. 사업정의",
    "section": "가치창출 유형",
    "text": "가치창출 유형\n1 자사 - 가치 - 고객\n2 자사 + 파트너 핵심가치 - 가치 \\(\\to\\) 고객\n3 자사 - 가치(&lt;- 파트너 부가가치) \\(\\to\\) 고객\n4 자사 - 가치(&lt;- 고객) \\(\\to\\) 고객 : 고객이 더 가치를 의미있게 만들어줌"
  },
  {
    "objectID": "posts/DX/09. 제안전략수립/2023-11-29-04. 사업정의.html#고객목표관리",
    "href": "posts/DX/09. 제안전략수립/2023-11-29-04. 사업정의.html#고객목표관리",
    "title": "04. 사업정의",
    "section": "고객목표관리",
    "text": "고객목표관리\n\n(1) KPI\n- Key Performance Indicator\n\nTop-down 방식으로 결정 : 실제 해옫ㅇ으로 옮겨야 하는 하부조직에서 결정한 행동이 아닌 위에서 시킨일을 해야하는 경우가 많음\n보상과 직접적으로 연결 ; 실제적인 이니셔티브 보다 보상을 받는 쪽으로 실행이 집중될 수 있음\n연단위로 수립 : 연중에 수정이 어려워 즉각적인 대응이 어려움\n\n과거의 결과에 대한 성적표를 기준으로 평가를 하기 때문에 조직을 미래지향적으로 만들기 어려움\n\n\n\n\n(2) OKR\n- 목표(Objective) + 핵심 결과(Key Results)\n\n고객이 중요한 의사결정을 하기전에 힐요한 정보를 제공 \\(\\to\\) 고객선호도, 인지도 \\(\\uparrow\\)\n이를 통해 판매가 이루어진다!\n\n\n\n(3) 성과목표의 기본 원칙(SMART)\nSpecific(구체적인 목표) 목표 달성 여부를 확인할 수 있는 명확한 기준을 수립\n\n육하원칙에 답하여 목표 설정\n\nMeasurable(측정 가능한 목표)\n\n목표의 정량화/수치화\n얼마나/어떻게 많이 측정할 것인가?\n달성했는지 어떻게 확인할 것인가?\n\nAchievable (달성 가능한 목표) : 달성할 수 있는 범위의 목표를 수립\n\n성취할 수 있는 범위인가\n통제 불가능한 요소는 무엇인가?\n도움을 줄 수 있는 사람은 누구인가?\n\nRealistic(실현 가능한 목표) : 한계점/장애물을 파악하고 실현할 수 있는 목표를 수립\n\n성취할 수 있는 범위인가\n통제 불가능한 요소는 무엇인가?\n도움을 줄 수 있는 사람은 누구인가?\n\nTime-bound(기한 설정된 목표) ; 목표 달성을 이룰 마감 기한을 설정\n\n언제 시작/끝낼 것인가?\n얼마 동안 지속할 것인가?\n벤치마크(단기목표 달성지점)는 언제인가?\n\n\n\n(4) 실행 모니터링\n1 단기목표와 실행지표 Gap 발생 여부(Yes)\n2 목표 설정의 적절성 평가\n3 경쟁우위 유지여부 평가\n4 실행 적합성 여부 평가\n\\(\\divideontimes\\) 실행 모니터링에 따른 시나리오 수립(ppt 241 참고)"
  },
  {
    "objectID": "posts/DX/2023-07-31-00.intro.html",
    "href": "posts/DX/2023-07-31-00.intro.html",
    "title": "00. Intro & setting",
    "section": "",
    "text": "- 어… 일단 평소에도 quarto를 이용해서 웹사이트를 관리했지만… 뭔가 처음 깃허브를 접하구 하시는 분들은 이 플랫폼을 사용할 때 되게 난항이 있을것 같다… (내가 그랬다…)\n- 그리고 원래 만들어 놓았던 사이트는 뭔가 좀 지저분한 느낌이 들어서….\n- 에이블스쿨 하면서 배운것들 기록할 때는 뭔가 깔끔한 공간에 하고 싶기도 하다.\n- 이참에 절차를 확실히 내가 적어두자!\n\n\n- quarto download link : 여기서 quarto를 다운받자!\n\n\n\n- Terminal을 켠다음에 아래와 같은 명령어를 입력한다!\n(그.. 명령어 입력할 때 현재 자기 주피터 킬때 켜지는 폴더로 옮긴 다음에 수행하자… 골치 아프다ㅜㅜ)\nquarto create project website gcsite\n- 그러면 다음과 같은 이미지가 보인다\n\n- 저기 open with 어찌고 보이는데 d버튼 누르면 (don’t open)으로 넘어가니 그걸 선택한 후 엔터를 눌러준다!\n- 그러면 아래 이미지처럼 맨 밑에 gcsite라는 폴더가 생긴 것을 볼 수 있다.\n\n\n\n\n- git bash 쓰는 사람들 많던데 난 github desktop이 훨씬 편하다.\n- git 알못이기 때문에 많은 것을 알기 위해 괴롭고 싶지 않다.\n- 뭐 여튼 깃허브 데스크탑을 킨다.\n- 상단 메뉴바 \\(\\to\\) File \\(\\to\\) Add local repository\n- 그러면 아래와 같은 경고문이 뜬다.\n\n- local하고 연결하고 싶은데 깃허브에는 gcsite가 없으니 대충 만들어 달라는 것임 “create a repository” 를 눌러주자.\n\n- 무시, 걍 create repository ㄱㄱ\n- 그러면 깃허브 데스크탑에서 너 방금 만든거 너꺼 깃허브에 Publish 할거냐고 물어봄\n\n- Publish repository 눌러주면 끝~~ (단, publish할 때 private 체크박스는 해제하구 하자!)\n- 그 다음 내가 생성산 gcsite 저장소 setting으로 넘어가서 pages를 클릭!\n- 아래와 같이 branch를 수정 후 save 버튼 눌러주자\n\n\n\n\n- quarto 원리 : 작성한 ipynb파일 html파일로 출력해서 그 html파일들로 웹사이트를 구성하는 것1\n- step1. posts와 docs라는 폴더를 만들자\n\nposts는 내가 작성하는 ipynb파일들이 들어갈거고, docs에는 html파일이 들어갈 것이다.\n\n- step2. index 파일 수정\n\nindex파일은 뭐랄까 네비게이터 역할이랄까 아래와 같이 바꿔주자\n\n---\ntitle: \"GC site\"\nlisting:\n  contents: posts\n  sort: [date desc, title]\n  type: table\n  categories: true\n  sort-ui: true\n  filter-ui: true\npage-layout: full\ntitle-block-banner: true\n---\n- step3. _quarto.uml 파일 수정 \\(\\to\\) 템플릿이랑 디자인 이쁜거 많으니 본인 입맛에 맞게 수정하면 됩니당\nproject:\n  type: website\n  output-dir : docs  \nwebsite:\n  title: \"GC site\"\n  page-navigation: true\n  navbar:\n    right:\n      - icon : github\n        href : https://github.com/gangcheol/\n  sidebar:\n    style: \"docked\"\n    search: True\n    contents: auto\n    \nformat:\n  html:\n    css: styles.css\n    toc: true\n    code-fold : False\n    code-line-numbers : True\n    code-copy : True\n\ntheme :\n  light : flatly\n  \neditor : visual\n- step4. 앞서 만든 posts폴더에 아무 파일이나 만들어보자\n\n- step5. 그 후 다시 터미널에서 내가 생성한 폴더로 이동\n필자의 경우는 cd gcsite\n- step6. quarto render 입력\n- step7. github desktop보면 난리가 났을 것이다. 막 일을 좀 많이 했음.\n\n로컬하고 연결되어 있으니 로컬이 하고 있는 걸 다적어서 그럼\n\n\n\n저기 내가 밑에 이러한 기록을 init이라고 써놨다. 저건 내가 로컬에서 한 행동을 내 깃허브 로컬에 저장할 건데, 그 행동을 init이라고 쓴거\n이제 저 Commit to main 버튼을 눌러주고 가운데 화면에 뜨는 push origin을 눌러주자!\n\n- 마지막!! 아까 깃허브 로컬 셋팅에서 pases란에 잠시 후에 들어가보면 다음과 같은 것을 볼 수 있다.\n\n- 저 링크로 들어가면 내가 만든 웹사이트 초안을 볼 수 있다.\n- 링크"
  },
  {
    "objectID": "posts/DX/2023-07-31-00.intro.html#install",
    "href": "posts/DX/2023-07-31-00.intro.html#install",
    "title": "00. Intro & setting",
    "section": "",
    "text": "- quarto download link : 여기서 quarto를 다운받자!"
  },
  {
    "objectID": "posts/DX/2023-07-31-00.intro.html#website-생성",
    "href": "posts/DX/2023-07-31-00.intro.html#website-생성",
    "title": "00. Intro & setting",
    "section": "",
    "text": "- Terminal을 켠다음에 아래와 같은 명령어를 입력한다!\n(그.. 명령어 입력할 때 현재 자기 주피터 킬때 켜지는 폴더로 옮긴 다음에 수행하자… 골치 아프다ㅜㅜ)\nquarto create project website gcsite\n- 그러면 다음과 같은 이미지가 보인다\n\n- 저기 open with 어찌고 보이는데 d버튼 누르면 (don’t open)으로 넘어가니 그걸 선택한 후 엔터를 눌러준다!\n- 그러면 아래 이미지처럼 맨 밑에 gcsite라는 폴더가 생긴 것을 볼 수 있다."
  },
  {
    "objectID": "posts/DX/2023-07-31-00.intro.html#깃허브-로컬-연결",
    "href": "posts/DX/2023-07-31-00.intro.html#깃허브-로컬-연결",
    "title": "00. Intro & setting",
    "section": "",
    "text": "- git bash 쓰는 사람들 많던데 난 github desktop이 훨씬 편하다.\n- git 알못이기 때문에 많은 것을 알기 위해 괴롭고 싶지 않다.\n- 뭐 여튼 깃허브 데스크탑을 킨다.\n- 상단 메뉴바 \\(\\to\\) File \\(\\to\\) Add local repository\n- 그러면 아래와 같은 경고문이 뜬다.\n\n- local하고 연결하고 싶은데 깃허브에는 gcsite가 없으니 대충 만들어 달라는 것임 “create a repository” 를 눌러주자.\n\n- 무시, 걍 create repository ㄱㄱ\n- 그러면 깃허브 데스크탑에서 너 방금 만든거 너꺼 깃허브에 Publish 할거냐고 물어봄\n\n- Publish repository 눌러주면 끝~~ (단, publish할 때 private 체크박스는 해제하구 하자!)\n- 그 다음 내가 생성산 gcsite 저장소 setting으로 넘어가서 pages를 클릭!\n- 아래와 같이 branch를 수정 후 save 버튼 눌러주자"
  },
  {
    "objectID": "posts/DX/2023-07-31-00.intro.html#문서-생성",
    "href": "posts/DX/2023-07-31-00.intro.html#문서-생성",
    "title": "00. Intro & setting",
    "section": "",
    "text": "- quarto 원리 : 작성한 ipynb파일 html파일로 출력해서 그 html파일들로 웹사이트를 구성하는 것1\n- step1. posts와 docs라는 폴더를 만들자\n\nposts는 내가 작성하는 ipynb파일들이 들어갈거고, docs에는 html파일이 들어갈 것이다.\n\n- step2. index 파일 수정\n\nindex파일은 뭐랄까 네비게이터 역할이랄까 아래와 같이 바꿔주자\n\n---\ntitle: \"GC site\"\nlisting:\n  contents: posts\n  sort: [date desc, title]\n  type: table\n  categories: true\n  sort-ui: true\n  filter-ui: true\npage-layout: full\ntitle-block-banner: true\n---\n- step3. _quarto.uml 파일 수정 \\(\\to\\) 템플릿이랑 디자인 이쁜거 많으니 본인 입맛에 맞게 수정하면 됩니당\nproject:\n  type: website\n  output-dir : docs  \nwebsite:\n  title: \"GC site\"\n  page-navigation: true\n  navbar:\n    right:\n      - icon : github\n        href : https://github.com/gangcheol/\n  sidebar:\n    style: \"docked\"\n    search: True\n    contents: auto\n    \nformat:\n  html:\n    css: styles.css\n    toc: true\n    code-fold : False\n    code-line-numbers : True\n    code-copy : True\n\ntheme :\n  light : flatly\n  \neditor : visual\n- step4. 앞서 만든 posts폴더에 아무 파일이나 만들어보자\n\n- step5. 그 후 다시 터미널에서 내가 생성한 폴더로 이동\n필자의 경우는 cd gcsite\n- step6. quarto render 입력\n- step7. github desktop보면 난리가 났을 것이다. 막 일을 좀 많이 했음.\n\n로컬하고 연결되어 있으니 로컬이 하고 있는 걸 다적어서 그럼\n\n\n\n저기 내가 밑에 이러한 기록을 init이라고 써놨다. 저건 내가 로컬에서 한 행동을 내 깃허브 로컬에 저장할 건데, 그 행동을 init이라고 쓴거\n이제 저 Commit to main 버튼을 눌러주고 가운데 화면에 뜨는 push origin을 눌러주자!\n\n- 마지막!! 아까 깃허브 로컬 셋팅에서 pases란에 잠시 후에 들어가보면 다음과 같은 것을 볼 수 있다.\n\n- 저 링크로 들어가면 내가 만든 웹사이트 초안을 볼 수 있다.\n- 링크"
  },
  {
    "objectID": "posts/DX/2023-10-26-02. BERTopic.html",
    "href": "posts/DX/2023-10-26-02. BERTopic.html",
    "title": "02. BERTopic",
    "section": "",
    "text": "- 진짜….역시 공식 깃허브나 문서를 보자\nmecab\n\n!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n\nCloning into 'Mecab-ko-for-Google-Colab'...\nremote: Enumerating objects: 138, done.\nremote: Counting objects: 100% (47/47), done.\nremote: Compressing objects: 100% (38/38), done.\nremote: Total 138 (delta 26), reused 22 (delta 8), pack-reused 91\nReceiving objects: 100% (138/138), 1.72 MiB | 28.79 MiB/s, done.\nResolving deltas: 100% (65/65), done.\n\n\n\ncd /content/Mecab-ko-for-Google-Colab\n\n/content/Mecab-ko-for-Google-Colab\n\n\n\n!bash install_mecab-ko_on_colab_light_220429.sh\n\nInstalling konlpy.....\nRequirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\nRequirement already satisfied: JPype1&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\nRequirement already satisfied: lxml&gt;=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\nRequirement already satisfied: numpy&gt;=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1&gt;=0.7.0-&gt;konlpy) (23.2)\nDone\nInstalling mecab-0.996-ko-0.9.2.tar.gz.....\nDownloading mecab-0.996-ko-0.9.2.tar.gz.......\nfrom https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n--2023-10-25 15:03:59--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\nResolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22cd:e0db\nConnecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNL5HFT22Z&Signature=nmXeqoZmBzETjaeXIke%2BSb9%2BDbM%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEDcaCXVzLWVhc3QtMSJIMEYCIQD0rM%2BnoPic266JPOIEaeM0t6vjyzWuw%2F7Q6WMxMjYSggIhALzVOYG39bkUweHkfMhSYWycnqy%2F0x0fGoXd8TecwyIFKqcCCGAQABoMOTg0NTI1MTAxMTQ2IgxX3576t4rCJL1u4XAqhAJAxMk82ZlQk3aMcrU7y6eKX0sK9JqvtH0p7xmqbTlLA02ulJQCRDdW4S%2FB91FmL0znI2TMJUvg%2FOjSNGAZqEREj47gMSdxjITNttjO1hsx4ykWrb7Hwkns012m9GObabLq5YlrH%2BXaD77TX2BGHIgonwIonPkm6aaJFHgsqmVonDvr1QnuDW0%2BbWl%2BULq9rggBmnO5FMyQdgdzX6DPMkwCEf%2BEVexG7%2BWLbdeDE9Zwe3Xs%2Fhg75EYq3WKPmBNOWK4IdQ0YM8hDyArHEim0c8dDCYjkFFdmKY57uoIJCvKA0Cr7JpWwm%2Fc%2FY8PdJmEKPwXbMdRcaaAF35V%2B3dABVXWm0T%2BAxzDg3OSpBjqcAS8anRjOjJf7WkPI6gQ4Lqn1somIsZ0DZMMKZwx2zhQ5rUaXC2z%2FeUw%2BI%2FH2p9jSjjwAelZuSamNzXJnQajxqQaJLfwch8chvUezORkUWrby8J3EF8oVLeGZ83Rg%2Fsvl1vb7G55cOdExm9e%2FyBuERSEnwtthmR68sAO26OgDtE4m1LiJiPdPjnsKHEy8yLdXy0F6gV65EIRWgHJB7g%3D%3D&Expires=1698248040 [following]\n--2023-10-25 15:04:00--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNL5HFT22Z&Signature=nmXeqoZmBzETjaeXIke%2BSb9%2BDbM%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEDcaCXVzLWVhc3QtMSJIMEYCIQD0rM%2BnoPic266JPOIEaeM0t6vjyzWuw%2F7Q6WMxMjYSggIhALzVOYG39bkUweHkfMhSYWycnqy%2F0x0fGoXd8TecwyIFKqcCCGAQABoMOTg0NTI1MTAxMTQ2IgxX3576t4rCJL1u4XAqhAJAxMk82ZlQk3aMcrU7y6eKX0sK9JqvtH0p7xmqbTlLA02ulJQCRDdW4S%2FB91FmL0znI2TMJUvg%2FOjSNGAZqEREj47gMSdxjITNttjO1hsx4ykWrb7Hwkns012m9GObabLq5YlrH%2BXaD77TX2BGHIgonwIonPkm6aaJFHgsqmVonDvr1QnuDW0%2BbWl%2BULq9rggBmnO5FMyQdgdzX6DPMkwCEf%2BEVexG7%2BWLbdeDE9Zwe3Xs%2Fhg75EYq3WKPmBNOWK4IdQ0YM8hDyArHEim0c8dDCYjkFFdmKY57uoIJCvKA0Cr7JpWwm%2Fc%2FY8PdJmEKPwXbMdRcaaAF35V%2B3dABVXWm0T%2BAxzDg3OSpBjqcAS8anRjOjJf7WkPI6gQ4Lqn1somIsZ0DZMMKZwx2zhQ5rUaXC2z%2FeUw%2BI%2FH2p9jSjjwAelZuSamNzXJnQajxqQaJLfwch8chvUezORkUWrby8J3EF8oVLeGZ83Rg%2Fsvl1vb7G55cOdExm9e%2FyBuERSEnwtthmR68sAO26OgDtE4m1LiJiPdPjnsKHEy8yLdXy0F6gV65EIRWgHJB7g%3D%3D&Expires=1698248040\nResolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 3.5.25.44, 52.216.41.17, 3.5.25.213, ...\nConnecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|3.5.25.44|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1414979 (1.3M) [application/x-tar]\nSaving to: ‘mecab-0.996-ko-0.9.2.tar.gz.1’\n\nmecab-0.996-ko-0.9. 100%[===================&gt;]   1.35M  2.25MB/s    in 0.6s    \n\n2023-10-25 15:04:01 (2.25 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz.1’ saved [1414979/1414979]\n\nDone\nUnpacking mecab-0.996-ko-0.9.2.tar.gz.......\nDone\nChange Directory to mecab-0.996-ko-0.9.2.......\ninstalling mecab-0.996-ko-0.9.2.tar.gz........\nconfigure\nmake\nmake check\nmake install\nldconfig\nDone\nChange Directory to /content\nDownloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\nfrom https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n--2023-10-25 15:04:24--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\nResolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22cd:e0db\nConnecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNFTX5OSNC&Signature=4u9De4BcGtIDIjM2MKJoP%2Bsho44%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEDgaCXVzLWVhc3QtMSJGMEQCIC5a3So2H477rapMEFTdv%2BOphG3nvgBrC%2BqEyz4iiPxIAiAgo5%2BosygsRwNamAEzphTItMW44cdyPJpHhW%2FWcS63zSqnAghgEAAaDDk4NDUyNTEwMTE0NiIMrs5e73uLyZhNJy4fKoQCScaGEpNw6Hx84UwM5MejtfY7CynGFfn2hm6vsZ2o2DcwbIwhNF7KEXr2N9JuE8puwrAcCbhklNl7qDb7YJ8iTMlS0ljTREKTnJC2xmSqRizSwwzW%2BNr5bpoJ%2BrcLm9htQqkTSJNax4nlI8BJYI6vaHB2LTKQJP8pBMGApYgbBmP2o547dsVEcS1BPbmYnXLdy0aNvLpOlDjypDlZOocBGYsShvp3NYzTo6AiYQo%2Br1AOmjdBn2EPD3LlXl4Ic2F7VckQQiQ%2FVyEQFGYG4abfvLkR5omRi3mWqpL54KATIXwy4LVi6vFmrpT0aLc7f0rw6wz1kxd%2FCiN6GBwDAgWr1fjGHhMw%2BdzkqQY6ngGTmZ0%2F7n1YIur3I3HQ%2FVpjbqo4nSigJW7Y7m0vrvoIW2xKssR%2Frx%2BspExIq1woJDLdAj7kKlSFThrYbS9E7%2Fbmd4KZZpYflg938FjrcvUNLm8KERgVGEf0dImMiYn%2BwALNAlRJxTE65%2BWxpZvUGvM6H3164ozEWpCTek1fXLGcqBADEIKwjqXZWMu5NDQgfS5QROqg5eBffrN4gcv7Mg%3D%3D&Expires=1698248065 [following]\n--2023-10-25 15:04:25--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNFTX5OSNC&Signature=4u9De4BcGtIDIjM2MKJoP%2Bsho44%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEDgaCXVzLWVhc3QtMSJGMEQCIC5a3So2H477rapMEFTdv%2BOphG3nvgBrC%2BqEyz4iiPxIAiAgo5%2BosygsRwNamAEzphTItMW44cdyPJpHhW%2FWcS63zSqnAghgEAAaDDk4NDUyNTEwMTE0NiIMrs5e73uLyZhNJy4fKoQCScaGEpNw6Hx84UwM5MejtfY7CynGFfn2hm6vsZ2o2DcwbIwhNF7KEXr2N9JuE8puwrAcCbhklNl7qDb7YJ8iTMlS0ljTREKTnJC2xmSqRizSwwzW%2BNr5bpoJ%2BrcLm9htQqkTSJNax4nlI8BJYI6vaHB2LTKQJP8pBMGApYgbBmP2o547dsVEcS1BPbmYnXLdy0aNvLpOlDjypDlZOocBGYsShvp3NYzTo6AiYQo%2Br1AOmjdBn2EPD3LlXl4Ic2F7VckQQiQ%2FVyEQFGYG4abfvLkR5omRi3mWqpL54KATIXwy4LVi6vFmrpT0aLc7f0rw6wz1kxd%2FCiN6GBwDAgWr1fjGHhMw%2BdzkqQY6ngGTmZ0%2F7n1YIur3I3HQ%2FVpjbqo4nSigJW7Y7m0vrvoIW2xKssR%2Frx%2BspExIq1woJDLdAj7kKlSFThrYbS9E7%2Fbmd4KZZpYflg938FjrcvUNLm8KERgVGEf0dImMiYn%2BwALNAlRJxTE65%2BWxpZvUGvM6H3164ozEWpCTek1fXLGcqBADEIKwjqXZWMu5NDQgfS5QROqg5eBffrN4gcv7Mg%3D%3D&Expires=1698248065\nResolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.126.25, 52.217.16.132, 3.5.28.38, ...\nConnecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.126.25|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 49775061 (47M) [application/x-tar]\nSaving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n\nmecab-ko-dic-2.1.1- 100%[===================&gt;]  47.47M  28.6MB/s    in 1.7s    \n\n2023-10-25 15:04:27 (28.6 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n\nDone\nUnpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\nDone\nChange Directory to mecab-ko-dic-2.1.1-20180720\nDone\ninstalling........\nconfigure\nmake\nmake install\nbash &lt;(curl -s https://raw.githubusercontent.com/konlpy/konlpy/v0.6.0/scripts/mecab.sh)\nhttps://github.com/konlpy/konlpy/issues/395#issue-1099168405 - 2022.01.11\nDone\nInstall mecab-python\nSuccessfully Installed\nNow you can use Mecab\nfrom konlpy.tag import Mecab\nmecab = Mecab()\n사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\nNameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n블로그에 해결 방법을 남겨주신 tana님 감사합니다.\nlight 버전 작성 : Dogdriip님 ( https://github.com/Dogdriip )\n문제를 해결해주신 combacsa님 감사합니다.\n\n\n\nfrom konlpy.tag import Mecab\nmecab = Mecab()"
  },
  {
    "objectID": "posts/DX/2023-10-26-02. BERTopic.html#데이터-전처리",
    "href": "posts/DX/2023-10-26-02. BERTopic.html#데이터-전처리",
    "title": "02. BERTopic",
    "section": "데이터 전처리",
    "text": "데이터 전처리\n- 공백 제거\n\ndoc[\"제목\"] = [i.strip() for i in doc[\"제목\"]]\n\n- 빈 문자열거나 숫자로만 이루어진 줄은 제외\n\npreprocessed_documents = []\n\nfor line in tqdm(doc[\"제목\"]):\n  # 빈 문자열이거나 숫자로만 이루어진 줄은 제외\n  if line and not line.replace(' ', '').isdecimal():\n    preprocessed_documents.append(line)\n\n100%|██████████| 1001/1001 [00:00&lt;00:00, 814609.68it/s]\n\n\n\ndoc[\"전처리\"] = preprocessed_documents\n\n\ndoc.tail()\n\n\n  \n    \n\n\n\n\n\n\n뉴스 식별자\n제목\n전처리\n\n\n\n\n996\n2100701\n이상일 용인시장, 집중호우 사망자 유가족 지방세 전액 면제\n이상일 용인시장, 집중호우 사망자 유가족 지방세 전액 면제\n\n\n997\n1500701\n'무사고' 불꽃축제 위해 올해 안전경호인력 7000여 명 투입(종합)\n'무사고' 불꽃축제 위해 올해 안전경호인력 7000여 명 투입(종합)\n\n\n998\n1500301\n주군시설관리공단, 전동킥보드 화재대비 지도소방훈련\n주군시설관리공단, 전동킥보드 화재대비 지도소방훈련\n\n\n999\n2100851\n북, 日야스쿠니 공물봉납 집단참배는 \"노골적 전쟁 선동 행위\"\n북, 日야스쿠니 공물봉납 집단참배는 \"노골적 전쟁 선동 행위\"\n\n\n1000\n7101201\nKT, 국가통신망 복구 훈련\nKT, 국가통신망 복구 훈련"
  },
  {
    "objectID": "posts/DX/2023-10-26-02. BERTopic.html#tokenizer-설계",
    "href": "posts/DX/2023-10-26-02. BERTopic.html#tokenizer-설계",
    "title": "02. BERTopic",
    "section": "tokenizer 설계",
    "text": "tokenizer 설계\n\nclass CustomTokenizer:\n    def __init__(self, tagger):\n        self.tagger = tagger\n    def __call__(self, sent):\n        sent = sent[:1000000]\n        word_tokens = self.tagger.morphs(sent)\n        result = [word for word in word_tokens if len(word) &gt; 1]\n        return result\n\n\ncustom_tokenizer = CustomTokenizer(Mecab())\n\n\nvectorizer = CountVectorizer(tokenizer=custom_tokenizer, max_features=3000)\n\n\nBERTopic?"
  },
  {
    "objectID": "posts/DX/2023-10-26-02. BERTopic.html#모델-설계",
    "href": "posts/DX/2023-10-26-02. BERTopic.html#모델-설계",
    "title": "02. BERTopic",
    "section": "모델 설계",
    "text": "모델 설계\n\nmodel = BERTopic(embedding_model=\"sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens\",\n                                  vectorizer_model=vectorizer,\n                                  nr_topics=10, ## 토픽 개수\n                                  top_n_words=10, ## 추출할 토픽 당 상위 단어 수\n                                  calculate_probabilities=True) ## Documnent 별로 특정 Topic에 속하는 확류를 계산"
  },
  {
    "objectID": "posts/DX/2023-10-26-02. BERTopic.html#학습",
    "href": "posts/DX/2023-10-26-02. BERTopic.html#학습",
    "title": "02. BERTopic",
    "section": "학습",
    "text": "학습\n- 데이터가 1000개도 안되서 그런가 그렇게 1분도채 안걸림\n\ntopics, probs = model.fit_transform(doc[\"전처리\"])"
  },
  {
    "objectID": "posts/DX/2023-10-26-02. BERTopic.html#시각화-2.-특정-문서의-토픽-분포-확인",
    "href": "posts/DX/2023-10-26-02. BERTopic.html#시각화-2.-특정-문서의-토픽-분포-확인",
    "title": "02. BERTopic",
    "section": "시각화 2. 특정 문서의 토픽 분포 확인",
    "text": "시각화 2. 특정 문서의 토픽 분포 확인\n- 200번쨰 문서의 토픽 확인\n\nmodel.visualize_distribution(probs[200], min_probability=0.015)"
  },
  {
    "objectID": "posts/DX/2023-10-26-02. BERTopic.html#시각화-3.-토픽-대표-단어-상위-5개",
    "href": "posts/DX/2023-10-26-02. BERTopic.html#시각화-3.-토픽-대표-단어-상위-5개",
    "title": "02. BERTopic",
    "section": "시각화 3. 토픽 대표 단어 상위 5개",
    "text": "시각화 3. 토픽 대표 단어 상위 5개\n\nmodel.visualize_barchart?\n\n\n# Topic을 대표하는 상위 단어 5개씩 보여준다.\nmodel.visualize_barchart(top_n_topics=5, n_words= 10)"
  },
  {
    "objectID": "posts/DX/2023-10-26-02. BERTopic.html#시각화-4.-토픽간-유사도-표현",
    "href": "posts/DX/2023-10-26-02. BERTopic.html#시각화-4.-토픽간-유사도-표현",
    "title": "02. BERTopic",
    "section": "시각화 4. 토픽간 유사도 표현",
    "text": "시각화 4. 토픽간 유사도 표현\n\nmodel.visualize_heatmap(n_clusters=4, width=500, height = 500)\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n# Topic내 대표하는 단어들에 대해서 c-tf-idf로 계산해서 각 단어가 Topic에서 차지하는 중요도를 계산했던 것을 Rank 순대로 보여준다.\nmodel.visualize_term_rank()\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\netc. 기타 파라미터\n- 분류한 토픽을 보여줌\n\nmodel.topics_[:5]\n\n[2, 2, 2, 2, 2]\n\n\n- HDBSCAN 기법을 이용하여 각 문서를 군집화했을 때, 해당 문서가 군집에 속할 확류을 보여줌\n\nmodel.probabilities_[:2]\n\narray([[5.32352913e-002, 5.90055063e-002, 6.89318188e-001,\n        1.04375284e-001, 9.40657308e-002],\n       [3.71440583e-309, 4.70787179e-309, 1.00000000e+000,\n        4.70423409e-309, 5.42962400e-309]])\n\n\n- 각 토픽들의 size\n\nmodel.topic_sizes_\n\nCounter({2: 901, 0: 44, 4: 19, -1: 10, 3: 12, 1: 15})\n\n\n- 토픽 내상위 단어들의 대한 c-TF_IDF값\n\nmodel.topic_representations_[0]\n\n[('2023', 0.32417088668560623),\n ('훈련', 0.2051842701113714),\n ('한국', 0.18245372133142587),\n ('실시', 0.17703575379330266),\n ('대응', 0.1565152758688988),\n ('재난', 0.15254680479769683),\n ('안전', 0.14396222867116093),\n ('지원', 0.06276754363422),\n ('교육', 0.06229370766924284),\n ('부산', 0.06127748188099344)]\n\n\n- 토픽 라벨 확인\n\nmodel.topic_labels_ ## 디폴트로 생긴 토픽 라벨\n\n{-1: '-1_오디오북_발간_포스코_esg',\n 0: '0_2023_훈련_한국_실시',\n 1: '1_2023_연구_미래_우주',\n 2: '2_재난_안전_참사_훈련',\n 3: '3_핼러윈_홍대_명동_관리',\n 4: '4_핼러윈_알람_인파_경고'}\n\n\n\nmodel.set_topic_labels? ## 뭐 대충 토픽 라벨 설정하는건데 딕셔너리 형태이니 별로 상관 없는 것 같다.\n\n- 각 토픽들의 임베딩 벡터\n\nmodel.topic_embeddings_\n\narray([[-0.13217519,  0.0063285 ,  0.95600444, ...,  0.5427133 ,\n        -0.03279791, -0.24064429],\n       [-0.34547415, -0.23547484,  1.002295  , ...,  0.4318177 ,\n         0.06947018, -0.39168218],\n       [-0.00538328,  0.03810034,  0.9684657 , ...,  0.06903452,\n        -0.09306312, -0.05956983],\n       [-0.19442223,  0.02605102,  0.8316406 , ...,  0.20028639,\n        -0.09091544, -0.22935173],\n       [-0.23698546,  0.07576042,  1.1194484 , ...,  0.23134069,\n         0.05843855, -0.3001703 ],\n       [ 0.0934517 ,  0.21052632,  1.0790582 , ..., -0.04979509,\n         0.04043319, -0.06839072]], dtype=float32)\n\n\n\nmodel.topic_embeddings_.shape\n\n(6, 768)\n\n\n- 각 토픽을 대표하는 문서들의 내용 확인\n\nmodel.representative_docs_\n\n{-1: ['포스코 ESG성과 귀로도 듣는다 오디오북 발간',\n  '포스코, 기업시민보고서 오디오북 발간 ESG 성과 듣는다',\n  '포스코, 기업시민보고서 오디오북 발간 ESG성과 귀로도 듣는다'],\n 0: ['전남도, 2023 재난대응 안전한국훈련 실시',\n  \"전남도, '2023 재난대응 안전한국훈련' 실시\",\n  '2023년 재난대응 안전한국훈련 실시'],\n 1: ['필로스, 이달 24일부터 ‘2023 A+A 안전산업박람회’ 참여',\n  '[2023 ITCE]우경정보기술, AI기반 재난 사회안전 솔루션 소개',\n  '신종 감염병 연구 우수성과 한자리에  ‘2023 GFID 연구성과 실증박람회’'],\n 2: ['경기도, 럼피스킨병 대응 긴급재난안전대책회의 열어',\n  '창원특례시, 재난 대응 안전한국훈련 실시  재난관리 ↑',\n  '관악구, 재난대응 안전한국훈련'],\n 3: ['핼러윈 축제 기간 이태원 홍대 등 합동 상황관리 나선다',\n  '정부, 핼러윈 기간 이태원 홍대 명동 동성로에 국장급 상황관리관 파견',\n  '정부, ‘핼러윈 대비’ 이태원 홍대 명동에 상황관리관 파견'],\n 4: ['이태원 참사 막는다 핼러윈 앞두고 1㎡당 4명 초과시 `경고 알람`',\n  \"핼러윈 이태원, 1㎡당 운집 인원 4명 넘으면 '경고 알람'\",\n  '핼러윈 앞두고 이태원 홍대 등 4곳 인파관리 총력']}\n\n\n- 참고\n\nBERTopic\n\n\nBERTopic??"
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-01. model train (2).html",
    "href": "posts/DX/BP/2023-12-28-01. model train (2).html",
    "title": "01. model train (2)",
    "section": "",
    "text": "- environment : local jupyter lab\n- 앞에서 주어진 데이터로 모델을 돌려봤으니까 이제 우리 데이터로 모델을 학습하고 돌려보자."
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-01. model train (2).html#save",
    "href": "posts/DX/BP/2023-12-28-01. model train (2).html#save",
    "title": "01. model train (2)",
    "section": "save",
    "text": "save\n\nPATH = \"D:/projects/mysite2/posts/DX/BP/model_save/test_model1.pth\"\n\n\ntorch.save(model_ft.state_dict(), PATH)"
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-01. model train (2).html#load",
    "href": "posts/DX/BP/2023-12-28-01. model train (2).html#load",
    "title": "01. model train (2)",
    "section": "load",
    "text": "load\n\nmodel_ft = torchvision.models.video.mc3_18(pretrained=True, progress=True)\n\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = torch.nn.Linear(num_ftrs, 2)\nmodel_ft.load_state_dict(torch.load(PATH))\nmodel_ft = model_ft.to(device)\n#model_ft.eval()"
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-01. model train (2).html#predict",
    "href": "posts/DX/BP/2023-12-28-01. model train (2).html#predict",
    "title": "01. model train (2)",
    "section": "predict",
    "text": "predict"
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-01. model train (2).html#원본비디오-재생",
    "href": "posts/DX/BP/2023-12-28-01. model train (2).html#원본비디오-재생",
    "title": "01. model train (2)",
    "section": "원본비디오 재생",
    "text": "원본비디오 재생\n\nos.listdir(\"D:/projects/mysite2/posts/DX/BP/test video/폭행\")\n\n['폭행29.mp4', '폭행30.mp4', '폭행31.mp4', '폭행32.mp4', '폭행33.mp4']\n\n\n\npath_output = \"D:/projects/mysite2/posts/DX/BP/test video/폭행/폭행31.mp4\"\nFight_utils.show_video(path_output, width=960)"
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-01. model train (2).html#비디오-클래스-추정",
    "href": "posts/DX/BP/2023-12-28-01. model train (2).html#비디오-클래스-추정",
    "title": "01. model train (2)",
    "section": "비디오 클래스 추정",
    "text": "비디오 클래스 추정\n\npath_1 = \"D:/projects/mysite2/posts/DX/BP/test video/폭행/폭행31.mp4\"\npath_2 = \"D:/projects/mysite2/posts/DX/BP/test video/실신/실신19.mp4\"\n\n\nSEQUENCE_LENGTH=16\n\n- 슈발 함수 수정하고 restart 하니까 된다…\n\nprint(Fight_utils.FightInference(path_1,model_ft,SEQUENCE_LENGTH))\nprint(Fight_utils.FightInference_Time(path_1, model_ft, SEQUENCE_LENGTH)) ## 추론하는데 걸린시간\n\nfight\n[('fight', 0.60055), ('fall', 0.39945)]\n***********\ntime is: 2.789564371109009\nfight"
  },
  {
    "objectID": "posts/DX/BP/2023-12-28-01. model train (2).html#클래스-추정-후-비디오-표시",
    "href": "posts/DX/BP/2023-12-28-01. model train (2).html#클래스-추정-후-비디오-표시",
    "title": "01. model train (2)",
    "section": "클래스 추정 후 비디오 표시",
    "text": "클래스 추정 후 비디오 표시\n\nimport Fight_utils\nfrom Fight_utils import *\n\n\noutput_path = \"D:/projects/mysite2/posts/DX/BP/test video/output/result1.mp4\"\n\n\nstart_time = time.time()\noutVideo_1=Fight_utils.showIference(model_ft, 16, 2, path_1, output_path, showInfo = False)\nelapsed = time.time() - start_time\nprint(\"time is:\",elapsed)\n\ntime is: 8.3247389793396\n\n\n\nVideoFileClip(outVideo_1, audio=False, target_resolution=(300,None)).ipython_display()\n\nMoviepy - Building video __temp__.mp4.\nMoviepy - Writing video __temp__.mp4\n\nMoviepy - Done !\nMoviepy - video ready __temp__.mp4\n\n\n                                                                                                                       \n\n\nSorry, seems like your browser doesn't support HTML5 audio/video"
  },
  {
    "objectID": "posts/DX/BP/2024-01-01-03. model theory.html",
    "href": "posts/DX/BP/2024-01-01-03. model theory.html",
    "title": "03. model theory",
    "section": "",
    "text": "abstract\n- pythoch의 torchvision에서 제공하는 모델 중 mc3_18 이라는 모델을 이번 빅프에서 사용했는데 해당 모델을 정리할 필요가 있음.\n\n초록을 읽어보니 2D CNN에서보다 3D CNN을 기반으로 만들어진 모델이 더욱 성능이 정확도가 높은 것을 보여주었음\n그리고 3D Conv Filter를 spatial(공간적)인 특징과 Temporal(시간적인) 특징으로 분리하면 정확도가 크게 향상된다는 것을 보여줌.\n여튼 몇몇 데이터 셋을 이용해서 새로운 spatiotemporal block R(2+1)D인 produces CNN을 설계해서 우리가 사용한 모델을 만들어준 것임.\n\n\n\n\nIntroduction\n1 기존의 2D CNN 기반 모델들(ex. Alexnet, I3D)은 비디오 분석에서 중요한 측면으로 간주되는 시간적 정보와 행동 패턴을 모델링 할 수 없다는 단점이 있음\n2 그래서 이러한 단점을 해결하기 위해 mixed convolution(MC)와 spatiotemporal block R(2+1)D을 이용함\n\n우리는 mixed convolution(MC)을 이용함\n\n3 mixed convolution\n\n네트워크의 초기 몇개의 layer에서만 3D conv를 사용하고 top layer에서 2D conv를 사용\n해당 설계의 근거는 동작에 대한 모델링의 경우 3D conv를 통해 구현될 수 있는 low/mid-level 작업임\n이러한 mid-levle에서 산출된 행동 features(frame을 말하는것 같음)에 대한 공간적 추론은 정확한 행동 인식으로 이어진다는 것임.\n\n4 spatiotemporal block R(2+1)D\n\n3D conv를 2D saptial convolution과 1D temporal convolution으로 분해하는 것을 뜻함.\n장점 1 : 3D conv 안에서 분해된 공간적인 특성과 시간적인 특성 사이에 추가적인 비선형계층을 추가해 복잡한 모델링을 할 수 있음\n\n이는 만약 full 3D conv가 10개의 파라미터를 사용했다면, 제안한 모델에서는 두 배 가량 복잡한 모델을 만들 수 있음\n\n장점 2 : 저런 시간적, 공간적 요소를 분해함에 있어 최적화를 촉진하고 실제로 train loss와 test loss를 줄일 수 있다는 것인데… (잠재적 이점이므로 skip하자.)\n\n5 머여튼 그래서 해당 논문에서 제안한 모델이 성능이 좋았단 것임\n\n\nMC3 모델이란 그래서?\n\n\n\n- resnet 모델과 mixed convolutions을 결합한 모델이라고 한다…\n\n이 부분은 나중에 그림 어케 그릴지 생각…(그냥 resnet + mixed conv 해서 mc3 모델 사용했다고?)\n\n\n\nreference\n[1] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, Manohar Paluri (2018). A Closer Look at Sptiotemporal Convolutions for Action Recognition"
  },
  {
    "objectID": "posts/DX/MP/2023-08-24-00. MP (1).html",
    "href": "posts/DX/MP/2023-08-24-00. MP (1).html",
    "title": "00. MP (1)",
    "section": "",
    "text": "pandas와 numpy 라라이브러리를 불러오기\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n\n\n\n\n- 데이터를 불러온 후 상위 10개 행 확인\n\n\nCode\nimport openpyxl\ndata = pd.read_excel(\"data04.xlsx\")\ndata.head(10)\n\n\n\n\n\n\n\n\n\nID\nSeq\nGender\nBirth_Year\nLC_Score\nRC_Score\nTotal Score\n학습목표\n학습방법\n강의 학습 교재 유형\n학습빈도\n기출문제 공부 횟수\n취약분야 인지 여부\n토익 모의테스트 횟수\nStudent ID\n\n\n\n\n0\n1\n1\nM\n1973\n181\n173\n354\n자기계발\n참고서\n일반적인 영어 텍스트 기반 교재\n주3-4회\n6.0\n알고 있지 않음\n6\nstudent1\n\n\n1\n1\n2\nM\n1973\n227\n213\n440\n자기계발\n오프라인강의\n뉴스/이슈 기반 교재\n주1-2회\n3.0\n알고 있음\n5\nstudent1\n\n\n2\n1\n3\nM\n1973\n345\n336\n681\n승진\n온라인강의\n영상 교재\n주5-6회\n7.0\n알고 있음\n10\nstudent1\n\n\n3\n2\n1\nF\n1982\n330\n290\n620\n자기계발\n오프라인강의\n뉴스/이슈 기반 교재\n매일(주 7회)\n8.0\n알고 있지 않음\n19\nstudent2\n\n\n4\n2\n2\nF\n1982\n354\n339\n693\n승진\n온라인강의\n영상 교재\n주5-6회\n2.0\n알고 있음\n15\nstudent2\n\n\n5\n2\n3\nF\n1982\n380\n368\n748\n승진\n온라인강의\n뉴스/이슈 기반 교재\n주5-6회\n4.0\n알고 있음\n14\nstudent2\n\n\n6\n3\n1\nF\n1995\n367\n309\n676\n취업\n온라인강의\n영상 교재\n매일(주 7회)\n9.0\n알고 있지 않음\n7\nstudent3\n\n\n7\n3\n2\nF\n1995\n396\n365\n761\n자기계발\n온라인강의\n영상 교재\n주3-4회\n7.0\n알고 있지 않음\n6\nstudent3\n\n\n8\n3\n3\nF\n1995\n416\n382\n798\n자기계발\n참고서\n일반적인 영어 텍스트 기반 교재\n주1-2회\n4.0\n알고 있음\n4\nstudent3\n\n\n9\n4\n1\nM\n1987\n470\n285\n755\n자기계발\n온라인강의\n뉴스/이슈 기반 교재\n주1-2회\n7.0\n알고 있지 않음\n4\nstudent4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(f\"전체 데이터는 {data.shape[0]}개의 행과 {data.shape[1]}열로 구성되어 있습니다.\")\n\n\n전체 데이터는 1500개의 행과 15열로 구성되어 있습니다.\n\n\n\n\n\n\n\nCode\ndata.tail(5)\n\n\n\n\n\n\n\n\n\nID\nSeq\nGender\nBirth_Year\nLC_Score\nRC_Score\nTotal Score\n학습목표\n학습방법\n강의 학습 교재 유형\n학습빈도\n기출문제 공부 횟수\n취약분야 인지 여부\n토익 모의테스트 횟수\nStudent ID\n\n\n\n\n1495\n499\n2\nF\n1990\n378\n326\n704\n승진\n온라인강의\n뉴스/이슈 기반 교재\n주5-6회\n6.0\n알고 있지 않음\n12\nstudent499\n\n\n1496\n499\n3\nF\n1990\n422\n370\n792\n자기계발\n오프라인강의\n비즈니스 시뮬레이션(Role Play)\n주3-4회\n4.0\n알고 있음\n7\nstudent499\n\n\n1497\n500\n1\nM\n1984\n169\n188\n357\n자기계발\n참고서\n일반적인 영어 텍스트 기반 교재\n주3-4회\n8.0\n알고 있지 않음\n2\nstudent500\n\n\n1498\n500\n2\nM\n1984\n172\n190\n362\n자기계발\n참고서\n뉴스/이슈 기반 교재\n매일(주 7회)\n10.0\n알고 있음\n16\nstudent500\n\n\n1499\n500\n3\nM\n1984\n235\n226\n461\n승진\n오프라인강의\n비즈니스 시뮬레이션(Role Play)\n주5-6회\n7.0\n알고 있음\n15\nstudent500\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(list(data.columns))\n\n\n['ID', 'Seq', 'Gender', 'Birth_Year', 'LC_Score', 'RC_Score', 'Total Score', '학습목표', '학습방법', '강의 학습 교재 유형', '학습빈도', '기출문제 공부 횟수', '취약분야 인지 여부', '토익 모의테스트 횟수', 'Student ID']\n\n\n\n\n\n\n\nCode\nprint(data.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   ID           1500 non-null   int64  \n 1   Seq          1500 non-null   int64  \n 2   Gender       1500 non-null   object \n 3   Birth_Year   1500 non-null   int64  \n 4   LC_Score     1500 non-null   int64  \n 5   RC_Score     1500 non-null   int64  \n 6   Total Score  1500 non-null   int64  \n 7   학습목표         1500 non-null   object \n 8   학습방법         1500 non-null   object \n 9   강의 학습 교재 유형  1500 non-null   object \n 10  학습빈도         1500 non-null   object \n 11  기출문제 공부 횟수   1497 non-null   float64\n 12  취약분야 인지 여부   1500 non-null   object \n 13  토익 모의테스트 횟수  1500 non-null   int64  \n 14  Student ID   1500 non-null   object \ndtypes: float64(1), int64(7), object(7)\nmemory usage: 175.9+ KB\nNone\n\n\n\n\n\n\n기출문제 공부횟수에서 총 3개의 결측치가 확인된다.\n\n\n\nCode\ndata.isna().sum()\n\n\nID             0\nSeq            0\nGender         0\nBirth_Year     0\nLC_Score       0\nRC_Score       0\nTotal Score    0\n학습목표           0\n학습방법           0\n강의 학습 교재 유형    0\n학습빈도           0\n기출문제 공부 횟수     3\n취약분야 인지 여부     0\n토익 모의테스트 횟수    0\nStudent ID     0\ndtype: int64\n\n\n\n\n\n\n\nCode\ndata.describe()\n\n\n\n\n\n\n\n\n\nID\nSeq\nBirth_Year\nLC_Score\nRC_Score\nTotal Score\n기출문제 공부 횟수\n토익 모의테스트 횟수\n\n\n\n\ncount\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n1497.000000\n1500.000000\n\n\nmean\n250.500000\n2.000000\n1992.906000\n340.079333\n340.164667\n680.260667\n5.286573\n9.784000\n\n\nstd\n144.385415\n0.816769\n8.218893\n86.807523\n87.143890\n159.110652\n2.797303\n5.324181\n\n\nmin\n1.000000\n1.000000\n1973.000000\n105.000000\n84.000000\n250.000000\n1.000000\n1.000000\n\n\n25%\n125.750000\n1.000000\n1986.750000\n279.000000\n280.000000\n564.000000\n3.000000\n5.000000\n\n\n50%\n250.500000\n2.000000\n1992.500000\n335.000000\n337.000000\n687.000000\n5.000000\n9.000000\n\n\n75%\n375.250000\n3.000000\n2000.000000\n404.000000\n406.000000\n800.000000\n8.000000\n14.000000\n\n\nmax\n500.000000\n3.000000\n2007.000000\n495.000000\n495.000000\n990.000000\n10.000000\n20.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(data.dtypes)\n\n\nID               int64\nSeq              int64\nGender          object\nBirth_Year       int64\nLC_Score         int64\nRC_Score         int64\nTotal Score      int64\n학습목표            object\n학습방법            object\n강의 학습 교재 유형     object\n학습빈도            object\n기출문제 공부 횟수     float64\n취약분야 인지 여부      object\n토익 모의테스트 횟수      int64\nStudent ID      object\ndtype: object\n\n\n\n\n\n\n\nCode\ndata.drop(\"Student ID\",axis=1,inplace=True)\n\n\n\n\nCode\ndata.head()\n\n\n\n\n\n\n\n\n\nID\nSeq\nGender\nBirth_Year\nLC_Score\nRC_Score\nTotal Score\n학습목표\n학습방법\n강의 학습 교재 유형\n학습빈도\n기출문제 공부 횟수\n취약분야 인지 여부\n토익 모의테스트 횟수\n\n\n\n\n0\n1\n1\nM\n1973\n181\n173\n354\n자기계발\n참고서\n일반적인 영어 텍스트 기반 교재\n주3-4회\n6.0\n알고 있지 않음\n6\n\n\n1\n1\n2\nM\n1973\n227\n213\n440\n자기계발\n오프라인강의\n뉴스/이슈 기반 교재\n주1-2회\n3.0\n알고 있음\n5\n\n\n2\n1\n3\nM\n1973\n345\n336\n681\n승진\n온라인강의\n영상 교재\n주5-6회\n7.0\n알고 있음\n10\n\n\n3\n2\n1\nF\n1982\n330\n290\n620\n자기계발\n오프라인강의\n뉴스/이슈 기반 교재\n매일(주 7회)\n8.0\n알고 있지 않음\n19\n\n\n4\n2\n2\nF\n1982\n354\n339\n693\n승진\n온라인강의\n영상 교재\n주5-6회\n2.0\n알고 있음\n15\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlen(data.columns)\n\n\n14\n\n\n\n\n\n\n\nCode\ndata[\"기출문제 공부 횟수\"].fillna(0,inplace=True)\n\n\n\n\n\n\n\nCode\ndata[\"기출문제 공부 횟수\"].unique()\n\n\narray([ 6.,  3.,  7.,  8.,  2.,  4.,  9.,  5., 10.,  1.,  0.])\n\n\n\n\n\n\ndf1(개인정보 데이터) features : 'ID', 'Gender', 'Birth_Year'\ndf2(토익시험 학습정보 데이터) features : 'ID','Seq', 'LC_Score', 'RC_Score', 'Total Score', '학습목표', '학습방법', '강의 학습 교재 유형', '학습빈도', '기출문제 공부 횟수', '취약분야 인지 여부', '토익 모의테스트 횟수'\n\n\n\nCode\ncol_1 = ['ID', 'Gender', 'Birth_Year']\ncol_2 = ['ID','Seq', 'LC_Score', 'RC_Score', 'Total Score',\n         '학습목표', '학습방법', '강의 학습 교재 유형', '학습빈도',\n         '기출문제 공부 횟수', '취약분야 인지 여부', '토익 모의테스트 횟수']\n\ndf1 = data.loc[:, map(lambda x : x  in col_1,data.columns )]\ndf2 = data.loc[:, map(lambda x : x  in col_2,data.columns )]\n\nprint(f\"df1의 컬럼 : {list(df1.columns)}\\n\")\nprint(\"*\"*100+\"\\n\")\nprint(f\"df2의 컬럼 : {list(df2.columns)}\")\n\n\ndf1의 컬럼 : ['ID', 'Gender', 'Birth_Year']\n\n****************************************************************************************************\n\ndf2의 컬럼 : ['ID', 'Seq', 'LC_Score', 'RC_Score', 'Total Score', '학습목표', '학습방법', '강의 학습 교재 유형', '학습빈도', '기출문제 공부 횟수', '취약분야 인지 여부', '토익 모의테스트 횟수']\n\n\n\n\n\n\n\n\nCode\ndf1 = df1.drop_duplicates()\n\n\n\n\nCode\ndf1.head()\n\n\n\n\n\n\n\n\n\nID\nGender\nBirth_Year\n\n\n\n\n0\n1\nM\n1973\n\n\n3\n2\nF\n1982\n\n\n6\n3\nF\n1995\n\n\n9\n4\nM\n1987\n\n\n12\n5\nM\n1994\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntemp = df2.loc[map(lambda x : x == 3,df2.Seq), :]\ntemp.Seq.unique()\n\n\narray([3], dtype=int64)\n\n\n\n\nCode\ntemp.head()\n\n\n\n\n\n\n\n\n\nID\nSeq\nLC_Score\nRC_Score\nTotal Score\n학습목표\n학습방법\n강의 학습 교재 유형\n학습빈도\n기출문제 공부 횟수\n취약분야 인지 여부\n토익 모의테스트 횟수\n\n\n\n\n2\n1\n3\n345\n336\n681\n승진\n온라인강의\n영상 교재\n주5-6회\n7.0\n알고 있음\n10\n\n\n5\n2\n3\n380\n368\n748\n승진\n온라인강의\n뉴스/이슈 기반 교재\n주5-6회\n4.0\n알고 있음\n14\n\n\n8\n3\n3\n416\n382\n798\n자기계발\n참고서\n일반적인 영어 텍스트 기반 교재\n주1-2회\n4.0\n알고 있음\n4\n\n\n11\n4\n3\n495\n397\n892\n승진\n온라인강의\n뉴스/이슈 기반 교재\n주3-4회\n9.0\n알고 있음\n8\n\n\n14\n5\n3\n398\n437\n835\n자기계발\n온라인강의\n영상 교재\n주3-4회\n6.0\n알고 있음\n4\n\n\n\n\n\n\n\n\n\n\n\nLC_Score, RC_Score, Total Score를 각각 ‘3st_LC_Score’, ‘3st_RC_Score’, ’3st_Total_Score’로 변경하고 확인해주세요.\n\n\n\nCode\ntemp = temp.rename(columns = {\"LC_Score\" : \"3st_LC_SCcore\",\n                        \"RC_Score\" : \"3st_RC_SCcore\",\n                        \"Total Score\" : \"3st_Total_SCcore\",\n                        })\ntemp.head()\n\n\n\n\n\n\n\n\n\nID\nSeq\n3st_LC_SCcore\n3st_RC_SCcore\n3st_Total_SCcore\n학습목표\n학습방법\n강의 학습 교재 유형\n학습빈도\n기출문제 공부 횟수\n취약분야 인지 여부\n토익 모의테스트 횟수\n\n\n\n\n2\n1\n3\n345\n336\n681\n승진\n온라인강의\n영상 교재\n주5-6회\n7.0\n알고 있음\n10\n\n\n5\n2\n3\n380\n368\n748\n승진\n온라인강의\n뉴스/이슈 기반 교재\n주5-6회\n4.0\n알고 있음\n14\n\n\n8\n3\n3\n416\n382\n798\n자기계발\n참고서\n일반적인 영어 텍스트 기반 교재\n주1-2회\n4.0\n알고 있음\n4\n\n\n11\n4\n3\n495\n397\n892\n승진\n온라인강의\n뉴스/이슈 기반 교재\n주3-4회\n9.0\n알고 있음\n8\n\n\n14\n5\n3\n398\n437\n835\n자기계발\n온라인강의\n영상 교재\n주3-4회\n6.0\n알고 있음\n4\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntemp1 = df2.loc[map(lambda x : x == 1,df2.Seq), :]\ntemp1.Seq.unique()\n\n\narray([1], dtype=int64)\n\n\n\n\n\n\n\nCode\ntemp1 = temp1.loc[:,['ID','LC_Score','RC_Score','Total Score']]\n\n\n\n\n\n\nLC_Score, RC_Score, Total Score를 각각 ‘1st_LC_Score’, ‘1st_RC_Score’, ’1st_Total_Score’로 변경하고 확인해주세요.\n\n\n\nCode\ntemp1 = temp1.rename(columns = {\"LC_Score\" : \"1st_LC_SCcore\",\n                        \"RC_Score\" : \"1st_RC_SCcore\",\n                        \"Total Score\" : \"1st_Total_SCcore\",\n                        })\ntemp1.head()\n\n\n\n\n\n\n\n\n\nID\n1st_LC_SCcore\n1st_RC_SCcore\n1st_Total_SCcore\n\n\n\n\n0\n1\n181\n173\n354\n\n\n3\n2\n330\n290\n620\n\n\n6\n3\n367\n309\n676\n\n\n9\n4\n470\n285\n755\n\n\n12\n5\n273\n372\n645\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntemp2 = df2.loc[map(lambda x : x == 2,df2.Seq), :]\ntemp2.Seq.unique()\n\n\narray([2], dtype=int64)\n\n\n\n\n\n\n\nCode\ntemp2 =  temp2.loc[:,['ID','LC_Score','RC_Score','Total Score']]\n\n\n\n\n\n\n\nCode\ntemp2 = temp2.rename(columns = {\"LC_Score\" : \"2st_LC_SCcore\",\n                        \"RC_Score\" : \"2st_RC_SCcore\",\n                        \"Total Score\" : \"2st_Total_SCcore\",\n                        })\ntemp2.head()\n\n\n\n\n\n\n\n\n\nID\n2st_LC_SCcore\n2st_RC_SCcore\n2st_Total_SCcore\n\n\n\n\n1\n1\n227\n213\n440\n\n\n4\n2\n354\n339\n693\n\n\n7\n3\n396\n365\n761\n\n\n10\n4\n495\n341\n836\n\n\n13\n5\n314\n426\n740\n\n\n\n\n\n\n\n\n\n\n- 합친 후 ’score_merged_data1’에 할당\n\n\nCode\nscore_merged_data1 = pd.merge(temp,temp1)\n\n\n\n\n\n\n\nCode\nscore_merged_data2 = pd.merge(score_merged_data1,temp2)\n\n\n\n\n\n\n\nCode\nprint(score_merged_data2.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 500 entries, 0 to 499\nData columns (total 18 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   ID                500 non-null    int64  \n 1   Seq               500 non-null    int64  \n 2   3st_LC_SCcore     500 non-null    int64  \n 3   3st_RC_SCcore     500 non-null    int64  \n 4   3st_Total_SCcore  500 non-null    int64  \n 5   학습목표              500 non-null    object \n 6   학습방법              500 non-null    object \n 7   강의 학습 교재 유형       500 non-null    object \n 8   학습빈도              500 non-null    object \n 9   기출문제 공부 횟수        500 non-null    float64\n 10  취약분야 인지 여부        500 non-null    object \n 11  토익 모의테스트 횟수       500 non-null    int64  \n 12  1st_LC_SCcore     500 non-null    int64  \n 13  1st_RC_SCcore     500 non-null    int64  \n 14  1st_Total_SCcore  500 non-null    int64  \n 15  2st_LC_SCcore     500 non-null    int64  \n 16  2st_RC_SCcore     500 non-null    int64  \n 17  2st_Total_SCcore  500 non-null    int64  \ndtypes: float64(1), int64(12), object(5)\nmemory usage: 70.4+ KB\nNone\n\n\n\n\n\n\n- 합친 데이터를 baseline_data에 할당\n\n\nCode\nbaseline_data = pd.merge(df1,score_merged_data2)\n\n\n\n\nCode\nbaseline_data.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 502 entries, 0 to 501\nData columns (total 20 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   ID                502 non-null    int64  \n 1   Gender            502 non-null    object \n 2   Birth_Year        502 non-null    int64  \n 3   Seq               502 non-null    int64  \n 4   3st_LC_SCcore     502 non-null    int64  \n 5   3st_RC_SCcore     502 non-null    int64  \n 6   3st_Total_SCcore  502 non-null    int64  \n 7   학습목표              502 non-null    object \n 8   학습방법              502 non-null    object \n 9   강의 학습 교재 유형       502 non-null    object \n 10  학습빈도              502 non-null    object \n 11  기출문제 공부 횟수        502 non-null    float64\n 12  취약분야 인지 여부        502 non-null    object \n 13  토익 모의테스트 횟수       502 non-null    int64  \n 14  1st_LC_SCcore     502 non-null    int64  \n 15  1st_RC_SCcore     502 non-null    int64  \n 16  1st_Total_SCcore  502 non-null    int64  \n 17  2st_LC_SCcore     502 non-null    int64  \n 18  2st_RC_SCcore     502 non-null    int64  \n 19  2st_Total_SCcore  502 non-null    int64  \ndtypes: float64(1), int64(13), object(6)\nmemory usage: 78.6+ KB\n\n\n\n\n\n\n‘Score_diff_total’ = ‘3st_Total_Score’ - ‘2st_Total_Score’\n\n\n\nCode\nbaseline_data[\"Score_diff_total\"] = baseline_data[\"3st_Total_SCcore\"] - baseline_data[\"2st_Total_SCcore\"] \n\n\n\n\n\n\n\nCode\nbaseline_data.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 502 entries, 0 to 501\nData columns (total 21 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   ID                502 non-null    int64  \n 1   Gender            502 non-null    object \n 2   Birth_Year        502 non-null    int64  \n 3   Seq               502 non-null    int64  \n 4   3st_LC_SCcore     502 non-null    int64  \n 5   3st_RC_SCcore     502 non-null    int64  \n 6   3st_Total_SCcore  502 non-null    int64  \n 7   학습목표              502 non-null    object \n 8   학습방법              502 non-null    object \n 9   강의 학습 교재 유형       502 non-null    object \n 10  학습빈도              502 non-null    object \n 11  기출문제 공부 횟수        502 non-null    float64\n 12  취약분야 인지 여부        502 non-null    object \n 13  토익 모의테스트 횟수       502 non-null    int64  \n 14  1st_LC_SCcore     502 non-null    int64  \n 15  1st_RC_SCcore     502 non-null    int64  \n 16  1st_Total_SCcore  502 non-null    int64  \n 17  2st_LC_SCcore     502 non-null    int64  \n 18  2st_RC_SCcore     502 non-null    int64  \n 19  2st_Total_SCcore  502 non-null    int64  \n 20  Score_diff_total  502 non-null    int64  \ndtypes: float64(1), int64(14), object(6)\nmemory usage: 82.5+ KB\n\n\n\n\n\n\n\n\n\n\n\nCode\nbaseline_data.to_csv(\"data04_baseline.csv\",index=False)\n\n\n\n\n\n\n\nCode\npd.read_csv(\"data04_baseline.csv\").info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 502 entries, 0 to 501\nData columns (total 21 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   ID                502 non-null    int64  \n 1   Gender            502 non-null    object \n 2   Birth_Year        502 non-null    int64  \n 3   Seq               502 non-null    int64  \n 4   3st_LC_SCcore     502 non-null    int64  \n 5   3st_RC_SCcore     502 non-null    int64  \n 6   3st_Total_SCcore  502 non-null    int64  \n 7   학습목표              502 non-null    object \n 8   학습방법              502 non-null    object \n 9   강의 학습 교재 유형       502 non-null    object \n 10  학습빈도              502 non-null    object \n 11  기출문제 공부 횟수        502 non-null    float64\n 12  취약분야 인지 여부        502 non-null    object \n 13  토익 모의테스트 횟수       502 non-null    int64  \n 14  1st_LC_SCcore     502 non-null    int64  \n 15  1st_RC_SCcore     502 non-null    int64  \n 16  1st_Total_SCcore  502 non-null    int64  \n 17  2st_LC_SCcore     502 non-null    int64  \n 18  2st_RC_SCcore     502 non-null    int64  \n 19  2st_Total_SCcore  502 non-null    int64  \n 20  Score_diff_total  502 non-null    int64  \ndtypes: float64(1), int64(14), object(6)\nmemory usage: 82.5+ KB"
  },
  {
    "objectID": "posts/DX/MP/2023-08-24-00. MP (1).html#환경설정",
    "href": "posts/DX/MP/2023-08-24-00. MP (1).html#환경설정",
    "title": "00. MP (1)",
    "section": "",
    "text": "pandas와 numpy 라라이브러리를 불러오기\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n\n\n\n\n- 데이터를 불러온 후 상위 10개 행 확인\n\n\nCode\nimport openpyxl\ndata = pd.read_excel(\"data04.xlsx\")\ndata.head(10)\n\n\n\n\n\n\n\n\n\nID\nSeq\nGender\nBirth_Year\nLC_Score\nRC_Score\nTotal Score\n학습목표\n학습방법\n강의 학습 교재 유형\n학습빈도\n기출문제 공부 횟수\n취약분야 인지 여부\n토익 모의테스트 횟수\nStudent ID\n\n\n\n\n0\n1\n1\nM\n1973\n181\n173\n354\n자기계발\n참고서\n일반적인 영어 텍스트 기반 교재\n주3-4회\n6.0\n알고 있지 않음\n6\nstudent1\n\n\n1\n1\n2\nM\n1973\n227\n213\n440\n자기계발\n오프라인강의\n뉴스/이슈 기반 교재\n주1-2회\n3.0\n알고 있음\n5\nstudent1\n\n\n2\n1\n3\nM\n1973\n345\n336\n681\n승진\n온라인강의\n영상 교재\n주5-6회\n7.0\n알고 있음\n10\nstudent1\n\n\n3\n2\n1\nF\n1982\n330\n290\n620\n자기계발\n오프라인강의\n뉴스/이슈 기반 교재\n매일(주 7회)\n8.0\n알고 있지 않음\n19\nstudent2\n\n\n4\n2\n2\nF\n1982\n354\n339\n693\n승진\n온라인강의\n영상 교재\n주5-6회\n2.0\n알고 있음\n15\nstudent2\n\n\n5\n2\n3\nF\n1982\n380\n368\n748\n승진\n온라인강의\n뉴스/이슈 기반 교재\n주5-6회\n4.0\n알고 있음\n14\nstudent2\n\n\n6\n3\n1\nF\n1995\n367\n309\n676\n취업\n온라인강의\n영상 교재\n매일(주 7회)\n9.0\n알고 있지 않음\n7\nstudent3\n\n\n7\n3\n2\nF\n1995\n396\n365\n761\n자기계발\n온라인강의\n영상 교재\n주3-4회\n7.0\n알고 있지 않음\n6\nstudent3\n\n\n8\n3\n3\nF\n1995\n416\n382\n798\n자기계발\n참고서\n일반적인 영어 텍스트 기반 교재\n주1-2회\n4.0\n알고 있음\n4\nstudent3\n\n\n9\n4\n1\nM\n1987\n470\n285\n755\n자기계발\n온라인강의\n뉴스/이슈 기반 교재\n주1-2회\n7.0\n알고 있지 않음\n4\nstudent4"
  },
  {
    "objectID": "posts/DX/MP/2023-08-24-00. MP (1).html#데이터프레임-탐색",
    "href": "posts/DX/MP/2023-08-24-00. MP (1).html#데이터프레임-탐색",
    "title": "00. MP (1)",
    "section": "",
    "text": "Code\nprint(f\"전체 데이터는 {data.shape[0]}개의 행과 {data.shape[1]}열로 구성되어 있습니다.\")\n\n\n전체 데이터는 1500개의 행과 15열로 구성되어 있습니다.\n\n\n\n\n\n\n\nCode\ndata.tail(5)\n\n\n\n\n\n\n\n\n\nID\nSeq\nGender\nBirth_Year\nLC_Score\nRC_Score\nTotal Score\n학습목표\n학습방법\n강의 학습 교재 유형\n학습빈도\n기출문제 공부 횟수\n취약분야 인지 여부\n토익 모의테스트 횟수\nStudent ID\n\n\n\n\n1495\n499\n2\nF\n1990\n378\n326\n704\n승진\n온라인강의\n뉴스/이슈 기반 교재\n주5-6회\n6.0\n알고 있지 않음\n12\nstudent499\n\n\n1496\n499\n3\nF\n1990\n422\n370\n792\n자기계발\n오프라인강의\n비즈니스 시뮬레이션(Role Play)\n주3-4회\n4.0\n알고 있음\n7\nstudent499\n\n\n1497\n500\n1\nM\n1984\n169\n188\n357\n자기계발\n참고서\n일반적인 영어 텍스트 기반 교재\n주3-4회\n8.0\n알고 있지 않음\n2\nstudent500\n\n\n1498\n500\n2\nM\n1984\n172\n190\n362\n자기계발\n참고서\n뉴스/이슈 기반 교재\n매일(주 7회)\n10.0\n알고 있음\n16\nstudent500\n\n\n1499\n500\n3\nM\n1984\n235\n226\n461\n승진\n오프라인강의\n비즈니스 시뮬레이션(Role Play)\n주5-6회\n7.0\n알고 있음\n15\nstudent500\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(list(data.columns))\n\n\n['ID', 'Seq', 'Gender', 'Birth_Year', 'LC_Score', 'RC_Score', 'Total Score', '학습목표', '학습방법', '강의 학습 교재 유형', '학습빈도', '기출문제 공부 횟수', '취약분야 인지 여부', '토익 모의테스트 횟수', 'Student ID']\n\n\n\n\n\n\n\nCode\nprint(data.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   ID           1500 non-null   int64  \n 1   Seq          1500 non-null   int64  \n 2   Gender       1500 non-null   object \n 3   Birth_Year   1500 non-null   int64  \n 4   LC_Score     1500 non-null   int64  \n 5   RC_Score     1500 non-null   int64  \n 6   Total Score  1500 non-null   int64  \n 7   학습목표         1500 non-null   object \n 8   학습방법         1500 non-null   object \n 9   강의 학습 교재 유형  1500 non-null   object \n 10  학습빈도         1500 non-null   object \n 11  기출문제 공부 횟수   1497 non-null   float64\n 12  취약분야 인지 여부   1500 non-null   object \n 13  토익 모의테스트 횟수  1500 non-null   int64  \n 14  Student ID   1500 non-null   object \ndtypes: float64(1), int64(7), object(7)\nmemory usage: 175.9+ KB\nNone\n\n\n\n\n\n\n기출문제 공부횟수에서 총 3개의 결측치가 확인된다.\n\n\n\nCode\ndata.isna().sum()\n\n\nID             0\nSeq            0\nGender         0\nBirth_Year     0\nLC_Score       0\nRC_Score       0\nTotal Score    0\n학습목표           0\n학습방법           0\n강의 학습 교재 유형    0\n학습빈도           0\n기출문제 공부 횟수     3\n취약분야 인지 여부     0\n토익 모의테스트 횟수    0\nStudent ID     0\ndtype: int64\n\n\n\n\n\n\n\nCode\ndata.describe()\n\n\n\n\n\n\n\n\n\nID\nSeq\nBirth_Year\nLC_Score\nRC_Score\nTotal Score\n기출문제 공부 횟수\n토익 모의테스트 횟수\n\n\n\n\ncount\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n1500.000000\n1497.000000\n1500.000000\n\n\nmean\n250.500000\n2.000000\n1992.906000\n340.079333\n340.164667\n680.260667\n5.286573\n9.784000\n\n\nstd\n144.385415\n0.816769\n8.218893\n86.807523\n87.143890\n159.110652\n2.797303\n5.324181\n\n\nmin\n1.000000\n1.000000\n1973.000000\n105.000000\n84.000000\n250.000000\n1.000000\n1.000000\n\n\n25%\n125.750000\n1.000000\n1986.750000\n279.000000\n280.000000\n564.000000\n3.000000\n5.000000\n\n\n50%\n250.500000\n2.000000\n1992.500000\n335.000000\n337.000000\n687.000000\n5.000000\n9.000000\n\n\n75%\n375.250000\n3.000000\n2000.000000\n404.000000\n406.000000\n800.000000\n8.000000\n14.000000\n\n\nmax\n500.000000\n3.000000\n2007.000000\n495.000000\n495.000000\n990.000000\n10.000000\n20.000000"
  },
  {
    "objectID": "posts/DX/MP/2023-08-24-00. MP (1).html#데이터-전처리-수행",
    "href": "posts/DX/MP/2023-08-24-00. MP (1).html#데이터-전처리-수행",
    "title": "00. MP (1)",
    "section": "",
    "text": "Code\nprint(data.dtypes)\n\n\nID               int64\nSeq              int64\nGender          object\nBirth_Year       int64\nLC_Score         int64\nRC_Score         int64\nTotal Score      int64\n학습목표            object\n학습방법            object\n강의 학습 교재 유형     object\n학습빈도            object\n기출문제 공부 횟수     float64\n취약분야 인지 여부      object\n토익 모의테스트 횟수      int64\nStudent ID      object\ndtype: object\n\n\n\n\n\n\n\nCode\ndata.drop(\"Student ID\",axis=1,inplace=True)\n\n\n\n\nCode\ndata.head()\n\n\n\n\n\n\n\n\n\nID\nSeq\nGender\nBirth_Year\nLC_Score\nRC_Score\nTotal Score\n학습목표\n학습방법\n강의 학습 교재 유형\n학습빈도\n기출문제 공부 횟수\n취약분야 인지 여부\n토익 모의테스트 횟수\n\n\n\n\n0\n1\n1\nM\n1973\n181\n173\n354\n자기계발\n참고서\n일반적인 영어 텍스트 기반 교재\n주3-4회\n6.0\n알고 있지 않음\n6\n\n\n1\n1\n2\nM\n1973\n227\n213\n440\n자기계발\n오프라인강의\n뉴스/이슈 기반 교재\n주1-2회\n3.0\n알고 있음\n5\n\n\n2\n1\n3\nM\n1973\n345\n336\n681\n승진\n온라인강의\n영상 교재\n주5-6회\n7.0\n알고 있음\n10\n\n\n3\n2\n1\nF\n1982\n330\n290\n620\n자기계발\n오프라인강의\n뉴스/이슈 기반 교재\n매일(주 7회)\n8.0\n알고 있지 않음\n19\n\n\n4\n2\n2\nF\n1982\n354\n339\n693\n승진\n온라인강의\n영상 교재\n주5-6회\n2.0\n알고 있음\n15\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlen(data.columns)\n\n\n14\n\n\n\n\n\n\n\nCode\ndata[\"기출문제 공부 횟수\"].fillna(0,inplace=True)\n\n\n\n\n\n\n\nCode\ndata[\"기출문제 공부 횟수\"].unique()\n\n\narray([ 6.,  3.,  7.,  8.,  2.,  4.,  9.,  5., 10.,  1.,  0.])\n\n\n\n\n\n\ndf1(개인정보 데이터) features : 'ID', 'Gender', 'Birth_Year'\ndf2(토익시험 학습정보 데이터) features : 'ID','Seq', 'LC_Score', 'RC_Score', 'Total Score', '학습목표', '학습방법', '강의 학습 교재 유형', '학습빈도', '기출문제 공부 횟수', '취약분야 인지 여부', '토익 모의테스트 횟수'\n\n\n\nCode\ncol_1 = ['ID', 'Gender', 'Birth_Year']\ncol_2 = ['ID','Seq', 'LC_Score', 'RC_Score', 'Total Score',\n         '학습목표', '학습방법', '강의 학습 교재 유형', '학습빈도',\n         '기출문제 공부 횟수', '취약분야 인지 여부', '토익 모의테스트 횟수']\n\ndf1 = data.loc[:, map(lambda x : x  in col_1,data.columns )]\ndf2 = data.loc[:, map(lambda x : x  in col_2,data.columns )]\n\nprint(f\"df1의 컬럼 : {list(df1.columns)}\\n\")\nprint(\"*\"*100+\"\\n\")\nprint(f\"df2의 컬럼 : {list(df2.columns)}\")\n\n\ndf1의 컬럼 : ['ID', 'Gender', 'Birth_Year']\n\n****************************************************************************************************\n\ndf2의 컬럼 : ['ID', 'Seq', 'LC_Score', 'RC_Score', 'Total Score', '학습목표', '학습방법', '강의 학습 교재 유형', '학습빈도', '기출문제 공부 횟수', '취약분야 인지 여부', '토익 모의테스트 횟수']\n\n\n\n\n\n\n\n\nCode\ndf1 = df1.drop_duplicates()\n\n\n\n\nCode\ndf1.head()\n\n\n\n\n\n\n\n\n\nID\nGender\nBirth_Year\n\n\n\n\n0\n1\nM\n1973\n\n\n3\n2\nF\n1982\n\n\n6\n3\nF\n1995\n\n\n9\n4\nM\n1987\n\n\n12\n5\nM\n1994\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntemp = df2.loc[map(lambda x : x == 3,df2.Seq), :]\ntemp.Seq.unique()\n\n\narray([3], dtype=int64)\n\n\n\n\nCode\ntemp.head()\n\n\n\n\n\n\n\n\n\nID\nSeq\nLC_Score\nRC_Score\nTotal Score\n학습목표\n학습방법\n강의 학습 교재 유형\n학습빈도\n기출문제 공부 횟수\n취약분야 인지 여부\n토익 모의테스트 횟수\n\n\n\n\n2\n1\n3\n345\n336\n681\n승진\n온라인강의\n영상 교재\n주5-6회\n7.0\n알고 있음\n10\n\n\n5\n2\n3\n380\n368\n748\n승진\n온라인강의\n뉴스/이슈 기반 교재\n주5-6회\n4.0\n알고 있음\n14\n\n\n8\n3\n3\n416\n382\n798\n자기계발\n참고서\n일반적인 영어 텍스트 기반 교재\n주1-2회\n4.0\n알고 있음\n4\n\n\n11\n4\n3\n495\n397\n892\n승진\n온라인강의\n뉴스/이슈 기반 교재\n주3-4회\n9.0\n알고 있음\n8\n\n\n14\n5\n3\n398\n437\n835\n자기계발\n온라인강의\n영상 교재\n주3-4회\n6.0\n알고 있음\n4\n\n\n\n\n\n\n\n\n\n\n\nLC_Score, RC_Score, Total Score를 각각 ‘3st_LC_Score’, ‘3st_RC_Score’, ’3st_Total_Score’로 변경하고 확인해주세요.\n\n\n\nCode\ntemp = temp.rename(columns = {\"LC_Score\" : \"3st_LC_SCcore\",\n                        \"RC_Score\" : \"3st_RC_SCcore\",\n                        \"Total Score\" : \"3st_Total_SCcore\",\n                        })\ntemp.head()\n\n\n\n\n\n\n\n\n\nID\nSeq\n3st_LC_SCcore\n3st_RC_SCcore\n3st_Total_SCcore\n학습목표\n학습방법\n강의 학습 교재 유형\n학습빈도\n기출문제 공부 횟수\n취약분야 인지 여부\n토익 모의테스트 횟수\n\n\n\n\n2\n1\n3\n345\n336\n681\n승진\n온라인강의\n영상 교재\n주5-6회\n7.0\n알고 있음\n10\n\n\n5\n2\n3\n380\n368\n748\n승진\n온라인강의\n뉴스/이슈 기반 교재\n주5-6회\n4.0\n알고 있음\n14\n\n\n8\n3\n3\n416\n382\n798\n자기계발\n참고서\n일반적인 영어 텍스트 기반 교재\n주1-2회\n4.0\n알고 있음\n4\n\n\n11\n4\n3\n495\n397\n892\n승진\n온라인강의\n뉴스/이슈 기반 교재\n주3-4회\n9.0\n알고 있음\n8\n\n\n14\n5\n3\n398\n437\n835\n자기계발\n온라인강의\n영상 교재\n주3-4회\n6.0\n알고 있음\n4\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntemp1 = df2.loc[map(lambda x : x == 1,df2.Seq), :]\ntemp1.Seq.unique()\n\n\narray([1], dtype=int64)\n\n\n\n\n\n\n\nCode\ntemp1 = temp1.loc[:,['ID','LC_Score','RC_Score','Total Score']]\n\n\n\n\n\n\nLC_Score, RC_Score, Total Score를 각각 ‘1st_LC_Score’, ‘1st_RC_Score’, ’1st_Total_Score’로 변경하고 확인해주세요.\n\n\n\nCode\ntemp1 = temp1.rename(columns = {\"LC_Score\" : \"1st_LC_SCcore\",\n                        \"RC_Score\" : \"1st_RC_SCcore\",\n                        \"Total Score\" : \"1st_Total_SCcore\",\n                        })\ntemp1.head()\n\n\n\n\n\n\n\n\n\nID\n1st_LC_SCcore\n1st_RC_SCcore\n1st_Total_SCcore\n\n\n\n\n0\n1\n181\n173\n354\n\n\n3\n2\n330\n290\n620\n\n\n6\n3\n367\n309\n676\n\n\n9\n4\n470\n285\n755\n\n\n12\n5\n273\n372\n645\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntemp2 = df2.loc[map(lambda x : x == 2,df2.Seq), :]\ntemp2.Seq.unique()\n\n\narray([2], dtype=int64)\n\n\n\n\n\n\n\nCode\ntemp2 =  temp2.loc[:,['ID','LC_Score','RC_Score','Total Score']]\n\n\n\n\n\n\n\nCode\ntemp2 = temp2.rename(columns = {\"LC_Score\" : \"2st_LC_SCcore\",\n                        \"RC_Score\" : \"2st_RC_SCcore\",\n                        \"Total Score\" : \"2st_Total_SCcore\",\n                        })\ntemp2.head()\n\n\n\n\n\n\n\n\n\nID\n2st_LC_SCcore\n2st_RC_SCcore\n2st_Total_SCcore\n\n\n\n\n1\n1\n227\n213\n440\n\n\n4\n2\n354\n339\n693\n\n\n7\n3\n396\n365\n761\n\n\n10\n4\n495\n341\n836\n\n\n13\n5\n314\n426\n740\n\n\n\n\n\n\n\n\n\n\n- 합친 후 ’score_merged_data1’에 할당\n\n\nCode\nscore_merged_data1 = pd.merge(temp,temp1)\n\n\n\n\n\n\n\nCode\nscore_merged_data2 = pd.merge(score_merged_data1,temp2)\n\n\n\n\n\n\n\nCode\nprint(score_merged_data2.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 500 entries, 0 to 499\nData columns (total 18 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   ID                500 non-null    int64  \n 1   Seq               500 non-null    int64  \n 2   3st_LC_SCcore     500 non-null    int64  \n 3   3st_RC_SCcore     500 non-null    int64  \n 4   3st_Total_SCcore  500 non-null    int64  \n 5   학습목표              500 non-null    object \n 6   학습방법              500 non-null    object \n 7   강의 학습 교재 유형       500 non-null    object \n 8   학습빈도              500 non-null    object \n 9   기출문제 공부 횟수        500 non-null    float64\n 10  취약분야 인지 여부        500 non-null    object \n 11  토익 모의테스트 횟수       500 non-null    int64  \n 12  1st_LC_SCcore     500 non-null    int64  \n 13  1st_RC_SCcore     500 non-null    int64  \n 14  1st_Total_SCcore  500 non-null    int64  \n 15  2st_LC_SCcore     500 non-null    int64  \n 16  2st_RC_SCcore     500 non-null    int64  \n 17  2st_Total_SCcore  500 non-null    int64  \ndtypes: float64(1), int64(12), object(5)\nmemory usage: 70.4+ KB\nNone\n\n\n\n\n\n\n- 합친 데이터를 baseline_data에 할당\n\n\nCode\nbaseline_data = pd.merge(df1,score_merged_data2)\n\n\n\n\nCode\nbaseline_data.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 502 entries, 0 to 501\nData columns (total 20 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   ID                502 non-null    int64  \n 1   Gender            502 non-null    object \n 2   Birth_Year        502 non-null    int64  \n 3   Seq               502 non-null    int64  \n 4   3st_LC_SCcore     502 non-null    int64  \n 5   3st_RC_SCcore     502 non-null    int64  \n 6   3st_Total_SCcore  502 non-null    int64  \n 7   학습목표              502 non-null    object \n 8   학습방법              502 non-null    object \n 9   강의 학습 교재 유형       502 non-null    object \n 10  학습빈도              502 non-null    object \n 11  기출문제 공부 횟수        502 non-null    float64\n 12  취약분야 인지 여부        502 non-null    object \n 13  토익 모의테스트 횟수       502 non-null    int64  \n 14  1st_LC_SCcore     502 non-null    int64  \n 15  1st_RC_SCcore     502 non-null    int64  \n 16  1st_Total_SCcore  502 non-null    int64  \n 17  2st_LC_SCcore     502 non-null    int64  \n 18  2st_RC_SCcore     502 non-null    int64  \n 19  2st_Total_SCcore  502 non-null    int64  \ndtypes: float64(1), int64(13), object(6)\nmemory usage: 78.6+ KB\n\n\n\n\n\n\n‘Score_diff_total’ = ‘3st_Total_Score’ - ‘2st_Total_Score’\n\n\n\nCode\nbaseline_data[\"Score_diff_total\"] = baseline_data[\"3st_Total_SCcore\"] - baseline_data[\"2st_Total_SCcore\"] \n\n\n\n\n\n\n\nCode\nbaseline_data.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 502 entries, 0 to 501\nData columns (total 21 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   ID                502 non-null    int64  \n 1   Gender            502 non-null    object \n 2   Birth_Year        502 non-null    int64  \n 3   Seq               502 non-null    int64  \n 4   3st_LC_SCcore     502 non-null    int64  \n 5   3st_RC_SCcore     502 non-null    int64  \n 6   3st_Total_SCcore  502 non-null    int64  \n 7   학습목표              502 non-null    object \n 8   학습방법              502 non-null    object \n 9   강의 학습 교재 유형       502 non-null    object \n 10  학습빈도              502 non-null    object \n 11  기출문제 공부 횟수        502 non-null    float64\n 12  취약분야 인지 여부        502 non-null    object \n 13  토익 모의테스트 횟수       502 non-null    int64  \n 14  1st_LC_SCcore     502 non-null    int64  \n 15  1st_RC_SCcore     502 non-null    int64  \n 16  1st_Total_SCcore  502 non-null    int64  \n 17  2st_LC_SCcore     502 non-null    int64  \n 18  2st_RC_SCcore     502 non-null    int64  \n 19  2st_Total_SCcore  502 non-null    int64  \n 20  Score_diff_total  502 non-null    int64  \ndtypes: float64(1), int64(14), object(6)\nmemory usage: 82.5+ KB"
  },
  {
    "objectID": "posts/DX/MP/2023-08-24-00. MP (1).html#데이터셋-저장하기",
    "href": "posts/DX/MP/2023-08-24-00. MP (1).html#데이터셋-저장하기",
    "title": "00. MP (1)",
    "section": "",
    "text": "Code\nbaseline_data.to_csv(\"data04_baseline.csv\",index=False)\n\n\n\n\n\n\n\nCode\npd.read_csv(\"data04_baseline.csv\").info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 502 entries, 0 to 501\nData columns (total 21 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   ID                502 non-null    int64  \n 1   Gender            502 non-null    object \n 2   Birth_Year        502 non-null    int64  \n 3   Seq               502 non-null    int64  \n 4   3st_LC_SCcore     502 non-null    int64  \n 5   3st_RC_SCcore     502 non-null    int64  \n 6   3st_Total_SCcore  502 non-null    int64  \n 7   학습목표              502 non-null    object \n 8   학습방법              502 non-null    object \n 9   강의 학습 교재 유형       502 non-null    object \n 10  학습빈도              502 non-null    object \n 11  기출문제 공부 횟수        502 non-null    float64\n 12  취약분야 인지 여부        502 non-null    object \n 13  토익 모의테스트 횟수       502 non-null    int64  \n 14  1st_LC_SCcore     502 non-null    int64  \n 15  1st_RC_SCcore     502 non-null    int64  \n 16  1st_Total_SCcore  502 non-null    int64  \n 17  2st_LC_SCcore     502 non-null    int64  \n 18  2st_RC_SCcore     502 non-null    int64  \n 19  2st_Total_SCcore  502 non-null    int64  \n 20  Score_diff_total  502 non-null    int64  \ndtypes: float64(1), int64(14), object(6)\nmemory usage: 82.5+ KB"
  },
  {
    "objectID": "posts/DX/MP/2023-08-24-00. MP (1).html#환경설정-1",
    "href": "posts/DX/MP/2023-08-24-00. MP (1).html#환경설정-1",
    "title": "00. MP (1)",
    "section": "1. 환경설정",
    "text": "1. 환경설정\n\n(1) 폰트설치\n\n\nCode\n!pip install matplotlib\n!pip install --upgrade matplotlib\n\nimport matplotlib.pyplot as plt\n\n\nRequirement already satisfied: matplotlib in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (3.7.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (from matplotlib) (1.1.0)\nRequirement already satisfied: cycler&gt;=0.10 in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (from matplotlib) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (from matplotlib) (4.42.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (from matplotlib) (1.4.4)\nRequirement already satisfied: numpy&gt;=1.20 in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (from matplotlib) (1.25.2)\nRequirement already satisfied: packaging&gt;=20.0 in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (from matplotlib) (23.0)\nRequirement already satisfied: pillow&gt;=6.2.0 in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (from matplotlib) (10.0.0)\nRequirement already satisfied: pyparsing&lt;3.1,&gt;=2.3.1 in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil&gt;=2.7 in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: six&gt;=1.5 in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.16.0)\nRequirement already satisfied: matplotlib in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (3.7.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (from matplotlib) (1.1.0)\nRequirement already satisfied: cycler&gt;=0.10 in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (from matplotlib) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (from matplotlib) (4.42.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (from matplotlib) (1.4.4)\nRequirement already satisfied: numpy&gt;=1.20 in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (from matplotlib) (1.25.2)\nRequirement already satisfied: packaging&gt;=20.0 in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (from matplotlib) (23.0)\nRequirement already satisfied: pillow&gt;=6.2.0 in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (from matplotlib) (10.0.0)\nRequirement already satisfied: pyparsing&lt;3.1,&gt;=2.3.1 in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil&gt;=2.7 in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: six&gt;=1.5 in c:\\users\\rkdcj\\anaconda3\\envs\\dx\\lib\\site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.16.0)\n\n\n\n\n(2) 라이브러리 불러오기\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.rc('font', family='Malgun Gothic')\n\n\n\n\n(3)-1. 데이터 불러오기\n\n\nCode\ndata = pd.read_csv(\"data04_baseline.csv\")\n\n\n\n\n(3)-2 데이터 확인\n\n\nCode\ndata.head()\n\n\n\n\n\n\n\n\n\nID\nGender\nBirth_Year\nSeq\n3st_LC_SCcore\n3st_RC_SCcore\n3st_Total_SCcore\n학습목표\n학습방법\n강의 학습 교재 유형\n...\n기출문제 공부 횟수\n취약분야 인지 여부\n토익 모의테스트 횟수\n1st_LC_SCcore\n1st_RC_SCcore\n1st_Total_SCcore\n2st_LC_SCcore\n2st_RC_SCcore\n2st_Total_SCcore\nScore_diff_total\n\n\n\n\n0\n1\nM\n1973\n3\n345\n336\n681\n승진\n온라인강의\n영상 교재\n...\n7.0\n알고 있음\n10\n181\n173\n354\n227\n213\n440\n241\n\n\n1\n2\nF\n1982\n3\n380\n368\n748\n승진\n온라인강의\n뉴스/이슈 기반 교재\n...\n4.0\n알고 있음\n14\n330\n290\n620\n354\n339\n693\n55\n\n\n2\n3\nF\n1995\n3\n416\n382\n798\n자기계발\n참고서\n일반적인 영어 텍스트 기반 교재\n...\n4.0\n알고 있음\n4\n367\n309\n676\n396\n365\n761\n37\n\n\n3\n4\nM\n1987\n3\n495\n397\n892\n승진\n온라인강의\n뉴스/이슈 기반 교재\n...\n9.0\n알고 있음\n8\n470\n285\n755\n495\n341\n836\n56\n\n\n4\n5\nM\n1994\n3\n398\n437\n835\n자기계발\n온라인강의\n영상 교재\n...\n6.0\n알고 있음\n4\n273\n372\n645\n314\n426\n740\n95\n\n\n\n\n5 rows × 21 columns\n\n\n\n\n\n(3)-3. 열과 행확인\n\n\nCode\ndata.shape\n\n\n(502, 21)\n\n\n\n\n(3)-4. 데이터의 자료구조(Row, Colu,n, Not-null, type)을 확인\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 502 entries, 0 to 501\nData columns (total 21 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   ID                502 non-null    int64  \n 1   Gender            502 non-null    object \n 2   Birth_Year        502 non-null    int64  \n 3   Seq               502 non-null    int64  \n 4   3st_LC_SCcore     502 non-null    int64  \n 5   3st_RC_SCcore     502 non-null    int64  \n 6   3st_Total_SCcore  502 non-null    int64  \n 7   학습목표              502 non-null    object \n 8   학습방법              502 non-null    object \n 9   강의 학습 교재 유형       502 non-null    object \n 10  학습빈도              502 non-null    object \n 11  기출문제 공부 횟수        502 non-null    float64\n 12  취약분야 인지 여부        502 non-null    object \n 13  토익 모의테스트 횟수       502 non-null    int64  \n 14  1st_LC_SCcore     502 non-null    int64  \n 15  1st_RC_SCcore     502 non-null    int64  \n 16  1st_Total_SCcore  502 non-null    int64  \n 17  2st_LC_SCcore     502 non-null    int64  \n 18  2st_RC_SCcore     502 non-null    int64  \n 19  2st_Total_SCcore  502 non-null    int64  \n 20  Score_diff_total  502 non-null    int64  \ndtypes: float64(1), int64(14), object(6)\nmemory usage: 82.5+ KB\n\n\n\n\n(3)-5. 인덱스 확인\n\n\nCode\ndata.index\n\n\nRangeIndex(start=0, stop=502, step=1)\n\n\n\n\n(3)-6. 컬럼명을 확인\n\n\nCode\ndata.columns\n\n\nIndex(['ID', 'Gender', 'Birth_Year', 'Seq', '3st_LC_SCcore', '3st_RC_SCcore',\n       '3st_Total_SCcore', '학습목표', '학습방법', '강의 학습 교재 유형', '학습빈도', '기출문제 공부 횟수',\n       '취약분야 인지 여부', '토익 모의테스트 횟수', '1st_LC_SCcore', '1st_RC_SCcore',\n       '1st_Total_SCcore', '2st_LC_SCcore', '2st_RC_SCcore',\n       '2st_Total_SCcore', 'Score_diff_total'],\n      dtype='object')\n\n\n\n\n(3)-7. 상단 5행을 확인\n\n\nCode\ndata.head(5)\n\n\n\n\n\n\n\n\n\nID\nGender\nBirth_Year\nSeq\n3st_LC_SCcore\n3st_RC_SCcore\n3st_Total_SCcore\n학습목표\n학습방법\n강의 학습 교재 유형\n...\n기출문제 공부 횟수\n취약분야 인지 여부\n토익 모의테스트 횟수\n1st_LC_SCcore\n1st_RC_SCcore\n1st_Total_SCcore\n2st_LC_SCcore\n2st_RC_SCcore\n2st_Total_SCcore\nScore_diff_total\n\n\n\n\n0\n1\nM\n1973\n3\n345\n336\n681\n승진\n온라인강의\n영상 교재\n...\n7.0\n알고 있음\n10\n181\n173\n354\n227\n213\n440\n241\n\n\n1\n2\nF\n1982\n3\n380\n368\n748\n승진\n온라인강의\n뉴스/이슈 기반 교재\n...\n4.0\n알고 있음\n14\n330\n290\n620\n354\n339\n693\n55\n\n\n2\n3\nF\n1995\n3\n416\n382\n798\n자기계발\n참고서\n일반적인 영어 텍스트 기반 교재\n...\n4.0\n알고 있음\n4\n367\n309\n676\n396\n365\n761\n37\n\n\n3\n4\nM\n1987\n3\n495\n397\n892\n승진\n온라인강의\n뉴스/이슈 기반 교재\n...\n9.0\n알고 있음\n8\n470\n285\n755\n495\n341\n836\n56\n\n\n4\n5\nM\n1994\n3\n398\n437\n835\n자기계발\n온라인강의\n영상 교재\n...\n6.0\n알고 있음\n4\n273\n372\n645\n314\n426\n740\n95\n\n\n\n\n5 rows × 21 columns"
  },
  {
    "objectID": "posts/DX/MP/2023-08-24-00. MP (1).html#데이터프레임-탐색-개별-변수-분석",
    "href": "posts/DX/MP/2023-08-24-00. MP (1).html#데이터프레임-탐색-개별-변수-분석",
    "title": "00. MP (1)",
    "section": "2. 데이터프레임 탐색 : 개별 변수 분석",
    "text": "2. 데이터프레임 탐색 : 개별 변수 분석\n\n(1)-1 : 열별 누락값 확인\n\n\nCode\ndata.isna().sum()\n\n\nID                  0\nGender              0\nBirth_Year          0\nSeq                 0\n3st_LC_SCcore       0\n3st_RC_SCcore       0\n3st_Total_SCcore    0\n학습목표                0\n학습방법                0\n강의 학습 교재 유형         0\n학습빈도                0\n기출문제 공부 횟수          0\n취약분야 인지 여부          0\n토익 모의테스트 횟수         0\n1st_LC_SCcore       0\n1st_RC_SCcore       0\n1st_Total_SCcore    0\n2st_LC_SCcore       0\n2st_RC_SCcore       0\n2st_Total_SCcore    0\nScore_diff_total    0\ndtype: int64\n\n\n\n\n(1)-2. 열별 통계량 요약하여 출력\n\n\nCode\ndata.describe()\n\n\n\n\n\n\n\n\n\nID\nBirth_Year\nSeq\n3st_LC_SCcore\n3st_RC_SCcore\n3st_Total_SCcore\n기출문제 공부 횟수\n토익 모의테스트 횟수\n1st_LC_SCcore\n1st_RC_SCcore\n1st_Total_SCcore\n2st_LC_SCcore\n2st_RC_SCcore\n2st_Total_SCcore\nScore_diff_total\n\n\n\n\ncount\n502.000000\n502.000000\n502.0\n502.000000\n502.000000\n502.000000\n502.000000\n502.000000\n502.000000\n502.000000\n502.000000\n502.000000\n502.000000\n502.000000\n502.000000\n\n\nmean\n250.587649\n1992.948207\n3.0\n368.023904\n369.318725\n737.382470\n5.087649\n9.452191\n313.697211\n312.798805\n626.496016\n337.868526\n338.045817\n675.924303\n61.458167\n\n\nstd\n144.199862\n8.236603\n0.0\n82.052339\n81.659228\n155.752174\n2.790826\n4.952137\n85.483105\n86.522443\n148.318758\n84.141542\n83.817809\n152.986694\n39.684902\n\n\nmin\n1.000000\n1973.000000\n3.0\n141.000000\n135.000000\n280.000000\n0.000000\n1.000000\n105.000000\n84.000000\n250.000000\n120.000000\n129.000000\n260.000000\n0.000000\n\n\n25%\n126.250000\n1987.000000\n3.0\n295.000000\n295.000000\n591.250000\n3.000000\n5.000000\n259.250000\n250.000000\n519.000000\n279.000000\n280.500000\n558.750000\n30.000000\n\n\n50%\n251.500000\n1993.000000\n3.0\n372.000000\n375.000000\n760.000000\n5.000000\n8.000000\n308.000000\n311.500000\n641.000000\n332.500000\n335.000000\n690.500000\n63.000000\n\n\n75%\n374.750000\n2000.000000\n3.0\n434.000000\n437.000000\n860.000000\n7.000000\n13.000000\n369.000000\n377.750000\n734.750000\n395.000000\n400.000000\n790.000000\n83.000000\n\n\nmax\n500.000000\n2007.000000\n3.0\n495.000000\n495.000000\n990.000000\n10.000000\n20.000000\n495.000000\n491.000000\n970.000000\n495.000000\n495.000000\n990.000000\n281.000000\n\n\n\n\n\n\n\n\n\n(1)-3. 출력한 요약통계량을 행과 열을 바꿔서 출력\n\n\nCode\ndata.describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nID\n502.0\n250.587649\n144.199862\n1.0\n126.25\n251.5\n374.75\n500.0\n\n\nBirth_Year\n502.0\n1992.948207\n8.236603\n1973.0\n1987.00\n1993.0\n2000.00\n2007.0\n\n\nSeq\n502.0\n3.000000\n0.000000\n3.0\n3.00\n3.0\n3.00\n3.0\n\n\n3st_LC_SCcore\n502.0\n368.023904\n82.052339\n141.0\n295.00\n372.0\n434.00\n495.0\n\n\n3st_RC_SCcore\n502.0\n369.318725\n81.659228\n135.0\n295.00\n375.0\n437.00\n495.0\n\n\n3st_Total_SCcore\n502.0\n737.382470\n155.752174\n280.0\n591.25\n760.0\n860.00\n990.0\n\n\n기출문제 공부 횟수\n502.0\n5.087649\n2.790826\n0.0\n3.00\n5.0\n7.00\n10.0\n\n\n토익 모의테스트 횟수\n502.0\n9.452191\n4.952137\n1.0\n5.00\n8.0\n13.00\n20.0\n\n\n1st_LC_SCcore\n502.0\n313.697211\n85.483105\n105.0\n259.25\n308.0\n369.00\n495.0\n\n\n1st_RC_SCcore\n502.0\n312.798805\n86.522443\n84.0\n250.00\n311.5\n377.75\n491.0\n\n\n1st_Total_SCcore\n502.0\n626.496016\n148.318758\n250.0\n519.00\n641.0\n734.75\n970.0\n\n\n2st_LC_SCcore\n502.0\n337.868526\n84.141542\n120.0\n279.00\n332.5\n395.00\n495.0\n\n\n2st_RC_SCcore\n502.0\n338.045817\n83.817809\n129.0\n280.50\n335.0\n400.00\n495.0\n\n\n2st_Total_SCcore\n502.0\n675.924303\n152.986694\n260.0\n558.75\n690.5\n790.00\n990.0\n\n\nScore_diff_total\n502.0\n61.458167\n39.684902\n0.0\n30.00\n63.0\n83.00\n281.0\n\n\n\n\n\n\n\n\n\n(1)-4. Gender 컬럼의 값 별 개수를 확인\n\n\nCode\ndata.Gender.value_counts()\n\n\nGender\nM    251\nF    251\nName: count, dtype: int64\n\n\n\n\n(1)-5. Gender 컬럼의 [‘M’, ‘F’] –&gt; [1,2]로 변경\n\n\nCode\ndata.Gender = [1 if i ==\"M\" else 2 for i in data.Gender]\n\n\n\n\nCode\ndata.Gender.unique()\n\n\narray([1, 2], dtype=int64)\n\n\n\n\n(1)-6. Gender 컬럼의 값 별 개수를 다시 확인해주세요.\n\n\nCode\ndata.Gender.value_counts()\n\n\nGender\n1    251\n2    251\nName: count, dtype: int64\n\n\n\n\n(1)-7. Gender컬럼 타입을 int로 변경\n- 리스트 컴프리헨션을 통해 위에서 수행\n\n\n(1)-8. 데이터 프레임의 Null 데이터가 있는지 확인\n\n\nCode\ndata.isnull().sum()\n\n\nID                  0\nGender              0\nBirth_Year          0\nSeq                 0\n3st_LC_SCcore       0\n3st_RC_SCcore       0\n3st_Total_SCcore    0\n학습목표                0\n학습방법                0\n강의 학습 교재 유형         0\n학습빈도                0\n기출문제 공부 횟수          0\n취약분야 인지 여부          0\n토익 모의테스트 횟수         0\n1st_LC_SCcore       0\n1st_RC_SCcore       0\n1st_Total_SCcore    0\n2st_LC_SCcore       0\n2st_RC_SCcore       0\n2st_Total_SCcore    0\nScore_diff_total    0\ndtype: int64\n\n\n\n\n\n(2)-1. 변수 sdt 문자열 “Score_diff_total”을 할당\n\n\nCode\nsdt  =\"Score_diff_total\"\n\n\n\n\n(2)-2. Score_diff_total에 대한 기술 통계 정보를 데이터 프레임의 형태로 출력\n\n\nCode\ndata[[sdt]].describe()\n\n\n\n\n\n\n\n\n\nScore_diff_total\n\n\n\n\ncount\n502.000000\n\n\nmean\n61.458167\n\n\nstd\n39.684902\n\n\nmin\n0.000000\n\n\n25%\n30.000000\n\n\n50%\n63.000000\n\n\n75%\n83.000000\n\n\nmax\n281.000000\n\n\n\n\n\n\n\n\n\n(2)-3. 위의 결과를 행과 열을 변환하여 출력\n\n\nCode\ndata[[sdt]].describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nScore_diff_total\n502.0\n61.458167\n39.684902\n0.0\n30.0\n63.0\n83.0\n281.0\n\n\n\n\n\n\n\n\n\n(2)-4. 변수 BY에 문자열 ’Birth_Year’을 할당\n\n\nCode\nBY = \"Birth_Year\"\n\n\n\n\n(2)-5. ’Birth_Year’열에 대한 기술 통계 정보를 행과열을 변환해 출력\n\n\nCode\ndata[[BY]].describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nBirth_Year\n502.0\n1992.948207\n8.236603\n1973.0\n1987.0\n1993.0\n2000.0\n2007.0\n\n\n\n\n\n\n\n\n\n(2)-6. ‘data’ 데이터프레임의 ‘Birth_Year’ 컬럼의 연도별 개수를 Bar 차트로 그리세요.\n\n\nCode\ndata[BY]\n\n\n0      1973\n1      1982\n2      1995\n3      1987\n4      1994\n       ... \n497    2006\n498    1988\n499    2006\n500    1990\n501    1984\nName: Birth_Year, Length: 502, dtype: int64\n\n\n\n\nCode\ndata.groupby(BY)[[BY]].count().\\\n                    rename(columns = {\"Birth_Year\" : \"Count\"}).\\\n                    reset_index().plot(kind= \"bar\",x=\"Birth_Year\",y=\"Count\",title=  \"Count of Birth_Year\")\n\n\n&lt;Axes: title={'center': 'Count of Birth_Year'}, xlabel='Birth_Year'&gt;\n\n\n\n\n\n\n\n(2)-7. 데이터 타입이 object인 컬럼만 추출\n\n\nCode\ndata.select_dtypes(\"O\")\n\n\n\n\n\n\n\n\n\n학습목표\n학습방법\n강의 학습 교재 유형\n학습빈도\n취약분야 인지 여부\n\n\n\n\n0\n승진\n온라인강의\n영상 교재\n주5-6회\n알고 있음\n\n\n1\n승진\n온라인강의\n뉴스/이슈 기반 교재\n주5-6회\n알고 있음\n\n\n2\n자기계발\n참고서\n일반적인 영어 텍스트 기반 교재\n주1-2회\n알고 있음\n\n\n3\n승진\n온라인강의\n뉴스/이슈 기반 교재\n주3-4회\n알고 있음\n\n\n4\n자기계발\n온라인강의\n영상 교재\n주3-4회\n알고 있음\n\n\n...\n...\n...\n...\n...\n...\n\n\n497\n자기계발\n온라인강의\n일반적인 영어 텍스트 기반 교재\n매일(주 7회)\n알고 있음\n\n\n498\n승진\n온라인강의\n비즈니스 시뮬레이션(Role Play)\n매일(주 7회)\n알고 있음\n\n\n499\n자기계발\n오프라인강의\n일반적인 영어 텍스트 기반 교재\n주1-2회\n알고 있음\n\n\n500\n자기계발\n오프라인강의\n비즈니스 시뮬레이션(Role Play)\n주3-4회\n알고 있음\n\n\n501\n승진\n오프라인강의\n비즈니스 시뮬레이션(Role Play)\n주5-6회\n알고 있음\n\n\n\n\n502 rows × 5 columns\n\n\n\n\n\n(2)-8. 데이터 타입이 Object 형태인 컬럼의 컬럼명만 추출해서 출력\n\n\nCode\ndata.select_dtypes(\"O\").columns.values\n\n\narray(['학습목표', '학습방법', '강의 학습 교재 유형', '학습빈도', '취약분야 인지 여부'], dtype=object)\n\n\n\n\n(2)-9. 학습목표의 값들의 빈도수를 계산하여 출력\n\n\nCode\ndata.select_dtypes(\"O\")[[\"학습목표\"]].value_counts().reset_index()\n\n\n\n\n\n\n\n\n\n학습목표\ncount\n\n\n\n\n0\n자기계발\n331\n\n\n1\n승진\n155\n\n\n2\n취업\n16\n\n\n\n\n\n\n\n\n\n(2)-10. data 데이터 프레임의 전체 열과 행 개수를 출력\n\n\nCode\nr,c = data.shape\n\n\n\n\nCode\nprint(f\"전체 행의 수 : {r}, 전체 열의 수 : {c}\")\n\n\n전체 행의 수 : 502, 전체 열의 수 : 21\n\n\n\n\n(2)-10. 변수 ’학습목표’의 값들의 빈도수를 전체 데이터의 개수로 나누어서 해당 값들이 전체 데이터에서 차지하는 비율을 구하기\n\n\nCode\ntemp = data.select_dtypes(\"O\")[[\"학습목표\"]].value_counts().reset_index()\n\n\n\n\nCode\ntemp[\"비율(%)\"] = round(temp[\"count\"]/r*100,2)\n\n\n\n\nCode\ntemp\n\n\n\n\n\n\n\n\n\n학습목표\ncount\n비율(%)\n\n\n\n\n0\n자기계발\n331\n65.94\n\n\n1\n승진\n155\n30.88\n\n\n2\n취업\n16\n3.19\n\n\n\n\n\n\n\n\n\n(2)-11. 학습목표 열에 대한 Bar 차트를 확인\n\n\nCode\ntemp.plot(kind= \"bar\", x= \"학습목표\",y=\"비율(%)\", title =\"토익 학습 목표 비율\")\n\n\n&lt;Axes: title={'center': '토익 학습 목표 비율'}, xlabel='학습목표'&gt;\n\n\n\n\n\n\n\n(2)-12. data 데이터 프레임에서 숫자형 컬럼에 대하여 검색\n\n\nCode\n#data.info()\n\n\n\n\nCode\ndata.select_dtypes(include= [\"float64\", \"int64\"])\n\n\n\n\n\n\n\n\n\nID\nGender\nBirth_Year\nSeq\n3st_LC_SCcore\n3st_RC_SCcore\n3st_Total_SCcore\n기출문제 공부 횟수\n토익 모의테스트 횟수\n1st_LC_SCcore\n1st_RC_SCcore\n1st_Total_SCcore\n2st_LC_SCcore\n2st_RC_SCcore\n2st_Total_SCcore\nScore_diff_total\n\n\n\n\n0\n1\n1\n1973\n3\n345\n336\n681\n7.0\n10\n181\n173\n354\n227\n213\n440\n241\n\n\n1\n2\n2\n1982\n3\n380\n368\n748\n4.0\n14\n330\n290\n620\n354\n339\n693\n55\n\n\n2\n3\n2\n1995\n3\n416\n382\n798\n4.0\n4\n367\n309\n676\n396\n365\n761\n37\n\n\n3\n4\n1\n1987\n3\n495\n397\n892\n9.0\n8\n470\n285\n755\n495\n341\n836\n56\n\n\n4\n5\n1\n1994\n3\n398\n437\n835\n6.0\n4\n273\n372\n645\n314\n426\n740\n95\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n497\n496\n1\n2006\n3\n364\n336\n700\n10.0\n13\n347\n315\n662\n349\n321\n670\n30\n\n\n498\n497\n2\n1988\n3\n187\n252\n439\n9.0\n17\n112\n250\n362\n120\n251\n371\n68\n\n\n499\n498\n1\n2006\n3\n255\n167\n422\n0.0\n4\n252\n150\n402\n254\n158\n412\n10\n\n\n500\n499\n2\n1990\n3\n422\n370\n792\n4.0\n7\n371\n324\n695\n378\n326\n704\n88\n\n\n501\n500\n1\n1984\n3\n235\n226\n461\n7.0\n15\n169\n188\n357\n172\n190\n362\n99\n\n\n\n\n502 rows × 16 columns\n\n\n\n\n\n(2)-13. 변수 ’강의 학습 교재 유형’의 값들의 빈도수, 비율을 계산해서 출력\n\n\nCode\ntemp2 = data[[\"강의 학습 교재 유형\"]].value_counts().reset_index()\n\n\n\n\nCode\ntemp2[\"비율(%)\"] = temp2[\"count\"]/r\n\n\n\n\nCode\ntemp2\n\n\n\n\n\n\n\n\n\n강의 학습 교재 유형\ncount\n비율(%)\n\n\n\n\n0\n일반적인 영어 텍스트 기반 교재\n137\n0.272908\n\n\n1\n영상 교재\n129\n0.256972\n\n\n2\n뉴스/이슈 기반 교재\n122\n0.243028\n\n\n3\n비즈니스 시뮬레이션(Role Play)\n114\n0.227092\n\n\n\n\n\n\n\n\n\n(2)-14. 취약분야 인지 여부 문자열의 값을 알고 있음은 1, 알고 있지 않음은 0으로 변경\n\n\nCode\ndata[\"취약분야 인지 여부\"].replace(\"알고 있음\",\"1\").replace(\"알고 있지 않음\",\"0\").unique()\n\n\narray(['1', '0'], dtype=object)\n\n\n\n\nCode\ndata[\"취약분야 인지 여부\"] = data[\"취약분야 인지 여부\"].replace(\"알고 있음\",\"1\").replace(\"알고 있지 않음\",\"0\")"
  },
  {
    "objectID": "posts/DX/MP/2023-08-24-00. MP (1).html#데이터-저장",
    "href": "posts/DX/MP/2023-08-24-00. MP (1).html#데이터-저장",
    "title": "00. MP (1)",
    "section": "3. 데이터 저장",
    "text": "3. 데이터 저장\n\n\nCode\ndata.to_csv(\"data04_featured.csv\",index=False)\n\n\n\n\nCode\npd.read_csv(\"data04_featured.csv\").info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 502 entries, 0 to 501\nData columns (total 21 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   ID                502 non-null    int64  \n 1   Gender            502 non-null    int64  \n 2   Birth_Year        502 non-null    int64  \n 3   Seq               502 non-null    int64  \n 4   3st_LC_SCcore     502 non-null    int64  \n 5   3st_RC_SCcore     502 non-null    int64  \n 6   3st_Total_SCcore  502 non-null    int64  \n 7   학습목표              502 non-null    object \n 8   학습방법              502 non-null    object \n 9   강의 학습 교재 유형       502 non-null    object \n 10  학습빈도              502 non-null    object \n 11  기출문제 공부 횟수        502 non-null    float64\n 12  취약분야 인지 여부        502 non-null    int64  \n 13  토익 모의테스트 횟수       502 non-null    int64  \n 14  1st_LC_SCcore     502 non-null    int64  \n 15  1st_RC_SCcore     502 non-null    int64  \n 16  1st_Total_SCcore  502 non-null    int64  \n 17  2st_LC_SCcore     502 non-null    int64  \n 18  2st_RC_SCcore     502 non-null    int64  \n 19  2st_Total_SCcore  502 non-null    int64  \n 20  Score_diff_total  502 non-null    int64  \ndtypes: float64(1), int64(16), object(4)\nmemory usage: 82.5+ KB"
  },
  {
    "objectID": "posts/EX/2023-10-31-01. VOC.html",
    "href": "posts/EX/2023-10-31-01. VOC.html",
    "title": "01. VOC",
    "section": "",
    "text": "#!pip install tensorflow\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.dafault = \"plotly_mimetype+notebook_connected\"\nimport scipy.stats as spst\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\n\nplt.rcParams[\"font.family\"] = \"Malgun Gothic\"\nplt.rcParams[\"axes.unicode_minus\"] = False\n\n\n\nimport warnings\nwarnings.filterwarnings(action =\"ignore\")"
  },
  {
    "objectID": "posts/EX/2023-10-31-01. VOC.html#sklearn의-모듈을-읽어와-범주형-데이터를-카테고리화starstarstar",
    "href": "posts/EX/2023-10-31-01. VOC.html#sklearn의-모듈을-읽어와-범주형-데이터를-카테고리화starstarstar",
    "title": "01. VOC",
    "section": "sklearn의 모듈을 읽어와 범주형 데이터를 카테고리화(\\(\\star\\star\\star\\))",
    "text": "sklearn의 모듈을 읽어와 범주형 데이터를 카테고리화(\\(\\star\\star\\star\\))\n- cat_cols 데이터프레임에서 cust_clas_itg_cd 열의 범주형 데이터를 숫자로 인코딩하고, 그 결과를 le_cust_clas_itg_cd 열에 저장\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ncat_cols[\"le_cust_clas_itg_cd\"] = le.fit_transform(cat_cols[\"cust_clas_itg_cd\"])\n\n\ncat_cols.head()\n\n\n\n\n\n\n\n\ncust_clas_itg_cd\ncont_sttus_itg_cd\ncust_dtl_ctg_itg_cd\ntrm_yn\nle_cust_clas_itg_cd\n\n\n\n\n0\nF\n10001\n10003\nN\n0\n\n\n1\nG\n10001\n10002\nN\n1\n\n\n2\nG\n10001\n10003\nN\n1\n\n\n3\nL\n10001\n90024\nN\n5\n\n\n4\nG\n10001\n90024\nN\n1"
  },
  {
    "objectID": "posts/EX/2023-11-01-03. Titanic.html",
    "href": "posts/EX/2023-11-01-03. Titanic.html",
    "title": "03. Titanic",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams[\"font.family\"] = \"Malgun Gothic\"\nplt.rcParams[\"axes.unicode_minus\"] = False\n\n\ndf = sns.load_dataset(\"titanic\")\ndf.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   survived     891 non-null    int64   \n 1   pclass       891 non-null    int64   \n 2   sex          891 non-null    object  \n 3   age          714 non-null    float64 \n 4   sibsp        891 non-null    int64   \n 5   parch        891 non-null    int64   \n 6   fare         891 non-null    float64 \n 7   embarked     889 non-null    object  \n 8   class        891 non-null    category\n 9   who          891 non-null    object  \n 10  adult_male   891 non-null    bool    \n 11  deck         203 non-null    category\n 12  embark_town  889 non-null    object  \n 13  alive        891 non-null    object  \n 14  alone        891 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(4), object(5)\nmemory usage: 80.7+ KB"
  },
  {
    "objectID": "posts/EX/2023-11-01-03. Titanic.html#pclass분포-확인",
    "href": "posts/EX/2023-11-01-03. Titanic.html#pclass분포-확인",
    "title": "03. Titanic",
    "section": "pclass분포 확인",
    "text": "pclass분포 확인\n\ndf.pclass.value_counts()\n\npclass\n3    491\n1    216\n2    184\nName: count, dtype: int64\n\n\n\ndf.pclass.value_counts().plot(kind = \"bar\")\n\n&lt;Axes: xlabel='pclass'&gt;"
  },
  {
    "objectID": "posts/EX/2023-11-01-03. Titanic.html#sex-값-분포-확인",
    "href": "posts/EX/2023-11-01-03. Titanic.html#sex-값-분포-확인",
    "title": "03. Titanic",
    "section": "sex 값 분포 확인",
    "text": "sex 값 분포 확인\n\ndf.sex.value_counts().plot(kind = \"bar\")\n\n&lt;Axes: xlabel='sex'&gt;"
  },
  {
    "objectID": "posts/EX/2023-11-01-03. Titanic.html#age-boxplot",
    "href": "posts/EX/2023-11-01-03. Titanic.html#age-boxplot",
    "title": "03. Titanic",
    "section": "age boxplot",
    "text": "age boxplot\n\ndf.age.plot(kind = \"box\")\n\n&lt;Axes: &gt;\n\n\n\n\n\n\npclass별 age 분포확인\n\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n\ndf.plot(y = \"age\", kind = \"box\", backend = \"plotly\",\n        color = \"pclass\")\n\n\n                                                \n\n\n\n\npclass별 age 분포 + Survived 추가\n\ndf.plot(x = \"pclass\", y = \"age\", kind = \"box\", backend = \"plotly\",\n        color = \"survived\")"
  },
  {
    "objectID": "posts/EX/2023-11-01-03. Titanic.html#pclass별-생존률-평균-구하기",
    "href": "posts/EX/2023-11-01-03. Titanic.html#pclass별-생존률-평균-구하기",
    "title": "03. Titanic",
    "section": "pclass별 생존률 평균 구하기",
    "text": "pclass별 생존률 평균 구하기\n- 결과를 pclass_grp 변수에 저장\n\ntarget = \"survived\"\n\n\npclass_grp = df.groupby(\"pclass\", as_index = True)[[target]].mean().reset_index()\n\n\npclass_grp\n\n\n\n\n\n\n\n\npclass\nsurvived\n\n\n\n\n0\n1\n0.629630\n\n\n1\n2\n0.472826\n\n\n2\n3\n0.242363\n\n\n\n\n\n\n\n\npclass_grp.plot(kind = \"bar\", backend = \"plotly\",\n                x = \"pclass\", y = target,color = \"pclass\", width = 400, height = 400)"
  },
  {
    "objectID": "posts/EX/2023-11-01-03. Titanic.html#중복컬럼삭제",
    "href": "posts/EX/2023-11-01-03. Titanic.html#중복컬럼삭제",
    "title": "03. Titanic",
    "section": "(1) 중복컬럼삭제",
    "text": "(1) 중복컬럼삭제\n\ndf.drop([\"class\",\"alive\"], axis = 1, inplace = True)\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 13 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   survived     891 non-null    int64   \n 1   pclass       891 non-null    int64   \n 2   sex          891 non-null    object  \n 3   age          714 non-null    float64 \n 4   sibsp        891 non-null    int64   \n 5   parch        891 non-null    int64   \n 6   fare         891 non-null    float64 \n 7   embarked     889 non-null    object  \n 8   who          891 non-null    object  \n 9   adult_male   891 non-null    bool    \n 10  deck         203 non-null    category\n 11  embark_town  889 non-null    object  \n 12  alone        891 non-null    bool    \ndtypes: bool(2), category(1), float64(2), int64(4), object(4)\nmemory usage: 72.7+ KB"
  },
  {
    "objectID": "posts/EX/2023-11-01-03. Titanic.html#필요없는-열-삭제",
    "href": "posts/EX/2023-11-01-03. Titanic.html#필요없는-열-삭제",
    "title": "03. Titanic",
    "section": "(2) 필요없는 열 삭제",
    "text": "(2) 필요없는 열 삭제\n\nd_col = ['embarked', 'who', 'adult_male', 'deck', 'embark_town', 'alone'] \n\ndf.drop(d_col, axis = 1, inplace = True)\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   survived  891 non-null    int64  \n 1   pclass    891 non-null    int64  \n 2   sex       891 non-null    object \n 3   age       714 non-null    float64\n 4   sibsp     891 non-null    int64  \n 5   parch     891 non-null    int64  \n 6   fare      891 non-null    float64\ndtypes: float64(2), int64(4), object(1)\nmemory usage: 48.9+ KB\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500"
  },
  {
    "objectID": "posts/EX/2023-11-01-03. Titanic.html#성별-컬럼-정수-인코딩",
    "href": "posts/EX/2023-11-01-03. Titanic.html#성별-컬럼-정수-인코딩",
    "title": "03. Titanic",
    "section": "(3) 성별 컬럼 정수 인코딩",
    "text": "(3) 성별 컬럼 정수 인코딩\n\ndf[\"sex\"].replace([\"male\",\"female\"], [0, 1], inplace = True)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\n\n\n\n\n0\n0\n3\n0\n22.0\n1\n0\n7.2500\n\n\n1\n1\n1\n1\n38.0\n1\n0\n71.2833\n\n\n2\n1\n3\n1\n26.0\n0\n0\n7.9250\n\n\n3\n1\n1\n1\n35.0\n1\n0\n53.1000\n\n\n4\n0\n3\n0\n35.0\n0\n0\n8.0500"
  },
  {
    "objectID": "posts/EX/2023-11-01-03. Titanic.html#결측치-확인",
    "href": "posts/EX/2023-11-01-03. Titanic.html#결측치-확인",
    "title": "03. Titanic",
    "section": "(4) 결측치 확인",
    "text": "(4) 결측치 확인\n\ndf.isnull().sum()\n\nsurvived      0\npclass        0\nsex           0\nage         177\nsibsp         0\nparch         0\nfare          0\ndtype: int64"
  },
  {
    "objectID": "posts/EX/2023-11-01-03. Titanic.html#결측치-평균값으로-대체",
    "href": "posts/EX/2023-11-01-03. Titanic.html#결측치-평균값으로-대체",
    "title": "03. Titanic",
    "section": "(5) 결측치 평균값으로 대체",
    "text": "(5) 결측치 평균값으로 대체\n\nm = df.age.mean()\n\ndf.age.fillna(m, inplace = True)\n\n\ndf.isnull().sum()\n\nsurvived    0\npclass      0\nsex         0\nage         0\nsibsp       0\nparch       0\nfare        0\ndtype: int64"
  },
  {
    "objectID": "posts/EX/2023-11-01-03. Titanic.html#tree",
    "href": "posts/EX/2023-11-01-03. Titanic.html#tree",
    "title": "03. Titanic",
    "section": "(1) tree",
    "text": "(1) tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(max_depth = 5)\n\ntree.fit(X_train, y_train)\n\ntree_pred = tree.predict(X_test)\n\nfrom sklearn.metrics import *\n\naccuracy_score(y_test, tree_pred)\n\n0.8044692737430168"
  },
  {
    "objectID": "posts/EX/2023-11-01-03. Titanic.html#rf",
    "href": "posts/EX/2023-11-01-03. Titanic.html#rf",
    "title": "03. Titanic",
    "section": "(2) RF",
    "text": "(2) RF\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(max_depth=5, n_estimators = 10)\n\nrf.fit(X_train, y_train)\n\nrf_pred = rf.predict(X_test)\n\naccuracy_score(y_test, rf_pred)\n\n0.7932960893854749"
  },
  {
    "objectID": "posts/EX/PRO/2024-01-21-00. Associate review.html",
    "href": "posts/EX/PRO/2024-01-21-00. Associate review.html",
    "title": "00. Associate Review",
    "section": "",
    "text": "1 반드시 답안을 작성하라는 셀안에 코드를 작성하기\n2 변수명이 제시된 경우 반드시 해당 변수명을 사용하기\n3 데이터를 분석 및 전처리한 후 머신러닝과 딥러닝으로 VOC를 제기한 고객의 해지여부를 예측하고 결과를 분석하세요."
  },
  {
    "objectID": "posts/EX/PRO/2024-01-21-00. Associate review.html#voc_trt_perd_itg_cd-변수의-고유값-count",
    "href": "posts/EX/PRO/2024-01-21-00. Associate review.html#voc_trt_perd_itg_cd-변수의-고유값-count",
    "title": "00. Associate Review",
    "section": "voc_trt_perd_itg_cd 변수의 고유값 count",
    "text": "voc_trt_perd_itg_cd 변수의 고유값 count\n\ndf[\"voc_trt_perd_itg_cd\"].value_counts()\n\n_        5422\n10000    4283\n10001     163\n10002      58\n10003      25\n10004      16\n10005      10\n10006       6\n10008       3\n10009       3\n10016       2\n10011       2\n10012       2\n10007       2\n10014       1\n10013       1\n10015       1\nName: voc_trt_perd_itg_cd, dtype: int64\n\n\n\ndf[\"voc_trt_perd_itg_cd\"].value_counts(normalize = True)\n\n_        0.5422\n10000    0.4283\n10001    0.0163\n10002    0.0058\n10003    0.0025\n10004    0.0016\n10005    0.0010\n10006    0.0006\n10008    0.0003\n10009    0.0003\n10016    0.0002\n10011    0.0002\n10012    0.0002\n10007    0.0002\n10014    0.0001\n10013    0.0001\n10015    0.0001\nName: voc_trt_perd_itg_cd, dtype: float64"
  },
  {
    "objectID": "posts/EX/PRO/2024-01-21-00. Associate review.html#라벨인코딩",
    "href": "posts/EX/PRO/2024-01-21-00. Associate review.html#라벨인코딩",
    "title": "00. Associate Review",
    "section": "라벨인코딩",
    "text": "라벨인코딩\ncat_cols 데이터프레임에서 cust_clas_itg_cd 열의 범주형 데이터를 숫자로 인코딩하고, 그 결과를 le_cust_clas_itg_cd 열에 저장\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ncat_cols[\"le_cust_clas_itg_cd\"] = le.fit_transform(cat_cols[\"cust_clas_itg_cd\"])\n\n\ncat_cols.head()\n\n\n  \n    \n\n\n\n\n\n\ncust_clas_itg_cd\ncont_sttus_itg_cd\ncust_dtl_ctg_itg_cd\ntrm_yn\nle_cust_clas_itg_cd\n\n\n\n\n0\nF\n10001\n10003\nN\n0\n\n\n1\nG\n10001\n10002\nN\n1\n\n\n2\nG\n10001\n10003\nN\n1\n\n\n3\nL\n10001\n90024\nN\n5\n\n\n4\nG\n10001\n90024\nN\n1"
  }
]