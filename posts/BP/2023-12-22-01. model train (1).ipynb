{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title : \"00. model train (1)\"\n",
    "author : \"gc\"\n",
    "date : \"12/22/23\"\n",
    "categories : [BP]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mf_Da7QuEPtC"
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igWvDVfXI08H"
   },
   "source": [
    "```python\n",
    "!pip install remotezip tqdm opencv-python==4.5.2.52 opencv-python-headless==4.5.2.52 tf-models-official\n",
    "!pip install remotezip\n",
    "!pip install tf-models-official\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "L1FtL9jCI5xo"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import random\n",
    "import pathlib\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import remotezip as rz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from pathlib import PosixPath\n",
    "\n",
    "# Import the MoViNet model from TensorFlow Models (tf-models-official) for the MoViNet model\n",
    "from official.projects.movinet.modeling import movinet\n",
    "from official.projects.movinet.modeling import movinet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-TxdPOYCJJRv",
    "outputId": "a8f2938e-1581-4c3b-aaed-d50c82140cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Colab Notebooks/big project\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/Colab Notebooks/big project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9J9y1HIJmLf"
   },
   "source": [
    "# 필요한 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FOeqB-9wJ_6H"
   },
   "outputs": [],
   "source": [
    "def format_frames(frame, output_size):\n",
    "  \"\"\"\n",
    "    Pad and resize an image from a video.\n",
    "\n",
    "    Args:\n",
    "      frame: Image that needs to resized and padded.\n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      Formatted frame with padding of specified output size.\n",
    "  \"\"\"\n",
    "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "  return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hHGfPCUdJonA"
   },
   "outputs": [],
   "source": [
    "def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n",
    "  \"\"\"\n",
    "    Creates frames from each video file present for each category.\n",
    "\n",
    "    Args:\n",
    "      video_path: File path to the video.\n",
    "      n_frames: Number of frames to be created per video file.\n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "  \"\"\"\n",
    "  # Read each video frame by frame\n",
    "  result = []\n",
    "  src = cv2.VideoCapture(str(video_path))\n",
    "\n",
    "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "  need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "  if need_length > video_length:\n",
    "    start = 0\n",
    "  else:\n",
    "    max_start = video_length - need_length\n",
    "    start = random.randint(0, max_start + 1)\n",
    "\n",
    "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "  ret, frame = src.read()\n",
    "  result.append(format_frames(frame, output_size))\n",
    "\n",
    "  for _ in range(n_frames - 1):\n",
    "    for _ in range(frame_step):\n",
    "      ret, frame = src.read()\n",
    "    if ret:\n",
    "      frame = format_frames(frame, output_size)\n",
    "      result.append(frame)\n",
    "    else:\n",
    "      result.append(np.zeros_like(result[0]))\n",
    "  src.release()\n",
    "  result = np.array(result)[..., [2, 1, 0]]\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2EhsC59lJ3CA"
   },
   "outputs": [],
   "source": [
    "class FrameGenerator:\n",
    "  def __init__(self, path, n_frames, training = False):\n",
    "    \"\"\" Returns a set of frames with their associated label.\n",
    "\n",
    "      Args:\n",
    "        path: Video file paths.\n",
    "        n_frames: Number of frames.\n",
    "        training: Boolean to determine if training dataset is being created.\n",
    "    \"\"\"\n",
    "    self.path = path\n",
    "    self.n_frames = n_frames\n",
    "    self.training = training\n",
    "    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
    "    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
    "\n",
    "  def get_files_and_class_names(self):\n",
    "    video_paths = list(self.path.glob('*/*.mp4'))\n",
    "    classes = [p.parent.name for p in video_paths]\n",
    "    return video_paths, classes\n",
    "\n",
    "  def __call__(self):\n",
    "    video_paths, classes = self.get_files_and_class_names()\n",
    "\n",
    "    pairs = list(zip(video_paths, classes))\n",
    "\n",
    "    if self.training:\n",
    "      random.shuffle(pairs)\n",
    "\n",
    "    for path, name in pairs:\n",
    "      video_frames = frames_from_video_file(path, self.n_frames)\n",
    "      label = self.class_ids_for_name[name] # Encode labels\n",
    "      yield video_frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bzBA51ftJRa6"
   },
   "outputs": [],
   "source": [
    "subset_paths  = {'train': PosixPath('DATA(20231222)/train'),\n",
    "                              'test': PosixPath('DATA(20231222)/test')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWyePEnuKhsA"
   },
   "source": [
    "# 잡다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "w5-IgeTvJeBf"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_frames = 8\n",
    "\n",
    "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
    "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], num_frames, training = True),\n",
    "                                          output_signature = output_signature)\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['test'], num_frames),\n",
    "                                         output_signature = output_signature)\n",
    "test_ds = test_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmNvooYGKHMQ",
    "outputId": "de3284fd-d543-4225-e1f8-ac1f7df8908b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 0 3 2 3 2 0 0], shape=(8,), dtype=int16)\n",
      "tf.Tensor([3 0 2 4 0 0 1 1], shape=(8,), dtype=int16)\n",
      "tf.Tensor([2 2 2 0 3 1 4 3], shape=(8,), dtype=int16)\n",
      "tf.Tensor([1 3 4 0 0 3 4 2], shape=(8,), dtype=int16)\n",
      "tf.Tensor([2 1 1 0 0 1 1 3], shape=(8,), dtype=int16)\n",
      "tf.Tensor([2 1 2 3 2 4 0 1], shape=(8,), dtype=int16)\n",
      "tf.Tensor([3 4 3 2 1 2 2 1], shape=(8,), dtype=int16)\n",
      "tf.Tensor([2 3 4 3 4 4 3 3], shape=(8,), dtype=int16)\n",
      "tf.Tensor([1 4 3 0 0 2 0 3], shape=(8,), dtype=int16)\n",
      "tf.Tensor([1 3 3 0 1 2 4 3], shape=(8,), dtype=int16)\n"
     ]
    }
   ],
   "source": [
    "for frames, labels in train_ds.take(10):\n",
    "  print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJKy_DYdKQkg",
    "outputId": "b7aacf65-a3ee-4bf2-9402-f43efdad5f82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (8, 8, 224, 224, 3)\n",
      "Label: (8,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape: {frames.shape}\")\n",
    "print(f\"Label: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "bmeVrgk4KZvX"
   },
   "outputs": [],
   "source": [
    "gru = layers.GRU(units=4, return_sequences=True, return_state=True)\n",
    "\n",
    "inputs = tf.random.normal(shape=[1, 10, 8]) # (batch, sequence, channels)\n",
    "\n",
    "result, state = gru(inputs) # Run it all at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJAInF6xKcwe",
    "outputId": "652a6444-bc94-49d0-e030-5386eb9a1a41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10, 4), dtype=float32, numpy=\n",
       "array([[[ 0.78977203, -0.7666972 , -0.18226078, -0.11051563],\n",
       "        [ 0.7447158 , -0.5760373 , -0.3625499 , -0.32132182],\n",
       "        [ 0.70870763, -0.8776552 , -0.40342405, -0.20605183],\n",
       "        [ 0.8300309 , -0.7762664 , -0.66221005, -0.15138052],\n",
       "        [ 0.7058173 , -0.601603  , -0.697772  ,  0.21956235],\n",
       "        [ 0.2517114 , -0.8170609 , -0.6245837 ,  0.2573869 ],\n",
       "        [-0.09677514, -0.753946  , -0.44089788, -0.1442756 ],\n",
       "        [-0.13653909, -0.5563181 ,  0.04367654,  0.1682852 ],\n",
       "        [ 0.16750489, -0.43640408, -0.30911005,  0.16984233],\n",
       "        [ 0.50685465, -0.84352696, -0.295877  ,  0.06026528]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zXmw4_PyKeA-",
    "outputId": "4ad1613c-7fe4-485d-e5ef-37a39807a765"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "first_half, state = gru(inputs[:, :5, :])   # run the first half, and capture the state\n",
    "second_half, _ = gru(inputs[:,5:, :], initial_state=state)  # Use the state to continue where you left off.\n",
    "\n",
    "print(np.allclose(result[:, :5,:], first_half))\n",
    "print(np.allclose(result[:, 5:,:], second_half))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duJDsVW8KgQI"
   },
   "source": [
    "# 사전학습된 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OtbDt0LYKmI_",
    "outputId": "0cc7644a-2c97-401a-fd24-58d3fc31c57d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movinet_a0_base/\n",
      "movinet_a0_base/checkpoint\n",
      "movinet_a0_base/ckpt-1.data-00000-of-00001\n",
      "movinet_a0_base/ckpt-1.index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x78f47a527970>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = 'a0'\n",
    "resolution = 224\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "backbone = movinet.Movinet(model_id=model_id)\n",
    "backbone.trainable = False\n",
    "\n",
    "# Set num_classes=600 to load the pre-trained weights from the original model\n",
    "model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=600)\n",
    "model.build([None, None, None, None, 3])\n",
    "\n",
    "# Load pre-trained weights\n",
    "!wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a0_base.tar.gz -O movinet_a0_base.tar.gz -q\n",
    "!tar -xvf movinet_a0_base.tar.gz\n",
    "\n",
    "checkpoint_dir = f'movinet_{model_id}_base'\n",
    "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "status = checkpoint.restore(checkpoint_path)\n",
    "status.assert_existing_objects_matched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "yW9TxbJQKqz5"
   },
   "outputs": [],
   "source": [
    "def build_classifier(batch_size, num_frames, resolution, backbone, num_classes):\n",
    "  \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n",
    "  model = movinet_model.MovinetClassifier(\n",
    "      backbone=backbone,\n",
    "      num_classes=num_classes)\n",
    "  model.build([batch_size, num_frames, resolution, resolution, 3])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "lz6-iZnIKug_"
   },
   "outputs": [],
   "source": [
    "model = build_classifier(batch_size, num_frames, resolution, backbone, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88aMDIEKK4hA"
   },
   "source": [
    "## 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "p7ZIbUGPK2GY"
   },
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "model.compile(loss=loss_obj, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-VqccsJK9b_"
   },
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qnWHsswLXMe",
    "outputId": "016b2ae9-53d3-42be-d974-fa87f6b6166a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 262s 11s/step - loss: 0.2182 - accuracy: 0.9250 - val_loss: 1.8610 - val_accuracy: 0.5300\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 236s 10s/step - loss: 0.1098 - accuracy: 0.9550 - val_loss: 2.0431 - val_accuracy: 0.4700\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 241s 10s/step - loss: 0.0851 - accuracy: 0.9650 - val_loss: 2.2936 - val_accuracy: 0.4800\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 245s 10s/step - loss: 0.0808 - accuracy: 0.9700 - val_loss: 2.2675 - val_accuracy: 0.4300\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 240s 10s/step - loss: 0.0554 - accuracy: 0.9800 - val_loss: 2.7530 - val_accuracy: 0.4700\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 237s 10s/step - loss: 0.0691 - accuracy: 0.9750 - val_loss: 2.4889 - val_accuracy: 0.4900\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 235s 10s/step - loss: 0.0576 - accuracy: 0.9800 - val_loss: 2.6091 - val_accuracy: 0.4300\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 236s 10s/step - loss: 0.0273 - accuracy: 0.9950 - val_loss: 3.0559 - val_accuracy: 0.4500\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 238s 10s/step - loss: 0.0222 - accuracy: 0.9950 - val_loss: 2.4566 - val_accuracy: 0.4600\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 234s 9s/step - loss: 0.0236 - accuracy: 0.9950 - val_loss: 2.6278 - val_accuracy: 0.4500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds,\n",
    "                    validation_data=test_ds,\n",
    "                    epochs=num_epochs,\n",
    "                    validation_freq=1,\n",
    "                    verbose=1).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "i1fs2Mc_WtbB"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8L_BItUW7rB",
    "outputId": "bba8eeb2-a1f6-4c24-bfb1-55ebe9b91148"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.21817944943904877,\n",
       " 0.10981059074401855,\n",
       " 0.08505159616470337,\n",
       " 0.08084923028945923,\n",
       " 0.05538821220397949,\n",
       " 0.06905801594257355,\n",
       " 0.05758459120988846,\n",
       " 0.027318188920617104,\n",
       " 0.022160962224006653,\n",
       " 0.023646153509616852]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g58waC2_WwhJ",
    "outputId": "b9bcb46b-4cf2-414b-c15b-f97159065b86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x78f3dc26e530>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize = (12, 4))\n",
    "plt.plot(history[\"loss\"], \"--.\", label = \"train_loss\", alpha = 0.3)\n",
    "plt.plot(history[\"val_loss\"], \"--.\", label = \"val_loss\",alpha = 0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPT8GhOHU98E"
   },
   "source": [
    "## 모델 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3kEiuGoVU_Jq",
    "outputId": "bd79825a-ce76-41c3-a0c3-df1bd640eefd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 80s 6s/step - loss: 2.5152 - accuracy: 0.4400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 2.515213966369629, 'accuracy': 0.4399999976158142}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApvETRTNWpmR"
   },
   "source": [
    "`-` 모델 성능이 너무 안나온다..."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
